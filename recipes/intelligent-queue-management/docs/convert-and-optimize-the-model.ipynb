{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e7d62b-5779-4211-822c-457c77321f8b",
   "metadata": {},
   "source": [
    "# Convert and Optimize YOLOv8 with OpenVINOâ„¢\n",
    "\n",
    "The YOLOv8 algorithm developed by Ultralytics is a cutting-edge, state-of-the-art (SOTA) model that is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection, image segmentation, and image classification tasks.\n",
    "\n",
    "YOLO stands for \"You Only Look Once\", it is a popular family of real-time object detection algorithms. The original YOLO object detector was first released in 2016. Since then, different versions and variants of YOLO have been proposed, each providing a significant increase in performance and efficiency. YOLOv8 builds upon the success of previous YOLO versions and introduces new features to boost performance and flexibility further. More details about its realization can be found in the original model [repository](https://github.com/ultralytics/ultralytics).\n",
    "\n",
    "Real-time object detection is often used as key component in computer vision systems. Applications that use real-time object detection models include video analytics, robotics, autonomous vehicles, multi-object tracking and object counting, medical image analysis, and many others.\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to convert and optimize PyTorch YOLOv8 with OpenVINO. We consider the steps required for an object detection scenario.\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights. We will use the YOLOv8 medium model (also known as `yolov8m`) pre-trained on the COCO dataset, which is available in this [repo](https://github.com/ultralytics/ultralytics). Similar steps are also applicable to other YOLOv8 models.\n",
    "Typical steps to obtain a pre-trained model:\n",
    "1. Create an instance of a model class\n",
    "2. Load checkpoint state dict, which contains pre-trained model weights\n",
    "3. Turn the model to evaluation for switching some operations to inference mode\n",
    "\n",
    "In this case, the model creators provide an API that enables converting the YOLOv8 model to ONNX and then to OpenVINO IR, so we don't need to do these steps manually.\n",
    "\n",
    "## Define an image for sanity testing\n",
    "\n",
    "Let's use the following image to ensure YOLOv8 is able to detect people in the queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373658bd-7e64-4479-914e-f2742d330afd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T17:10:46.760210Z",
     "start_time": "2023-04-25T17:10:46.658607Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "IMAGE_PATH = \"../data/pexels-catia-matos-1604200.jpg\"\n",
    "\n",
    "Image.open(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee32fd08-650c-4751-bb41-d8afccb2495e",
   "metadata": {},
   "source": [
    "## Instantiate the model\n",
    "\n",
    "Several models are available in the original repository, targeted for different tasks. We need to specify a path to the model checkpoint to load the model. It can be some local path or name available on the models' hub (in this case model checkpoint will be downloaded automatically).\n",
    "\n",
    "Making prediction, the model accepts a path to input image and returns a list with the Result class object. Also, it includes utilities for processing results, e.g., `plot()` method for drawing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fdb05e-02a6-48f6-ac64-7199f0c331fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T17:10:48.865286Z",
     "start_time": "2023-04-25T17:10:46.760022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# the name of the model we want to use\n",
    "DET_MODEL_NAME = \"yolov8m\"\n",
    "\n",
    "# create a model\n",
    "det_model = YOLO(f\"../model/{DET_MODEL_NAME}.pt\")\n",
    "\n",
    "# class 0 means person\n",
    "res = det_model(IMAGE_PATH, classes=[0])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa2440",
   "metadata": {},
   "source": [
    "Let's show the result to be sure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae879e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T17:10:49.020748Z",
     "start_time": "2023-04-25T17:10:48.867333Z"
    }
   },
   "outputs": [],
   "source": [
    "Image.fromarray(res.plot(line_width=3)[:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f21f1b9",
   "metadata": {},
   "source": [
    "## Convert model to OpenVINO IR\n",
    "\n",
    "YOLOv8 provides API for convenient model exporting to different formats, including OpenVINO IR. `model.export` is responsible for model conversion. We need to specify the format, and additionally, we could preserve dynamic shapes in the model. It would limit us to use CPU only, so we're not doing this. Also, we specify we want to use half-precision (FP16) to get better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a690611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-25T17:10:51.385621Z",
     "start_time": "2023-04-25T17:10:49.020595Z"
    }
   },
   "outputs": [],
   "source": [
    "# export model to OpenVINO format\n",
    "out_dir = det_model.export(format=\"openvino\", dynamic=False, half=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70275b",
   "metadata": {},
   "source": [
    "And we're done! The model has been exported to `model/yolov8_openvino_model` directory. Now it's time to use it in production!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469627dc-5564-4e30-ad42-d4c83c784cea",
   "metadata": {},
   "source": [
    "### Check model accuracy on the dataset\n",
    "\n",
    "For comparing the optimized model result with the original, it is good to know some measurable results in terms of model accuracy on the validation dataset. \n",
    "\n",
    "\n",
    "#### Download the validation dataset\n",
    "\n",
    "YOLOv8 is pre-trained on the COCO dataset, so to evaluate the model accuracy we need to download it. According to the instructions provided in the YOLOv8 repo, we also need to download annotations in the format used by the author of the model, for use with the original model evaluation function.\n",
    "\n",
    ">**Note**: In the first time, dataset downloading could take some minutes. Downloading speed depends on your internet connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c486f68d-7d59-4377-b6c4-a2215a31b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "sys.path.append('../')\n",
    "from utils import download_file\n",
    "from ultralytics.yolo.utils import ops\n",
    "\n",
    "DATA_URL = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "LABELS_URL = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip\"\n",
    "CFG_URL = \"https://raw.githubusercontent.com/ultralytics/ultralytics/main/ultralytics/datasets/coco.yaml\"\n",
    "\n",
    "OUT_DIR = Path('./datasets')\n",
    "\n",
    "download_file(DATA_URL, directory=OUT_DIR, show_progress=True)\n",
    "download_file(LABELS_URL, directory=OUT_DIR, show_progress=True)\n",
    "download_file(CFG_URL, directory=OUT_DIR, show_progress=True)\n",
    "\n",
    "if not (OUT_DIR / \"coco/labels\").exists():\n",
    "    with ZipFile(OUT_DIR / 'coco2017labels-segments.zip' , \"r\") as zip_ref:\n",
    "        zip_ref.extractall(OUT_DIR)\n",
    "    with ZipFile(OUT_DIR / 'val2017.zip' , \"r\") as zip_ref:\n",
    "        zip_ref.extractall(OUT_DIR / 'coco/images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cde5253-e1f1-4e07-b43d-1130b8290246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.yolo.utils import ops\n",
    "OUT_DIR = Path('./datasets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e540e02-94ac-41f4-a90b-3bb9fce3dcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics.yolo.utils import DEFAULT_CFG\n",
    "from ultralytics.yolo.cfg import get_cfg\n",
    "from ultralytics.yolo.data.utils import check_det_dataset\n",
    "from ultralytics import YOLO\n",
    "\n",
    "args = get_cfg(cfg=DEFAULT_CFG)\n",
    "args.data = str(OUT_DIR / \"coco.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2256ebc-eb12-4957-a83d-d2da0a72d156",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_validator = det_model.ValidatorClass(args=args)\n",
    "det_validator.data = check_det_dataset(args.data)\n",
    "det_data_loader = det_validator.get_dataloader(\"datasets/coco\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558a4bf-ea34-487f-b4cc-02df6dffbdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core, Model\n",
    "det_model_path = Path(f\"../model/{DET_MODEL_NAME}_openvino_model/{DET_MODEL_NAME}.xml\")\n",
    "core = Core()\n",
    "det_ov_model = core.read_model(det_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8574353d-b9d1-4e4c-8457-5cf519f54d9b",
   "metadata": {},
   "source": [
    "### Optimize model using NNCF Post-training Quantization API\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "We will use 8-bit quantization in post-training mode (without the fine-tuning pipeline) to optimize YOLOv8.\n",
    "\n",
    "> **Note**: NNCF Post-training Quantization is available as a preview feature in OpenVINO 2022.3 release.\n",
    "Fully functional support will be provided in the next releases.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a Dataset for quantization.\n",
    "2. Run `nncf.quantize` for getting an optimized model.\n",
    "3. Serialize OpenVINO IR model, using the `openvino.runtime.serialize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2493211-0631-48a4-95f9-11b771503d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf  # noqa: F811\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def transform_fn(data_item:Dict):\n",
    "    \"\"\"\n",
    "    Quantization transform function. Extracts and preprocess input data from dataloader item for quantization.\n",
    "    Parameters:\n",
    "       data_item: Dict with data item produced by DataLoader during iteration\n",
    "    Returns:\n",
    "        input_tensor: Input data for quantization\n",
    "    \"\"\"\n",
    "    input_tensor = det_validator.preprocess(data_item)['img'].numpy()\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "quantization_dataset = nncf.Dataset(det_data_loader, transform_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537d057-6b4e-4972-af8d-1c09059e1874",
   "metadata": {},
   "source": [
    "The `nncf.quantize` function provides an interface for model quantization. It requires an instance of the OpenVINO Model and quantization dataset. \n",
    "Optionally, some additional parameters for the configuration quantization process (number of samples for quantization, preset, ignored scope, etc.) can be provided. YOLOv8 model contains non-ReLU activation functions, which require asymmetric quantization of activations. To achieve a better result, we will use a `mixed` quantization preset. It provides symmetric quantization of weights and asymmetric quantization of activations. For more accurate results, we should keep the operation in the postprocessing subgraph in floating point precision, using the `ignored_scope` parameter.\n",
    "\n",
    ">**Note**: Model post-training quantization is time-consuming process. Be patient, it can take several minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8d3c81-f1a3-4b30-8bef-5f57f943796c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_scope = nncf.IgnoredScope(\n",
    "    types=[\"Multiply\", \"Subtract\", \"Sigmoid\"],  # ignore operations\n",
    "    names=[\n",
    "        \"/model.22/dfl/conv/Conv\",           # in the post-processing subgraph\n",
    "        \"/model.22/Add\",\n",
    "        \"/model.22/Add_1\",\n",
    "        \"/model.22/Add_2\",\n",
    "        \"/model.22/Add_3\",\n",
    "        \"/model.22/Add_4\",   \n",
    "        \"/model.22/Add_5\",\n",
    "        \"/model.22/Add_6\",\n",
    "        \"/model.22/Add_7\",\n",
    "        \"/model.22/Add_8\",\n",
    "        \"/model.22/Add_9\",\n",
    "        \"/model.22/Add_10\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Detection model\n",
    "quantized_det_model = nncf.quantize(\n",
    "    det_ov_model, \n",
    "    quantization_dataset,\n",
    "    preset=nncf.QuantizationPreset.MIXED,\n",
    "    ignored_scope=ignored_scope\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a9b0d2-ec8d-4f23-9917-a9d9918db7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import serialize\n",
    "int8_model_det_path = Path(f'../model/{DET_MODEL_NAME}_openvino_int8_model/{DET_MODEL_NAME}.xml')\n",
    "print(f\"Quantized detection model will be saved to {int8_model_det_path}\")\n",
    "serialize(quantized_det_model, str(int8_model_det_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f3534-db54-4dc3-8bc3-a420b3b3b8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"CPU\"  # \"GPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f7bc8b-8370-48c7-b104-014bf5d79103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP32 model (OpenVINO IR)\n",
    "!benchmark_app -m $det_model_path -d $device -api async -shape \"[1,3,640,640]\" -t 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0123551a-e908-4900-a8ce-d2453c890c4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "!benchmark_app -m $int8_model_det_path -d $device -api async -shape \"[1,3,640,640]\" -t 30"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
