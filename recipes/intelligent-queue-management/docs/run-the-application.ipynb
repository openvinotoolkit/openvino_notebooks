{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Queue Management with OpenVINOâ„¢\n",
    "\n",
    "An intelligent queue management system is a system that provides real-time insights to businesses to manage checkout queues in an intelligent and efficient way. The system can provide real-time information to customers about queue lengths and average customer count, allowing them to plan their time better. It can also optimize the use of resources, such as staff and equipment, to reduce wait times and increase customer satisfaction.\n",
    "\n",
    "By analyzing data such as wait times, queue lengths, and customer behavior, businesses can make informed decisions to optimize their operations, reduce wait times, and improve the overall customer experience. \n",
    "\n",
    "This notebook demonstrates an intelligent queue management system that utilizes the YOLOv8 object detection model, which can be found in this [repository](https://github.com/ultralytics/ultralytics). The system is designed to detect people in a video stream and count the number of individuals in a queue at any given time. By leveraging this real-time data on queue lengths and average customer count, the system can optimize queue management and reduce waiting times, ultimately leading to improved customer experiences.\n",
    "\n",
    "Please to note that if you intend to use a webcam for this system, you will need to run the notebook on a computer with a built-in or external webcam. However, if you're running the notebook on a server, you can still perform inference on a pre-recorded video file.\n",
    "\n",
    "> **NOTE**: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server, the webcam will not work. However, you can still do inference on a video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import json\n",
    "import logging as log\n",
    "from collections import defaultdict, deque\n",
    "from typing import Tuple, List, Union\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import torch\n",
    "from openvino import runtime as ov\n",
    "from ultralytics.yolo.utils import ops\n",
    "\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "\n",
    "log.basicConfig(level=log.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "The model input is a tensor with the shape `[1, 3, 640, 640]` in the `N, C, H, W` format, where:\n",
    "* `N` - the number of images in a batch (batch size)\n",
    "* `C` - the number of image channels\n",
    "* `H` - the image height\n",
    "* `W` - the image width\n",
    "\n",
    "The model expects images in RGB channel format and normalized in the `[0, 1]` range. Although the YOLOv8 model itself supports dynamic input shapes while preserving input divisibility by 32, it is recommended to use static shapes, such as `640x640`, for better efficiency. To resize images to fit the model size, the `letterbox` resize approach is used, where the aspect ratio of width and height is preserved.\n",
    "\n",
    "To maintain a specific shape, preprocessing automatically enables padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def letterbox(img: np.ndarray, new_shape: Tuple[int, int]) -> Tuple[np.ndarray, Tuple[float, float], Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "        Resize image and padding for detection. Takes image as input,\n",
    "         resizes image to fit into new shape with saving original aspect ratio and pads it to meet stride-multiple constraints\n",
    "\n",
    "        Parameters:\n",
    "          img: image for preprocessing\n",
    "          new_shape: image size after preprocessing in format [width, height]\n",
    "        Returns:\n",
    "          img: image after preprocessing\n",
    "          ratio: hight and width scaling ratio\n",
    "          padding_size: height and width padding size\n",
    "    \"\"\"\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = img.shape[1::-1]  # current shape [width, height]\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "\n",
    "    # Compute padding\n",
    "    ratio = r, r  # width, height ratios\n",
    "    new_unpad = int(round(shape[0] * r)), int(round(shape[1] * r))\n",
    "    dw, dh = new_shape[0] - new_unpad[0], new_shape[1] - new_unpad[1]  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape != new_unpad:  # resize\n",
    "        img = cv2.resize(img, dsize=new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(128, 128, 128))  # add border\n",
    "    return img, ratio, (dw, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocess` function prepares an image for detection using a YOLOv8 model. First, it adds padding to the image to meet the input size requirements of the model. Then, the function converts the image to float32 format and normalizes it to a range of (0,1). Additionally, the data layout of the image is changed from HWC to CHW, and an extra dimension is added before returning it for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(image: np.ndarray, input_size: Tuple[int, int]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "        Preprocess image according to YOLOv8 input requirements.\n",
    "\n",
    "        Parameters:\n",
    "          image: image for preprocessing\n",
    "          input_size: image size after preprocessing in format [width, height]\n",
    "        Returns:\n",
    "          img: image after preprocessing\n",
    "    \"\"\"\n",
    "    # add padding to the image\n",
    "    image = letterbox(image, new_shape=input_size)[0]\n",
    "    # convert to float32\n",
    "    image = image.astype(np.float32)\n",
    "    # normalize to (0, 1)\n",
    "    image /= 255.0\n",
    "    # changes data layout from HWC to CHW\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    # add one more dimension\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "\n",
    "The model output contains detection boxes candidates, it is a tensor with shape `[1,84,8400]` in format `B,84,N` where:\n",
    "\n",
    "- `B` - batch size\n",
    "- `N` - number of detection boxes\n",
    "\n",
    "Detection box (second dimension) has format [`x`, `y`, `h`, `w`, `class_no_1`, ..., `class_no_80`], where:\n",
    "\n",
    "- (`x`, `y`) - raw coordinates of box center\n",
    "- `h`, `w` - raw height and width of the box\n",
    "- `class_no_1`, ..., `class_no_80` - probability distribution over the classes.\n",
    "\n",
    "Fot getting the final prediction, we apply a non-maximum suppression algorithm and rescale box coordinates to the original image size. The function also filters out other predictions than people and returns detected boxes in `supervision` (dependency) format.\n",
    "\n",
    "The function takes the following parameters:\n",
    "\n",
    "- `pred_boxes`: model output prediction boxes\n",
    "- `input_size`: image size after preprocessing in format [width, height]\n",
    "- `orig_img`: image before preprocessing\n",
    "- `min_conf_threshold`: minimal accepted confidence for object filtering (default: 0.25)\n",
    "- `nms_iou_threshold`: minimal overlap score for removing objects duplicates in NMS (default: 0.75)\n",
    "- `agnostic_nms`: apply class agnostic NMS approach or not (default: False)\n",
    "- `max_detections`: maximum detections after NMS (default: 100)\n",
    "\n",
    "The function returns a list of detected boxes in supervision format (sv.Detections), with the following attributes:\n",
    "\n",
    "- `xyxy`: a numpy array of shape `(N, 4)` containing the coordinates of the detected boxes in format `[x0, y0, x1, y1]`\n",
    "- `confidence`: a numpy array of shape `(N,)` containing the confidence scores of the detected boxes\n",
    "- `class_id`: a numpy array of shape `(N,)` containing the class IDs of the detected boxes (only `0` for people in this case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def postprocess(pred_boxes: np.ndarray, input_size: Tuple[int, int], orig_img, min_conf_threshold=0.25, nms_iou_threshold=0.75, agnostic_nms=False, max_detections=100) -> sv.Detections:\n",
    "    \"\"\"\n",
    "        YOLOv8 model postprocessing function. Applied non-maximum supression algorithm to detections and rescale boxes to original image size,\n",
    "         filtering out other classes than person\n",
    "\n",
    "         Parameters:\n",
    "            pred_boxes: model output prediction boxes\n",
    "            input_size: image size after preprocessing in format [width, height]\n",
    "            orig_img: image before preprocessing\n",
    "            min_conf_threshold: minimal accepted confidence for object filtering\n",
    "            nms_iou_threshold: minimal overlap score for removing objects duplicates in NMS\n",
    "            agnostic_nms: apply class agnostinc NMS approach or not\n",
    "            max_detections:  maximum detections after NMS\n",
    "        Returns:\n",
    "           det: list of detected boxes in sv.Detections format\n",
    "    \"\"\"\n",
    "    nms_kwargs = {\"agnostic\": agnostic_nms, \"max_det\": max_detections}\n",
    "    # non-maximum suppresion\n",
    "    pred = ops.non_max_suppression(torch.from_numpy(pred_boxes), min_conf_threshold, nms_iou_threshold, nc=80, **nms_kwargs)[0]\n",
    "\n",
    "    # no predictions in the image\n",
    "    if not len(pred):\n",
    "        return sv.Detections(xyxy=np.empty((0, 4), dtype=np.float32), confidence=np.array([], dtype=np.float32), class_id=np.array([], dtype=int))\n",
    "\n",
    "    # transform boxes to pixel coordinates\n",
    "    pred[:, :4] = ops.scale_boxes(input_size, pred[:, :4], orig_img.shape).round()\n",
    "    # numpy array from torch tensor\n",
    "    pred = np.array(pred)\n",
    "    # create detections in supervision format\n",
    "    det = sv.Detections(pred[:, :4], pred[:, 5], pred[:, 4])\n",
    "    # filter out other predictions than people\n",
    "    return det[det.class_id == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model\n",
    "\n",
    "Only a few lines of code are required to run the model. First, initialize OpenVINO Runtime. Then, read the network architecture and model weights from the `.bin` and `.xml` files to compile for the desired device. If you choose `GPU` you need to wait for a while, as the startup time is a little longer than in the case of `CPU`.\n",
    "\n",
    "There is a possibility to allow OpenVINO to decide which hardware offers the best performance. In that case, just use `AUTO` to automatically select the best available device for the model based on the system configuration and available devices.\n",
    "\n",
    "The config parameter specifies additional configuration options for the model compilation. In this case, the `PERFORMANCE_HINT` option is set to `LATENCY` mode, which optimizes the model for low latency inference, another option is to set to `THROUGHPUT` or `CUMULATIVE_THROUGHPUT` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model(model_path: str) -> ov.CompiledModel:\n",
    "    \"\"\"\n",
    "        Initialize OpenVINO and compile model for latency processing\n",
    "\n",
    "        Parameters:\n",
    "            model_path: path to the model to load\n",
    "        Returns:\n",
    "           model: compiled and ready OpenVINO model\n",
    "    \"\"\"\n",
    "    # initialize OpenVINO\n",
    "    core = ov.Core()\n",
    "    # read the model from file\n",
    "    model = core.read_model(model_path)\n",
    "    # compile the model for latency mode\n",
    "    model = core.compile_model(model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads a JSON file containing a list of zones, where the keys are the zone names and the values are lists of points defining the zone. It returns a list of numpy arrays, where each array contains the points of a zone in the format `(n, 2)`. `n` is the number of points and the second dimension consist of two values, the x and y coordinates of each point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_zones(json_path: str) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "        Load zones specified in an external json file\n",
    "\n",
    "        Parameters:\n",
    "            json_path: path to the json file with defined zones\n",
    "        Returns:\n",
    "           zones: a list of arrays with zone points\n",
    "    \"\"\"\n",
    "    # load json file\n",
    "    with open(json_path) as f:\n",
    "        zones_dict = json.load(f)\n",
    "\n",
    "    # return a list of zones defined by points\n",
    "    return [np.array(zone[\"points\"], np.int32) for zone in zones_dict.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a path to a JSON file that defines zones and their boundaries, as well as the resolution of the frame. It loads the zones, assigns colors to them, and creates `PolygonZone`, `PolygonZoneAnnotator`, and `BoxAnnotator` objects for each zone. These objects are returned in three lists: `zones`, `zone_annotators`, and `box_annotators`. The objects are used for visualizing and counting people in the specified zones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_annotators(json_path: str, resolution_wh: Tuple[int, int]) -> Tuple[List, List, List]:\n",
    "    \"\"\"\n",
    "        Load zones specified in an external json file\n",
    "\n",
    "        Parameters:\n",
    "            json_path: path to the json file with defined zones\n",
    "            resolution_wh: width and height of the frame\n",
    "        Returns:\n",
    "           zones, zone_annotators, box_annotators: lists of zones and their annotators\n",
    "    \"\"\"\n",
    "    # list of points\n",
    "    polygons = load_zones(json_path)\n",
    "\n",
    "    # colors for zones\n",
    "    colors = sv.ColorPalette.default()\n",
    "\n",
    "    zones = []\n",
    "    zone_annotators = []\n",
    "    box_annotators = []\n",
    "    for index, polygon in enumerate(polygons):\n",
    "        # a zone to count people in\n",
    "        zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=resolution_wh)\n",
    "        zones.append(zone)\n",
    "        # the annotator - visual part of the zone\n",
    "        zone_annotators.append(sv.PolygonZoneAnnotator(zone=zone, color=colors.by_idx(index), thickness=4))\n",
    "        # box annotator, showing boxes around people\n",
    "        box_annotators.append(sv.BoxAnnotator(color=colors.by_idx(index)))\n",
    "\n",
    "    return zones, zone_annotators, box_annotators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `draw_text` function calculates the size of the text and the size of the rectangle that will be drawn around the text based on the image size. It uses the `cv2.rectangle()` function to draw the rectangle and the `cv2.putText()` function to draw the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text(image, text, point, color=(255, 255, 255)) -> None:\n",
    "    \"\"\"\n",
    "    Draws \"Store assistant required\" in the bottom-right corner\n",
    "\n",
    "    Parameters:\n",
    "        image: image to draw on\n",
    "        text: text to draw\n",
    "        point: top left corner of the text\n",
    "        color: text color\n",
    "    \"\"\"\n",
    "    _, f_width = image.shape[:2]\n",
    "    text_size, _ = cv2.getTextSize(text, fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=f_width / 1500, thickness=2)\n",
    "\n",
    "    rect_width = text_size[0] + 20\n",
    "    rect_height = text_size[1] + 20\n",
    "    rect_x, rect_y = point\n",
    "\n",
    "    cv2.rectangle(image, pt1=(rect_x, rect_y), pt2=(rect_x + rect_width, rect_y + rect_height), color=(0, 0, 0), thickness=cv2.FILLED)\n",
    "\n",
    "    text_x = rect_x + (rect_width - text_size[0]) // 2\n",
    "    text_y = rect_y + (rect_height + text_size[1]) // 2\n",
    "\n",
    "    cv2.putText(image, text=text, org=(text_x, text_y), fontFace=cv2.FONT_HERSHEY_SIMPLEX, fontScale=f_width / 1500, color=color, thickness=2, lineType=cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Function\n",
    "\n",
    "Run queue management on the specified source. Either a webcam or a video file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def queue_management(video_path: Union[str, int], model_path: str = \"../model/yolov8m_openvino_model/yolov8m.xml\", zones_config_file: str = \"zones.json\", customers_limit: int = 3) -> None:\n",
    "    \"\"\"\n",
    "    Main processing function.\n",
    "\n",
    "    Parameters:\n",
    "        video_path: path to the video file or camera number\n",
    "        model_path: path to the object detection OV model\n",
    "        zones_config_file: path to zones config JSON file\n",
    "        customers_limit: limit of customers in every queue\n",
    "    \"\"\"\n",
    "    # initialize and load model\n",
    "    model = get_model(model_path)\n",
    "    # input shape of the model (w, h, d)\n",
    "    input_shape = tuple(model.inputs[0].shape)[:0:-1]\n",
    "\n",
    "    # initialize video player to deliver frames\n",
    "    if isinstance(video_path, str) and video_path.isnumeric():\n",
    "        video_path = int(video_path)\n",
    "    player = utils.VideoPlayer(video_path, fps=60)\n",
    "\n",
    "    # get zones, and zone and box annotators for zones\n",
    "    zones, zone_annotators, box_annotators = get_annotators(json_path=zones_config_file,\n",
    "                                                            resolution_wh=(player.width, player.height))\n",
    "    \n",
    "    # people counter\n",
    "    queue_count = defaultdict(deque)\n",
    "    # keep at most 100 last times\n",
    "    processing_times = deque(maxlen=100)\n",
    "    \n",
    "    # initialize a video writer with codec and fps\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter('output.mp4', fourcc, player.fps, (player.width, player.height))\n",
    "\n",
    "\n",
    "    # start a video stream\n",
    "    player.start()\n",
    "    while True:\n",
    "        # Grab the frame.\n",
    "        frame = player.next()\n",
    "        if frame is None:\n",
    "            print(\"Source ended\")\n",
    "            break\n",
    "        # If the frame is larger than full HD, reduce size to improve the performance.\n",
    "        scale = 1280 / max(frame.shape)\n",
    "        if scale < 1:\n",
    "            frame = cv2.resize(\n",
    "                src=frame,\n",
    "                dsize=None,\n",
    "                fx=scale,\n",
    "                fy=scale,\n",
    "                interpolation=cv2.INTER_AREA,\n",
    "            )\n",
    "        # Get the results.\n",
    "        frame = np.array(frame)\n",
    "        f_height, f_width = frame.shape[:2]\n",
    "        \n",
    "        start_time = time.time()\n",
    "        # preprocessing\n",
    "        input_image = preprocess(image=frame, input_size=input_shape[:2])\n",
    "        # prediction\n",
    "        prediction = model(input_image)[model.outputs[0]]\n",
    "        # postprocessing\n",
    "        detections = postprocess(pred_boxes=prediction, input_size=input_shape[:2], orig_img=frame)\n",
    "        processing_times.append(time.time() - start_time)\n",
    "\n",
    "        # annotate the frame with the detected persons within each zone\n",
    "        for zone_id, (zone, zone_annotator, box_annotator) in enumerate(zip(zones, zone_annotators, box_annotators), start=1):\n",
    "            # visualize polygon for the zone\n",
    "            frame = zone_annotator.annotate(scene=frame)\n",
    "\n",
    "            # get detections relevant only for the zone\n",
    "            mask = zone.trigger(detections=detections)\n",
    "            detections_filtered = detections[mask]\n",
    "            # visualize boxes around people in the zone\n",
    "            frame = box_annotator.annotate(scene=frame, detections=detections_filtered, skip_label=True)\n",
    "            # count how many people detected\n",
    "            det_count = len(detections_filtered)\n",
    "\n",
    "            # add the count to the list\n",
    "            queue_count[zone_id].append(det_count)\n",
    "            # store the results from last 300 frames (approx. 5-10s)\n",
    "            if len(queue_count[zone_id]) > 300:\n",
    "                queue_count[zone_id].popleft()\n",
    "            # calculate the mean number of customers in the queue\n",
    "            mean_customer_count = np.mean(queue_count[zone_id], dtype=np.int32)\n",
    "            \n",
    "            # add alert text to the frame if necessary\n",
    "            if mean_customer_count > customers_limit:\n",
    "                draw_text(frame, text=f\"Store assistant required on cash desk {zone_id}!\", point=(f_width // 2, f_height - 50), color=(0, 0, 255))\n",
    "\n",
    "            # print an info about number of customers in the queue, ask for the more assistants if required\n",
    "            log.info(f\"Checkout queue: {zone_id}, avg customer count: {mean_customer_count} {'Store assistant required!' if mean_customer_count > customers_limit else ''}\")\n",
    "\n",
    "        # Mean processing time [ms].\n",
    "        processing_time = np.mean(processing_times) * 1000\n",
    "        fps = 1000 / processing_time\n",
    "\n",
    "        draw_text(frame, text=f\"Inference time: {processing_time:.0f}ms ({fps:.1f} FPS)\", point=(f_width * 3 // 5, 10))\n",
    "\n",
    "        # write frame to the output video\n",
    "        out.write(frame)\n",
    "        # show the output live\n",
    "        cv2.imshow(\"Intelligent Queue Management System\", frame)\n",
    "        key = cv2.waitKey(1)\n",
    "        # escape = 27 or 'q' to close the app\n",
    "        if key == 27 or key == ord('q'):\n",
    "            break\n",
    "\n",
    "    # stop the stream\n",
    "    player.stop()\n",
    "    # clean-up windows\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run\n",
    "\n",
    "#### Run Intelligent Queue Management\n",
    "\n",
    "To run Intelligent Queue Management, use a webcam or a video file as the video input. By default, the primary webcam is set with `video_path=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0.\n",
    "\n",
    "If you do not have a webcam, you can still run this demo with a video file. Any [format supported by OpenCV](https://docs.opencv.org/4.5.1/dd/d43/tutorial_py_video_display.html) will work.\n",
    "\n",
    "`queue_management()` is a function that uses the OpenVINO toolkit to perform object detection on a video file and count the number of customers in each zone based on the detected objects. It takes four arguments: the path to the video file, the path to the model file, the path to the JSON file containing the zone definitions, and the maximum number of customers allowed in a zone at any given time. The function displays the output video with the detected objects and zone annotations overlaid, and sends an alert notification to store management if any zones are over capacity.\n",
    "\n",
    "> NOTE: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server (for example, Binder), the webcam will not work. Popup mode may not work if you run this notebook on a remote computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_path = \"../video_file.mp4\"  # Provide Path to the video file or camera number (0, 1, 2, etc.)\n",
    "model_path = \"../model/yolov8m_openvino_model/yolov8m.xml\"\n",
    "zones_config_file = \"../zones.json\"\n",
    "customers_limit = 3\n",
    "\n",
    "queue_management(video_path, model_path, zones_config_file, customers_limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
