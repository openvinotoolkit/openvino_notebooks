{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "550bf249-9df2-4078-965d-7651261a1d4d",
   "metadata": {},
   "source": [
    "# Local ReAct Agent on Intel Lunar Lake\n",
    "In this notebook we will show how to build a ReAct Agent on your Intel Lunar Lake laptop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d8554-68e1-4df7-b4f9-c79489de5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "# ! pip install langchain datasets pandas nltk sentence-transformers langchain-community faiss-cpu\n",
    "# import nltk\n",
    "\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf152da-e25e-4732-bdef-6d3e20e776bf",
   "metadata": {},
   "source": [
    "Build a local database to store local files.\n",
    "The difference here from RAG is that we won't pass every user message to the retriever.\n",
    "This Retriever will be used as a tool for the agent to access local files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed9f199-e2d4-4681-8a55-9342b0686842",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from functools import reduce\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from langchain_community.embeddings import OpenVINOBgeEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "# If there is a problem with import with onnx please install onnx version under 1.16.2:\n",
    "# ! pip install onnx<1.16.2\n",
    "\n",
    "def articles_to_passages(articles, sent_count_per_passage=3):\n",
    "    \"\"\"Split a list of articles to a list of passages\"\"\"\n",
    "    def map(text):\n",
    "        \n",
    "        sents = sent_tokenize(text)\n",
    "        sentence_df = pd.DataFrame(sents, columns=[\"sentence\"]).reset_index()\n",
    "        sentence_df[\"batch\"] = sentence_df[\"index\"] // sent_count_per_passage\n",
    "        passages = list(sentence_df.groupby(\"batch\")[\"sentence\"].apply(lambda x: \" \".join(x)))\n",
    "        return passages\n",
    "    return reduce(lambda l1, l2: l1 + l2, [map(p) for p in articles], [])\n",
    "\n",
    "model_name =  \"BAAI/bge-small-en-v1.5\"\n",
    "save_name = './bge-small-en-v1.5_openvino'\n",
    "saved = os.path.exists(save_name)\n",
    "load_name = save_name if saved else model_name\n",
    "embedding_function = OpenVINOBgeEmbeddings(\n",
    "    model_name_or_path=load_name,\n",
    "    model_kwargs={\"device\": \"CPU\"},\n",
    ")\n",
    "\n",
    "if not saved:\n",
    "    embedding_function.save_model(save_name)\n",
    "\n",
    "def parse_dataset_month(dataset):\n",
    "    sports_articles = dataset.filter(lambda e: \"sport\" in e[\"link\"])[\"content\"]\n",
    "    sports_articles = pd.DataFrame(sports_articles).drop_duplicates()[0].to_list()\n",
    "    # Split documents to passages\n",
    "    sport_passages = articles_to_passages(sports_articles)\n",
    "    return sport_passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936d05d3-8981-4052-861a-e4e71a11f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "index_path = \"./faiss_index\"\n",
    "if os.path.exists(index_path):\n",
    "    database = FAISS.load_local(index_path, embedding_function, allow_dangerous_deserialization=True)\n",
    "else:\n",
    "    all_ds = []\n",
    "    for month in [\"2024-02\",\"2024-03\",\"2024-04\",\"2024-05\"]:\n",
    "        ds = load_dataset('RealTimeData/bbc_news_alltime', month)\n",
    "        all_ds.append(ds)\n",
    "        \n",
    "    sport_passages = []\n",
    "    for ds in all_ds:\n",
    "        sport_passages += parse_dataset_month(ds[\"train\"])\n",
    "    database = FAISS.from_documents([Document(page_content=doc) for doc in sport_passages], embedding_function)\n",
    "    database.save_local(index_path)\n",
    "    print(f'Number of sports arcticles found: {len(sport_passages)}\\nNumber of embedded passages: {len(sport_passages)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070bc2c-23cb-439c-ae05-1bdf5953dd2b",
   "metadata": {},
   "source": [
    "Next we will initilize a retriever from the dataset.\n",
    "We override the `_get_relevant_documents` method to enable a control over the number of documents the retriever will return for every query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ad351-85b1-41b9-8f93-a98327025c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_relevant_documents(self, query, *, run_manager):\n",
    "    search_kwargs = {k:v for k,v in self.search_kwargs.items()}\n",
    "\n",
    "    if \"top_k\" in run_manager.metadata:\n",
    "        search_kwargs[\"k\"] = run_manager.metadata[\"top_k\"]\n",
    "    if self.search_type == \"similarity\":\n",
    "        docs = self.vectorstore.similarity_search(query, **search_kwargs)\n",
    "    elif self.search_type == \"similarity_score_threshold\":\n",
    "        docs_and_similarities = (\n",
    "            self.vectorstore.similarity_search_with_relevance_scores(\n",
    "                query, **search_kwargs\n",
    "            )\n",
    "        )\n",
    "        docs = [doc for doc, _ in docs_and_similarities]\n",
    "    elif self.search_type == \"mmr\":\n",
    "        docs = self.vectorstore.max_marginal_relevance_search(\n",
    "            query, **search_kwargs\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"search_type of {self.search_type} not allowed.\")\n",
    "    return [d.page_content for d in docs]\n",
    "\n",
    "\n",
    "retriever = database.as_retriever()\n",
    "type(retriever)._get_relevant_documents = _get_relevant_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94638569-4b83-4de3-b8f0-e72462a6e4b2",
   "metadata": {},
   "source": [
    "Next we will initialize our LLM which will be the backbone of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ef00cf-82d5-4020-8d7d-6ac41e5e4fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "# ! pip install optimum[openvino,nncf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbc954-f523-4c12-b90c-0bb70062b728",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "from threading import Thread\n",
    "from transformers import (\n",
    "    TextIteratorStreamer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    GenerationConfig,\n",
    ")\n",
    "\n",
    "class SuffixCriteria(StoppingCriteria):\n",
    "    def __init__(self, start_length, eof_strings, tokenizer, check_fn=None):\n",
    "        self.start_length = start_length\n",
    "        self.eof_strings = eof_strings\n",
    "        self.tokenizer = tokenizer\n",
    "        if check_fn is None:\n",
    "            check_fn = lambda decoded_generation: any(\n",
    "                [decoded_generation.endswith(stop_string) for stop_string in self.eof_strings]\n",
    "            )\n",
    "        self.check_fn = check_fn\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        \"\"\"Returns True if generated sequence ends with any of the stop strings\"\"\"\n",
    "        decoded_generations = self.tokenizer.batch_decode(input_ids[:, self.start_length :])\n",
    "        return all([self.check_fn(decoded_generation) for decoded_generation in decoded_generations])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5718757-885c-4bd6-bb4f-3cee1182115b",
   "metadata": {},
   "source": [
    "Here you can choose to how many bits you want to quantize your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902061b8-53c6-4f18-815c-22feeb4b0533",
   "metadata": {},
   "outputs": [],
   "source": [
    "bits = 8  # 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a5086-9ed7-45f1-8837-bc1b2d174616",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from optimum.intel import OVModelForCausalLM, OVWeightQuantizationConfig\n",
    "from functools import wraps\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "\n",
    "\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "save_name = model_name.split(\"/\")[-1] + f\"_openvino_{bits}bit\"\n",
    "quantization_config = OVWeightQuantizationConfig(\n",
    "    bits=4,\n",
    "    sym=False,\n",
    "    group_size=128,\n",
    "    ratio=0.8,\n",
    ")\n",
    "device = \"gpu\"\n",
    "saved = os.path.exists(save_name)\n",
    "load_kwargs = {\n",
    "    \"device\": device,\n",
    "    \"export\": not saved,\n",
    "}\n",
    "if bits == 4:\n",
    "    load_kwargs[\"quantization_config\"] = quantization_config\n",
    "elif bits == 8:\n",
    "    load_kwargs[\"load_in_8bit\"] = True\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "s = \"\"\"```\n",
    "\n",
    "```\"\"\"\n",
    "stop_words_list =[\"Observation:\", s]\n",
    "stopping_criteria = SuffixCriteria(0, stop_words_list, tokenizer)\n",
    "\n",
    "ov_llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id=model_name if not saved else save_name,\n",
    "    task=\"text-generation\",\n",
    "    backend=\"openvino\",\n",
    "    model_kwargs=load_kwargs,\n",
    "    pipeline_kwargs={\n",
    "        \"stopping_criteria\": StoppingCriteriaList([stopping_criteria]),\n",
    "        \"eos_token_id\": tokenizer.convert_tokens_to_ids([\"<|endoftext|>\", \"<|end|>\", \"<|system|>\", \"<|user|>\", \"<|assistant|>\"]),\n",
    "        \"pad_token_id\": tokenizer.eos_token_id,\n",
    "        \"return_full_text\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "if not saved:\n",
    "    # For some reason LC passes the model_kwargs to the tokenizer aswell and this can cause issues when saving\n",
    "    for k in load_kwargs:\n",
    "        ov_llm.pipeline.tokenizer.__dict__['init_kwargs'].pop(k, None)\n",
    "    ov_llm.pipeline.save_pretrained(save_name)\n",
    "    \n",
    "\n",
    "original_generate = HuggingFacePipeline._generate\n",
    "\n",
    "@wraps(original_generate)\n",
    "def _generate_with_kwargs(*args, **kwargs):\n",
    "    pipeline_kwargs = kwargs.get(\"run_manager\").metadata.get(\"pipeline_kwargs\", {})\n",
    "    return original_generate(*args, **kwargs, pipeline_kwargs=pipeline_kwargs)\n",
    "\n",
    "HuggingFacePipeline._generate = _generate_with_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5dd8b5d-c787-41bc-aed4-bbb91923679d",
   "metadata": {},
   "source": [
    "## React\n",
    "Next, we will define the system prompt of the agent. \n",
    "This will give the LLM the instructions on how to act when recieving a prompt and present the tools that are availalbe for the agent to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aec1278-2fb4-4d43-9d9e-0ba2e6fb3159",
   "metadata": {},
   "outputs": [],
   "source": [
    "AGENT_PROMPT = \"\"\"<|system|>You are designed to help with a variety of tasks, from answering questions to providing summaries to other types of analyses.\n",
    "\n",
    "## Tools\n",
    "\n",
    "You have access to a wide variety of tools. You are responsible for using the tools in any sequence you deem appropriate to complete the task at hand.\n",
    "This may require breaking the task into subtasks and using different tools to complete each subtask.\n",
    "\n",
    "You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "\n",
    "## Output Format\n",
    "\n",
    "### Tool Format\n",
    "If you have enough information to answer the question and don't need to use any tool, skip to the Answer Format section.\n",
    "Please use the following format when using a tool:\n",
    "\n",
    "```\n",
    "Thought: I need to use a tool to help me answer the question.\n",
    "Action: [tool name (one of {tool_names}) if using a tool.]\n",
    "Action Input: [the input to the tool, in a JSON format representing the kwargs]\n",
    "Observation:\n",
    "```\n",
    "\n",
    "Please ALWAYS start with a Thought.\n",
    "\n",
    "NEVER surround your response with markdown code markers. You may use code markers within your response if you need to.\n",
    "\n",
    "Please use a valid JSON format for the Action Input.\n",
    "\n",
    "You should keep repeating the above format till you have enough information to answer the question without using any more tools. At that point, you MUST respond in the one of the following two answer formats.\n",
    "\n",
    "### Answer Format\n",
    "\n",
    "```\n",
    "Thought: I can answer without using any more tools.\n",
    "Answer: [your answer here]<|end|>\n",
    "```\n",
    "\n",
    "```\n",
    "Thought: I cannot answer the question with the provided tools.\n",
    "Answer: [your answer here]<|end|>\n",
    "```\n",
    "\n",
    "## Current Conversation\n",
    "\n",
    "Below is the current conversation consisting of interleaving human and assistant messages.\n",
    "{chat_history}\n",
    "<|user|>\n",
    "Human: {input}<|end|>\n",
    "<|assistant|>\n",
    "{agent_scratchpad}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb07ac78-56e7-4306-bdaa-3bd09d16b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "mod_readct_temp = PromptTemplate(template=AGENT_PROMPT, input_variables=['agent_scratchpad', 'chat_history', 'input', 'tools'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70fd52e-5487-4357-bd40-affacb59e1f0",
   "metadata": {},
   "source": [
    "Now we can define the tools that the agent will be able to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f1db2e-c6b9-48e2-8482-d1842c9fa2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "\n",
    "rag_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"search_my_local_files\",\n",
    "    \"\"\"Searches over the user's local documents. Use the following format: {\"query\": [your input goes here]}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7129130c-0966-4218-b1c9-e5b836471040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import base as prompts_base_imp, BasePromptTemplate\n",
    "from typing import Dict, Union\n",
    "from langchain_core.documents.base import Document\n",
    "\n",
    "\n",
    "def _get_document_info(doc: Union[Document,str], prompt: BasePromptTemplate[str]) -> Dict:\n",
    "    if type(doc) == str:\n",
    "        doc_modified = f\"\"\"Success! I have found the information that I need:\n",
    "{doc}\n",
    "I will now use this information to ansewer the question.\"\"\"\n",
    "        base_info = {\"page_content\": doc_modified}\n",
    "    else:\n",
    "        base_info = {\"page_content\": doc.page_content, **doc.metadata}\n",
    "    missing_metadata = set(prompt.input_variables).difference(base_info)\n",
    "    if len(missing_metadata) > 0:\n",
    "        required_metadata = [\n",
    "            iv for iv in prompt.input_variables if iv != \"page_content\"\n",
    "        ]\n",
    "        raise ValueError(\n",
    "            f\"Document prompt requires documents to have metadata variables: \"\n",
    "            f\"{required_metadata}. Received document with missing metadata: \"\n",
    "            f\"{list(missing_metadata)}.\"\n",
    "        )\n",
    "    return {k: base_info[k] for k in prompt.input_variables}\n",
    "prompts_base_imp._get_document_info = _get_document_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516183be-64a3-4c8b-9c3e-c6d819498924",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bing Search\n",
    "from langchain.tools.bing_search import BingSearchRun\n",
    "import os\n",
    "from langchain_community.utilities import BingSearchAPIWrapper\n",
    "\n",
    "# Follow these instructions to acquite a subscription key:\n",
    "# https://learn.microsoft.com/en-us/bing/search-apis/bing-web-search/create-bing-search-service-resource\n",
    "os.environ[\"BING_SUBSCRIPTION_KEY\"] = \"<<<Enter your Bing subscription key here>>>\"\n",
    "os.environ[\"BING_SEARCH_URL\"] = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "\n",
    "class ReactBingSearchAPIWrapper(BingSearchAPIWrapper):\n",
    "    def run(self, query: str) -> str:\n",
    "        \"\"\"Run query through BingSearch and parse result.\"\"\"\n",
    "        joined_snippets = super().run(query)\n",
    "        return f\"\"\"The answer to the query '{query}' is:\n",
    "{joined_snippets.replace('...','')}\n",
    "\"\"\"\n",
    "        \n",
    "bing_search = ReactBingSearchAPIWrapper(k=2)\n",
    "\n",
    "bing_search_tool = BingSearchRun(\n",
    "    api_wrapper=bing_search,\n",
    "    name=\"bing_search\",\n",
    "    description=\"\"\"Searches over the internet with Bing. Use the following format: {{\"query\": [your input goes here]}}\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fcc3ef-7c06-4b48-8d70-cc783c4fd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_for_agent = [rag_tool, bing_search_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b35188e-c4e6-482f-8d8d-9c90ee6657c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "agent = create_react_agent(ov_llm, tools_for_agent, mod_readct_temp)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools_for_agent, verbose=True, handle_parsing_errors=False, return_intermediate_steps=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cfbfd6-c0d1-4df7-a424-21e9e4b6ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers.react_single_input import ReActSingleInputOutputParser, FINAL_ANSWER_ACTION, MISSING_ACTION_AFTER_THOUGHT_ERROR_MESSAGE, MISSING_ACTION_INPUT_AFTER_ACTION_ERROR_MESSAGE\n",
    "import json\n",
    "import re\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.agents import AgentAction, AgentFinish\n",
    "from langchain_core.exceptions import OutputParserException\n",
    "import logging\n",
    "from langchain.agents.agent import AgentOutputParser\n",
    "from langchain.agents.mrkl.prompt import FORMAT_INSTRUCTIONS\n",
    "\n",
    "from typing import Union\n",
    "\n",
    "def extract_json_from_string(string):\n",
    "    found = False\n",
    "    for index, char in enumerate(string):\n",
    "        if char in \"{[\":\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        return {}\n",
    "    string = string[index:]\n",
    "    try:\n",
    "        json_parsed = json.loads(string)\n",
    "        return json_parsed\n",
    "    except json.JSONDecodeError as json_exception:\n",
    "        json_end_index = json_exception.pos\n",
    "        json_parsed = json.loads(string[:json_end_index])\n",
    "        return json_parsed\n",
    "\n",
    "def parse(self, text: str) -> Union[AgentAction, AgentFinish]:\n",
    "    includes_answer = FINAL_ANSWER_ACTION in text or \"Answer:\" in text\n",
    "    regex = (\n",
    "        r\"Action\\s*\\d*\\s*:[\\s]*(.*?)[\\s]*Action\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "    )\n",
    "    action_match = re.search(regex, text, re.DOTALL)\n",
    "    if action_match:\n",
    "        action = action_match.group(1).strip()\n",
    "        action_input = action_match.group(2)\n",
    "        tool_input = action_input.strip(\" \")\n",
    "        tool_input = tool_input.strip('\"')\n",
    "        \n",
    "        tool_input = tool_input.replace(\"Observation:\",\"\")\n",
    "        tool_input = tool_input.replace(\"{{\",\"{\").replace(\"}}\",\"}\")\n",
    "        try:    \n",
    "            tool_input = extract_json_from_string(tool_input)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        return AgentAction(action, tool_input, text)\n",
    "\n",
    "    elif includes_answer:\n",
    "        return AgentFinish(\n",
    "            {\"output\": text.split(FINAL_ANSWER_ACTION)[-1].strip()}, text\n",
    "        )\n",
    "    else:\n",
    "        return AgentFinish(\n",
    "            {\"output\": text}, text\n",
    "        )\n",
    "\n",
    "ReActSingleInputOutputParser.parse = parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f27cd7-12ab-4dd8-acca-c87830bd85be",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.agent.stream_runnable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126b60ff-983e-4c37-b9a6-7cf7755e528b",
   "metadata": {},
   "source": [
    "## Chatbot with ReAct Agent\n",
    "We are now ready to build our chatbot demo with the agent.\n",
    "We will use [Gradio](https://www.gradio.app/) to build our demo.\n",
    "\n",
    "First, we will define our chat memory and modify our template and chain to be able to handle chat memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "395d148f-6049-4e00-b4d5-9d43efd41c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install dependencies\n",
    "# ! pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194855c3-4d89-49ef-8ef3-d328406b9889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextIteratorStreamer, AutoTokenizer\n",
    "\n",
    "class TextStreamerFlagException(Exception):\n",
    "    pass\n",
    "\n",
    "class FlaggedTextIteratorStreamer(TextIteratorStreamer):\n",
    "    class Flag:\n",
    "        pass\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.flag = self.Flag()\n",
    "    \n",
    "    def put_flag(self):\n",
    "        self.text_queue.put(self.flag)\n",
    "\n",
    "    def __next__(self):\n",
    "        value = super().__next__()\n",
    "        if value is self.flag:\n",
    "            raise TextStreamerFlagException()\n",
    "        else:\n",
    "            return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a01dbb-9fd9-4853-9972-ceb7d08f3ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages.base import BaseMessage\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "\n",
    "def parse_chat_history(chat_history):\n",
    "    role_map = {\"human\": \"<|user|> Human:\", \"ai\": \"<|assistant|>\", \"context\": \"\"}\n",
    "    buffer = \"\"\n",
    "    for dialogue_turn in chat_history:\n",
    "        assert isinstance(dialogue_turn, BaseMessage)\n",
    "        role_prefix = role_map[dialogue_turn.type]\n",
    "        role_suffix = \"<|end|>\" if dialogue_turn.type in [\"context\", \"ai\"] else \"\"\n",
    "        buffer += f\"\\n{role_prefix} {dialogue_turn.content} {role_suffix}\"\n",
    "        buffer = buffer[:-1] if buffer[-1] == \"\\n\" else buffer\n",
    "    return buffer\n",
    "\n",
    "def add_to_memory(memory, question, context, answer):\n",
    "    memory.chat_memory.add_messages([\n",
    "        BaseMessage(\n",
    "            content=question,\n",
    "            type=\"human\"\n",
    "        ),\n",
    "        BaseMessage(\n",
    "            content=context,\n",
    "            type=\"context\"\n",
    "        ),\n",
    "        BaseMessage(\n",
    "            content=answer,\n",
    "            type=\"ai\"\n",
    "        )\n",
    "    ])\n",
    "\n",
    "def delete_last_message_from_memory(memory):\n",
    "    del memory.chat_memory.messages[-3:]\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", ai_prefix=\"Assistant\", human_prefix=\"User\")\n",
    "prompt = mod_readct_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79381021-130e-452e-bb59-f87c521fe94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from operator import itemgetter\n",
    "\n",
    "rag_chain_agent = (\n",
    "    {\n",
    "        \"input\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\") | RunnableLambda(parse_chat_history),\n",
    "    }\n",
    "    | RunnableLambda(func=lambda x: x)\n",
    "    | {\n",
    "        \"answer\": agent_executor\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d8d5d4-f075-4d4f-9203-c3f52e0b714e",
   "metadata": {},
   "source": [
    "Next we will write our core functions generation function for our demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a48961-5cb4-42fc-9a76-4cc6cdd7326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "from threading import Thread\n",
    "from transformers import (\n",
    "    TextIteratorStreamer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    GenerationConfig,\n",
    ")\n",
    "from langchain.schema.runnable import RunnableConfig\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "class ThreadWithResult(Thread):\n",
    "    \"\"\"\n",
    "    Modified Thread class to save the return value of the target function\n",
    "    Based on https://stackoverflow.com/a/65447493\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, group=None, target=None, name=None, args=(), kwargs={}, *, daemon=None):\n",
    "        def function():\n",
    "            self._result = target(*args, **kwargs)\n",
    "        super().__init__(group=group, target=function, name=name, daemon=daemon)\n",
    "\n",
    "    @property\n",
    "    def result(self):\n",
    "        self.join()\n",
    "        return self._result\n",
    "\n",
    "def is_partial_stop(output, stop_str):\n",
    "    \"\"\"\n",
    "    Check whether the output contains a partial stop str.\n",
    "\n",
    "    Params:\n",
    "      output: current output from the model\n",
    "      stop_str: a string we will want to generation on\n",
    "    Returns:\n",
    "      True if the suffix of the output is a prefix of the stop_str\n",
    "    \"\"\"\n",
    "    for i in range(0, min(len(output), len(stop_str))):\n",
    "        if stop_str.startswith(output[-i:]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def format_context(context):\n",
    "    \"\"\"\n",
    "    Utility function to format retrieved documents inside the chatbot window\n",
    "\n",
    "    Params:\n",
    "      context: retrived documents\n",
    "    Returns:\n",
    "      Formated string with the retrieved documents\n",
    "    \"\"\"\n",
    "    if len(context) == 0:\n",
    "        return \"\"\n",
    "    blockquote_style = \"\"\"font-size: 12px;\n",
    "background: #e4e4e4;\n",
    "border-left: 10px solid #ccc; \n",
    "margin: 0.5em 30px;\n",
    "padding: 0.5em 10px;\n",
    "color: black;\"\"\"\n",
    "    summary_style = \"\"\"font-weight: bold;\n",
    "font-size: 14px;\n",
    "list-style-position: outside;\n",
    "margin: 0.5em 15px;\n",
    "padding: 0px 0px 10px 15px;\"\"\"\n",
    "    s = f'<details style=\"margin:0px;padding:0px;\"><summary style=\"{summary_style}\">Retrieved documents:</summary>'\n",
    "    for doc in context:\n",
    "        d = doc.replace(\"\\n\", \" \")\n",
    "        s += f'<blockquote style=\"{blockquote_style}\"><p>{d}</p></blockquote>'\n",
    "    s += \"</details>\"\n",
    "    return s\n",
    "\n",
    "def prepare_for_regenerate(history):\n",
    "    \"\"\"\n",
    "    Delete last assistant response from memory in order to regenerate it\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      Updated history\n",
    "    \"\"\"\n",
    "    history[-1][1] = None\n",
    "    delete_last_message_from_memory(memory)\n",
    "    return history, *([gr.update(interactive=False)] * 6)\n",
    "\n",
    "def add_user_text(message, history):\n",
    "    \"\"\"\n",
    "    Add user's message to chatbot history\n",
    "\n",
    "    Params:\n",
    "      message: current user message\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      Updated history, clears user message and status\n",
    "    \"\"\"\n",
    "    # Append current user message to history with a blank assistant message which will be generated by the model\n",
    "    history.append([message.strip(), None])\n",
    "    return \"\", history, *([gr.update(interactive=False)] * 5)\n",
    "\n",
    "def reset_chatbot():\n",
    "    \"\"\"Clears demo contents and resets chat history\"\"\"\n",
    "    memory.clear()\n",
    "    return None, None, \"Status: Idle\"\n",
    "\n",
    "def get_all_contexts_used(agent_result):\n",
    "    return \"\\n\".join([str(agent_res[1]) for agent_res in agent_result[\"intermediate_steps\"]])\n",
    "\n",
    "def clean_text(text):\n",
    "    removed_ticks = text.replace(\"```\",\"\")\n",
    "    for s_word in stop_words_list:\n",
    "        removed_ticks = removed_ticks.rstrip(s_word)\n",
    "    return removed_ticks\n",
    "\n",
    "def generate(\n",
    "    history,\n",
    "    temperature,\n",
    "    max_new_tokens,\n",
    "    top_p,\n",
    "    repetition_penalty,\n",
    "    num_retrieved_docs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the assistant's reponse given the chatbot history and generation parameters\n",
    "\n",
    "    Params:\n",
    "      history: conversation history formated in pairs of user and assistant messages `[user_message, assistant_message]`\n",
    "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
    "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
    "      max_new_tokens: The maximum number of tokens we allow the model to generate as a response.\n",
    "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
    "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
    "      num_retrieved_docs: number of documents to retrieve in case of RAG\n",
    "    Yields:\n",
    "      Updated history and generation status.\n",
    "    \"\"\"\n",
    "    if len(history) == 0 or history[-1][1] is not None:\n",
    "        yield history, \"Status: Idle\", *([gr.update(interactive=True)] * 6)\n",
    "        return\n",
    "    prompt_char = '▌'\n",
    "    history[-1][1] = prompt_char\n",
    "    yield history, \"Status: Generating...\", *([gr.update(interactive=False)] * 6)\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    user_query = history[-1][0]\n",
    "    current_chain = rag_chain_agent\n",
    "    tokenizer = ov_llm.pipeline.tokenizer\n",
    "    streamer = FlaggedTextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    stop_str = [\"```\\n\\n```\"]\n",
    "    \n",
    "    # Prepare input for generate\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=temperature > 0.0,\n",
    "        temperature=temperature if temperature > 0.0 else 1.0,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        top_p=top_p,\n",
    "    )\n",
    "    generate_kwargs = dict(\n",
    "        streamer=streamer,\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    chain_kwargs = {\n",
    "        \"config\": RunnableConfig(metadata={\n",
    "            \"top_k\": num_retrieved_docs,\n",
    "            \"pipeline_kwargs\": generate_kwargs\n",
    "        })}\n",
    "\n",
    "    def target(*args, **kwargs):\n",
    "        out = current_chain.invoke(*args, **kwargs)\n",
    "        streamer.put_flag()\n",
    "        return out\n",
    "    # Call chain\n",
    "    t1 = ThreadWithResult(\n",
    "        target=target,\n",
    "        args=[{\"question\": user_query, \"chat_history\": memory.chat_memory.messages}],\n",
    "        kwargs=chain_kwargs,\n",
    "    )\n",
    "    t1.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text.\n",
    "    partial_text = \"\"\n",
    "    generated_tokens = 0\n",
    "    try:\n",
    "        while True:\n",
    "            for new_text in streamer:\n",
    "                partial_text += new_text\n",
    "                generated_tokens += 1\n",
    "                history[-1][1] = partial_text + prompt_char\n",
    "                pos = -1\n",
    "                for s in stop_str:\n",
    "                    if (pos := partial_text.rfind(s)) != -1:\n",
    "                        break\n",
    "                if pos != -1:\n",
    "                    partial_text = partial_text[:pos]\n",
    "                    raise TextStreamerFlagException()\n",
    "                elif any([is_partial_stop(partial_text, s) for s in stop_str]):\n",
    "                    continue\n",
    "                yield history, \"Status: Generating...\", *([gr.update(interactive=False)] * 6)\n",
    "            partial_text += \"\\n\"\n",
    "    except TextStreamerFlagException:\n",
    "        pass\n",
    "    history[-1][1] = partial_text\n",
    "    chain_out = t1.result\n",
    "    if \"intermediate_steps\" in chain_out[\"answer\"]:\n",
    "        current_context = get_all_contexts_used(chain_out[\"answer\"])\n",
    "        if current_context != \"\":\n",
    "            history[-1][1] = partial_text + format_context([current_context])\n",
    "    else:\n",
    "        current_context = \"\"\n",
    "\n",
    "    add_to_memory(memory, user_query, current_context, partial_text)\n",
    "    generation_time = time.perf_counter() - start\n",
    "    yield history, f'Generation time: {generation_time:.2f} sec', *([gr.update(interactive=True)] * 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8afd92-1c31-4429-8fac-4b2f6b40d2ca",
   "metadata": {},
   "source": [
    "Let's add an option to chat with our own documents by loading them to our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c9549-e714-4892-9a7c-f91e6f2d2877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ba3eb4-5e8b-4d45-b4b9-cae901bb683d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader \n",
    "\n",
    "\n",
    "added_documents_ids = []\n",
    "\n",
    "\n",
    "def pdf_to_docs(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    texts = [page.extract_text() for page in reader.pages]\n",
    "    return [Document(page_content=p) for p in articles_to_passages(texts)]\n",
    "\n",
    "\n",
    "def load_files(files):\n",
    "    yield (\n",
    "        f'Loading...', \n",
    "        *([gr.update(interactive=False)] * 6),\n",
    "    )\n",
    "    start = time.perf_counter()\n",
    "    for fp in files:\n",
    "        documents = pdf_to_docs(fp)\n",
    "        added_documents_ids.append(database.add_documents(documents))\n",
    "    upload_time = time.perf_counter() - start\n",
    "    yield (\n",
    "        f'Load time: {upload_time * 1000:.2f}ms', \n",
    "        *([gr.update(interactive=True)] * 5),\n",
    "        gr.update(value=f\"Delete documents 〈{len(added_documents_ids)}〉\", interactive=True),\n",
    "    )\n",
    "\n",
    "\n",
    "def delete_documents():\n",
    "    yield (\n",
    "        f'Deleting...', \n",
    "        *([gr.update(interactive=False)] * 6),\n",
    "    )\n",
    "    global added_documents_ids\n",
    "    for l in added_documents_ids:\n",
    "        database.delete(l)\n",
    "    added_documents_ids = []\n",
    "    yield (\n",
    "        f'Status: Idle',\n",
    "        *([gr.update(interactive=True)] * 5),\n",
    "        gr.update(value=f\"Delete documents 〈{len(added_documents_ids)}〉\", interactive=True),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff747f-1ca1-4b31-9ef5-608673b37d0a",
   "metadata": {},
   "source": [
    "Now we can build the actual demo using Gradio.\n",
    "The layout will be simple, a chatbow window followed by a text prompt with controls that will let you submit a message, clear chat and regenerate the last answer, this is pretty standard for a chatbot demo.\n",
    "We have also added the option to add PDF documents to the database and delete them if required.\n",
    "You can extend the add documents option to support other formats than PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c95e58-7080-494a-84c9-0df4e68bbbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "try:\n",
    "    demo.close()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "EXAMPLES_EDUCATION = [\n",
    "    \"Lily drops a rubber ball from the top of a wall. The wall is 2 meters tall. How long will it take for the ball to reach the ground?\",\n",
    "    \"Mark has 15 notebooks in his backpack. Each day, he uses 3 notebooks for his classes. After 4 days, how many notebooks will Mark have left in his backpack?\",\n",
    "]\n",
    "EXAMPLES_BBC = [\n",
    "    \"How many teams will The 2024-25 Champions League feature?\",\n",
    "    \"Search in my files how many teams will the Champions League feature this year?\",    \n",
    "]\n",
    "\n",
    "AGENT = [\n",
    "    \"What are the best three locations to visit in Seattle today?\"\n",
    "]\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown('<h1 style=\"text-align: center;\">Intel Labs Demo: Prompt ReAct with Phi-3 on Intel Lunar Lake iGPU</h1>')\n",
    "    chatbot = gr.Chatbot(height=800)\n",
    "    with gr.Row():\n",
    "        msg = gr.Textbox(placeholder=\"Enter message here...\", show_label=False, autofocus=True, scale=75)\n",
    "        status = gr.Textbox(\"Status: Idle\", show_label=False, max_lines=1, scale=20, visible=False)\n",
    "    with gr.Row():\n",
    "        submit = gr.Button(\"Submit\", variant=\"primary\")\n",
    "        regenerate = gr.Button(\"Regenerate\")\n",
    "        clear = gr.Button(\"Clear\")\n",
    "        load = gr.UploadButton(\"Load Document\", file_types=['pdf'], file_count='multiple')\n",
    "        delete_docs = gr.Button(lambda: f\"Delete documents {f'〈{len(added_documents_ids)}〉'}\", interactive=True)\n",
    "    with gr.Accordion(\"Advanced Options:\", open=False):\n",
    "        with gr.Row():\n",
    "            with gr.Column():\n",
    "                temperature = gr.Slider(\n",
    "                    label=\"Temperature\",\n",
    "                    value=0.0,\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    step=0.05,\n",
    "                    interactive=True,\n",
    "                )\n",
    "                max_new_tokens = gr.Slider(\n",
    "                    label=\"Max new tokens\",\n",
    "                    value=128,\n",
    "                    minimum=0,\n",
    "                    maximum=512,\n",
    "                    step=32,\n",
    "                    interactive=True,\n",
    "                )\n",
    "            with gr.Column():\n",
    "                top_p = gr.Slider(\n",
    "                    label=\"Top-p (nucleus sampling)\",\n",
    "                    value=1.0,\n",
    "                    minimum=0.0,\n",
    "                    maximum=1.0,\n",
    "                    step=0.05,\n",
    "                    interactive=True,\n",
    "                )\n",
    "                repetition_penalty = gr.Slider(\n",
    "                    label=\"Repetition penalty\",\n",
    "                    value=1.0,\n",
    "                    minimum=1.0,\n",
    "                    maximum=2.0,\n",
    "                    step=0.1,\n",
    "                    interactive=True,\n",
    "                )\n",
    "            num_documents = gr.Slider(\n",
    "                label=\"Retrieved documents numbers\",\n",
    "                value=1,\n",
    "                minimum=1,\n",
    "                maximum=10,\n",
    "                step=1,\n",
    "                interactive=True\n",
    "                )\n",
    "    gr.Examples(\n",
    "        AGENT, inputs=msg, label=\"Agent examples\"\n",
    "    )\n",
    "    gr.Examples(\n",
    "        EXAMPLES_EDUCATION, inputs=msg, label=\"Non-RAG examples\"\n",
    "    )\n",
    "    gr.Examples(\n",
    "        EXAMPLES_BBC, inputs=msg, label=\"RAG with BBC Sports examples\"\n",
    "    )       \n",
    "    buttons = [submit, regenerate, clear, load, delete_docs]\n",
    "    # Sets generate function to be triggered when the user submit a new message\n",
    "    gr.on(\n",
    "        triggers=[submit.click, msg.submit],\n",
    "        fn=add_user_text,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot, *buttons],\n",
    "        concurrency_limit=1,\n",
    "        queue=True,\n",
    "    ).then(\n",
    "        fn=generate,\n",
    "        inputs=[chatbot, temperature, max_new_tokens, top_p, repetition_penalty, num_documents],\n",
    "        outputs=[chatbot, status, msg, *buttons],\n",
    "        concurrency_limit=1,\n",
    "        queue=True\n",
    "    )\n",
    "    regenerate.click(\n",
    "        fn=prepare_for_regenerate,\n",
    "        inputs=chatbot,\n",
    "        outputs=[chatbot, msg, *buttons],\n",
    "        concurrency_limit=1,\n",
    "        queue=True,\n",
    "    ).then(\n",
    "        fn=generate,\n",
    "        inputs=[chatbot, temperature, max_new_tokens, top_p, repetition_penalty, num_documents],\n",
    "        outputs=[chatbot, status, msg, *buttons],\n",
    "        concurrency_limit=1,\n",
    "        queue=True\n",
    "    )\n",
    "    clear.click(fn=reset_chatbot, inputs=None, outputs=[chatbot, msg, status], queue=True)\n",
    "    load.upload(\n",
    "        fn=load_files,\n",
    "        inputs=[load],\n",
    "        outputs=[status, msg, *buttons],\n",
    "        concurrency_limit=1,\n",
    "        queue=True,\n",
    "    )\n",
    "    delete_docs.click(fn=delete_documents, outputs=[status, msg, *buttons], concurrency_limit=1, queue=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27efd8d1-2b4d-437e-9062-77ee6f3d99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.clear()\n",
    "demo.launch(inline=False, inbrowser=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee004285-50fb-43cc-b04a-2bd996fd47d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# demo.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
