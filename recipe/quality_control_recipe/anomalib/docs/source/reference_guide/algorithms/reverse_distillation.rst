Reverse Distillation
--------------------

This is the implementation of the `Anomaly Detection via Reverse Distillation from One-Class Embedding <https://arxiv.org/pdf/2201.10703v2.pdf>`_ paper.

Model Type: Segmentation

Description
***********

Reverse Distillation model consists of three networks. The first is a pre-trained feature extractor (E). The next two are the one-class bottleneck embedding (OCBE) and the student decoder network (D). The backbone E is a ResNet model pre-trained on ImageNet dataset. During the forward pass, features from three ResNet block are extracted. These features are encoded by concatenating the three feature maps using the multi-scale feature fusion block of OCBE and passed to the decoder D. The decoder network is symmetrical to the feature extractor but reversed. During training, outputs from these symmetrical blocks are forced to be similar to the corresponding feature extractor layers by using cosine distance as the loss metric.

During testing, a similar step is followed but this time the cosine distance between the feature maps is used to indicate the presence of anomalies. The distance maps from all the three layers are up-sampled to the image size and added (or multiplied) to produce the final feature map. Gaussian blur is applied to the output map to make it smoother. Finally, the anomaly map is generated by applying min-max normalization on the output map.

Architecture
************

.. image:: ../../images/reverse_distillation/architecture.png
    :alt: Reverse Distillation Architecture

Usage
*****

.. code-block:: bash

    $ python tools/train.py --model reverse_distillation


.. automodule:: anomalib.models.reverse_distillation.torch_model
   :members:
   :undoc-members:
   :show-inheritance:

.. automodule:: anomalib.models.reverse_distillation.lightning_model
   :members:
   :undoc-members:
   :show-inheritance:

.. automodule:: anomalib.models.reverse_distillation.anomaly_map
   :members:
   :undoc-members:
   :show-inheritance:
