{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports modules required to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from openvino.inference_engine import IECore\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "In this part you have to set up all variables further used in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"model\"\n",
    "download_folder = \"output\"\n",
    "data_folder = \"data\"\n",
    "\n",
    "precision = \"FP16\"\n",
    "model_name = \"quartznet-15x5-en\"\n",
    "model_extensions = (\"bin\", \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models and convert public model\n",
    "\n",
    "We use `omz_downloader` and `omz_converter`, which are command-line tools from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. This step is skipped if the model is already downloaded. The selected model comes from the public directory, which means it must be converted into Intermediate Representation (IR).\n",
    "\n",
    "`omz_converter` is needed to convert pre-trainded `PyTorch` model to ONNX model format, which is further converted to OpenVINO IR format. \n",
    "\n",
    "If it is your first run models will download and convert here. It might take up to ten minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if models are already downloaded in download directory\n",
    "if not path.isdir(f'{download_folder}/public/{model_name}'):\n",
    "    download_command = f\"omz_downloader --name {model_name} --output_dir {download_folder} --precision {precision} --num_attempts 3\"\n",
    "    ! $download_command\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfaf7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.isdir(f'{model_folder}/public/{model_name}/{precision}'):\n",
    "    convert_command = f\"omz_converter --name {model_name} --precisions {precision} --download_dir {download_folder} --output_dir {model_folder}\"\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load audio file\n",
    "\n",
    "Now, when model files are downloaded and converted, you need to load audio file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ad393",
   "metadata": {},
   "source": [
    "### Defining constants\n",
    "\n",
    "First step will be locating audio file and defining alphabet used by model. In this case you will use latin alphabet begining with space symbol and ending with blank symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audio_file_name = \"how_are_you_doing.wav\"\n",
    "alphabet = \" abcdefghijklmnopqrstuvwxyz~\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22895cf3",
   "metadata": {},
   "source": [
    "### Load audio file\n",
    "\n",
    "Next step is opening defined in previous cell audio file and getting params that will allow you to decide if file needs adjustments before placing into preprocessing function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4183aa",
   "metadata": {},
   "source": [
    "### All assertions met, whats next?\n",
    "\n",
    "Now we need to read audio frames and change type of variables in buffer to int16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5070e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_formats = list(sf.available_formats().keys())\n",
    "print(f\"Availble audio formats: {', '.join(audio_formats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0baf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if audio_file_name.split('.')[-1].upper() not in audio_formats:\n",
    "    raise Exception(f\"Invalid file format. Availble formats: {', '.join(audio_formats)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio, sampling_rate = librosa.load(path=f'{data_folder}/{audio_file_name}', sr=16000, mono=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd9d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "librosa.display.waveplot(audio ,sr=sampling_rate, max_points=50000.0, x_axis='time', offset=0.0, max_sr=1000);\n",
    "plt.show()\n",
    "specto_audio = librosa.stft(audio)\n",
    "specto_audio = librosa.amplitude_to_db(np.abs(specto_audio), ref=np.max)\n",
    "print(specto_audio.shape)\n",
    "librosa.display.specshow(specto_audio, sr=sampling_rate);\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb5cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if max(np.abs(audio)) <= 1:\n",
    "    audio = (audio * (2**15 - 1))\n",
    "audio = audio.astype(np.int16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a47cd",
   "metadata": {},
   "source": [
    "### Here comes magic!\n",
    "\n",
    "After all those small convertion now we need to convert our pre-pre-processed audio to [Mel Spectrum](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53). Explaination why are you doing that is covered in multiple articles like [this one](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_melbasis(audio, sampling_rate,):\n",
    "    assert sampling_rate == 16000, \"Only 16 KHz audio supported\"\n",
    "    preemph = 0.97\n",
    "    preemphased = np.concatenate([audio[:1], audio[1:] - preemph * audio[:-1].astype(np.float32)])\n",
    "\n",
    "    win_length = round(sampling_rate * 0.02)\n",
    "    spec = np.abs(librosa.core.spectrum.stft(preemphased, n_fft=512, hop_length=round(sampling_rate * 0.01), \n",
    "                  win_length=win_length, center=True, window=scipy.signal.windows.hann(win_length), pad_mode='reflect'))\n",
    "    mel_basis = librosa.filters.mel(sampling_rate, 512, n_mels=64, fmin=0.0, fmax=8000.0, htk=False)\n",
    "    return mel_basis, spec\n",
    "\n",
    "def melbasis_to_melspectrum(mel_basis, spec, padding=16):\n",
    "    log_melspectrum = np.log(np.dot(mel_basis, np.power(spec, 2)) + 2 ** -24)\n",
    "\n",
    "    normalized = (log_melspectrum - log_melspectrum.mean(1)[:, None]) / (log_melspectrum.std(1)[:, None] + 1e-5)\n",
    "    remainder = normalized.shape[1] % padding\n",
    "    if remainder != 0:\n",
    "        return np.pad(normalized, ((0, 0), (0, padding - remainder)))[None]\n",
    "    return normalized[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mel_basis, spec = audio_to_melbasis(audio.flatten(), sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef57088",
   "metadata": {},
   "outputs": [],
   "source": [
    "librosa.display.specshow(spec, sr=sampling_rate);\n",
    "plt.show()\n",
    "librosa.display.specshow(mel_basis, sr=sampling_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a63159b",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = melbasis_to_melspectrum(mel_basis, spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aec612",
   "metadata": {},
   "source": [
    "## Running network\n",
    "\n",
    "When everything is prepared, you can finally read and load network. You may choose to run the network on multiple devices by default it will load the model on the CPU (you can choose manually CPU, GPU, MYRIAD, etc.) or let the engine choose the best available device (AUTO).\n",
    "\n",
    "To list all available devices that you can use, uncomment and run line `print(ie.available_devices)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5dc5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a69236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ie.available_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ie.read_network(\n",
    "    model=f\"{model_folder}/public/{model_name}/{precision}/{model_name}.xml\"\n",
    ")\n",
    "net.reshape({next(iter(net.input_info)): audio.shape})\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566de49",
   "metadata": {},
   "source": [
    "### Run the inference!\n",
    "\n",
    "Everything is set up. Now only thing remaining is passing input to previously loaded network and running inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_ir = next(iter(exec_net.input_info))\n",
    "\n",
    "character_probabilities = exec_net.infer({input_layer_ir: audio}).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc761843",
   "metadata": {},
   "source": [
    "### Read output\n",
    "\n",
    "After inference you need to reach out the output. Default output format for `quartznet 15x5` are per-frame probabilities (after LogSoftmax) for every symbol in the alphabet, name - output, shape - 1, 64, 29, output data format is B, N, C, where:\n",
    "\n",
    "* B - batch size\n",
    "* N - number of audio frames\n",
    "* C - alphabet size, including the CTC blank symbol\n",
    "\n",
    "You need to make it in a more human-readable format. To do this you need to get a symbol with the highest probability. When you hold a list of indexes that are predicted to have the highest probability, due to limitations given by [CTC Decoding](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7) you will remove concurrent symbols and then remove all the blanks.\n",
    "\n",
    "The last step is getting symbols from corresponding indexes in charlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_probabilities = next(iter(character_probabilities))\n",
    "\n",
    "# Remove unnececery dimension\n",
    "character_probabilities = np.squeeze(character_probabilities)\n",
    "\n",
    "# Run argmax to pick most possible symbols\n",
    "character_probabilities = np.argmax(character_probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d8b66",
   "metadata": {},
   "source": [
    "### Implementation of CTC Decoding\n",
    "\n",
    "To decode previously explained output we need [CTC decode](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_greedy_decode(predictions):\n",
    "    previous_letter_id = blank_id = len(alphabet)\n",
    "    transcription = list()\n",
    "    for letter_index in predictions:\n",
    "        if previous_letter_id != letter_index != blank_id:\n",
    "            transcription.append(alphabet[letter_index])\n",
    "        previous_letter_id = letter_index\n",
    "    return ''.join(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9d9b9",
   "metadata": {},
   "source": [
    "### Run CTC decoding and print output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = ctc_greedy_decode(character_probabilities)\n",
    "print(transcription)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
