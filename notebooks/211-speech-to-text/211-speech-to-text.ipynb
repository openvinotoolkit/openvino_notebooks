{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports modules required to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import librosa\n",
    "except OSError:\n",
    "    import sys\n",
    "    import types\n",
    "    sys.modules['soundfile'] = types.ModuleType('fake_soundfile')\n",
    "    import librosa\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import wave\n",
    "from openvino.inference_engine import IECore\n",
    "from os import path, makedirs, listdir\n",
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "In this part you have to set up all variables further used in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"model\"\n",
    "download_folder = \"output\"\n",
    "data_folder = \"data\"\n",
    "\n",
    "precision = \"FP16\"\n",
    "model_name = \"quartznet-15x5-en\"\n",
    "model_extensions = (\"bin\", \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models and convert public model\n",
    "\n",
    "We use `omz_downloader` and `omz_converter`, which are command-line tools from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. This step is skipped if the model is already downloaded. The selected model comes from the public directory, which means it must be converted into Intermediate Representation (IR).\n",
    "\n",
    "`omz_converter` is needed to convert pre-trainded `PyTorch` model to OpenVINO IR format. \n",
    "\n",
    "If it is your first run models will download and convert here. It might take up to ten minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Check if models are already downloaded in download directory\n",
    "for extension in model_extensions:\n",
    "    if not path.isfile(f'{model_folder}/{model_name}.{extension}'):\n",
    "        download_command = f\"omz_downloader --name {model_name} --output_dir {download_folder} --precision {precision} --num_attempts 3\"\n",
    "        convert_command = f\"omz_converter --name {model_name} --precisions {precision} --download_dir {download_folder} --output_dir {download_folder}\"\n",
    "        # Run commands, first download model than convert it to inferable \n",
    "        ! $download_command\n",
    "        # Models are downloaded straight to output folder, we will keep all not used files outside of models directory\n",
    "        ! $convert_command\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy models to model folder\n",
    "\n",
    "At this point both models are kept in download_folder (by default named ```output```). We need only .bin and .xml files from there that we will copy to ```model directory```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab17c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(model_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in listdir(f\"{download_folder}/public/{model_name}/{precision}\"):\n",
    "    copy(src=f\"{download_folder}/public/{model_name}/{precision}/{file_name}\", dst=model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load audio file\n",
    "\n",
    "Now, when model files are downloaded and converted, you need to load audio file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ad393",
   "metadata": {},
   "source": [
    "### Defining constants\n",
    "\n",
    "First step will be locating audio file and defining alphabet used by model. In this case you will use latin alphabet begining with space symbol and ending with blank symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audio_file_name = \"how_are_you_doing.wav\"\n",
    "alphabet = \" abcdefghijklmnopqrstuvwxyz'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22895cf3",
   "metadata": {},
   "source": [
    "### Load audio file\n",
    "\n",
    "Next step is opening defined in previous cell audio file and getting params that will allow you to decide if file needs adjustments before placing into preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addacba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage\n",
    "wave_read = wave.open(f'{data_folder}/{audio_file_name}')\n",
    "channel_num, sample_width, sampling_rate, pcm_length, compression_type, _ = wave_read.getparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337adee3",
   "metadata": {},
   "source": [
    "### Assertions about audio file\n",
    "\n",
    "For this model we can use audio files that meets those requirements:\n",
    "* 16-bit WAV PCM\n",
    "* without any compression type (linear PCM WAV)\n",
    "* single channel (mono WAV PCM)\n",
    "* 16 KHz audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_width == 2, \"Only 16-bit WAV PCM supported\"\n",
    "assert compression_type == 'NONE', \"Only linear PCM WAV files supported\"\n",
    "assert channel_num == 1, \"Only mono WAV PCM supported\"\n",
    "assert sampling_rate == 16000, \"Only 16 KHz audio supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4183aa",
   "metadata": {},
   "source": [
    "### All assertions met, whats next?\n",
    "\n",
    "Now we need to read audio frames and change type of variables in buffer to int16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = wave_read.readframes(pcm_length * channel_num)\n",
    "audio = np.frombuffer(audio, dtype=np.int16)\n",
    "audio = audio.reshape((pcm_length, channel_num))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666a47cd",
   "metadata": {},
   "source": [
    "### Here comes magic!\n",
    "\n",
    "After all those small convertion now we need to convert our pre-pre-processed audio to [Mel Spectrum](https://medium.com/analytics-vidhya/understanding-the-mel-spectrogram-fca2afa2ce53). Explaination why are you doing that is covered in multiple articles like [this one](https://towardsdatascience.com/audio-deep-learning-made-simple-part-2-why-mel-spectrograms-perform-better-aad889a93505)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_melspectrum(audio, sampling_rate, padding=16):\n",
    "    assert sampling_rate == 16000, \"Only 16 KHz audio supported\"\n",
    "    preemph = 0.97\n",
    "    preemphased = np.concatenate([audio[:1], audio[1:] - preemph * audio[:-1].astype(np.float32)])\n",
    "\n",
    "    win_length = round(sampling_rate * 0.02)\n",
    "    spec = np.abs(librosa.core.spectrum.stft(preemphased, n_fft=512, hop_length=round(sampling_rate * 0.01), \n",
    "                  win_length=win_length, center=True, window=scipy.signal.windows.hann(win_length), pad_mode='reflect'))\n",
    "    mel_basis = librosa.filters.mel(sampling_rate, 512, n_mels=64, fmin=0.0, fmax=8000.0, htk=False)\n",
    "    log_melspectrum = np.log(np.dot(mel_basis, np.power(spec, 2)) + 2 ** -24)\n",
    "\n",
    "    normalized = (log_melspectrum - log_melspectrum.mean(1)[:, None]) / (log_melspectrum.std(1)[:, None] + 1e-5)\n",
    "    remainder = normalized.shape[1] % padding\n",
    "    if remainder != 0:\n",
    "        return np.pad(normalized, ((0, 0), (0, padding - remainder)))[None]\n",
    "    return normalized[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audio_to_melspectrum(audio.flatten(), sampling_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4aec612",
   "metadata": {},
   "source": [
    "## Running network\n",
    "\n",
    "When everything is prepared, you can finally read and load network. You may choose to run the network on multiple devices by default it will load the model on the CPU (you can choose manually CPU, GPU, MYRIAD, etc.) or let the engine choose the best available device (AUTO).\n",
    "\n",
    "To list all available devices that you can use, uncomment and run line `print(ie.available_devices)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "\n",
    "# print(ie.available_devices)\n",
    "\n",
    "net = ie.read_network(\n",
    "    model=f\"{model_folder}/{model_name}.xml\"\n",
    ")\n",
    "net.reshape({next(iter(net.input_info)): audio.shape})\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a566de49",
   "metadata": {},
   "source": [
    "### Run the inference!\n",
    "\n",
    "Everything is set up. Now only thing remaining is passing input to previously loaded network and running inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_ir = next(iter(exec_net.input_info))\n",
    "\n",
    "character_probabilities = exec_net.infer({input_layer_ir: audio}).values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc761843",
   "metadata": {},
   "source": [
    "### Read output\n",
    "\n",
    "After inference you need to reach out the output. Default output format for `quartznet 15x5` are per-frame probabilities (after LogSoftmax) for every symbol in the alphabet, name - output, shape - 1, 64, 29, output data format is B, N, C, where:\n",
    "\n",
    "* B - batch size\n",
    "* N - number of audio frames\n",
    "* C - alphabet size, including the CTC blank symbol\n",
    "\n",
    "You need to make it in a more human-readable format. To do this you need to get a symbol with the highest probability. When you hold a list of indexes that are predicted to have the highest probability, due to limitations given by [CTC Decoding](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7) you will remove concurrent symbols and then remove all the blanks.\n",
    "\n",
    "The last step is getting symbols from corresponding indexes in charlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e3604",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_probabilities = next(iter(character_probabilities))\n",
    "\n",
    "# Remove unnececery dimension\n",
    "character_probabilities = np.squeeze(character_probabilities)\n",
    "\n",
    "# Run argmax to pick most possible symbols\n",
    "character_probabilities = np.argmax(character_probabilities, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477d8b66",
   "metadata": {},
   "source": [
    "### Implementation of CTC Decoding\n",
    "\n",
    "To decode previously explained output we need [CTC decode](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd8a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_greedy_decode(predictions):\n",
    "    previous_letter_id = blank_id = len(alphabet)\n",
    "    transcription = list()\n",
    "    for letter_index in predictions:\n",
    "        if previous_letter_id != letter_index != blank_id:\n",
    "            transcription.append(alphabet[letter_index])\n",
    "        previous_letter_id = letter_index\n",
    "    return ''.join(transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e9d9b9",
   "metadata": {},
   "source": [
    "### Run CTC decoding and print output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = ctc_greedy_decode(character_probabilities)\n",
    "print(transcription)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
