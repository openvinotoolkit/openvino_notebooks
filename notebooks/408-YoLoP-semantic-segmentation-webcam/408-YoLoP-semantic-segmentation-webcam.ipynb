{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65c3763b-4544-41b7-9631-73d7a736d9a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Panoptic segmentation using OpenVINOâ„¢\n",
    "\n",
    "Understand the _state-of-the-art_ computer vision technology which enables driverless vehicles to perceive their environments in real time and helps it make reasonable decisions while driving!! \n",
    "\n",
    "We will be consulting this [repo](https://github.com/hustvl/YOLOP) for the code, models and other data assets. At the same time we will learn how to use the OpenVINO [API](https://docs.openvino.ai/nightly/notebooks/003-hello-segmentation-with-output.html) for converting a model in a different format to the standard OpenVINOâ„¢ IR model representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e2489-039c-46a4-8a3e-2f94757043aa",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4157c21-9b8b-4a09-8e6d-b01c13195a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional,Union\n",
    "from openvino.runtime import Core, serialize\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f60d26-fab2-41d6-b817-f128bc83ce02",
   "metadata": {},
   "source": [
    "### Loading an ONXX model\n",
    "\n",
    "[ONNX](https://onnx.ai/) is an open format built to represent machine learning models. ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers. OpenVINO supports reading models in ONNX format directly,that means they can be used with OpenVINO Runtime without any prior conversion.\n",
    "\n",
    "Reading and loading an ONNX model, which is a single .onnx file, works the same way as with an OpenVINO IR model. The model argument points to the filename of an ONNX model. \n",
    "\n",
    "**The YoLoP models we would be dealing with today are indeed .onxx model files optimised for different image sizes**. We shall directly point to this directory for acquiring the model(s): [https://github.com/hustvl/YOLOP/blob/d37e600cf71ecac20b08865ddfe923d76fd02d55/weights/](https://github.com/hustvl/YOLOP/blob/d37e600cf71ecac20b08865ddfe923d76fd02d55/weights/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb00bfc3-e319-4bd8-a1b9-e4b08596e9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c636468e41904a0891019a4f811c7cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model\\yolop-320-320.onnx: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('G:/PRONOY/openvino_notebooks/notebooks/408-YoLoP-semantic-segmentation-webcam/model/yolop-320-320.onnx')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a model directory adjacent to the notebook and suppress errors if the directory already exists\n",
    "Path('model').mkdir(exist_ok=True)\n",
    "\n",
    "# Download the model file corresponding to the one trained for images of size 320 x 320 from the link \n",
    "# into the directory created above. Suppress errors if the file already exists\n",
    "utils.download_file(\"https://github.com/hustvl/YOLOP/blob/main/weights/yolop-320-320.onnx\", directory='model', silent=True) \n",
    "\n",
    "# Uncomment the following lines if you want to download the remaining models as well\n",
    "\n",
    "# utils.download_file(\"https://github.com/hustvl/YOLOP/blob/main/weights/yolop-640-640.onnx\", directory='model', silent=True) \n",
    "# utils.download_file(\"https://github.com/hustvl/YOLOP/blob/main/weights/yolop-1280-1280.onnx\", directory='model', silent=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf644c9-616c-42e9-941c-1161a976b534",
   "metadata": {},
   "source": [
    "### OpenVINOâ„¢ magic ðŸª„\n",
    "\n",
    "This is what we have come here for. Now we shall convert the downloaded model into the OpenVINO [IR](https://docs.openvino.ai/nightly/notebooks/002-openvino-api-with-output.html#onnx-model) representation by first compiling it on our device and then serializing the same into the ```.xml``` format. We will take one more step and delete the ```.onxx``` file which was used to do the same. The entirety of the process shall be defined in a custom function as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e8c16c-6d60-4643-bc0d-49d61185aaac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "def convert_to_ir_and_remove(model_path: str, compile_device: Union[str, int] = \"CPU\") -> None:\n",
    "    onnx_model_path = Path(model_path)\n",
    "    \n",
    "    if not Path(onnx_model_path.stem + '.xml').is_file():\n",
    "        \n",
    "        model_onnx_yolop = ie.read_model(model=onnx_model_path)\n",
    "\n",
    "        # Compilation has been done on the default CPU device. This may be quick for loading models, however, GPUs perform the best when it comes to actual inference.\n",
    "        compiled_model_onnx_yolop = ie.compile_model(model=model_onnx_yolop, device_name=compile_device)\n",
    "        \n",
    "        serialize(compiled_model_onnx_yolop, xml_path=Path(onnx_model_path.stem + '.xml'))\n",
    "    \n",
    "    \n",
    "    # Remove the .onxx file if it exists\n",
    "    # suppress errors if the file is not found\n",
    "    onnx_model_path.unlink(missing_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6b988cc-0ff0-4f6d-8812-77b9bc2737ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[ NETWORK_NOT_READ ] Unable to read the model: model/yolop-320-320.onnx Please check that model format: onnx is supported and the model is correct. Available frontends: ir onnx paddle tf ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/yolop-320-320.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m model_onnx \u001b[38;5;241m=\u001b[39m \u001b[43mie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=0)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [ NETWORK_NOT_READ ] Unable to read the model: model/yolop-320-320.onnx Please check that model format: onnx is supported and the model is correct. Available frontends: ir onnx paddle tf "
     ]
    }
   ],
   "source": [
    "onnx_model_path = \"model/yolop-320-320.onnx\"\n",
    "model_onnx = ie.read_model(model=onnx_model_path)\n",
    "# compiled_model_onnx = ie.compile_model(model=model_onnx, device_name=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "da5dfab7-9e08-4a6d-b377-3a9be8cfc7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Model file model\\yolop-320-320.onxx cannot be opened!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconvert_to_ir_and_remove\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel/yolop-320-320.onxx\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 8\u001b[0m, in \u001b[0;36mconvert_to_ir_and_remove\u001b[1;34m(model_path, compile_device)\u001b[0m\n\u001b[0;32m      4\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m Path(model_path)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(onnx_model_path\u001b[38;5;241m.\u001b[39mstem \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.xml\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m----> 8\u001b[0m     model_onnx_yolop \u001b[38;5;241m=\u001b[39m \u001b[43mie\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43monnx_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Compilation has been done on the default CPU device. This may be quick for loading models, however, GPUs perform the best when it comes to actual inference.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     compiled_model_onnx_yolop \u001b[38;5;241m=\u001b[39m ie\u001b[38;5;241m.\u001b[39mcompile_model(model\u001b[38;5;241m=\u001b[39mmodel_onnx_yolop, device_name\u001b[38;5;241m=\u001b[39mcompile_device)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Model file model\\yolop-320-320.onxx cannot be opened!"
     ]
    }
   ],
   "source": [
    "convert_to_ir_and_remove('model/yolop-320-320.onxx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe3f182-478a-4e8d-abdf-d7dd9527685b",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "This is a critical step. It is known that as per the [guidelines](https://github.com/pronoym99/openvino_notebooks/blob/main/CONTRIBUTING.md#file-structure), ```data``` may be loaded into a directory of the same name adjacent to the [README](README.md) file and must not be large in size. Keeping in mind that the creation of this directory is an optional step, we will directly use the link to the [data](https://github.com/hustvl/YOLOP/tree/main/pictures) to get our job done and avoid creation of unnecessary directories when the images and videos are openly accessible across the internet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
