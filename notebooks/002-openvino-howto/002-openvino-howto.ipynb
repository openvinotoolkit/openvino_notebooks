{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "927f9ad4-4b12-4540-b39b-4ec4b8e13e20",
   "metadata": {},
   "source": [
    "# OpenVINO API guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c8001-9c5b-4f3a-bc26-0645bf534311",
   "metadata": {},
   "source": [
    "## Preparation: Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab92fa3f-05f5-41b7-aa64-2edc114a1977",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install openvino-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d09fe88-8f56-45a3-8536-ef55aa2a4de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "segmentation_model_xml = \"segmentation.xml\"\n",
    "classification_model_xml = \"classification.xml\"\n",
    "onnx_model = \"segmentation.onnx\"\n",
    "\n",
    "image_filename = \"coco_hollywood.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c87522e-4bde-4b13-b3f5-06a652625e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_attributes(instance):\n",
    "    attributes = [item for item in dir(instance) if not item.startswith(\"__\")]\n",
    "    pprint(attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af99c6e-10e8-47b0-8334-55e533a8b2cf",
   "metadata": {},
   "source": [
    "## Load Inference Engine and show info\n",
    "\n",
    "Initialize Inference Engine with IECore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25906d74-a720-4956-abdf-023275843eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e3bee-6807-469d-896e-74eabffe57f4",
   "metadata": {},
   "source": [
    "Inference Engine can load a network on a device. A device in this context means a CPU, an Intel GPU, a Neural Compute Stick 2, etc. The `available_devices` property shows the devices that are available on your system. `ie.get_metric` can show the name of the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718161f-f3c2-4dd5-bce1-ce4a92a3ed0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = ie.available_devices\n",
    "for device in devices:\n",
    "    device_name = ie.get_metric(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f280fa-0ad5-46a3-b5b4-aeae32eec5b4",
   "metadata": {},
   "source": [
    "## Loading a network\n",
    "\n",
    "After initializing Inference Engine, first read the model file with `read_network`, then load it to the specified device with `load_network`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad8ebe-88c0-43b2-b61f-41623bcf9a65",
   "metadata": {},
   "source": [
    "### IR model\n",
    "\n",
    "An IR model consists of an .xml file, containing model information, and a .bin file, containing the weights. `read_network` expects the weights file to be located in the same directory as the xml file, with the same filename, and the extension .bin: `model_weights_file == Path(model_xml).with_suffix(\".bin\")`. If this is the case, specifying the weights file is optional. If the weights file has a different filename, it can be specified as a parameter to `read_network`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84ff092-5ec1-40fc-b1ea-8d8f59617c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "net = ie.read_network(model=classification_model_xml)\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbfd6ae4-c138-4d55-aeea-7483af13ed9b",
   "metadata": {},
   "source": [
    "### ONNX model\n",
    "\n",
    "An ONNX model is a single file. Reading and loading an ONNX model works the same way as reading and loading an IR model. The `model` argument points to the ONNX filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58213aaf-3f6f-42cb-bd33-8b1169b29249",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "net_onnx = ie.read_network(model=onnx_model)\n",
    "exec_net_onnx = ie.load_network(network=net_onnx, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc882c5c-68a2-408d-90b5-4a2827a0d605",
   "metadata": {},
   "source": [
    "## Getting information about a model\n",
    "\n",
    "The OpenVINO IENetwork instance stores information about the model. Information about the inputs and outputs of the model are in `net.input_info` and `net.outputs`. These are also properties of the ExecutableNetwork instance. Where we use `net.input_info` and `net.outputs` in the cells below, you can also use `exec_net.input_info` and `exec_net.outputs`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2835206a-f934-46c2-aa66-14e7ca9e9260",
   "metadata": {},
   "source": [
    "### Model Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d38416-5292-42c3-9222-8a1e4cf3f278",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.input_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b55e7e-3c97-4c68-ab13-609a4cfbbff0",
   "metadata": {},
   "source": [
    "The cell above shows that the model loaded expects one input, with the name _input_. If you loaded a different model, you may see a different input layer name, and you may see more inputs.\n",
    "\n",
    "It is often useful to have a reference to the name of the first input layer. For a model with one input, `next(iter(net.input_info))` gets this name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7feb2d3-a714-4e34-a816-a5e347e8ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = next(iter(net.input_info))\n",
    "input_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510b9c4-39ef-4e15-8f5f-a9c3d0e5f7af",
   "metadata": {},
   "source": [
    "Information for this input layer is stored in `input_info`. The next cell prints the input layout, precision and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2265d18-b47b-4cf8-8c8d-f453b2c47db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"input layout: {net.input_info[input_layer].layout}\")\n",
    "print(f\"input precision: {net.input_info[input_layer].precision}\")\n",
    "print(f\"input shape: {net.input_info[input_layer].tensor_desc.dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccec019-6932-41e1-8f77-834508385dc2",
   "metadata": {},
   "source": [
    "This cell output tells us that the model expects inputs with a shape of [1,3,224,244], and that this is NCHW layout. This means that the model expects input data with a batch size (N) of 1, 3 channels (C) and images of a height (H) and width (W) of 224. The input data is expected to be of FP32 (floating point) precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a0ade-3a5f-4bef-8fff-c901c4dd227c",
   "metadata": {},
   "source": [
    "### Model Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1db434-8cff-4038-a9a8-c623299bdc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e711fd-0201-4304-8ea7-94a7a5e30027",
   "metadata": {},
   "source": [
    "Model output info is stored in `net.outputs`. The cell above shows that the model returns one output, with the name _MobilenetV3/Predictions/Softmax_. If you loaded a different model, you will probably see a different output layer name, and you may see more outputs.\n",
    "\n",
    "To get a reference to the name of the output layer, we follow the same method as for the input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb6bde4-e491-4662-85f3-bdfdde4660a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = next(iter(net.outputs))\n",
    "output_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678af62-f4e6-4211-ae62-7060b408b921",
   "metadata": {},
   "source": [
    "Getting the output layout, precision and shape is similar to getting the input layout, precision and shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71497db0-937a-444b-8a51-a662b23cd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"output layout: {net.outputs[output_layer].layout}\")\n",
    "print(f\"output precision: {net.outputs[output_layer].precision}\")\n",
    "print(f\"output shape: {net.outputs[output_layer].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0b7528-eb12-4389-b9b6-46ee97864e90",
   "metadata": {},
   "source": [
    "This cell output shows that the model returns outputs with a shape of [1, 1001], where 1 is the batch size (N) and 1001 the number of classes (C). The output is returned as 32 bit floating point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f064b86f-6081-4010-83b8-f883f4455c31",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "To do inference on a model, call the `infer()` method of the _ExecutableNetwork_, the `exec_net` that we loaded with `load_network`. `infer()` expects one argument: _inputs_. This is a dictionary, mapping input layer names to input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1560e6-03ab-428b-8fc8-464f5bd7f795",
   "metadata": {},
   "source": [
    "**Preparation: load network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae629a6-6cda-4643-8e2f-8a074466f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "net = ie.read_network(model=classification_model_xml)\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa253339-e14f-44d0-a35c-e32834640fa9",
   "metadata": {},
   "source": [
    "**Preparation: load image and convert to input shape**\n",
    "\n",
    "To propagate an image to the network, it needs to be loaded, resized to the shape that the network expects, and converted to the layout that the network expects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81aa510-c0f8-441c-883e-ec378743244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_filename)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e1c12-913d-469a-bdcd-974f5d7e7da7",
   "metadata": {},
   "source": [
    "The image has a shape of (664,994,3). It is 663 pixels in height, 994 pixels in width, and has 3 color channels. \n",
    "\n",
    "We get a reference to the height and width that the network expects, and resize to that size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7d8990-164c-4bf9-985e-a44fa60ab32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# N,C,H,W = batch size, number of channels, height, width\n",
    "N, C, H, W = net.input_info[input_layer].tensor_desc.dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682be1dc-dcae-49df-afed-d5fcadd8e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_image = cv2.resize(src=image, dsize=(W, H))  # OpenCV resize expects the destination size as (width, height)\n",
    "print(f\"resized image shape: {resized_image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9355a90-24ee-4792-ba8f-0b347f491bb2",
   "metadata": {},
   "source": [
    "Now the image has the width and height that the network expects. It is still in H,C,W format, we change it to N,C,H,W format (where N=1) by first calling `np.transpose` to change to C,H,W and then adding the N dimension by calling `np.expand_dims`. Convert the data to FP32 with `np.astype()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf888ff-5a3c-45d2-87ac-5ed856390b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0).astype(np.float32)\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e8fac6-905a-4ccc-b7b8-859a947e3d2a",
   "metadata": {},
   "source": [
    "### Do inference\n",
    "\n",
    "Now that the input data is in the right shape, doing inference is one simple command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c6548-4c62-489d-b255-3714332e0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = exec_net.infer({input_layer: input_data})\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051260a8-3fd2-42e4-8865-672e50c0d7f1",
   "metadata": {},
   "source": [
    "`.infer()` returns a dictionary, mapping output layers to data. Since we know this network returns one output, and we stored the reference to the output layer in the `output_layer` variable, we can get the data with `result[output_layer]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c919cd5b-0cc7-431e-b238-27d6fe5008dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = result[output_layer]\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9d2008-2a2b-4d30-837b-653a035a6315",
   "metadata": {},
   "source": [
    "The output shape is (1,1001), which we saw is indeed the expected shape of the output. This output shape indicates that the network returns probabilities for 1001 classes. To transform this in meaningful information, check out the [hello world notebook](../001-hello-world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62958073-f29f-4953-952a-4a07a1772c19",
   "metadata": {},
   "source": [
    "## Reshaping a network\n",
    "\n",
    "### Change image size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185266c4-f599-4ddb-a594-c6b19554c67b",
   "metadata": {},
   "source": [
    "Instead of reshaping the image to fit the model, you can also reshape the model to fit the image. Note that not input shapes will work for every model, and the model accuracy may also suffer if you reshape the model inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976be688-7277-4b20-b5cf-df815140fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_net = ie.read_network(model=segmentation_model_xml)\n",
    "segmentation_input_layer = next(iter(segmentation_net.input_info))\n",
    "segmentation_output_layer = next(iter(segmentation_net.outputs))\n",
    "\n",
    "print(f\"input shape: {segmentation_net.input_info[segmentation_input_layer].tensor_desc.dims}\")\n",
    "print(f\"input layout: {net.input_info[input_layer].layout}\")\n",
    "print(f\"output shape: {segmentation_net.outputs[segmentation_output_layer].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb66d427-6682-4a90-8f0b-d4ade2731843",
   "metadata": {},
   "source": [
    "The input shape for the network is [1,3,256,256], with NHCW layout: the network expects 3-channel images with width and height of 256 and a batch size of 1. We can reshape the network to make it accept input images with width and height of 512 with the `.reshape()` method of `IENetwork`. After reshaping, load the network to the device again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b542ccf-4375-4e22-9428-610219fc9551",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_shape = (1, 3, 512, 512)\n",
    "\n",
    "segmentation_net.reshape({segmentation_input_layer: new_shape})\n",
    "segmentation_exec_net = ie.load_network(network=segmentation_net, device_name=\"CPU\")\n",
    "print(f\"net input shape: {segmentation_net.input_info[segmentation_input_layer].tensor_desc.dims}\")\n",
    "print(f\"exec_net input shape: {segmentation_exec_net.input_info[segmentation_input_layer].tensor_desc.dims}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4bf0e8-aa26-4a7a-bdf3-43ad6c6547c3",
   "metadata": {},
   "source": [
    "### Change batch size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96c8686-961e-4a77-80fb-5c2b2d9d84b5",
   "metadata": {},
   "source": [
    "We can also use reshape to set the batch size, by increasing the first element of _new_shape_. For example, to set a batch size of two, set `new_shape = (2,3,512,512)` in the cell above. If you only want to change the batch size, you can also set the `batch_size` property directly. The batch size will be treated as the maximum batch size for the network: setting a batch size of 2 means that you can provide input data with a batch size of 1 or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06068dd8-de48-4509-9aab-b56dd7f53ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_net = ie.read_network(model=segmentation_model_xml)\n",
    "segmentation_net.batch_size = 2\n",
    "segmentation_exec_net = ie.load_network(network=segmentation_net, device_name=\"CPU\")\n",
    "print(f\"input shape: {segmentation_net.input_info[segmentation_input_layer].tensor_desc.dims}\")\n",
    "print(f\"input layout: {net.input_info[input_layer].layout}\")\n",
    "print(f\"output shape: {segmentation_net.outputs[segmentation_output_layer].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74461f1-cd6c-42d9-9e1f-bdfd64999ade",
   "metadata": {},
   "source": [
    "Notice that the output above shows that by setting the batch size, the output shape now shows a batch size of 2 indeed. Let's see what happens if we propagate our input image through the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e311f245-db0c-41d1-83c4-48e7b11c1e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ie.read_network(model=classification_model_xml)\n",
    "net.batch_size = 2\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "result_batch_1 = exec_net.infer({input_layer: input_data})\n",
    "print(f\"input data shape: {input_data.shape}\")\n",
    "print(\n",
    "    f\"Result of first and second output are the same: {np.allclose(result_batch_1[output_layer][0], result_batch_1[output_layer][1])}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39203162-8544-48b0-a182-00c053541d24",
   "metadata": {},
   "source": [
    "Create input data with a batch size of two by concatenating random data to the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5452fdb6-2b3c-433a-a3f2-21c5b7f579cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_data = np.empty_like(input_data)\n",
    "input_data_batch = np.concatenate((input_data, random_data), axis=0)\n",
    "net = ie.read_network(model=classification_model_xml)\n",
    "net.batch_size = 2\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "result_batch_2 = exec_net.infer({input_layer: input_data_batch})\n",
    "print(f\"input data shape: {input_data_batch.shape}\")\n",
    "print(\n",
    "    f\"Result of first and second output are the same: {np.allclose(result_batch_2[output_layer][0], result_batch_2[output_layer][1])}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
