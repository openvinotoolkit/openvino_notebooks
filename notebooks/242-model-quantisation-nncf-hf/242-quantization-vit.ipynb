{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6570fc5cc0394e3d9dd94e05fd304b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22ef062e78404ceebef0cbe11dbb75c9",
              "IPY_MODEL_fcbea40b1ec64d35bd57dad49f8c3aaf",
              "IPY_MODEL_a2918b8f5b244576964938f1a6ba5e5a"
            ],
            "layout": "IPY_MODEL_8f9c5f59832842c685c36f6020f3281f"
          }
        },
        "22ef062e78404ceebef0cbe11dbb75c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_697bfd1385724c75b9cefa348655029c",
            "placeholder": "​",
            "style": "IPY_MODEL_b58fac611c3a487893491473c485e2b8",
            "value": "/content/dataset/tiny-imagenet-200.zip: 100%"
          }
        },
        "fcbea40b1ec64d35bd57dad49f8c3aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bc6628206324a3bbb9d95e0cf1b4e55",
            "max": 248100043,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1eb2bfe07b824551b06df20a1c4abfd4",
            "value": 248100043
          }
        },
        "a2918b8f5b244576964938f1a6ba5e5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_285e2acee2fc4a4ca73d53acf3014966",
            "placeholder": "​",
            "style": "IPY_MODEL_06f38eab2b4c4898855d6a85a814413e",
            "value": " 237M/237M [00:13&lt;00:00, 12.5MB/s]"
          }
        },
        "8f9c5f59832842c685c36f6020f3281f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "697bfd1385724c75b9cefa348655029c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b58fac611c3a487893491473c485e2b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5bc6628206324a3bbb9d95e0cf1b4e55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1eb2bfe07b824551b06df20a1c4abfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "285e2acee2fc4a4ca73d53acf3014966": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06f38eab2b4c4898855d6a85a814413e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/GSOC/\n",
        "!pip install onnx\n",
        "!pip install openvino\n",
        "\n",
        "!pip install openvino-dev[all]\n",
        "!pip install nncf\n",
        "!pip install timm\n",
        "!mkdir /content/drive/MyDrive/GSOC/model"
      ],
      "metadata": {
        "id": "bNprfDZwtQ13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b929188-99d8-4f6a-965a-e0e20d248f9f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/GSOC\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting onnx\n",
            "  Downloading onnx-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.9/dist-packages (from onnx) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from onnx) (1.22.4)\n",
            "Requirement already satisfied: protobuf<4,>=3.20.2 in /usr/local/lib/python3.9/dist-packages (from onnx) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.13.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openvino\n",
            "  Downloading openvino-2022.3.0-9052-cp39-cp39-manylinux_2_17_x86_64.whl (36.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<=1.23.4,>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from openvino) (1.22.4)\n",
            "Installing collected packages: openvino\n",
            "Successfully installed openvino-2022.3.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openvino-dev[all]\n",
            "  Downloading openvino_dev-2022.3.0-9052-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: openvino-dev 2022.3.0 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: openvino==2022.3.0 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (2022.3.0)\n",
            "Requirement already satisfied: requests>=2.25.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (2.27.1)\n",
            "Requirement already satisfied: defusedxml>=0.7.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (0.7.1)\n",
            "Requirement already satisfied: opencv-python>=4.5 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (4.7.0.72)\n",
            "Collecting networkx<=2.8.8\n",
            "  Downloading networkx-2.8.8-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting openvino-telemetry>=2022.1.0\n",
            "  Downloading openvino_telemetry-2022.3.0-py3-none-any.whl (20 kB)\n",
            "Collecting jstyleson>=0.0.2\n",
            "  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (1.22.4)\n",
            "Collecting addict>=2.4.0\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting pandas~=1.3.5\n",
            "  Downloading pandas-1.3.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (6.0)\n",
            "Collecting texttable>=1.6.3\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (4.65.0)\n",
            "Requirement already satisfied: pillow>=8.1.2 in /usr/local/lib/python3.9/dist-packages (from openvino-dev[all]) (8.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.3.5->openvino-dev[all]) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas~=1.3.5->openvino-dev[all]) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[all]) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[all]) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[all]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.25.1->openvino-dev[all]) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas~=1.3.5->openvino-dev[all]) (1.16.0)\n",
            "Building wheels for collected packages: jstyleson\n",
            "  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2398 sha256=7338ec74203a07452234fadf889819c9d47c9347514b30ff5b527e8117739e9a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d4/2a/06/11202ea86be0f51f34e9411d691e25b991d188d93ab4d3e551\n",
            "Successfully built jstyleson\n",
            "Installing collected packages: texttable, jstyleson, addict, networkx, pandas, openvino-telemetry, openvino-dev\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.0\n",
            "    Uninstalling networkx-3.0:\n",
            "      Successfully uninstalled networkx-3.0\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.4.4\n",
            "    Uninstalling pandas-1.4.4:\n",
            "      Successfully uninstalled pandas-1.4.4\n",
            "Successfully installed addict-2.4.0 jstyleson-0.0.2 networkx-2.8.8 openvino-dev-2022.3.0 openvino-telemetry-2022.3.0 pandas-1.3.5 texttable-1.6.7\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nncf\n",
            "  Downloading nncf-2.4.0-py3-none-any.whl (904 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m904.5/904.5 KB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: texttable>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from nncf) (1.6.7)\n",
            "Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from nncf) (4.3.3)\n",
            "Requirement already satisfied: openvino-telemetry in /usr/local/lib/python3.9/dist-packages (from nncf) (2022.3.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.19.1 in /usr/local/lib/python3.9/dist-packages (from nncf) (1.22.4)\n",
            "Requirement already satisfied: jstyleson>=0.0.2 in /usr/local/lib/python3.9/dist-packages (from nncf) (0.0.2)\n",
            "Requirement already satisfied: pandas<=1.5.2,>=1.1.5 in /usr/local/lib/python3.9/dist-packages (from nncf) (1.3.5)\n",
            "Collecting networkx<=2.8.2,>=2.6\n",
            "  Downloading networkx-2.8.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymoo==0.5.0\n",
            "  Downloading pymoo-0.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja<1.11,>=1.10.0.post2\n",
            "  Downloading ninja-1.10.2.4-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (120 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m120.7/120.7 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydot>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from nncf) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from nncf) (1.2.2)\n",
            "Collecting scipy<=1.10.0,>=1.3.2\n",
            "  Downloading scipy-1.10.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyparsing<3.0\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 KB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.54.1 in /usr/local/lib/python3.9/dist-packages (from nncf) (4.65.0)\n",
            "Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.9/dist-packages (from nncf) (8.3.1)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.9/dist-packages (from pymoo==0.5.0->nncf) (3.7.1)\n",
            "Requirement already satisfied: autograd>=1.3 in /usr/local/lib/python3.9/dist-packages (from pymoo==0.5.0->nncf) (1.5)\n",
            "Collecting cma==2.7\n",
            "  Downloading cma-2.7.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.1/239.1 KB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->nncf) (22.2.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=3.2.0->nncf) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.9/dist-packages (from pandas<=1.5.2,>=1.1.5->nncf) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.9/dist-packages (from pandas<=1.5.2,>=1.1.5->nncf) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.24.0->nncf) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.24.0->nncf) (3.1.0)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.9/dist-packages (from openvino-telemetry->nncf) (2.27.1)\n",
            "Requirement already satisfied: future>=0.15.2 in /usr/local/lib/python3.9/dist-packages (from autograd>=1.3->pymoo==0.5.0->nncf) (0.18.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (23.0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (5.12.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (4.39.3)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->pymoo==0.5.0->nncf) (8.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7.3->pandas<=1.5.2,>=1.1.5->nncf) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->openvino-telemetry->nncf) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->openvino-telemetry->nncf) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->openvino-telemetry->nncf) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.20.0->openvino-telemetry->nncf) (3.4)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3->pymoo==0.5.0->nncf) (3.15.0)\n",
            "Installing collected packages: ninja, cma, scipy, pyparsing, networkx, pymoo, nncf\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: pyparsing\n",
            "    Found existing installation: pyparsing 3.0.9\n",
            "    Uninstalling pyparsing-3.0.9:\n",
            "      Successfully uninstalled pyparsing-3.0.9\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 2.8.8\n",
            "    Uninstalling networkx-2.8.8:\n",
            "      Successfully uninstalled networkx-2.8.8\n",
            "Successfully installed cma-2.7.0 networkx-2.8.2 ninja-1.10.2.4 nncf-2.4.0 pymoo-0.5.0 pyparsing-2.4.7 scipy-1.10.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pyparsing"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm) (1.13.1+cu116)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm) (0.14.1+cu116)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (23.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (3.10.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.13.3 timm-0.6.13\n",
            "mkdir: cannot create directory ‘/content/drive/MyDrive/GSOC/model’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORT LIBRARIES"
      ],
      "metadata": {
        "id": "c8gOK2N6eTSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "import zipfile\n",
        "import pickle\n",
        "from pathlib import Path\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from nncf import NNCFConfig  # Important - should be imported directly after torch\n",
        "from nncf.common.logging.logger import set_log_level\n",
        "\n",
        "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages\n",
        "from nncf.torch import create_compressed_model, register_default_init_args\n",
        "from openvino.runtime import Core\n",
        "from openvino.tools import mo\n",
        "from openvino.runtime import serialize\n",
        "from torch.jit import TracerWarning\n",
        "from copy import deepcopy\n",
        "import timm\n",
        "\n",
        "sys.path.append(\"/content/drive/MyDrive/GSOC/openvino_notebooks-main/notebooks/utils\")\n",
        "from notebook_utils import download_file"
      ],
      "metadata": {
        "id": "Hunnubl3O1m8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f57d21-01a7-4207-8b2b-3b76548a8c75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/openvino/offline_transformations/__init__.py:10: FutureWarning: The module is private and following namespace `offline_transformations` will be removed in the future, use `openvino.runtime.passes` instead!\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "79d6mZ76CsnG"
      },
      "outputs": [],
      "source": [
        "# swin_small_patch4_window7_224\n",
        "# vit_tiny_patch16_224\n",
        "# print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#configs for the notebook"
      ],
      "metadata": {
        "id": "wfR9jtUceYNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMAGE_SIZE = (64, 64)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "DATASET_DIR = Path(\"/content/dataset\")\n",
        "MODEL_OUTPUT_DIR=Path(\"/content/drive/MyDrive/GSOC/model\")\n",
        "BASE_MODEL_NAME='vit_tiny_patch16_224'\n",
        "\n",
        "\n",
        "fp32_checkpoint_filename = Path(MODEL_OUTPUT_DIR/( BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".pth\")\n",
        "\n",
        "fp32_onnx_path = Path(MODEL_OUTPUT_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".onnx\")\n",
        "fp32_ir_path = fp32_onnx_path.with_suffix(\".xml\")\n",
        "\n",
        "int8_onnx_path = Path(MODEL_OUTPUT_DIR / (BASE_MODEL_NAME + \"_int8\")).with_suffix(\".onnx\")\n",
        "int8_ir_path = int8_onnx_path.with_suffix(\".xml\")\n",
        "\n",
        "int4_onnx_path= Path(MODEL_OUTPUT_DIR / (BASE_MODEL_NAME + \"_int4\")).with_suffix(\".onnx\")\n",
        "int4_ir_path = int4_onnx_path.with_suffix(\".xml\")\n",
        "# !mkdir $str(MODEL_OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "YL3jIx7XwhHO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset and Dataloader"
      ],
      "metadata": {
        "id": "lio79FHnecmk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def download_tiny_imagenet_200(\n",
        "    output_dir: Path,\n",
        "    url: str = \"http://cs231n.stanford.edu/tiny-imagenet-200.zip\",\n",
        "    tarname: str = \"tiny-imagenet-200.zip\",):\n",
        "    archive_path = output_dir / tarname\n",
        "    download_file(url, directory=output_dir, filename=tarname)\n",
        "    zip_ref = zipfile.ZipFile(archive_path, \"r\")\n",
        "    zip_ref.extractall(path=output_dir)\n",
        "    zip_ref.close()\n",
        "    print(f\"Successfully downloaded and extracted dataset to: {output_dir}\")\n",
        "\n",
        "\n",
        "def create_validation_dir(dataset_dir: Path):\n",
        "    VALID_DIR = dataset_dir / \"val\"\n",
        "    val_img_dir = VALID_DIR / \"images\"\n",
        "\n",
        "    fp = open(VALID_DIR / \"val_annotations.txt\", \"r\")\n",
        "    data = fp.readlines()\n",
        "\n",
        "    val_img_dict = {}\n",
        "    for line in data:\n",
        "        words = line.split(\"\\t\")\n",
        "        val_img_dict[words[0]] = words[1]\n",
        "    fp.close()\n",
        "\n",
        "    for img, folder in val_img_dict.items():\n",
        "        newpath = val_img_dir / folder\n",
        "        if not newpath.exists():\n",
        "            os.makedirs(newpath)\n",
        "        if (val_img_dir / img).exists():\n",
        "            os.rename(val_img_dir / img, newpath / img)\n",
        "\n",
        "if not DATASET_DIR.exists():\n",
        "    download_tiny_imagenet_200(DATASET_DIR)\n",
        "    create_validation_dir(Path('/content/dataset/tiny-imagenet-200'))"
      ],
      "metadata": {
        "id": "HXuxFI0bJEjo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6570fc5cc0394e3d9dd94e05fd304b8b",
            "22ef062e78404ceebef0cbe11dbb75c9",
            "fcbea40b1ec64d35bd57dad49f8c3aaf",
            "a2918b8f5b244576964938f1a6ba5e5a",
            "8f9c5f59832842c685c36f6020f3281f",
            "697bfd1385724c75b9cefa348655029c",
            "b58fac611c3a487893491473c485e2b8",
            "5bc6628206324a3bbb9d95e0cf1b4e55",
            "1eb2bfe07b824551b06df20a1c4abfd4",
            "285e2acee2fc4a4ca73d53acf3014966",
            "06f38eab2b4c4898855d6a85a814413e"
          ]
        },
        "outputId": "e9fb56b8-b864-4084-9d71-161636ccc4f4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/content/dataset/tiny-imagenet-200.zip:   0%|          | 0.00/237M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6570fc5cc0394e3d9dd94e05fd304b8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded and extracted dataset to: /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloaders(batch_size: int = 128):\n",
        "    \"\"\"Creates train dataloader that is used for quantization initialization and validation dataloader for computing the model accruacy\"\"\"\n",
        "    train_dir = DATASET_DIR /'tiny-imagenet-200' /\"train\"\n",
        "    val_dir = DATASET_DIR /'tiny-imagenet-200'/ \"val\" /'images'\n",
        "    normalize = transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "    train_dataset = datasets.ImageFolder(\n",
        "        train_dir,\n",
        "        transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(IMAGE_SIZE),\n",
        "                transforms.ToTensor(),\n",
        "                normalize,\n",
        "            ]\n",
        "        ),\n",
        "    )\n",
        "    val_dataset = datasets.ImageFolder(\n",
        "        val_dir,\n",
        "        transforms.Compose(\n",
        "            [transforms.Resize(IMAGE_SIZE), transforms.ToTensor(), normalize]\n",
        "        ),\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "        sampler=None,\n",
        "    )\n",
        "\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=0,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "train_loader, val_loader = create_dataloaders()"
      ],
      "metadata": {
        "id": "no6ieI__ekm2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#UTILITY functions"
      ],
      "metadata": {
        "id": "m5z0Kqqteola"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, fmt: str = \":f\"):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val: float, n: int = 1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = \"{name} {val\" + self.fmt + \"} ({avg\" + self.fmt + \"})\"\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "class ProgressMeter(object):\n",
        "    \"\"\"Displays the progress of validation process\"\"\"\n",
        "\n",
        "    def __init__(self, num_batches: int, meters: List[AverageMeter], prefix: str = \"\"):\n",
        "        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n",
        "        self.meters = meters\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def display(self, batch: int):\n",
        "        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n",
        "        entries += [str(meter) for meter in self.meters]\n",
        "        print(\"\\t\".join(entries))\n",
        "\n",
        "    def _get_batch_fmtstr(self, num_batches: int):\n",
        "        num_digits = len(str(num_batches // 1))\n",
        "        fmt = \"{:\" + str(num_digits) + \"d}\"\n",
        "        return \"[\" + fmt + \"/\" + fmt.format(num_batches) + \"]\"\n",
        "\n",
        "def get_model_size(ir_path: str, m_type: str = 'Mb',\n",
        "                   verbose: bool = True) -> float:\n",
        "    xml_size = os.path.getsize(ir_path)\n",
        "    bin_size = os.path.getsize(os.path.splitext(ir_path)[0] + '.bin')\n",
        "    for t in ['bytes', 'Kb', 'Mb']:\n",
        "        if m_type == t:\n",
        "            break\n",
        "        xml_size /= 1024\n",
        "        bin_size /= 1024\n",
        "    model_size = xml_size + bin_size\n",
        "    if verbose:\n",
        "        print(f'Model graph (xml):   {xml_size:.3f} Mb')\n",
        "        print(f'Model weights (bin): {bin_size:.3f} Mb')\n",
        "        print(f'Model size:          {model_size:.3f} Mb')\n",
        "    return model_size\n",
        "\n",
        "def accuracy(output: torch.Tensor, target: torch.Tensor, topk: Tuple[int] = (1,)):\n",
        "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        _, pred = output.topk(maxk, 1, True, True)\n",
        "        pred = pred.t()\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size))\n",
        "        return res"
      ],
      "metadata": {
        "id": "eSPTF9YuzcVS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train and Test functions"
      ],
      "metadata": {
        "id": "Os1w8FuFetZT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
        "    losses = AverageMeter(\"Loss\", \":2.3f\")\n",
        "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
        "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
        "    progress = ProgressMeter(\n",
        "        len(train_loader), [batch_time, losses, top1, top5], prefix=\"Epoch:[{}]\".format(epoch)\n",
        "    )\n",
        "\n",
        "    # Switch to train mode.\n",
        "    model.train()\n",
        "\n",
        "    end = time.time()\n",
        "    for i, (images, target) in enumerate(train_loader):\n",
        "        images = images.to(device)\n",
        "        target = target.to(device)\n",
        "\n",
        "        # Compute output.\n",
        "        output = model(images)\n",
        "        loss = criterion(output, target)\n",
        "\n",
        "        # Measure accuracy and record loss.\n",
        "        acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "        losses.update(loss.item(), images.size(0))\n",
        "        top1.update(acc1[0], images.size(0))\n",
        "        top5.update(acc5[0], images.size(0))\n",
        "\n",
        "        # Compute gradient and do opt step.\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Measure elapsed time.\n",
        "        batch_time.update(time.time() - end)\n",
        "        end = time.time()\n",
        "\n",
        "        print_frequency = 50\n",
        "        if i % print_frequency == 0:\n",
        "            progress.display(i)\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "GNaSCWk26pMi"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(val_loader: torch.utils.data.DataLoader, model: torch.nn.Module):\n",
        "    \"\"\"Compute the metrics using data from val_loader for the model\"\"\"\n",
        "    batch_time = AverageMeter(\"Time\", \":3.3f\")\n",
        "    top1 = AverageMeter(\"Acc@1\", \":2.2f\")\n",
        "    top5 = AverageMeter(\"Acc@5\", \":2.2f\")\n",
        "    progress = ProgressMeter(len(val_loader), [batch_time, top1, top5], prefix=\"Test: \")\n",
        "\n",
        "    # Switch to evaluate mode.\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        end = time.time()\n",
        "        for i, (images, target) in enumerate(val_loader):\n",
        "            images = images.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            # Compute the output.\n",
        "            output = model(images)\n",
        "\n",
        "            # Measure accuracy and record loss.\n",
        "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
        "            top1.update(acc1[0], images.size(0))\n",
        "            top5.update(acc5[0], images.size(0))\n",
        "\n",
        "            # Measure elapsed time.\n",
        "            batch_time.update(time.time() - end)\n",
        "            end = time.time()\n",
        "\n",
        "            print_frequency = 10\n",
        "            if i % print_frequency == 0:\n",
        "                progress.display(i)\n",
        "\n",
        "        print(\n",
        "            \" * Acc@1 {top1.avg:.3f} Acc@5 {top5.avg:.3f}\".format(top1=top1, top5=top5)\n",
        "        )\n",
        "    return top1.avg"
      ],
      "metadata": {
        "id": "YnIZQf5B3elh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_the_model(model):\n",
        "    best_acc1 = 0\n",
        "    # Training loop.\n",
        "    init_lr=1e-4\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
        "    epochs=3\n",
        "    for epoch in range(0, epochs):\n",
        "        # Run a single training epoch.\n",
        "        model=train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "        # Evaluate on validation set.\n",
        "        acc1 = validate(val_loader, model)\n",
        "\n",
        "        is_best = acc1 > best_acc1\n",
        "        best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "        if is_best:\n",
        "            checkpoint = {\"state_dict\": model.state_dict(), \"acc1\": acc1}\n",
        "            # torch.save(checkpoint, fp32_pth_path)\n",
        "            torch.save( model.state_dict(), fp32_checkpoint_filename)\n",
        "    acc1_fp32 = best_acc1\n",
        "    print(f\"Accuracy of FP32 model: {acc1_fp32:.3f}\")"
      ],
      "metadata": {
        "id": "6gZxOxyAXI2U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model=timm.create_model(BASE_MODEL_NAME, pretrained=True, num_classes=10,  input_size=(3, 64, 256))\n",
        "\n",
        "# model(torch.rand((5,3,64,64)).to(device))"
      ],
      "metadata": {
        "id": "5mxeSTRY5u7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 200\n",
        "pretrained=True\n",
        "model=timm.create_model(BASE_MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES,  img_size=64)\n",
        "model.to(device)\n",
        "if not pretrained:\n",
        "    train_the_model(model)\n",
        "model.load_state_dict(torch.load(fp32_checkpoint_filename))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ft6hsfbceYH",
        "outputId": "8a74059a-dd3a-47a8-85ce-cb20822eb25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Convert to ONNX form"
      ],
      "metadata": {
        "id": "X2yQPtHEe3N0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def export_model_to_onnx(model, fp32_onnx_path):\n",
        "    dummy_input = torch.randn(1, 3, *IMAGE_SIZE).to(device)\n",
        "    torch.onnx.export(model, dummy_input, fp32_onnx_path, operator_export_type=torch.onnx.OperatorExportTypes.ONNX_ATEN_FALLBACK)\n",
        "    print(f\"FP32 ONNX model was exported to {fp32_onnx_path}.\")\n",
        "\n",
        "# if not fp32_onnx_path.exists():\n",
        "export_model_to_onnx(model, fp32_onnx_path)"
      ],
      "metadata": {
        "id": "cUX5WJPIG5Qg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd735b53-8615-4375-b144-a02f08e03e29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP32 ONNX model was exported to /content/drive/MyDrive/GSOC/model/vit_tiny_patch16_224_fp32.onnx.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nncf_config_dict_01 = {\n",
        "    \"input_info\": {\"sample_size\": [1, 3, *IMAGE_SIZE  ]},\n",
        "    \"log_dir\": str(MODEL_OUTPUT_DIR),  # The log directory for NNCF-specific logging outputs.\n",
        "    \"target_device\":\"TRIAL\",\n",
        "    \"compression\": {\n",
        "        \"algorithm\": \"quantization\",\n",
        "        \"overflow_fix\":\"enable\"}  # Specify the algorithm here.}\n",
        "}\n",
        "nncf_config01 = NNCFConfig.from_dict(nncf_config_dict_01)\n",
        "nncf_config01 = register_default_init_args(\n",
        "    nncf_config01, \n",
        "    deepcopy(train_loader), \n",
        "    criterion=nn.CrossEntropyLoss())\n"
      ],
      "metadata": {
        "id": "BPpnkQg-I2WQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compression_ctrl01, quantized_model = create_compressed_model(model, nncf_config01)\n",
        "compression_ctrl01.export_model(int8_onnx_path)\n"
      ],
      "metadata": {
        "id": "W1v8PwURJiqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_the_quantisation():\n",
        "    acc1 = validate(val_loader, model)\n",
        "    print(f\"Accuracy of initialized FP32 model: {acc1:.3f}\")\n",
        "\n",
        "    acc2 = validate(val_loader, quantized_model)\n",
        "    print(f\"Accuracy of initialized INT8 model: {acc2:.3f}\")\n",
        "\n",
        "test_the_quantisation()"
      ],
      "metadata": {
        "id": "XVCwkSUVJldn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e59bfd1a-bae8-4884-e598-511360636892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: [ 0/79]\tTime 0.132 (0.132)\tAcc@1 71.09 (71.09)\tAcc@5 87.50 (87.50)\n",
            "Test: [10/79]\tTime 0.136 (0.136)\tAcc@1 45.31 (56.46)\tAcc@5 80.47 (81.25)\n",
            "Test: [20/79]\tTime 0.155 (0.139)\tAcc@1 45.31 (53.61)\tAcc@5 73.44 (80.13)\n",
            "Test: [30/79]\tTime 0.152 (0.148)\tAcc@1 40.62 (51.81)\tAcc@5 64.84 (77.87)\n",
            "Test: [40/79]\tTime 0.173 (0.154)\tAcc@1 61.72 (50.67)\tAcc@5 82.81 (76.87)\n",
            "Test: [50/79]\tTime 0.193 (0.158)\tAcc@1 46.88 (50.00)\tAcc@5 77.34 (76.39)\n",
            "Test: [60/79]\tTime 0.175 (0.161)\tAcc@1 53.91 (49.18)\tAcc@5 81.25 (75.37)\n",
            "Test: [70/79]\tTime 0.167 (0.162)\tAcc@1 46.09 (48.61)\tAcc@5 75.00 (75.02)\n",
            " * Acc@1 49.730 Acc@5 75.970\n",
            "Accuracy of initialized FP32 model: 49.730\n",
            "Test: [ 0/79]\tTime 0.291 (0.291)\tAcc@1 61.72 (61.72)\tAcc@5 85.16 (85.16)\n",
            "Test: [10/79]\tTime 0.260 (0.287)\tAcc@1 47.66 (52.49)\tAcc@5 75.78 (77.34)\n",
            "Test: [20/79]\tTime 0.261 (0.285)\tAcc@1 42.19 (49.55)\tAcc@5 65.62 (76.23)\n",
            "Test: [30/79]\tTime 0.288 (0.287)\tAcc@1 41.41 (47.25)\tAcc@5 60.16 (74.47)\n",
            "Test: [40/79]\tTime 0.428 (0.310)\tAcc@1 57.81 (46.34)\tAcc@5 84.38 (73.40)\n",
            "Test: [50/79]\tTime 0.468 (0.336)\tAcc@1 44.53 (45.73)\tAcc@5 73.44 (72.78)\n",
            "Test: [60/79]\tTime 0.293 (0.343)\tAcc@1 50.00 (44.94)\tAcc@5 78.12 (71.86)\n",
            "Test: [70/79]\tTime 0.278 (0.335)\tAcc@1 39.84 (44.45)\tAcc@5 72.66 (71.39)\n",
            " * Acc@1 45.640 Acc@5 72.420\n",
            "Accuracy of initialized INT8 model: 45.640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_bin_and_xml_files(onnx_model_path, ir_path):\n",
        "    onnx_model = mo.convert_model(str(onnx_model_path))\n",
        "    serialize(model=onnx_model, xml_path=str(ir_path))\n",
        "    \n",
        "create_bin_and_xml_files(int8_onnx_path, int8_ir_path)\n",
        "create_bin_and_xml_files(fp32_onnx_path, fp32_ir_path)"
      ],
      "metadata": {
        "id": "NJFCe6orwXXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(get_model_size(int8_ir_path, verbose=True))\n",
        "print(get_model_size(fp32_ir_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgb6UEKjn7uc",
        "outputId": "1d47bd20-f329-49b4-fb84-0f0ae5b7e4e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model graph (xml):   0.673 Mb\n",
            "Model weights (bin): 5.371 Mb\n",
            "Model size:          6.044 Mb\n",
            "6.044435501098633\n",
            "Model graph (xml):   0.317 Mb\n",
            "Model weights (bin): 21.090 Mb\n",
            "Model size:          21.406 Mb\n",
            "21.406378746032715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_benchmark_output(benchmark_output: str):\n",
        "    \"\"\"Prints the output from benchmark_app in human-readable format\"\"\"\n",
        "    parsed_output = [line for line in benchmark_output if 'FPS' in line]\n",
        "    print(*parsed_output, sep='\\n')\n",
        "\n",
        "print('Benchmark FP32 model (OpenVINO IR)')\n",
        "benchmark_output = ! benchmark_app -m \"$fp32_ir_path\" -d CPU -api async -t 15\n",
        "parse_benchmark_output(benchmark_output)\n",
        "\n",
        "print('Benchmark INT8 model (OpenVINO IR)')\n",
        "benchmark_output = ! benchmark_app -m \"$int8_ir_path\" -d CPU -api async -t 15\n",
        "parse_benchmark_output(benchmark_output)"
      ],
      "metadata": {
        "id": "lpRbY8oIqQDA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2effc7e0-f4b6-4db1-9a5a-a3c3f227f5bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmark FP32 model (OpenVINO IR)\n",
            "[ INFO ] Throughput:   164.38 FPS\n",
            "Benchmark INT8 model (OpenVINO IR)\n",
            "[ INFO ] Throughput:   188.12 FPS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# https://openvinotoolkit.github.io/nncf/\n",
        "\n",
        "nncf_config_dict02={\n",
        "    \"input_info\": { \"sample_size\": [1, 3,*IMAGE_SIZE] },\n",
        "\n",
        "    \"compression\": {\n",
        "       \"algorithm\": \"quantization\",\n",
        "       \"weights\":{\"bits\":4},\n",
        "       \"activations\":{\"bits\":8},\n",
        "       \"overflow_fix\":\"enable\"\n",
        "       },\n",
        "    \"target_device\": \"TRIAL\"\n",
        "}\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "nncf_config02 = NNCFConfig.from_dict(nncf_config_dict02)\n",
        "\n",
        "\n",
        "nncf_config02 = register_default_init_args(nncf_config02, (train_loader),\n",
        "                                    criterion=criterion)\n",
        "compression_ctrl, four_bit_quantized_model = create_compressed_model(deepcopy(model), nncf_config02)\n",
        "compression_ctrl.export_model(int4_onnx_path)"
      ],
      "metadata": {
        "id": "dfPfjsJ37xZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc3 = validate(val_loader, four_bit_quantized_model)\n",
        "print(f\"Accuracy of initialized INT8 model: {acc3:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVhzi3-OCMRi",
        "outputId": "d0c8f18b-3a9d-4f18-dfc0-45168dafc837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test: [ 0/79]\tTime 0.522 (0.522)\tAcc@1 0.00 (0.00)\tAcc@5 0.78 (0.78)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Benchmark int4 model (OpenVINO IR)')\n",
        "create_bin_and_xml_files(int4_onnx_path, int4_ir_path)\n",
        "\n",
        "benchmark_output = ! benchmark_app -m \"$int4_ir_path\" -d CPU -api async -t 15\n",
        "parse_benchmark_output(benchmark_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMhtXFZMfLAl",
        "outputId": "f793089e-2edd-4705-9077-0fb897116c2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Benchmark int4 model (OpenVINO IR)\n",
            "[ INFO ] Throughput:   151.93 FPS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(get_model_size(int4_ir_path))\n",
        "print(get_model_size(int8_ir_path))\n",
        "print(get_model_size(fp32_ir_path))\n",
        "\n",
        "# int4_onnx_path= Path(MODEL_OUTPUT_DIR / (BASE_MODEL_NAME + \"_int8\")).with_suffix(\".onnx\")\n",
        "# int4_ir_path = int4_onnx_path.with_suffix(\".xml\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HiYY0hF4sKr",
        "outputId": "964cc1b7-975b-4ec7-fa3b-153d35835ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model graph (xml):   0.713 Mb\n",
            "Model weights (bin): 2.751 Mb\n",
            "Model size:          3.465 Mb\n",
            "3.464742660522461\n",
            "Model graph (xml):   0.674 Mb\n",
            "Model weights (bin): 5.371 Mb\n",
            "Model size:          6.045 Mb\n",
            "6.045186996459961\n",
            "Model graph (xml):   0.317 Mb\n",
            "Model weights (bin): 21.090 Mb\n",
            "Model size:          21.407 Mb\n",
            "21.406603813171387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_the_model(model):\n",
        "    lr=1e-5\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_acc1 = 0\n",
        "    criterion = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    epochs=3\n",
        "    for epoch in range(0, epochs):\n",
        "        # Run a single training epoch.\n",
        "        model=train(train_loader, model, criterion, optimizer, epoch)\n",
        "\n",
        "        # Evaluate on validation set.\n",
        "        acc1 = validate(val_loader, model)\n",
        "\n",
        "        is_best = acc1 > best_acc1\n",
        "        best_acc1 = max(acc1, best_acc1)\n",
        "\n",
        "        # if is_best:\n",
        "        #     checkpoint = {\"state_dict\": model.state_dict(), \"acc1\": acc1}\n",
        "        #     # torch.save(checkpoint, fp32_pth_path)\n",
        "        #     torch.save( model.state_dict(), fp32_checkpoint_filename)\n",
        "        print(best_acc1)\n",
        "    acc1_fp32 = best_acc1\n",
        "    return model\n",
        "# four_bit_quantized_model=fine_tune_the_model(four_bit_quantized_model)"
      ],
      "metadata": {
        "id": "gXr2U-m0N1FO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "four_bit_quantized_model.parameters"
      ],
      "metadata": {
        "id": "FNF0rqGTQXBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int4_ir_path"
      ],
      "metadata": {
        "id": "TPjwfVNwCX_i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4aaf12d3-8b3a-474e-f347-f11b426f775a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/GSOC/model/vit_tiny_patch16_224_int4.xml')"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "int8_ir_path"
      ],
      "metadata": {
        "id": "flrr7UYNNZSK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c9582d5-dfa3-4f50-d748-4f1f14d43ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/drive/MyDrive/GSOC/model/vit_tiny_patch16_224_int8.xml')"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EoOMUK_iNb5q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}