{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190e8e4c-461f-4521-ae7f-3491fa827ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automatic Device Selection\n",
    "\n",
    "The Auto Device (or AUTO in short) selects the most suitable device from the available compute devices by considering the network model inference precision, power efficiency and processing capability. The network model inference precision (if the network is quantized or not) is the first consideration to filter out the devices that cannot run the network efficiently.\n",
    "\n",
    "Next, the dedicated accelerator devices are preferred, e.g., discrete GPU, integrated GPU, or VPU. CPU is used as the default “fallback device”. Please note that AUTO does this selection only once at the network load time. \n",
    "\n",
    "When choosing the accelerator device like GPUs, loading the network to these devices may take long time. To address this challenge for application that requires fast initial inference response the AUTO starts inferencing immediately on the CPU and then transparently shifts inferencing to the GPU once ready, dramatically reducing time to first inference.\n",
    "\n",
    "More information about Automatic Device Selection: [click >>>](https://docs.openvino.ai/latest/openvino_docs_IE_DG_supported_plugins_AUTO.html)\n",
    "\n",
    "![Auto Device Selection logic](data/auto_device_selection.png \"Auto Device Selection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8054d-1f4a-4337-bd2e-9e00d9e33820",
   "metadata": {},
   "source": [
    "## Prepare the network model files\n",
    "The following demostrations use the [googlenet-v1](https://docs.openvino.ai/latest/omz_models_model_googlenet_v1.html) model from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). The googlenet-v1 model is the first of the Inception family of models designed to perform image classification. Like the other Inception models, the googlenet-v1 model has been pre-trained on the ImageNet image database. For details about this family of models, check out the paper.\n",
    "\n",
    "The following code downaloads googlenet-v1 network model files and convert them to IR files (model/public/googlenet-v1/FP16/googlenet-v1.xml). More details about network model tools, please refer to [104-model-tools](../104-model-tools/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e0582-a77f-4621-ba92-429a9c5f5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "model_name = \"googlenet-v1\"\n",
    "base_model_dir = Path(\"./model\").expanduser()\n",
    "precision = \"FP16\"\n",
    "\n",
    "download_command = (\n",
    "    f\"omz_downloader --name {model_name} --output_dir {base_model_dir}\"\n",
    ")\n",
    "display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "display(Markdown(f\"Downloading {model_name}...\"))\n",
    "\n",
    "# Depends on your network connection, proxy may be required.\n",
    "# Uncomment following 2 lines and add correct proxies if they are required.\n",
    "# %env https_proxy=http://proxy\n",
    "# %env http_proxy=http://proxy\n",
    "! $download_command\n",
    "\n",
    "convert_command = f\"omz_converter --name {model_name} --precisions {precision} --download_dir {base_model_dir}\"\n",
    "display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "display(Markdown(f\"Converting {model_name}...\"))\n",
    "\n",
    "! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc461c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core, CompiledModel, AsyncInferQueue, InferRequest\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bfdd8",
   "metadata": {},
   "source": [
    "## Load the model with AUTO device\n",
    "### Default behavior of Core::compile_model API without device_name\n",
    "By default compile_model API will select AUTO as device_name if it is not specificed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "\n",
    "# set LOG_LEVEL to LOG_INFO\n",
    "ie.set_property(\"AUTO\", {\"LOG_LEVEL\":\"LOG_INFO\"})\n",
    "\n",
    "# read the model into ngraph representation\n",
    "model = ie.read_model(model=\"model/public/googlenet-v1/FP16/googlenet-v1.xml\")\n",
    "\n",
    "# load the model to target device\n",
    "compiled_model = ie.compile_model(model=model)\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model without device_name successfully.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfa571-8241-4623-9e9e-01bf2d0f89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "del compiled_model # Delete model will wait for selected device compiling network done.\n",
    "print(\"Deleted compiled_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b48949",
   "metadata": {},
   "source": [
    "### Explicitly pass AUTO as device_name to Core::compile_model API\n",
    "It is up to you, it may improve the code readiblity to explicitly pass AUTO as device_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\")\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model with AUTO successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e765f5-8e31-4eb4-bed8-9fba30bb29e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "del compiled_model # Delete model will wait for selected device compiling network done.\n",
    "print(\"Deleted compiled_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ce668",
   "metadata": {},
   "source": [
    "## First inference latency benefit with AUTO\n",
    "One of the key performance benefits of AUTO is on first inference latency (FIL = compile model time + fist inference execution time). Directly using CPU device would produce the shortest first inference latency as the OpenVINO graph representations can really quickly be JIT-compiled to CPU. The challenge is with the GPU. Since the OpenCL complication of graph to GPU-optimized kernels takes a few seconds to complete for this platform. If AUTO selects GPU as the device, this initialization time may be intolerable to some applications, which is the reason for AUTO to transparently use the CPU as the first inference device until GPU is ready. \n",
    "### Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For demo purpose, load model to CPU and get the input information for input buffer preparation.\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = next(iter(compiled_model.inputs))\n",
    "\n",
    "# Expect image in BGR format\n",
    "image = cv2.imread(\"../001-hello-world/data/coco.jpg\")\n",
    "\n",
    "# N, C, H, W = batch size, number of channels, height, width\n",
    "N, C, H, W = input_layer_ir.shape\n",
    "\n",
    "# Resize image to meet network expected input sizes\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to network input shape\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcaba9",
   "metadata": {},
   "source": [
    "### Load network model to GPU Device and do one inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to compile model, time point 1\n",
    "gpu_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"GPU\")  # load to GPU\n",
    "\n",
    "# get input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# get the first inference result\n",
    "results = compiled_model([input_image])[output_layer]\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "gpu_fil_end_time = time.perf_counter()\n",
    "gpu_fil_span = gpu_fil_end_time - gpu_load_start_time\n",
    "print(f\"Loaded model to GPU and get first inference in {gpu_fil_end_time-gpu_load_start_time:.2f} seconds.\")\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfac5d",
   "metadata": {},
   "source": [
    "### Load network model to AUTO Device and do one inference\n",
    "Since GPU is the selected device and CPU is taken as acceleration device, the 1st inference and some following inferences are executed on CPU until GPU is ready (model compiline done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to compile model, time point 1\n",
    "auto_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model)  # device_name is AUTO by default\n",
    "\n",
    "# get input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# get the first inference result\n",
    "results = compiled_model([input_image])[output_layer]\n",
    "\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "auto_fil_end_time = time.perf_counter()\n",
    "auto_fil_span = auto_fil_end_time - auto_load_start_time\n",
    "print(f\"Loaded model to AUTO and get first inference in {auto_fil_end_time-auto_load_start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06673210-5af9-4209-acc3-7b2fed1eb165",
   "metadata": {},
   "outputs": [],
   "source": [
    "del compiled_model # Delete model will wait for selected device compiling network done."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2de05c-6b75-4bc2-814a-08aa4924c2a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Performance hint\n",
    "The next highlight is the differentiation of performance hint with AUTO. By specifying LATENCY hint or THROUGHTPUT hint, AUTO demonstrate significant performance results towards the desired metric. THROUGHTPUT hint delivers much higher frame per second (FPS) performance than LATENCY hint. In contrast, the LATENCY hint delivers much lower latency than THROUGHTPUT hint. Notice that the hints do not require low-level device-specific settings, and are also completely portable between the devices, which allows the AUTO just to expedite the hint value directly to the selected device.\n",
    "\n",
    "More information about AUTO with performance hint, please go to [AUTO#performance-hints](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html#performance-hints)\n",
    "\n",
    "### Class and callback definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bc1059-5a3c-4d11-a340-09c224a9d5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceMetrics:\n",
    "    \"\"\"\n",
    "    Record the latest performance metrics (fps and latency), update the metrics in each @interval seconds\n",
    "    :member: fps: Frame per second, indicates the average accomplished inference number in each second during last @interval seconds.\n",
    "    :member: latency: Average latency of accomplished inferences in last @interval seconds.\n",
    "    :member: start_time: Record the start timestamp of onging @interval seconds duration.\n",
    "    :member: latency_list: Record the latecny of each accomplished inferences of onging @interval seconds duration.\n",
    "    :member: interval: The metrics will be updated in each @interval seconds\n",
    "    \"\"\"\n",
    "    def __init__ (self, interval):\n",
    "        \"\"\"\n",
    "        Create and initilize one instance of class PerformanceMetrics.\n",
    "        :param: interval: The metrics will be updated in each @interval seconds\n",
    "        :returns:\n",
    "            Instance of PerformanceMetrics\n",
    "        \"\"\"\n",
    "        self.fps = 0\n",
    "        self.latency = 0\n",
    "        \n",
    "        self.start_time = time.perf_counter()\n",
    "        self.latency_list = []\n",
    "        self.interval = interval\n",
    "        \n",
    "    def update(self, infer_request: InferRequest) -> bool:\n",
    "        \"\"\"\n",
    "        Update the metrics if current ongoing @interval seconds duration is expired. Record the latency only if it is not expired.\n",
    "        :param: infer_request: InferRequest returned from inference callback, which includes the result of inference request.\n",
    "        :returns:\n",
    "            True, if metrics are updated.\n",
    "            False, if @interval seconds duration is not expired and metrics are not updated.\n",
    "        \"\"\"\n",
    "        self.latency_list.append(infer_request.latency)\n",
    "        exec_time = time.perf_counter() - self.start_time\n",
    "        if exec_time >= self.interval:\n",
    "            # update the performance metrics\n",
    "            self.start_time = time.perf_counter()\n",
    "            self.fps = len(self.latency_list) / exec_time\n",
    "            self.latency = sum(self.latency_list) / len(self.latency_list)\n",
    "            print(f\"fps: {self.fps: .2f}, latency: {self.latency: .2f}, time taken:{exec_time: .2f}s\")\n",
    "            sys.stdout.flush()\n",
    "            self.latency_list = []\n",
    "            return True\n",
    "        else :\n",
    "            return False\n",
    "        \n",
    "class InferContext:\n",
    "    \"\"\"\n",
    "    Inference context. Record and update peforamnce metrics via @metrics, set @feed_inference to False once @remaining_update_num <=0\n",
    "    :member: metrics: instance of class PerformanceMetrics \n",
    "    :member: remaining_update_num: the remaining times for peforamnce metrics updating.\n",
    "    :member: feed_inference: if feed inference request is required or not.\n",
    "    \"\"\"\n",
    "    def __init__ (self, update_interval, num):\n",
    "        \"\"\"\n",
    "        Create and initilize one instance of class InferContext.\n",
    "        :param: update_interval: The performance metrics will be updated in each @update_interval seconds. This parameter will be passed to class PerformanceMetrics dirctly.\n",
    "        :param: num: The number of times for performance metrics updating.\n",
    "        :returns:\n",
    "            Instance of InferContext.\n",
    "        \"\"\"\n",
    "        self.metrics = PerformanceMetrics(update_interval)\n",
    "        self.remaining_update_num = num\n",
    "        self.feed_inference = True\n",
    "        \n",
    "    def update (self, infer_request: InferRequest):\n",
    "        \"\"\"\n",
    "        Update the conext. Set @feed_inference to False if remining performance metrcis updating times (@remaining_update_num) reaches 0\n",
    "        :param: infer_request: InferRequest returned from inference callback, which includes the result of inference request.\n",
    "        :returns: None\n",
    "        \"\"\"\n",
    "        if self.remaining_update_num <= 0 :\n",
    "            self.feed_inference = False\n",
    "            \n",
    "        if self.metrics.update(infer_request) :\n",
    "            self.remaining_update_num = self.remaining_update_num - 1\n",
    "            if self.remaining_update_num <= 0 :\n",
    "                self.feed_inference = False\n",
    "                \n",
    "def completion_callback(infer_request: InferRequest, context) -> None:\n",
    "    \"\"\"\n",
    "    callback for the inference request, pass the @infer_request to @context for updating\n",
    "    :param: infer_request: InferRequest returned for the callback, which includes the result of inference request.\n",
    "    :param: context: user data which is passed as 2nd parameter to AsyncInferQueue:start_async()\n",
    "    :returns: None\n",
    "    \"\"\"\n",
    "    context.update(infer_request)\n",
    "            \n",
    "# Performance metrics update interval (seconds) and times\n",
    "metrics_update_inerval = 10\n",
    "metrics_update_num = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbaebca-d7a3-43bd-a3d3-aeab1e177171",
   "metadata": {},
   "source": [
    "### Inference with THROUGHTPUT hint\n",
    "\n",
    "Loop for the inference and update the FPS/Latency for each @metrics_update_inerval sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "THROUGHPUT_hint_context = InferContext(metrics_update_inerval, metrics_update_num)\n",
    "\n",
    "print(\"Compiling Model for AUTO Device with THROUGHPUT hint\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "compiled_model = ie.compile_model(model=model, config={\"PERFORMANCE_HINT\":\"THROUGHPUT\"})\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0)  # set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "print(f\"Start inference, {metrics_update_num: .0f} groups fps/latency will be out with {metrics_update_inerval: .0f}s interval\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "while THROUGHPUT_hint_context.feed_inference:\n",
    "    infer_queue.start_async({input_layer_ir.any_name: input_image}, THROUGHPUT_hint_context)\n",
    "    \n",
    "infer_queue.wait_all()\n",
    "\n",
    "# Take the fps and latency of latest period\n",
    "THROUGHPUT_hint_fps = THROUGHPUT_hint_context.metrics.fps\n",
    "THROUGHPUT_hint_latency = THROUGHPUT_hint_context.metrics.latency\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d446b-0c7c-4f2d-baac-21d5ea92089d",
   "metadata": {},
   "source": [
    "### Inference with LATENCY hint\n",
    "\n",
    "Loop for the inference and update the FPS/Latency for each @metrics_update_inerval sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0396c-d7cb-4176-844c-1d5d94ae788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "LATENCY_hint_context = InferContext(metrics_update_inerval, metrics_update_num)\n",
    "\n",
    "print(\"Compiling Model for AUTO Device with LATENCY hint\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "compiled_model = ie.compile_model(model=model, config={\"PERFORMANCE_HINT\":\"LATENCY\"})\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0)  #set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "print(f\"Start inference, {metrics_update_num: .0f} groups fps/latency will be out with {metrics_update_inerval: .0f}s interval\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "while LATENCY_hint_context.feed_inference:\n",
    "    infer_queue.start_async({input_layer_ir.any_name: input_image}, LATENCY_hint_context)\n",
    "    \n",
    "infer_queue.wait_all()\n",
    "\n",
    "# Take the fps and latency of latest period\n",
    "LATENCY_hint_fps = LATENCY_hint_context.metrics.fps\n",
    "LATENCY_hint_latency = LATENCY_hint_context.metrics.latency\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1907e4-e71b-471f-891a-66161c03bc85",
   "metadata": {},
   "source": [
    "### FPS and latency difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5261e-581e-4b91-a8fb-9715981d450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the difference\n",
    "TPUT = 0\n",
    "LAT = 1\n",
    "labels = [\"THROUGHPUT hint\", \"LATENCY hint\"]\n",
    "\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "rects1 = ax[0].bar([0], THROUGHPUT_hint_fps, width, label=labels[TPUT], color='#557f2d')\n",
    "rects2 = ax[0].bar([width], LATENCY_hint_fps, width, label=labels[LAT])\n",
    "ax[0].set_ylabel(\"frame per second\")\n",
    "ax[0].set_xticks([width / 2]) \n",
    "ax[0].set_xticklabels([\"fps\"])\n",
    "ax[0].set_xlabel(\"Higher is better\")\n",
    "\n",
    "rects1 = ax[1].bar([0], THROUGHPUT_hint_latency, width, label=labels[TPUT], color='#557f2d')\n",
    "rects2 = ax[1].bar([width], LATENCY_hint_latency, width, label=labels[LAT])\n",
    "ax[1].set_ylabel(\"millisecond\")\n",
    "ax[1].set_xticks([width / 2])\n",
    "ax[1].set_xticklabels([\"latency (ms)\"])\n",
    "ax[1].set_xlabel(\"Lower is better\")\n",
    "\n",
    "fig.suptitle('Performance Hints')\n",
    "fig.legend(labels)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae617ccb002f72b3ab6d0069d721eac67ac2a969e83c083c4321cfcab0437cd1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
