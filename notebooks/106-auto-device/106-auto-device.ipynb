{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190e8e4c-461f-4521-ae7f-3491fa827ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automatic Device Selection\n",
    "\n",
    "The Auto Device (or AUTO in short) selects the most suitable device from the available compute devices by considering the network precision, power efficiency and processing capability. The network precision (if the network is quantized or not) is the first consideration to filter out the devices that cannot run the network efficiently.\n",
    "\n",
    "Next, the dedicated accelerator devices are preferred, e.g., discrete GPU, integrated GPU, or VPU. CPU is used as the default “fallback device”. Please note that AUTO does this selection only once at the network load time. \n",
    "\n",
    "When choosing the accelerator device like GPUs, loading the network to these devices may take long time. To address this challenge for application that requires fast initial inference response the AUTO starts inferencing immediately on the CPU and then transparently shifts inferencing to the GPU once ready, dramatically reducing time to first inference.\n",
    "\n",
    "![Auto Device Selection logic](data/auto_device_selection.png \"Auto Device Selection\")\n",
    "\n",
    "The following demostrations use the [resnet-50-tf](https://docs.openvino.ai/latest/omz_models_model_resnet_50_tf.html) model from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). resnet-50-tf is a TensorFlow* implementation of ResNet-50 - an image classification model pre-trained on the ImageNet dataset. Originally redistributed in Saved model format, converted to frozen graph using tf.graph_util module. For details see [paper](https://arxiv.org/abs/1512.03385), [repository](https://github.com/tensorflow/models/tree/v2.2.0/official/r1/resnet).\n",
    "\n",
    "Please follow the [104-model-tools](../104-model-tools/README.md) to download resnet-50-tf and convert (--precisions FP16), copy the results (resnet-50-tf.bin and resnet-50-tf.xml) to notebooks\\106-auto-device\\model\\ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bfdd8",
   "metadata": {},
   "source": [
    "## compile_model with AUTO device\n",
    "### Default behavior of compile_model without device_name\n",
    "By default compile_model will select AUTO as device_name if it is not specificed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core, CompiledModel\n",
    "\n",
    "ie = Core()\n",
    "\n",
    "model = ie.read_model(model=\"model/resnet-50-tf.xml\")\n",
    "compiled_model = ie.compile_model(model=model, config={\"LOG_LEVEL\":\"LOG_INFO\"})\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model without device_name successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b48949",
   "metadata": {},
   "source": [
    "### compile_model with AUTO as device name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\")\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model with AUTO successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ce668",
   "metadata": {},
   "source": [
    "## First inference latency benifit with AUTO\n",
    "One of the key performance benefits of AUTO is on first inference latency (FIL = compile model time + fist inference execution time). Directly using CPU device would produce the shortest first inference latency as the OpenVINO graph representations can really quickly be JIT-compiled to CPU. The challenge is with the GPU. Since the OpenCL complication of graph to GPU-optimized kernels takes a few seconds to complete for this platform. If AUTO selects GPU as the device, this initialization time may be intolerable to some applications, which is the reason for AUTO to transparently use the CPU as the first inference device until GPU is ready. \n",
    "### Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_ir = next(iter(compiled_model.inputs))\n",
    "\n",
    "# Text detection models expects image in BGR format\n",
    "image = cv2.imread(\"data/intel_rnb.jpg\")\n",
    "\n",
    "# N,H,W,C = batch size, height, width, number of channels\n",
    "N, H, W, C = input_layer_ir.shape\n",
    "\n",
    "# Resize image to meet network expected input sizes\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to network input shape\n",
    "#input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "input_image = np.expand_dims(resized_image, 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcaba9",
   "metadata": {},
   "source": [
    "### compile_model with GPU Device and do first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# Start to compile model, time point 1\n",
    "\n",
    "# Compile model\n",
    "gpu_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"GPU\") # load to GPU\n",
    "\n",
    "input_layer_ir = next(iter(compiled_model.inputs))\n",
    "\n",
    "# Create inference request\n",
    "request = compiled_model.create_infer_request()\n",
    "request.infer({input_layer_ir.any_name: input_image})\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "gpu_fil_end_time = time.perf_counter()\n",
    "gpu_fil_span = gpu_fil_end_time - gpu_load_start_time\n",
    "print(f\"Loaded model to GPU and get first inference in {gpu_fil_end_time-gpu_load_start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfac5d",
   "metadata": {},
   "source": [
    "### compile_model with AUTO Device and do first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to compile model, time point 1\n",
    "\n",
    "# Compile model\n",
    "auto_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model) # device_name is AUTO by default\n",
    "input_layer_ir = next(iter(compiled_model.inputs))\n",
    "\n",
    "# Create inference request\n",
    "request = compiled_model.create_infer_request()\n",
    "request.infer({input_layer_ir.any_name: input_image})\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "auto_fil_end_time = time.perf_counter()\n",
    "auto_fil_span = auto_fil_end_time - auto_load_start_time\n",
    "print(f\"Loaded model to AUTO and get first inference in {auto_fil_end_time-auto_load_start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a238bbe-07f0-4623-addf-e2ef58216b26",
   "metadata": {},
   "source": [
    "### First inference latency benefit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6811f16-29a4-450a-81eb-fe4eb7909f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the latency difference\n",
    "device_list = [\"GPU\", \"AUTO\"]\n",
    "load_and_fil_list = [gpu_fil_span, auto_fil_span]\n",
    "plt.barh(range(len(load_and_fil_list)), load_and_fil_list, tick_label=device_list)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2de05c-6b75-4bc2-814a-08aa4924c2a7",
   "metadata": {},
   "source": [
    "## Performance hint\n",
    "The next highlight is the differentiation of performance hint with AUTO. By specifying LATENCY hint or THROUGHTPUT hint, AUTO demonstrate significant performance results towards the desired metric. THROUGHTPUT hint delivers much higher frame per second (FPS) performance than LATENCY hint. In contrast, the LATENCY hint delivers much lower latency than THROUGHTPUT hint. Notice that the hints do not require low-level device-specific settings, and are also completely portable between the devices, which allows the AUTO just to expedite the hint value directly to the selected device.\n",
    "\n",
    "### compile_model with THROUGHTPUT hint\n",
    "\n",
    "Loop for the inference and output the FPS/Latency for each n sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import AsyncInferQueue, InferRequest\n",
    "import sys\n",
    "\n",
    "# output period (seconds)\n",
    "period_seconds = 10\n",
    "end_after_seconds = 60\n",
    "\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\":\"THROUGHPUT\", \"LOG_LEVEL\":\"LOG_TRACE\"})\n",
    "latency_list = []\n",
    "overall_latency_list = []\n",
    "\n",
    "def completion_callback(infer_request: InferRequest, job_id) -> None:\n",
    "    latency_list.append(infer_request.latency)\n",
    "    overall_latency_list.append(infer_request.latency)\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0) #set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "exec_time = time.perf_counter()-start_time\n",
    "job_id = 0\n",
    "latest_fps_tput = 0\n",
    "print(\"Start\")\n",
    "sys.stdout.flush()\n",
    "while exec_time < end_after_seconds:\n",
    "    period_start_time = time.perf_counter()\n",
    "    period_exec_time = time.perf_counter()-period_start_time\n",
    "    while period_exec_time < period_seconds:\n",
    "        infer_queue.start_async({input_layer_ir.any_name: input_image}, job_id)\n",
    "        period_exec_time = time.perf_counter()-period_start_time\n",
    "        job_id+=1\n",
    "    infer_queue.wait_all()\n",
    "    duration = time.perf_counter()-period_start_time\n",
    "    latest_fps_tput = len(latency_list)/duration\n",
    "    print(\"fps:%.2f\"%latest_fps_tput)\n",
    "    latest_latency_tput = sum(latency_list)/len(latency_list)\n",
    "    print(\"latency:%.2f\"%latest_latency_tput)\n",
    "    exec_time = time.perf_counter()-start_time\n",
    "    sys.stdout.flush()\n",
    "    latency_list = []\n",
    "\n",
    "print(\"Done\")\n",
    "sys.stdout.flush()\n",
    "#print(overall_latency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d446b-0c7c-4f2d-baac-21d5ea92089d",
   "metadata": {},
   "source": [
    "### compile_model with LATENCY hint\n",
    "\n",
    "Loop for the inference and output the FPS/Latency for each n sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0396c-d7cb-4176-844c-1d5d94ae788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output period (seconds), share the same as previous one\n",
    "overall_latency_list = []\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\":\"LATENCY\"})\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0) #set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "start_time = time.perf_counter()\n",
    "exec_time = time.perf_counter()-start_time\n",
    "job_id = 0\n",
    "latest_fps_latency = 0\n",
    "print(\"Start\")\n",
    "sys.stdout.flush()\n",
    "while exec_time < end_after_seconds:\n",
    "    period_start_time = time.perf_counter()\n",
    "    period_exec_time = time.perf_counter()-period_start_time\n",
    "    while period_exec_time < period_seconds:\n",
    "        infer_queue.start_async({input_layer_ir.any_name: input_image}, job_id)\n",
    "        period_exec_time = time.perf_counter()-period_start_time\n",
    "        job_id+=1\n",
    "    infer_queue.wait_all()\n",
    "    duration = time.perf_counter()-period_start_time\n",
    "    latest_fps_latency = len(latency_list)/duration\n",
    "    print(\"fps:%.2f\"%latest_fps_latency)\n",
    "    latest_latency_latency = sum(latency_list)/len(latency_list)\n",
    "    print(\"latency:%.2f\"%latest_latency_latency)\n",
    "    exec_time = time.perf_counter()-start_time\n",
    "    sys.stdout.flush()\n",
    "    latency_list = []\n",
    "print(\"Done\")\n",
    "sys.stdout.flush()\n",
    "#print(overall_latency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1907e4-e71b-471f-891a-66161c03bc85",
   "metadata": {},
   "source": [
    "### FPS and Latency difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5261e-581e-4b91-a8fb-9715981d450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the difference\n",
    "labels = [\"fps\", \"latency\"]\n",
    "THROUGHPUT = [latest_fps_tput, latest_latency_tput]\n",
    "LATENCY = [latest_fps_latency, latest_latency_latency]\n",
    "print(THROUGHPUT)\n",
    "print(LATENCY)\n",
    "x = np.arange(len(labels))\n",
    "width = 0.25\n",
    "fig, ax = plt.subplots()\n",
    "r1 = np.arange(len(labels))\n",
    "r2 = [x + width for x in r1]\n",
    "#r3 = [x + width for x in r2]\n",
    "#r4 = [x + width for x in r3]\n",
    "rects1 = ax.bar(r1, THROUGHPUT, width, label='THROUGHPUT', color='#557f2d')\n",
    "rects2 = ax.bar(r2, LATENCY, width, label='LATENCY')\n",
    "#rects3 = ax.bar(r3, min_latency, width, label='min_latency')\n",
    "#rects4 = ax.bar(r4, max_latency, width, label='max_latency')\n",
    "ax.set_title('performance hints (THROUGHPUT/LATENCY) fps/latency')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "plt.xticks([r + width for r in range(len(labels))], ['fps', 'lantency'])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae617ccb002f72b3ab6d0069d721eac67ac2a969e83c083c4321cfcab0437cd1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
