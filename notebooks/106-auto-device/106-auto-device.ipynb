{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190e8e4c-461f-4521-ae7f-3491fa827ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automatic Device Selection\n",
    "\n",
    "The Auto Device (or AUTO in short) selects the most suitable device from the available compute devices by considering the network precision, power efficiency and processing capability. The network precision (if the network is quantized or not) is the first consideration to filter out the devices that cannot run the network efficiently.\n",
    "\n",
    "Next, the dedicated accelerator devices are preferred, e.g., discrete GPU, integrated GPU, or VPU. CPU is used as the default “fallback device”. Please note that AUTO does this selection only once at the network load time. \n",
    "\n",
    "When choosing the accelerator device like GPUs, loading the network to these devices may take long time. To address this challenge for application that requires fast initial inference response the AUTO starts inferencing immediately on the CPU and then transparently shifts inferencing to the GPU once ready, dramatically reducing time to first inference.\n",
    "\n",
    "![Auto Device Selection logic](data/auto_device_selection.png \"Auto Device Selection\")\n",
    "\n",
    "The following demostrations use the [googlenet-v1](https://docs.openvino.ai/latest/omz_models_model_googlenet_v1.html) model from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). The googlenet-v1 model is the first of the Inception family of models designed to perform image classification. Like the other Inception models, the googlenet-v1 model has been pre-trained on the ImageNet image database. For details about this family of models, check out the paper.\n",
    "\n",
    "Please follow the [104-model-tools](../104-model-tools/README.md) to download googlenet-v1 and convert (--precisions FP16), copy the results (googlenet-v1.bin and googlenet-v1.xml) to notebooks\\106-auto-device\\model\\ folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc461c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core, CompiledModel, AsyncInferQueue, InferRequest\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bfdd8",
   "metadata": {},
   "source": [
    "## Load the model with AUTO device\n",
    "### Default behavior of compile_model without device_name\n",
    "By default compile_model will select AUTO as device_name if it is not specificed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "# read the model into ngraph representation\n",
    "model = ie.read_model(model=\"model/googlenet-v1.xml\")\n",
    "# load the model to target device\n",
    "compiled_model = ie.compile_model(model=model, config={\"LOG_LEVEL\":\"LOG_INFO\"})\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model without device_name successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b48949",
   "metadata": {},
   "source": [
    "### compile_model with AUTO as device name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\")\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model with AUTO successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ce668",
   "metadata": {},
   "source": [
    "## First inference latency benifit with AUTO\n",
    "One of the key performance benefits of AUTO is on first inference latency (FIL = compile model time + fist inference execution time). Directly using CPU device would produce the shortest first inference latency as the OpenVINO graph representations can really quickly be JIT-compiled to CPU. The challenge is with the GPU. Since the OpenCL complication of graph to GPU-optimized kernels takes a few seconds to complete for this platform. If AUTO selects GPU as the device, this initialization time may be intolerable to some applications, which is the reason for AUTO to transparently use the CPU as the first inference device until GPU is ready. \n",
    "### Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_ir = next(iter(compiled_model.inputs))\n",
    "\n",
    "# Text detection models expects image in BGR format\n",
    "image = cv2.imread(\"data/intel_rnb.jpg\")\n",
    "\n",
    "# N, C, H, W = batch size, number of channels, height, width\n",
    "N, C, H, W = input_layer_ir.shape\n",
    "\n",
    "# Resize image to meet network expected input sizes\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to network input shape\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcaba9",
   "metadata": {},
   "source": [
    "### load to GPU Device and do first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start to compile model, time point 1\n",
    "gpu_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"GPU\") # load to GPU\n",
    "\n",
    "# get input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# get the first inference result\n",
    "results = compiled_model([input_image])[output_layer]\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "gpu_fil_end_time = time.perf_counter()\n",
    "gpu_fil_span = gpu_fil_end_time - gpu_load_start_time\n",
    "print(f\"Loaded model to GPU and get first inference in {gpu_fil_end_time-gpu_load_start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfac5d",
   "metadata": {},
   "source": [
    "### compile_model with AUTO Device and do first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to compile model, time point 1\n",
    "auto_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model) # device_name is AUTO by default\n",
    "\n",
    "# get input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# get the first inference result\n",
    "results = compiled_model([input_image])[output_layer]\n",
    "\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "auto_fil_end_time = time.perf_counter()\n",
    "auto_fil_span = auto_fil_end_time - auto_load_start_time\n",
    "print(f\"Loaded model to AUTO and get first inference in {auto_fil_end_time-auto_load_start_time:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a238bbe-07f0-4623-addf-e2ef58216b26",
   "metadata": {},
   "source": [
    "### First inference latency benefit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6811f16-29a4-450a-81eb-fe4eb7909f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the latency difference\n",
    "device_list = [\"GPU\", \"AUTO\"]\n",
    "load_and_fil_list = [gpu_fil_span, auto_fil_span]\n",
    "plt.barh(range(len(load_and_fil_list)), load_and_fil_list, tick_label=device_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2de05c-6b75-4bc2-814a-08aa4924c2a7",
   "metadata": {},
   "source": [
    "## Performance hint\n",
    "The next highlight is the differentiation of performance hint with AUTO. By specifying LATENCY hint or THROUGHTPUT hint, AUTO demonstrate significant performance results towards the desired metric. THROUGHTPUT hint delivers much higher frame per second (FPS) performance than LATENCY hint. In contrast, the LATENCY hint delivers much lower latency than THROUGHTPUT hint. Notice that the hints do not require low-level device-specific settings, and are also completely portable between the devices, which allows the AUTO just to expedite the hint value directly to the selected device.\n",
    "\n",
    "### compile_model with THROUGHTPUT hint\n",
    "\n",
    "Loop for the inference and output the FPS/Latency for each n sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output period (seconds)\n",
    "period_seconds = 10\n",
    "end_after_periods = 6  # Total time @period_seconds x @end_after_periods\n",
    "\n",
    "print(\"Compiling Model for AUTO Device with THROUGHPUT hint\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "compiled_model = ie.compile_model(model=model, config={\"PERFORMANCE_HINT\":\"THROUGHPUT\", \"LOG_LEVEL\":\"LOG_INFO\"})\n",
    "\n",
    "def completion_callback(infer_request: InferRequest, job_id) -> None:\n",
    "    global period_fps\n",
    "    global period_latency\n",
    "    global period_start_time\n",
    "    global period_count\n",
    "    global latency_list\n",
    "    global overall_latency_list\n",
    "    global feed_inference\n",
    "    \n",
    "    latency_list.append(infer_request.latency)\n",
    "    overall_latency_list.append(infer_request.latency)\n",
    "    period_exec_time = time.perf_counter()-period_start_time\n",
    "    if period_exec_time >= period_seconds:\n",
    "        period_start_time = time.perf_counter()\n",
    "        period_fps = len(latency_list)/period_exec_time\n",
    "        period_latency = sum(latency_list)/len(latency_list)\n",
    "        print(\"fps:%.2f, latency:%.2f, period time: %.2fs\"%(period_fps, period_latency, period_exec_time))\n",
    "        sys.stdout.flush()\n",
    "        latency_list = []\n",
    "        period_count = period_count + 1\n",
    "        if period_count >= end_after_periods: # Stop feed inference request\n",
    "            feed_inference = False\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0) #set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "print(\"Start inference, %.0f groups fps/latency will be out with %.0fs interval\"%(end_after_periods, period_seconds))\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Initilization for inference with THROUGHPUT hint\n",
    "period_fps = 0\n",
    "period_latency = 0\n",
    "period_start_time = time.perf_counter()\n",
    "period_count = 0\n",
    "latency_list = []\n",
    "overall_latency_list = []\n",
    "feed_inference = True\n",
    "\n",
    "job_id = 0\n",
    "while True == feed_inference:\n",
    "    infer_queue.start_async({input_layer_ir.any_name: input_image}, job_id)\n",
    "    period_exec_time = time.perf_counter()-period_start_time\n",
    "    job_id+=1\n",
    "    \n",
    "infer_queue.wait_all()\n",
    "\n",
    "# Take the fps and latency of latest period\n",
    "THROUGHPUT_fps = period_fps\n",
    "THROUGHPUT_latency = period_latency\n",
    "\n",
    "print(\"Done\")\n",
    "sys.stdout.flush()\n",
    "#print(overall_latency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d446b-0c7c-4f2d-baac-21d5ea92089d",
   "metadata": {},
   "source": [
    "### compile_model with LATENCY hint\n",
    "\n",
    "Loop for the inference and output the FPS/Latency for each n sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0396c-d7cb-4176-844c-1d5d94ae788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling Model for AUTO Device with LATENCY hint\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\":\"LATENCY\"})\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0) #set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "print(\"Start inference, %.0f groups fps/latency will be out with %.0fs interval\"%(end_after_periods, period_seconds))\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Initilization for inference with LATENCY hint\n",
    "period_fps = 0\n",
    "period_latency = 0\n",
    "period_start_time = time.perf_counter()\n",
    "period_count = 0\n",
    "latency_list = []\n",
    "overall_latency_list = []\n",
    "feed_inference = True\n",
    "\n",
    "job_id = 0\n",
    "while True == feed_inference:\n",
    "    infer_queue.start_async({input_layer_ir.any_name: input_image}, job_id)\n",
    "    period_exec_time = time.perf_counter()-period_start_time\n",
    "    job_id+=1\n",
    "    \n",
    "infer_queue.wait_all()\n",
    "\n",
    "# Take the fps and latency of latest period\n",
    "LATENCY_fps = period_fps\n",
    "LATENCY_latency = period_latency\n",
    "\n",
    "print(\"Done\")\n",
    "sys.stdout.flush()\n",
    "#print(overall_latency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1907e4-e71b-471f-891a-66161c03bc85",
   "metadata": {},
   "source": [
    "### FPS and Latency difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5261e-581e-4b91-a8fb-9715981d450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the difference\n",
    "labels = [\"fps\", \"latency\"]\n",
    "THROUGHPUT = [THROUGHPUT_fps, THROUGHPUT_latency]\n",
    "LATENCY = [LATENCY_fps, LATENCY_latency]\n",
    "\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "rects1 = ax[0].bar([0], THROUGHPUT_fps, width, label='THROUGHPUT', color='#557f2d')\n",
    "rects2 = ax[0].bar([width], LATENCY_fps, width, label='LATENCY')\n",
    "ax[0].set_ylabel(\"frame per second\")\n",
    "ax[0].set_xticks([width/2]) \n",
    "ax[0].set_xticklabels([\"fps\"])\n",
    "\n",
    "rects1 = ax[1].bar([0], THROUGHPUT_latency, width, label='THROUGHPUT', color='#557f2d')\n",
    "rects2 = ax[1].bar([width], LATENCY_latency, width, label='LATENCY')\n",
    "ax[1].set_ylabel(\"millisecond\")\n",
    "ax[1].set_xticks([width/2]) \n",
    "ax[1].set_xticklabels([\"latency (ms)\"])\n",
    "\n",
    "fig.suptitle('Performance Hints')\n",
    "ax[1].legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae617ccb002f72b3ab6d0069d721eac67ac2a969e83c083c4321cfcab0437cd1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
