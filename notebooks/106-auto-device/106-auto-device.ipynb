{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "190e8e4c-461f-4521-ae7f-3491fa827ab7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Automatic Device Selection\n",
    "\n",
    "The Auto Device (or AUTO in short) selects the most suitable device from the available compute devices by considering the network precision, power efficiency and processing capability. The network precision (if the network is quantized or not) is the first consideration to filter out the devices that cannot run the network efficiently.\n",
    "\n",
    "Next, the dedicated accelerator devices are preferred, e.g., discrete GPU, integrated GPU, or VPU. CPU is used as the default “fallback device”. Please note that AUTO does this selection only once at the network load time. \n",
    "\n",
    "When choosing the accelerator device like GPUs, loading the network to these devices may take long time. To address this challenge for application that requires fast initial inference response the AUTO starts inferencing immediately on the CPU and then transparently shifts inferencing to the GPU once ready, dramatically reducing time to first inference.\n",
    "\n",
    "![Auto Device Selection logic](data/auto_device_selection.png \"Auto Device Selection\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8054d-1f4a-4337-bd2e-9e00d9e33820",
   "metadata": {},
   "source": [
    "## Prepare the network model files\n",
    "The following demostrations use the [googlenet-v1](https://docs.openvino.ai/latest/omz_models_model_googlenet_v1.html) model from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). The googlenet-v1 model is the first of the Inception family of models designed to perform image classification. Like the other Inception models, the googlenet-v1 model has been pre-trained on the ImageNet image database. For details about this family of models, check out the paper.\n",
    "\n",
    "The following code downaloads googlenet-v1 network model files and convert them to IR files (model/public/googlenet-v1/FP16/googlenet-v1.xml). More details about network model tools, please refer to [104-model-tools](../104-model-tools/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58e0582-a77f-4621-ba92-429a9c5f5e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "model_name = \"googlenet-v1\"\n",
    "base_model_dir = Path(\"./model\").expanduser()\n",
    "precision = \"FP16\"\n",
    "\n",
    "download_command = (\n",
    "    f\"omz_downloader --name {model_name} --output_dir {base_model_dir}\"\n",
    ")\n",
    "display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "display(Markdown(f\"Downloading {model_name}...\"))\n",
    "\n",
    "# Depends on your network connection, proxy may be required.\n",
    "# Uncomment following 2 lines and add correct proxies if they are required.\n",
    "# %env https_proxy=http://proxy\n",
    "# %env http_proxy=http://proxy\n",
    "\n",
    "! $download_command\n",
    "\n",
    "convert_command = f\"omz_converter --name {model_name} --precisions {precision} --download_dir {base_model_dir}\"\n",
    "display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "display(Markdown(f\"Converting {model_name}...\"))\n",
    "\n",
    "! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfc461c",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c128a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.runtime import Core, CompiledModel, AsyncInferQueue, InferRequest\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "740bfdd8",
   "metadata": {},
   "source": [
    "## Load the model with AUTO device\n",
    "### Default behavior of compile_model without device_name\n",
    "By default compile_model will select AUTO as device_name if it is not specificed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "# read the model into ngraph representation\n",
    "model = ie.read_model(model=\"model/public/googlenet-v1/FP16/googlenet-v1.xml\")\n",
    "# load the model to target device\n",
    "compiled_model = ie.compile_model(model=model, config={\"LOG_LEVEL\":\"LOG_INFO\"})\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model without device_name successfully.\")\n",
    "    \n",
    "del compiled_model # Delete model will wait for selected device compiling network done.\n",
    "print(\"Deleted compiled_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b48949",
   "metadata": {},
   "source": [
    "### Explicitly load network model to AUTO device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0814073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\")\n",
    "\n",
    "if isinstance(compiled_model, CompiledModel):\n",
    "    print(\"Compile model with AUTO successfully.\")\n",
    "        \n",
    "del compiled_model # Delete model will wait for selected device compiling network done.\n",
    "print(\"Deleted compiled_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ce668",
   "metadata": {},
   "source": [
    "## First inference latency benifit with AUTO\n",
    "One of the key performance benefits of AUTO is on first inference latency (FIL = compile model time + fist inference execution time). Directly using CPU device would produce the shortest first inference latency as the OpenVINO graph representations can really quickly be JIT-compiled to CPU. The challenge is with the GPU. Since the OpenCL complication of graph to GPU-optimized kernels takes a few seconds to complete for this platform. If AUTO selects GPU as the device, this initialization time may be intolerable to some applications, which is the reason for AUTO to transparently use the CPU as the first inference device until GPU is ready. \n",
    "### Load an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1cfeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = ie.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer_ir = next(iter(compiled_model.inputs))\n",
    "\n",
    "# Expect image in BGR format\n",
    "image = cv2.imread(\"../001-hello-world/data/coco.jpg\")\n",
    "\n",
    "# N, C, H, W = batch size, number of channels, height, width\n",
    "N, C, H, W = input_layer_ir.shape\n",
    "\n",
    "# Resize image to meet network expected input sizes\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to network input shape\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));\n",
    "\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fcaba9",
   "metadata": {},
   "source": [
    "### Load network model to GPU Device and do first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start to compile model, time point 1\n",
    "gpu_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"GPU\")  # load to GPU\n",
    "\n",
    "# get input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# get the first inference result\n",
    "results = compiled_model([input_image])[output_layer]\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "gpu_fil_end_time = time.perf_counter()\n",
    "gpu_fil_span = gpu_fil_end_time - gpu_load_start_time\n",
    "print(f\"Loaded model to GPU and get first inference in {gpu_fil_end_time-gpu_load_start_time:.2f} seconds.\")\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dfac5d",
   "metadata": {},
   "source": [
    "### Load network model to AUTO Device and do first inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a52b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to compile model, time point 1\n",
    "auto_load_start_time = time.perf_counter()\n",
    "compiled_model = ie.compile_model(model=model)  # device_name is AUTO by default\n",
    "\n",
    "# get input and output nodes\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# get the first inference result\n",
    "results = compiled_model([input_image])[output_layer]\n",
    "\n",
    "\n",
    "# Get 1st inference, time point 2\n",
    "auto_fil_end_time = time.perf_counter()\n",
    "auto_fil_span = auto_fil_end_time - auto_load_start_time\n",
    "print(f\"Loaded model to AUTO and get first inference in {auto_fil_end_time-auto_load_start_time:.2f} seconds.\")\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a238bbe-07f0-4623-addf-e2ef58216b26",
   "metadata": {},
   "source": [
    "### First inference latency benefit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6811f16-29a4-450a-81eb-fe4eb7909f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the latency difference\n",
    "device_list = [\"GPU\", \"AUTO\"]\n",
    "load_and_fil_list = [gpu_fil_span, auto_fil_span]\n",
    "plt.barh(range(len(load_and_fil_list)), load_and_fil_list, tick_label=device_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2de05c-6b75-4bc2-814a-08aa4924c2a7",
   "metadata": {},
   "source": [
    "## Performance hint\n",
    "The next highlight is the differentiation of performance hint with AUTO. By specifying LATENCY hint or THROUGHTPUT hint, AUTO demonstrate significant performance results towards the desired metric. THROUGHTPUT hint delivers much higher frame per second (FPS) performance than LATENCY hint. In contrast, the LATENCY hint delivers much lower latency than THROUGHTPUT hint. Notice that the hints do not require low-level device-specific settings, and are also completely portable between the devices, which allows the AUTO just to expedite the hint value directly to the selected device.\n",
    "\n",
    "### Inference with THROUGHTPUT hint\n",
    "\n",
    "Loop for the inference and output the FPS/Latency for each @period_seconds sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b8e571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output period (seconds)\n",
    "period_seconds = 10\n",
    "end_after_periods = 6  # Total time @period_seconds x @end_after_periods\n",
    "\n",
    "class InferContext:\n",
    "    def __init__ (self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.period_fps = 0\n",
    "        self.period_latency = 0\n",
    "        self.period_start_time = time.perf_counter()\n",
    "        self.period_count = 0\n",
    "        self.latency_list = []\n",
    "        self.overall_latency_list = []\n",
    "        self.feed_inference = True\n",
    "\n",
    "class InferJob:\n",
    "    def __init__ (self, id, context):\n",
    "        self.id = id\n",
    "        self.context = context\n",
    "\n",
    "context = InferContext()\n",
    "\n",
    "print(\"Compiling Model for AUTO Device with THROUGHPUT hint\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "compiled_model = ie.compile_model(model=model, config={\"PERFORMANCE_HINT\":\"THROUGHPUT\", \"LOG_LEVEL\":\"LOG_INFO\"})\n",
    "\n",
    "\n",
    "def completion_callback(infer_request: InferRequest, job) -> None:\n",
    "    context = job.context\n",
    "    \n",
    "    context.latency_list.append(infer_request.latency)\n",
    "    context.overall_latency_list.append(infer_request.latency)\n",
    "    period_exec_time = time.perf_counter() - context.period_start_time\n",
    "    if period_exec_time >= period_seconds:\n",
    "        context.period_start_time = time.perf_counter()\n",
    "        context.period_fps = len(context.latency_list) / period_exec_time\n",
    "        context.period_latency = sum(context.latency_list) / len(context.latency_list)\n",
    "        print(f\"fps: {context.period_fps: .2f}, latency: {context.period_latency: .2f}, period time:{period_exec_time: .2f}s\")\n",
    "        sys.stdout.flush()\n",
    "        context.latency_list = []\n",
    "        context.period_count = context.period_count + 1\n",
    "        if context.period_count >= end_after_periods:  # Stop feed inference request\n",
    "            context.feed_inference = False\n",
    "\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0)  # set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "print(f\"Start inference, {end_after_periods: .0f} groups fps/latency will be out with {period_seconds: .0f}s interval\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Initilization for inference with THROUGHPUT hint\n",
    "context.reset()\n",
    "\n",
    "job_id = 0\n",
    "while context.feed_inference:\n",
    "    infer_queue.start_async({input_layer_ir.any_name: input_image}, InferJob(job_id, context))\n",
    "    period_exec_time = time.perf_counter() - context.period_start_time\n",
    "    job_id += 1\n",
    "    \n",
    "infer_queue.wait_all()\n",
    "\n",
    "# Take the fps and latency of latest period\n",
    "THROUGHPUT_fps = context.period_fps\n",
    "THROUGHPUT_latency = context.period_latency\n",
    "\n",
    "print(\"Done\")\n",
    "sys.stdout.flush()\n",
    "# print(overall_latency_list)\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d446b-0c7c-4f2d-baac-21d5ea92089d",
   "metadata": {},
   "source": [
    "### Inference with LATENCY hint\n",
    "\n",
    "Loop for the inference and output the FPS/Latency for each @period_seconds sencods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e0396c-d7cb-4176-844c-1d5d94ae788f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Compiling Model for AUTO Device with LATENCY hint\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "compiled_model = ie.compile_model(model=model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\":\"LATENCY\"})\n",
    "\n",
    "infer_queue = AsyncInferQueue(compiled_model, 0)  #set 0 will query optimal num by default\n",
    "infer_queue.set_callback(completion_callback)\n",
    "\n",
    "print(f\"Start inference, {end_after_periods:.0f} groups fps/latency will be out with {period_seconds:.0f}s interval\")\n",
    "sys.stdout.flush()\n",
    "\n",
    "# Initilization for inference with LATENCY hint\n",
    "context.reset()\n",
    "\n",
    "job_id = 0\n",
    "while context.feed_inference:\n",
    "    infer_queue.start_async({input_layer_ir.any_name: input_image}, InferJob(job_id, context))\n",
    "    period_exec_time = time.perf_counter() - context.period_start_time\n",
    "    job_id += 1\n",
    "    \n",
    "infer_queue.wait_all()\n",
    "\n",
    "# Take the fps and latency of latest period\n",
    "LATENCY_fps = context.period_fps\n",
    "LATENCY_latency = context.period_latency\n",
    "\n",
    "print(\"Done\")\n",
    "sys.stdout.flush()\n",
    "# print(overall_latency_list)\n",
    "del compiled_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1907e4-e71b-471f-891a-66161c03bc85",
   "metadata": {},
   "source": [
    "### FPS and latency difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f5261e-581e-4b91-a8fb-9715981d450e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output the difference\n",
    "labels = [\"fps\", \"latency\"]\n",
    "THROUGHPUT = [THROUGHPUT_fps, THROUGHPUT_latency]\n",
    "LATENCY = [LATENCY_fps, LATENCY_latency]\n",
    "\n",
    "width = 0.4\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "rects1 = ax[0].bar([0], THROUGHPUT_fps, width, label='THROUGHPUT', color='#557f2d')\n",
    "rects2 = ax[0].bar([width], LATENCY_fps, width, label='LATENCY')\n",
    "ax[0].set_ylabel(\"frame per second\")\n",
    "ax[0].set_xticks([width / 2]) \n",
    "ax[0].set_xticklabels([\"fps\"])\n",
    "\n",
    "rects1 = ax[1].bar([0], THROUGHPUT_latency, width, label='THROUGHPUT', color='#557f2d')\n",
    "rects2 = ax[1].bar([width], LATENCY_latency, width, label='LATENCY')\n",
    "ax[1].set_ylabel(\"millisecond\")\n",
    "ax[1].set_xticks([width / 2]) \n",
    "ax[1].set_xticklabels([\"latency (ms)\"])\n",
    "\n",
    "fig.suptitle('Performance Hints')\n",
    "ax[1].legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ae617ccb002f72b3ab6d0069d721eac67ac2a969e83c083c4321cfcab0437cd1"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
