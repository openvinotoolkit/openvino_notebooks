{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize a Segmentation Model and Show Live Inference\n",
    "\n",
    "## Kidney Segmentation with PyTorch Lightning and OpenVINO™ - Part 3\n",
    "\n",
    "This tutorial is a part of the series on how to train, optimize, quantize and show live inference on a medical segmentation model. The goal is to accelerate inference on a kidney segmentation model. The [UNet](https://arxiv.org/abs/1505.04597) model is trained from scratch; the data is from [KiTS19](https://github.com/neheller/kits19).\n",
    "\n",
    "This third tutorial in the series shows how to:\n",
    "\n",
    "- convert an ONNX model to OpenVINO IR with [Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html),\n",
    "- quantize a model with [Post-Training Optimization Tool](https://docs.openvino.ai/latest/pot_compression_api_README.html) API in OpenVINO, \n",
    "- evaluate the F1 score metric of the original model and the quantized model,\n",
    "- benchmark performance of the original model and the quantized model,\n",
    "- show live inference with async API and MULTI plugin in OpenVINO.\n",
    "\n",
    "All notebooks in this series:\n",
    "\n",
    "- [Data Preparation for 2D Segmentation of 3D Medical Data](data-preparation-ct-scan.ipynb)\n",
    "- [Train a 2D-UNet Medical Imaging Model with PyTorch Lightning](pytorch-monai-training.ipynb)\n",
    "- Convert and Quantize a Segmentation Model and Show Live Inference (this notebook)\n",
    "- [Live Inference and Benchmark CT-scan data](../210-ct-scan-live-inference/210-ct-scan-live-inference.ipynb) \n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook needs a trained UNet model that is converted to the [ONNX](https://github.com/onnx/onnx) format. Provide a pre-trained model, trained for 20 epochs with the full [KiTS19](https://github.com/neheller/kits19) frames dataset, which has an F1 score on the validation set of 0.9. The training code is available in [this notebook](pytorch-monai-training.ipynb). \n",
    "\n",
    "> **Note**: Running this notebook with the full dataset will take a long time. For demonstration purposes, this tutorial will download one converted CT scan and use that scan for quantization and inference. For production use, choose a larger dataset that will provide more generalizable results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "The Post Training Optimization API is implemented in the `compression` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from addict import Dict\n",
    "from compression.api import DataLoader, Metric\n",
    "from compression.engines.ie_engine import IEEngine\n",
    "from compression.graph import load_model, save_model\n",
    "from compression.graph.model_utils import compress_model_weights\n",
    "from compression.pipeline.initializer import create_pipeline\n",
    "from monai.transforms import LoadImage\n",
    "from openvino.inference_engine import IECore\n",
    "from yaspin import yaspin\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from models.custom_segmentation import SegmentationModel\n",
    "from notebook_utils import benchmark_model, download_file, show_live_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "To use the pre-trained models, set `ONNX_PATH` to `\"pretrained_model/unet_kits19.onnx\"`. To use a model that you trained or optimized yourself, adjust `ONNX_PATH`. Running the next cell will check if a model trained in the [training notebook](pytorch-monai-training.ipynb) is saved in `MODEL_DIR` and if so, use that. If there is no trained model, the pre-trained model will be used. The optimized OpenVINO Intermediate Representation (OpenVINO IR) model will be saved in the `MODEL_DIR` directory. \n",
    "\n",
    "By default, this notebook will download one CT scan from KiTS19 dataset, and use that for quantization. To use the full dataset, set `BASEDIR` to the path of the dataset, as prepared according to the [Data Preparation](data-preparation-ct-scan.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASEDIR = Path(\"kits19_frames_1\")\n",
    "# Uncomment the line below to use the full dataset, as prepared in the data preparation notebook.\n",
    "# BASEDIR = Path(\"~/kits19/kits19_frames\").expanduser()\n",
    "MODEL_DIR = Path(\"model\")\n",
    "ONNX_PATH = MODEL_DIR / \"unet_kits19.onnx\"\n",
    "if not ONNX_PATH.exists():\n",
    "    ONNX_PATH = Path(\"pretrained_model/pretrained_unet_kits19.onnx\")\n",
    "assert ONNX_PATH.exists(), f\"ONNX_PATH: {ONNX_PATH} does not exist\"\n",
    "\n",
    "ir_path = (MODEL_DIR / ONNX_PATH.stem).with_suffix(\".xml\")\n",
    "\n",
    "print(f\"ONNX model: {ONNX_PATH}\")\n",
    "print(f\"Optimized model will be saved to: {ir_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CT-scan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The CT scan case number. For example: 2 for data from the case_00002 directory.\n",
    "# Currently, only 117 is supported.\n",
    "CASE = 117\n",
    "if not (BASEDIR / f\"case_{CASE:05d}\").exists():\n",
    "    BASEDIR.mkdir(exist_ok=True)\n",
    "    filename = download_file(\n",
    "        f\"https://storage.openvinotoolkit.org/data/test_data/openvino_notebooks/kits19/case_{CASE:05d}.zip\"\n",
    "    )\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path=BASEDIR)\n",
    "    os.remove(filename)  # remove zipfile\n",
    "    print(f\"Downloaded and extracted data for case_{CASE:05d}\")\n",
    "else:\n",
    "    print(f\"Data for case_{CASE:05d} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model to OpenVINO IR\n",
    "Use Model Optimizer to convert the ONNX model to OpenVINO IR, with `FP16` precision. The model files are saved to the `MODEL_DIR` directory. For more information, see the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html).\n",
    "\n",
    "When model optimization is successful, the last lines of the output will include `[ SUCCESS ] Generated IR version 10 model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "!mo --input_model $ONNX_PATH --output_dir $MODEL_DIR --data_type FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Optimization Tool (POT) Quantization\n",
    "The Post-Training Optimization Tool (POT) `compression` API defines base classes for `Metric` and `DataLoader`. This notebook uses a custom Metric and DataLoader that show all the required methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "#### Metric\n",
    "Define a metric to determine the performance of the model. For the Default Quantization algorithm that is used in this tutorial, defining a metric is optional. The metric is used to compare the quantized `INT8` model with the original `FP16` OpenVINO IR model.\n",
    "\n",
    "A metric for POT inherits from `compression.api.Metric` and should implement all the methods in this example.\n",
    "\n",
    "For this demo, the [F1 score](https://en.wikipedia.org/wiki/F-score) or Dice coefficient is used. The same metric implementation is used in the [training notebook](pytorch-monai-training.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The sigmoid function is used to transform the result of the network\n",
    "# to binary segmentation masks\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "\n",
    "class BinaryF1(Metric):\n",
    "    \"\"\"\n",
    "    Metric to compute F1/Dice score for binary segmentation. F1 is computed as\n",
    "    (2 * precision * recall) / (precision + recall) where precision is computed as\n",
    "    the ratio of pixels that were correctly predicted as true and all actual true pixels,\n",
    "    and recall as the ratio of pixels that were correctly predicted as true and all\n",
    "    predicted true pixels.\n",
    "\n",
    "    See https://en.wikipedia.org/wiki/F-score\n",
    "    \"\"\"\n",
    "\n",
    "    # Required methods\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._name = \"F1\"\n",
    "        self.y_true = 0\n",
    "        self.y_pred = 0\n",
    "        self.correct_true = 0\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"\n",
    "        Returns metric value for the last model output.\n",
    "        Possible format: {metric_name: [metric_values_per_image]}\n",
    "        \"\"\"\n",
    "        return {self._name: [0, 0]}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\"\n",
    "        Returns average metric value for all model outputs as {metric_name: metric_value}\n",
    "        \"\"\"\n",
    "        recall = self.correct_true / self.y_pred\n",
    "        precision = self.correct_true / self.y_true\n",
    "\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return {self._name: f1}\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\"\n",
    "        Update the statistics for metric computation\n",
    "\n",
    "        :param output: model output\n",
    "        :param target: annotations for model output\n",
    "        \"\"\"\n",
    "        label = target[0].astype(np.byte)\n",
    "        prediction = sigmoid(output[0]).round().astype(np.byte)\n",
    "\n",
    "        self.y_true += np.sum(label)\n",
    "        self.y_pred += np.sum(prediction)\n",
    "\n",
    "        correct_true = np.sum(\n",
    "            (label == prediction).astype(np.byte) * (label == 1).astype(np.byte)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        self.correct_true += correct_true\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets metric to initial values\n",
    "        \"\"\"\n",
    "        self.y_true = 0\n",
    "        self.y_pred = 0\n",
    "        self.correct_true = 0\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
    "                             'type': metric type\n",
    "        \"\"\"\n",
    "        return {self._name: {\"direction\": \"higher-better\", \"type\": \"F1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset\n",
    "\n",
    "The KitsDataLoader in the next cell expects images and masks in the *basedir* directory, in a folder per patient. For more information about the data in the dataset, see the [data preparation notebook](data-preparation-cit-scan.ipynb). This dataset follows POT's `compression.api.DataLoader` interface, which should implement `__init__`, `__getitem__` and `__len__`, where `__getitem__(index)` returns `(annotion, image)` or `(annotation, image, metadata)` and `annotation` consists of `(index, label)`.\n",
    "\n",
    "Images are loaded with MONAI's [`LoadImage`](https://docs.monai.io/en/stable/transforms.html#loadimage), to align with the image loading method in the [training notebook](pytorch-monai-training.ipynb). This method rotates and flips the images. We define a `rotate_and_flip` method to display the images in the expected orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "return len(self.dataset)": "return 30"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_and_flip(image):\n",
    "    \"\"\"Rotate `image` by 90 degrees and flip horizontally\"\"\"\n",
    "    return cv2.flip(cv2.rotate(image, rotateCode=cv2.ROTATE_90_CLOCKWISE), flipCode=1)\n",
    "\n",
    "\n",
    "class KitsDataLoader(DataLoader):\n",
    "    def __init__(self, basedir: str):\n",
    "        \"\"\"\n",
    "        DataLoader class for prepared KiTS19 data, for binary segmentation (background/kidney)\n",
    "        Source data should exist in basedir, in subdirectories case_00000 until case_00210,\n",
    "        with each subdirectory containing directories imaging_frames, with jpg images, and\n",
    "        segmentation_frames with segmentation masks as png files.\n",
    "        See https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize/data-preparation-ct-scan.ipynb\n",
    "\n",
    "        For demonstration purposes, in this implementation we use images from the validation subset.\n",
    "\n",
    "        :param basedir: Directory that contains the prepared CT scans\n",
    "        \"\"\"\n",
    "        allmasks = sorted(BASEDIR.glob(\"case_*/segmentation_frames/*png\"))\n",
    "\n",
    "        if len(allmasks) == 0:\n",
    "            raise ValueError(f\"basedir: '{basedir}' does not contain data\")\n",
    "        val_indices = [7, 10, 14, 15, 30, 60, 71, 74, 75, 81, 92, 93,\n",
    "                       106, 115, 117, 119, 134, 161, 192, 196, 203]  # fmt: skip\n",
    "        val_cases = [f\"case_{i:05d}\" for i in val_indices]\n",
    "        masks = [mask for mask in allmasks if mask.parents[1].name in val_cases]\n",
    "\n",
    "        self.basedir = basedir\n",
    "        self.dataset = masks\n",
    "        print(\n",
    "            f\"Created dataset with {len(self.dataset)} items. \"\n",
    "            f\"Base directory for data: {basedir}\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "\n",
    "        :return: (annotation, input_image, metadata) where annotation is (index, segmentation_mask)\n",
    "                 and metadata a dictionary with case and slice number\n",
    "        \"\"\"\n",
    "        mask_path = self.dataset[index]\n",
    "        image_path = str(mask_path.with_suffix(\".jpg\")).replace(\n",
    "            \"segmentation_frames\", \"imaging_frames\"\n",
    "        )\n",
    "\n",
    "        # Load images with MONAI's LoadImage to match data loading in training notebook\n",
    "        mask = LoadImage(image_only=True, dtype=np.uint8)(str(mask_path))\n",
    "        img = LoadImage(image_only=True, dtype=np.float32)(str(image_path))\n",
    "\n",
    "        if img.shape[:2] != (512, 512):\n",
    "            img = cv2.resize(img.astype(np.uint8), (512, 512)).astype(np.float32)\n",
    "            mask = cv2.resize(mask, (512, 512))\n",
    "\n",
    "        annotation = (index, mask)\n",
    "        input_image = np.expand_dims(img, axis=0)\n",
    "        return (\n",
    "            annotation,\n",
    "            input_image,\n",
    "            {\"case\": Path(mask_path).parents[1].name, \"slice\": Path(mask_path).stem},\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that the data loader returns the expected output, we create a DataLoader instance and show an image and a mask. The image and mask are shown as returned by the dataloader, after resizing and preprocessing. Since this dataset contains a lot of slices without kidneys, we select a slice that contains at least 5000 kidney pixels to verify that the annotations look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_loader = KitsDataLoader(BASEDIR)\n",
    "\n",
    "# Find a slice that contains kidney annotations\n",
    "# item[0] is the annotation: (id, annotation_data)\n",
    "annotation, image_data, _ = next(\n",
    "    item for item in data_loader if np.count_nonzero(item[0][1]) > 5000\n",
    ")\n",
    "# Remove extra image dimension and rotate and flip the image for visualization\n",
    "image = rotate_and_flip(image_data.squeeze())\n",
    "\n",
    "# The data loader returns annotations as (index, mask) and mask in shape (H,W)\n",
    "mask = rotate_and_flip(annotation[1])\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(image, cmap=\"gray\")\n",
    "ax[1].imshow(mask, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Quantization Config\n",
    "\n",
    "POT methods expect configuration dictionaries as arguments, which are defined in the cell below. The variable `ir_path` is defined in the [Settings](#Settings) cell at the top of the notebook. The other variables are defined in the cell above.\n",
    "\n",
    "See [Post-Training Optimization Best Practices](https://docs.openvino.ai/2021.4/pot_docs_BestPractices.html) for more information on the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Model config specifies the model name and paths to model .xml and .bin file\n",
    "model_config = Dict(\n",
    "    {\n",
    "        \"model_name\": f\"quantized_{ir_path.stem}\",\n",
    "        \"model\": ir_path,\n",
    "        \"weights\": ir_path.with_suffix(\".bin\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Engine config\n",
    "engine_config = Dict({\"device\": \"CPU\"})\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        \"name\": \"DefaultQuantization\",\n",
    "        \"stat_subset_size\": 1000,\n",
    "        \"params\": {\n",
    "            \"target_device\": \"ANY\",\n",
    "            \"preset\": \"performance\",  # choose between \"mixed\" and \"performance\"\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"model_config: {model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Quantization Pipeline\n",
    "\n",
    "The POT pipeline needs a loaded model, an `IEEngine` instance, a POT implementation of OpenVINO Runtime, and finally a POT `Pipeline` instance. The POT classes and functions expect a config argument. These configs are created in the Config section in the cell above. The F1 metric and `SegmentationDataLoader` are defined earlier in this notebook.\n",
    "\n",
    "Running the POT quantization pipeline takes just two lines of code. Create the pipeline with the `create_pipeline` function, and then run that pipeline with the `pipeline.run()` method. To reuse the quantized model later, compress the model weights and save the compressed model to a disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Step 1: Create the data loader.\n",
    "data_loader = KitsDataLoader(BASEDIR)\n",
    "\n",
    "# Step 2: Load the model.\n",
    "ir_model = load_model(model_config=model_config)\n",
    "\n",
    "# Step 3: Initialize the metric.\n",
    "metric = BinaryF1()\n",
    "\n",
    "# Step 4: Initialize the engine for metric calculation and statistics collection.\n",
    "engine = IEEngine(config=engine_config, data_loader=data_loader, metric=metric)\n",
    "\n",
    "# Step 5: Create a pipeline of compression algorithms.\n",
    "# The `quantization_algorithm` parameter is defined in the Settings.\n",
    "pipeline = create_pipeline(algorithms, engine)\n",
    "\n",
    "# Step 6: Execute the pipeline to quantize the model.\n",
    "algorithm_name = pipeline.algo_seq[0].name\n",
    "preset = pipeline._algo_seq[0].config[\"preset\"]\n",
    "with yaspin(\n",
    "    text=f\"Executing POT pipeline on {model_config['model']} with {algorithm_name}, {preset} preset\"\n",
    ") as sp:\n",
    "    start_time = time.perf_counter()\n",
    "    compressed_model = pipeline.run(ir_model)\n",
    "    end_time = time.perf_counter()\n",
    "    sp.text = f\"Quantization finished in {end_time - start_time:.2f} seconds\"\n",
    "    sp.ok(\"✔\")\n",
    "\n",
    "# Step 7 (Optional): Compress model weights to quantized precision\n",
    "#                    in order to reduce the size of the final .bin file.\n",
    "compress_model_weights(compressed_model)\n",
    "\n",
    "# Step 8: Save the compressed model to the desired path.\n",
    "# Set `save_path` to the directory where the model should be saved.\n",
    "compressed_model_paths = save_model(\n",
    "    model=compressed_model,\n",
    "    save_path=\"optimized_model\",\n",
    "    model_name=f\"{ir_model.name}_{preset}_{algorithm_name}\",\n",
    ")\n",
    "\n",
    "compressed_model_path = compressed_model_paths[0][\"model\"]\n",
    "print(\"The quantized model is stored at\", compressed_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Metrics of FP16 and INT8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the F1 score on the original and quantized model.\n",
    "ir_model = load_model(model_config=model_config)\n",
    "evaluation_pipeline = create_pipeline(algo_config=algorithms, engine=engine)\n",
    "\n",
    "with yaspin(text=\"Evaluating original IR model\") as sp:\n",
    "    original_metric = evaluation_pipeline.evaluate(ir_model)\n",
    "\n",
    "with yaspin(text=\"Evaluating quantized IR model\") as sp:\n",
    "    quantized_metric = pipeline.evaluate(compressed_model)\n",
    "\n",
    "if original_metric:\n",
    "    for key, value in original_metric.items():\n",
    "        print(f\"The {key} score of the original FP16 model is {value:.3f}\")\n",
    "\n",
    "if quantized_metric:\n",
    "    for key, value in quantized_metric.items():\n",
    "        print(f\"The {key} score of the quantized INT8 model is {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "\n",
    "To measure the inference performance of the `FP16` and `INT8` models, use [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - inference performance measurement tool in OpenVINO. Benchmark tool is a command-line application that can be run in the notebook with `! benchmark_app` or `%sx benchmark_app` commands.\n",
    "\n",
    "This tutorial uses a wrapper function from [Notebook Utils](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/utils/notebook_utils.ipynb). It prints the `benchmark_app` command with the chosen parameters.\n",
    "\n",
    "> **Note**: For the most accurate performance estimation, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app --help` to see all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameters and docstring for `benchmark_model`.\n",
    "benchmark_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# By default, benchmark on MULTI:CPU,GPU if a GPU is available, otherwise on CPU.\n",
    "ie = IECore()\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "# Uncomment one of the options below to benchmark on other devices\n",
    "# device = \"GPU\"\n",
    "# device = \"CPU\"\n",
    "# device = \"AUTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "seconds=15": "seconds=3"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark FP16 model\n",
    "benchmark_model(model_path=ir_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "seconds=15": "seconds=3"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark INT8 model\n",
    "benchmark_model(model_path=compressed_model_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Visually Compare Inference Results\n",
    "\n",
    "Visualize the results of the model on four slices of the validation set. Compare the results of the FP16 IR model with the results of the quantized INT8 model and the reference segmentation annotation.\n",
    "\n",
    "Medical imaging datasets tend to be very imbalanced: most of the slices in a CT scan do not contain kidney data. The segmentation model should be good at finding kidneys where they exist (in medical terms: have good sensitivity) but also not find spurious kidneys that do not exist (have good specificity). In the next cell, we show four slices: two slices that have no kidney data, and two slices that contain kidney data. For this example, a slice has kidney data if at least 50 pixels in the slices are annotated as kidney.\n",
    "\n",
    "Run this cell again to show results on a different subset. The random seed is displayed to allow reproducing specific runs of this cell.\n",
    "\n",
    "> **Note**: the images are shown after optional augmenting and resizing. In the KiTS19 dataset all but one of the cases has input shape `(512, 512)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "colormap = \"gray\"\n",
    "\n",
    "ie = IECore()\n",
    "net_ir = ie.read_network(ir_path)\n",
    "net_pot = ie.read_network(compressed_model_path)\n",
    "\n",
    "exec_net_ir = ie.load_network(network=net_ir, device_name=\"CPU\")\n",
    "exec_net_pot = ie.load_network(network=net_pot, device_name=\"CPU\")\n",
    "input_layer = next(iter(net_ir.input_info))\n",
    "output_layer_ir = next(iter(net_ir.outputs))\n",
    "output_layer_pot = next(iter(net_pot.outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a dataset and make a subset of the dataset for visualization.\n",
    "# The dataset items are (annotation, image) where annotation is (index, mask).\n",
    "background_slices = (item for item in data_loader if np.count_nonzero(item[0][1]) == 0)\n",
    "kidney_slices = (item for item in data_loader if np.count_nonzero(item[0][1]) > 50)\n",
    "# Set `seed` to current time. To reproduce specific results, copy the printed seed\n",
    "# and manually set `seed` to that value.\n",
    "seed = int(time.time())\n",
    "random.seed(seed)\n",
    "print(f\"Visualizing results with seed {seed}\")\n",
    "data_subset = random.sample(list(background_slices), 2) + random.sample(list(kidney_slices), 2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_images, ncols=4, figsize=(24, num_images * 4))\n",
    "for i, (annotation, image, meta) in enumerate(data_subset):\n",
    "    display_image = rotate_and_flip(image.squeeze())\n",
    "    mask = rotate_and_flip(annotation[1])\n",
    "    res_ir = exec_net_ir.infer(inputs={input_layer: image})\n",
    "    res_pot = exec_net_pot.infer(inputs={input_layer: image})\n",
    "    target_mask = mask.astype(np.uint8)\n",
    "\n",
    "    result_mask_ir = sigmoid(res_ir[output_layer_ir]).round().astype(np.uint8)[0, 0, ::]\n",
    "    result_mask_pot = sigmoid(res_pot[output_layer_pot]).round().astype(np.uint8)[0, 0, ::]\n",
    "    result_mask_ir = rotate_and_flip(result_mask_ir)\n",
    "    result_mask_pot = rotate_and_flip(result_mask_pot)\n",
    "\n",
    "    ax[i, 0].imshow(display_image, cmap=colormap)\n",
    "    ax[i, 1].imshow(target_mask, cmap=colormap)\n",
    "    ax[i, 2].imshow(result_mask_ir, cmap=colormap)\n",
    "    ax[i, 3].imshow(result_mask_pot, cmap=colormap)\n",
    "    ax[i, 0].set_title(f\"{meta['slice']}\")\n",
    "    ax[i, 1].set_title(\"Annotation\")\n",
    "    ax[i, 2].set_title(\"Prediction on FP16 model\")\n",
    "    ax[i, 3].set_title(\"Prediction on INT8 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Show Live Inference\n",
    "\n",
    "To show live inference on the model in the notebook, use the asynchronous processing feature of OpenVINO Runtime.\n",
    "\n",
    "If you use a GPU device, with `device=\"GPU\"` or `device=\"MULTI:CPU,GPU\"` to do inference on an integrated graphics card, model loading will be slow the first time you run this code. The model will be cached, so model loading will be faster after the first time. For more information on OpenVINO Runtime, including Model Caching, see the [OpenVINO Runtime API tutorial](../002-openvino-api/002-openvino-api.ipynb).\n",
    "\n",
    "Use the `show_live_inference` function from [Notebook Utils](../utils/notebook_utils.ipynb) to show live inference. This function uses AsyncPipeline and [Model API](https://docs.openvino.ai/latest/omz_python_model_api.html) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) to perform asynchronous inference. Once inference on the specified CT scan has completed, the total time and throughput (fps), including preprocessing and displaying, will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and List of Image Files\n",
    "\n",
    "Load the segmentation model to OpenVINO Runtime with `SegmentationModel`, based on the Model API from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). This model implementation includes pre and post processing for the model. For `SegmentationModel`, this includes the code to create an overlay of the segmentation mask on the original image/frame. Uncomment the next cell to see the implementation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SegmentationModel??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "122wcKhzXn3z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CASE = 117\n",
    "\n",
    "ie = IECore()\n",
    "segmentation_model = SegmentationModel(\n",
    "    ie=ie, model_path=Path(compressed_model_path), sigmoid=True, rotate_and_flip=True\n",
    ")\n",
    "case_path = BASEDIR / f\"case_{CASE:05d}\"\n",
    "image_paths = sorted(case_path.glob(\"imaging_frames/*jpg\"))\n",
    "print(f\"{case_path.name}, {len(image_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Show Inference\n",
    "\n",
    "In the next cell, run the `show_live_inference` function, which loads the `segmentation_model` to the specified `device` (using caching for faster model loading on GPU devices), loads the images, performs inference, and displays the results on the frames loaded in the `images` in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "=image_paths": "=image_paths[:5]"
    }
   },
   "outputs": [],
   "source": [
    "# Possible options for a device include: \"CPU\", \"GPU\", \"AUTO\", \"MULTI\".\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "reader = LoadImage(image_only=True, dtype=np.uint8)\n",
    "show_live_inference(\n",
    "    ie=ie, image_paths=image_paths, model=segmentation_model, device=device, reader=reader\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**OpenVINO**\n",
    "\n",
    "- [Post Training Optimization Tool (POT)](https://docs.openvino.ai/2021.4/pot_README.html)\n",
    "- [OpenVINO MULTI device plugin](https://docs.openvino.ai/latest/openvino_docs_IE_DG_supported_plugins_MULTI.html)\n",
    "- [OpenVINO API Tutorial](../002-openvino-api/002-openvino-api.ipynb)\n",
    "- [OpenVINO PyPI (pip install openvino-dev)](https://pypi.org/project/openvino-dev/)\n",
    "\n",
    "**Kits19 Data**\n",
    "  - [KiTS19 Challenge Homepage](https://kits19.grand-challenge.org/)\n",
    "  - [KiTS19 Github Repository](https://github.com/neheller/kits19)\n",
    "  - [The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes](https://arxiv.org/abs/1904.00445)\n",
    "  - [The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge](https://www.sciencedirect.com/science/article/pii/S1361841520301857)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b3d35d6a072dc3a394ff616c9fe1a71d7b246bb5400d8f70c87361b733f2cd1b"
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
