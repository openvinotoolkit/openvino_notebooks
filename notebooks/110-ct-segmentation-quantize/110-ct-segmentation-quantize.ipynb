{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kidney Segmentation with PyTorch Lightning and OpenVINO™ Toolkit.\n",
    "\n",
    "## **Part 2:** Convert and Quantize Model and Show Live Inference\n",
    "\n",
    "This tutorial demonstrates training and inference with a kidney segmentation model. For training, [PyTorch Lightning](https://www.pytorchlightning.ai/) is used with a [UNet](https://arxiv.org/abs/1505.04597) segmentation model. The model is converted to OpenVINO IR with [Model Optimizer](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html), and quantized with OpenVINO's [Post-Training Optimization Tool](https://docs.openvinotoolkit.org/latest/pot_compression_api_README.html) API. \n",
    "\n",
    "Other notebooks in this series:\n",
    "\n",
    "- [Data Preparation for 2D Segmentation of 3D Medical Data](../210-ct-scan-data-preparation)\n",
    "- [Live Inference and Benchmark CT-scan data](../210-ct-scan-live-inference/210-ct-scan-live-inference.ipynb)\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook needs a trained UNet model that is converted to [ONNX](https://github.com/onnx/onnx) format. We provide a pretrained model trained for 20 epochs with the full [Kits-19](https://github.com/neheller/kits19) frames dataset, which has an F1 score on the validation set of 0.9. The training code will me made availble soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "The Post Training Optimization API is implemented in the `compression` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from addict import Dict\n",
    "from async_inference import CTAsyncPipeline, SegModel\n",
    "from compression.api import Metric\n",
    "from compression.engines.ie_engine import IEEngine\n",
    "from compression.graph import load_model, save_model\n",
    "from compression.graph.model_utils import compress_model_weights\n",
    "from compression.pipeline.initializer import create_pipeline\n",
    "from IPython.display import Image\n",
    "from omz_python.models import model as omz_model\n",
    "from openvino.inference_engine import IECore\n",
    "from yaspin import yaspin\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import benchmark_model, download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "To use the pretrained model, set MODEL_DIR to `Path(\"pretrained_model\")` in the cell below. This is the default. To use a model that you trained yourself, adjust the paths. By default, this notebook will quantize one CT scan from the KITS19 dataset. To use the full dataset, set BASEDIR to the path of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASEDIR = Path(\"kits19_frames_1\")\n",
    "MODEL_DIR = Path(\"pretrained_model\")\n",
    "\n",
    "onnx_path = MODEL_DIR / \"unet44.onnx\"\n",
    "ir_path = onnx_path.with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CT Scan Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CT scan case number. For example: 16 for data from the case_00016 directory\n",
    "# Currently only 16 is supported\n",
    "case = 16\n",
    "if not (BASEDIR / f\"case_{case:05d}\").exists():\n",
    "    BASEDIR.mkdir(exist_ok=True)\n",
    "    filename = download_file(\n",
    "        f\"https://s3.us-west-1.amazonaws.com/openvino.notebooks/case_{case:05d}.zip\"\n",
    "    )\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path=BASEDIR)\n",
    "        os.remove(filename)  # remove zipfile\n",
    "        print(f\"Downloaded and extracted data for case_{case:05d}\")\n",
    "else:\n",
    "    print(f\"Data for case_{case:05d} exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model to OpenVINO IR\n",
    "Call the Model Optimizer tool to convert the ONNX model to OpenVINO IR, with FP16 precision. The models are saved to the `model` directory. See the [Model Optimizer Developer Guide](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information.\n",
    "\n",
    "Model Optimization was successful if the last lines of the output include `[ SUCCESS ] Generated IR version 10 model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo --input_model $onnx_path --output_dir $MODEL_DIR --data_type FP16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-Training Optimization Tool (POT) Quantization\n",
    "The Post-Training Optimization Tool (POT) `compression` API defines base classes for `Metric` and `DataLoader`. In this notebook, we use a custom Metric and DataLoader that show all the required methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration\n",
    "\n",
    "#### Metric\n",
    "Define a metric to determine the performance of the model. For the Default Quantization algorithm that is used in this tutorial, defining a metric is optional. The metric is used to compare the quantized INT8 model with the original FP16 IR model.\n",
    "\n",
    "A metric for POT inherits from `compression.api.Metric` and should implement all the methods in this example.\n",
    "\n",
    "For this demo, the [F1 score](https://en.wikipedia.org/wiki/F-score) or Dice coefficient is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "\n",
    "class BinaryF1(Metric):\n",
    "    \"\"\"\n",
    "    Metric to compute F1/Dice score for binary segmentation. F1 is computed as\n",
    "    (2 * precision * recall) / (precision + recall) where precision is computed as\n",
    "    the ratio of pixels that were correctly predicted as true and all actual true pixels,\n",
    "    and recall as the ratio of pixels that were correctly predicted as true and all\n",
    "    predicted true pixels.\n",
    "\n",
    "    See https://en.wikipedia.org/wiki/F-score\n",
    "    \"\"\"\n",
    "\n",
    "    # Required methods\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._name = \"F1\"\n",
    "        self.y_true = 0\n",
    "        self.y_pred = 0\n",
    "        self.correct_true = 0\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"Returns metric value for the last model output.\n",
    "        Possible format: {metric_name: [metric_values_per_image]}\n",
    "        \"\"\"\n",
    "        return {self._name: [0, 0]}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\"Returns average metric value for all model outputs.\n",
    "        Possible format: {metric_name: metric_value}\n",
    "        \"\"\"\n",
    "\n",
    "        recall = self.correct_true / self.y_pred\n",
    "        precision = self.correct_true / self.y_true\n",
    "\n",
    "        f1 = (2 * precision * recall) / (precision + recall)\n",
    "        return {self._name: f1}\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\"\n",
    "        :param output: model output\n",
    "        :param target: annotations for model output\n",
    "        \"\"\"\n",
    "\n",
    "        label = target[0].astype(np.byte)\n",
    "        prediction = sigmoid(output[0]).round().astype(np.byte)\n",
    "\n",
    "        self.y_true += np.sum(label)\n",
    "        self.y_pred += np.sum(prediction)\n",
    "\n",
    "        correct_true = np.sum(\n",
    "            (label == prediction).astype(np.byte) * (label == 1).astype(np.byte)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        self.correct_true += correct_true\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Resets metric\"\"\"\n",
    "        self.y_true = 0\n",
    "        self.y_pred = 0\n",
    "        self.correct_true = 0\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
    "                             'type': metric type\n",
    "        \"\"\"\n",
    "        return {self._name: {\"direction\": \"higher-better\", \"type\": \"F1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data\n",
    "\n",
    "##### Dataset\n",
    "\n",
    "The dataset in the next cell is copied from the training notebook. It expects images and masks in the *basedir* directory, in a folder per patient. For more information about the dataset, see the data preparation notebook. This dataset follows POT's `compression.api.DataLoader` interface, which should implement `__init__`, `__getitem__` and `__len__`. It can therefore be used directly for POT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KitsDataset(object):\n",
    "    def __init__(self, basedir: str, dataset_type: str, transforms=None):\n",
    "        \"\"\"\n",
    "        Dataset class for prepared Kits19 data, for binary segmentation (background/kidney).\n",
    "\n",
    "        :param basedir: Directory that contains the prepared CT scans, in subdirectories\n",
    "                        case_00000 until case_00210\n",
    "        :param dataset_type: either \"train\" or \"val\"\n",
    "        :param transforms: Compose object with augmentations\n",
    "        \"\"\"\n",
    "        allmasks = sorted(glob.glob(f\"{basedir}/case_*/segmentation_frames/*png\"))\n",
    "\n",
    "        # Reserve 10% of the patients for the validation dataset\n",
    "        # Set a random seed to ensure that this list is reproducable\n",
    "        random.seed(2.71828)\n",
    "        self.valpatients = sorted(random.choices(range(210), k=21))\n",
    "\n",
    "        valcases = [f\"case_{i:05d}\" for i in self.valpatients]\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            masks = [mask for mask in allmasks if Path(mask).parents[1].name not in valcases]\n",
    "        elif dataset_type == \"val\":\n",
    "            masks = [mask for mask in allmasks if Path(mask).parents[1].name in valcases]\n",
    "        else:\n",
    "            raise ValueError(\"Please choose train or val dataset split\")\n",
    "\n",
    "        random.shuffle(masks)\n",
    "        self.basedir = basedir\n",
    "        self.dataset_type = dataset_type\n",
    "        self.dataset = masks\n",
    "        self.transforms = transforms\n",
    "        print(f\"Created {dataset_type} dataset with {len(self.dataset)} items.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "        Labels are converted to binary labels (background/kidney).\n",
    "\n",
    "        :return: (annotation, input_image) where annotation is (index, segmentation_mask)\n",
    "        \"\"\"\n",
    "        mask_path = self.dataset[index]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "        # The masks contain annotations for kidneys and tumors, in this tutorial we only segment\n",
    "        # kidneys so we can set all pixels that contain a non-background value to 1.\n",
    "        mask[mask > 0] = 1\n",
    "\n",
    "        image_path = str(Path(mask_path.replace(\"segmentation\", \"imaging\")).with_suffix(\".jpg\"))\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        if img.shape != (512, 512, 3):\n",
    "            img = cv2.resize(img, (512, 512))\n",
    "            mask = cv2.resize(mask, (512, 512))\n",
    "\n",
    "        # TODO: add transforms with torchvision.transforms instead of albumentations\n",
    "        # if self.transforms is not None:\n",
    "\n",
    "        annotation = (index, mask.astype(np.uint8))\n",
    "        input_image = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "        return annotation, input_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test that the data loader returns the expected output, we create a DataLoader instance and show an image and a mask. The image and mask are shown as returned by the dataloader, after resizing and preprocessing. Since this dataset contains a lot of slices without kidneys, we select a slice that contains at least 100 kidney pixels to verify that the annotations look correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loader\n",
    "data_loader = KitsDataset(BASEDIR, \"val\")\n",
    "\n",
    "# Find a slice that contains kidney annotations\n",
    "# item[0] is the annotation: (id, annotation_data)\n",
    "annotation, image_data = next(item for item in data_loader if np.count_nonzero(item[0][1]) > 100)\n",
    "\n",
    "# The data loader returns images as floating point data with (C,H,W) layout. Convert to 8-bit\n",
    "# integer data and transpose to (H,C,W) for visualization\n",
    "image = image_data.astype(np.uint8).transpose(1, 2, 0)\n",
    "\n",
    "# The data loader returns annotations as (index, mask) and mask in shape (1,H,W)\n",
    "# grab only the mask, and remove the channel dimension for visualization\n",
    "mask = annotation[1].squeeze()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(image, cmap=\"gray\")\n",
    "ax[1].imshow(mask, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Quantization Config\n",
    "\n",
    "POT methods expect configuration dictionaries as arguments, which are defined in the cell below. The variable `ir_path` is defined in the [Settings](#Settings) cell at the top of the notebook. The other variables are defined in the cell above.\n",
    "\n",
    "See [Post-Training Optimization Best Practices](https://docs.openvino.ai/2021.4/pot_docs_BestPractices.html) for more information on the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model config specifies the model name and paths to model .xml and .bin file\n",
    "model_config = Dict(\n",
    "    {\n",
    "        \"model_name\": f\"quantized_{ir_path.stem}\",\n",
    "        \"model\": ir_path,\n",
    "        \"weights\": ir_path.with_suffix(\".bin\"),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Engine config\n",
    "engine_config = Dict({\"device\": \"CPU\"})\n",
    "\n",
    "algorithms = [\n",
    "    {\n",
    "        \"name\": \"DefaultQuantization\",\n",
    "        \"stat_subset_size\": 300,\n",
    "        \"params\": {\n",
    "            \"target_device\": \"ANY\",\n",
    "            \"preset\": \"mixed\",  # choose between \"mixed\" and \"performance\"\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"model_config: {model_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Quantization Pipeline: DataLoader, Model, Metric, Inference Engine\n",
    "\n",
    "The POT pipeline uses the functions `load_model()`, `IEEngine` and `create_pipeline()`. `load_model()` loads an IR model, specified in `model_config`, `IEEngine` is a POT implementation of Inference Engine, that will be passed to the POT pipeline created by `create_pipeline()`. The POT classes and functions expect a config argument. These configs are created in the Config section. The F1 metric and SegmentationDataLoader are defined earlier in this notebook.\n",
    "\n",
    "Running the POT quantization pipeline takes just two lines of code. We create the pipeline with the `create_pipeline` function, and then run that pipeline with `pipeline.run()`. To reuse the quantized model later, we compress the model weights and save the compressed model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: create data loader\n",
    "data_loader = KitsDataset(BASEDIR, \"val\")\n",
    "\n",
    "# Step 2: load model\n",
    "ir_model = load_model(model_config=model_config)\n",
    "\n",
    "# Step 3: initialize the metric\n",
    "metric = BinaryF1()\n",
    "\n",
    "# Step 4: Initialize the engine for metric calculation and statistics collection.\n",
    "engine = IEEngine(config=engine_config, data_loader=data_loader, metric=metric)\n",
    "\n",
    "# Step 5: Create a pipeline of compression algorithms.\n",
    "# quantization_algorithm is defined in the Settings\n",
    "pipeline = create_pipeline(algorithms, engine)\n",
    "algorithm_name = pipeline.algo_seq[0].name\n",
    "\n",
    "# Step 6: Execute the pipeline to quantize the model\n",
    "with yaspin(text=f\"Executing POT pipeline on {model_config['model']} with {algorithm_name}\") as sp:\n",
    "    start_time = time.perf_counter()\n",
    "    compressed_model = pipeline.run(ir_model)\n",
    "    end_time = time.perf_counter()\n",
    "    sp.text = f\"Quantization finished in {end_time - start_time:.2f} seconds\"\n",
    "    sp.ok(\"✔\")\n",
    "\n",
    "# Step 7 (Optional): Compress model weights to quantized precision\n",
    "#                    in order to reduce the size of the final .bin file.\n",
    "compress_model_weights(compressed_model)\n",
    "\n",
    "# Step 8: Save the compressed model to the desired path.\n",
    "# Set save_path to the directory where the directory\n",
    "compressed_model_paths = save_model(\n",
    "    model=compressed_model, save_path=\"optimized_model\", model_name=ir_model.name\n",
    ")\n",
    "\n",
    "compressed_model_path = compressed_model_paths[0][\"model\"]\n",
    "print(\"The quantized model is stored at\", compressed_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Metric of FP16 and INT8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the F1 score on the quantized model and compare with the F1 score on the FP16 IR model.\n",
    "\n",
    "ir_model = load_model(model_config=model_config)\n",
    "evaluation_pipeline = create_pipeline(algo_config=algorithms, engine=engine)\n",
    "\n",
    "original_metric = None\n",
    "with yaspin(text=\"Evaluating original IR model\") as sp:\n",
    "    start_time = time.time()\n",
    "    original_metric = evaluation_pipeline.evaluate(ir_model)\n",
    "    stop_time = time.time()\n",
    "    sp.text = f\"Finished evaluating original IR model in {stop_time-start_time:.2f} seconds\"\n",
    "    sp.ok(\"✔\")\n",
    "\n",
    "with yaspin(text=\"Evaluating quantized IR model\") as sp:\n",
    "    start_time = time.time()\n",
    "    quantized_metric = evaluation_pipeline.evaluate(compressed_model)\n",
    "    stop_time = time.time()\n",
    "    sp.text = f\"Finished evaluating quantized IR model in {stop_time-start_time:.2f} seconds\"\n",
    "    sp.ok(\"✔\")\n",
    "\n",
    "if quantized_metric:\n",
    "    for key, value in quantized_metric.items():\n",
    "        print(f\"The {key} score of the quantized INT8 model is {value:.3f}\")\n",
    "\n",
    "if original_metric:\n",
    "    for key, value in original_metric.items():\n",
    "        print(f\"The {key} score of the original FP16 model is {value:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "\n",
    "To measure the inference performance of the FP16 and INT8 models, we use [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html), OpenVINO's inference performance measurement tool. Benchmark tool is a command line application that can be run in the notebook with `! benchmark_app` or `%sx benchmark_app`.\n",
    "\n",
    "In this tutorial, we use a wrapper function from [Notebook Utils](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/utils/notebook_utils.ipynb). It prints the `benchmark_app` command with the chosen parameters.\n",
    "\n",
    "> NOTE: For the most accurate performance estimation, we recommended running `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app --help` to see all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the parameters and docstring for `benchmark_model`\n",
    "benchmark_model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, benchmark on MULTI:CPU,GPU if a GPU is available, otherwise on CPU.\n",
    "ie = IECore()\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "# Uncomment one of the options below to benchmark on other devices\n",
    "# device = \"GPU\"\n",
    "# device = \"CPU\"\n",
    "# device = \"AUTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark FP16 model\n",
    "benchmark_model(model_path=ir_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark INT8 model\n",
    "benchmark_model(model_path=compressed_model_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Show Live Inference\n",
    "\n",
    "To show live inference on the model in the notebook, we use the asynchronous processing feature of OpenVINO Inference Engine.\n",
    "\n",
    "If you use a GPU device, with `device=\"GPU\"` or `device=\"MULTI:CPU,GPU\"` to do inference on an integrated graphics card, model loading will be slow the first time you run this code. The model will be cached, so after the first time model loading will be fast. See the [OpenVINO API tutorial](../002-openvino-api/002-openvino-api.ipynb) for more information on Inference Engine, including Model Caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Functions\n",
    "\n",
    "We define a helper function `show_array` to efficiently show images in the notebook. The `do_inference` function uses [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)'s AsyncPipeline to perform asynchronous inference. After inference on the specified CT scan has completed, the total time and throughput (fps), including preprocessing and displaying, will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(frame: np.ndarray, display_handle=None):\n",
    "    \"\"\"\n",
    "    Display array `frame`. Replace information at `display_handle` with `frame`\n",
    "    encoded as jpeg image\n",
    "\n",
    "    Create a display_handle with: `display_handle = display(display_id=True)`\n",
    "    \"\"\"\n",
    "    _, frame = cv2.imencode(ext=\".jpeg\", img=frame)\n",
    "    if display_handle is None:\n",
    "        display_handle = display(Image(data=frame.tobytes()), display_id=True)\n",
    "    else:\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "    return display_handle\n",
    "\n",
    "\n",
    "def do_inference(imagelist: List, model: omz_model.Model, device: str):\n",
    "    \"\"\"\n",
    "    Do inference of images in `imagelist` on `model` on the given `device` and show\n",
    "    the results in real time in a Jupyter Notebook\n",
    "\n",
    "    :param imagelist: list of images/frames to do inference on\n",
    "    :param model: Model instance for inference\n",
    "    :param device: Name of device to perform inference on. For example: \"CPU\"\n",
    "    \"\"\"\n",
    "    display_handle = None\n",
    "    next_frame_id = 0\n",
    "    next_frame_id_to_show = 0\n",
    "\n",
    "    input_layer = next(iter(model.net.input_info))\n",
    "\n",
    "    # Create asynchronous pipeline and print time it takes to load the model\n",
    "    load_start_time = time.perf_counter()\n",
    "    pipeline = CTAsyncPipeline(\n",
    "        ie=ie, model=model, plugin_config={}, device=device, max_num_requests=0\n",
    "    )\n",
    "    load_end_time = time.perf_counter()\n",
    "\n",
    "    # Perform asynchronous inference\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    while next_frame_id < len(imagelist) - 1:\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "\n",
    "        if results:\n",
    "            # Show next result from async pipeline\n",
    "            result, meta = results\n",
    "            display_handle = showarray(result, display_handle)\n",
    "\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "        if pipeline.is_ready():\n",
    "            # Submit new image to async pipeline\n",
    "            image = imagelist[next_frame_id]\n",
    "            pipeline.submit_data(\n",
    "                inputs={input_layer: image}, id=next_frame_id, meta={\"frame\": image}\n",
    "            )\n",
    "            next_frame_id += 1\n",
    "        else:\n",
    "            # If the pipeline is not ready yet and there are no results: wait\n",
    "            pipeline.await_any()\n",
    "\n",
    "    pipeline.await_all()\n",
    "\n",
    "    # Show all frames that are in the pipeline after all images have been submitted\n",
    "    while pipeline.has_completed_request():\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            result, meta = results\n",
    "            display_handle = showarray(result, display_handle)\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    fps = len(imagelist) / duration\n",
    "    print(f\"Loaded model to {device} in {load_end_time-load_start_time:.2f} seconds.\")\n",
    "    print(f\"Total time for {next_frame_id+1} frames: {duration:.2f} seconds, fps:{fps:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model and Images\n",
    "\n",
    "Load the segmentation model with `SegModel`, based on the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) Model API. Load a  CT scan from the `BASEDIR` directory (by default: _kits19_frames_) to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "segmentation_model = SegModel(ie=ie, model_path=Path(compressed_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = 16\n",
    "demopattern = f\"{BASEDIR}/case_{case:05d}/imaging_frames/*jpg\"\n",
    "imlist = sorted(glob.glob(demopattern))\n",
    "images = [cv2.imread(im, cv2.IMREAD_UNCHANGED) for im in imlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Show Inference\n",
    "\n",
    "In the next cell, we run the `do inference` function, which loads the model to the specified device (using caching for faster model loading on GPU devices), performs inference, and displays the results in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible options for device include \"CPU\", \"GPU\", \"AUTO\", \"MULTI\"\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "do_inference(imagelist=images, model=segmentation_model, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Functions\n",
    "\n",
    "We define a helper function `show_array` to efficiently show images in the notebook. The `do_inference` function uses [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)'s AsyncPipeline to perform asynchronous inference. After inference on the specified CT scan has completed, the total time and throughput (fps), including preprocessing and displaying, will be printed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Show Inference\n",
    "\n",
    "In the next cell, we run the `do inference` function, which loads the model to the specified device (using caching for faster model loading on GPU devices), performs inference, and displays the results in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible options for device include \"CPU\", \"GPU\", \"AUTO\", \"MULTI\"\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "do_inference(imagelist=images, model=segmentation_model, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
