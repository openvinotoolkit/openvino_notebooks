{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize a Segmentation Model and Show Live Inference\n",
    "\n",
    "## Kidney Segmentation with PyTorch Lightning and OpenVINO™ - Part 3\n",
    "\n",
    "This tutorial is a part of a series on how to train, optimize, quantize and show live inference on a medical segmentation model. The goal is to accelerate inference on a kidney segmentation model. The [UNet](https://arxiv.org/abs/1505.04597) model is trained from scratch; the data is from [Kits19](https://github.com/neheller/kits19).\n",
    "\n",
    "This third tutorial in the series shows how to:\n",
    "\n",
    "- Convert an Original model to OpenVINO IR with [model conversion API](https://docs.openvino.ai/2023.0/openvino_docs_model_processing_introduction.html)\n",
    "- Quantize a PyTorch model with NNCF\n",
    "- Evaluate the F1 score metric of the original model and the quantized model\n",
    "- Benchmark performance of the FP32 model and the INT8 quantized model\n",
    "- Show live inference with OpenVINO's async API\n",
    "\n",
    "All notebooks in this series:\n",
    "\n",
    "- [Data Preparation for 2D Segmentation of 3D Medical Data](data-preparation-ct-scan.ipynb)\n",
    "- [Train a 2D-UNet Medical Imaging Model with PyTorch Lightning](pytorch-monai-training.ipynb)\n",
    "- Convert and Quantize a Segmentation Model and Show Live Inference (this notebook)\n",
    "- [Live Inference and Benchmark CT-scan data](110-ct-scan-live-inference.ipynb) \n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook needs a trained UNet model. We provide a pre-trained model, trained for 20 epochs with the full [Kits-19](https://github.com/neheller/kits19) frames dataset, which has an F1 score on the validation set of 0.9. The training code is available in [this notebook](pytorch-monai-training.ipynb). \n",
    "\n",
    "NNCF for PyTorch models requires a C++ compiler. On Windows, install [Microsoft Visual Studio 2019](https://docs.microsoft.com/en-us/visualstudio/releases/2019/release-notes). During installation, choose Desktop development with C++ in the Workloads tab. On macOS, \n",
    "run `xcode-select –install` from a Terminal. On Linux, install `gcc`.\n",
    "\n",
    "Running this notebook with the full dataset will take a long time. For demonstration purposes, this tutorial will download one converted CT scan and use that scan for quantization and inference. For production purposes, use a representative dataset for quantizing the model.\n",
    "\n",
    "#### Table of content:\n",
    "- [Imports](#Imports-Uparrow)\n",
    "- [Settings](#Settings-Uparrow)\n",
    "- [Load PyTorch Model](#Load-PyTorch-Model-Uparrow)\n",
    "- [Download CT-scan Data](#Download-CT-scan-Data-Uparrow)\n",
    "- [Configuration](#Configuration-Uparrow)\n",
    "    - [Dataset](#Dataset-Uparrow)\n",
    "    - [Metric](#Metric-Uparrow)\n",
    "- [Quantization](#Quantization-Uparrow)\n",
    "- [Compare FP32 and INT8 Model](#Compare-FP32-and-INT8-Model-Uparrow)\n",
    "    - [Compare File Size](#Compare-File-Size-Uparrow)\n",
    "    - [Compare Metrics for the original model and the quantized model to be sure that there no degradation.](#Compare-Metrics-for-the-original-model-and-the-quantized-model-to-be-sure-that-there-no-degradation.-Uparrow)\n",
    "    - [Compare Performance of the FP32 IR Model and Quantized Models](#Compare-Performance-of-the-FP32-IR-Model-and-Quantized-Models-Uparrow)\n",
    "    - [Visually Compare Inference Results](#Visually-Compare-Inference-Results-Uparrow)\n",
    "- [Show Live Inference](#Show-Live-Inference-Uparrow)\n",
    "    - [Load Model and List of Image Files](#Load-Model-and-List-of-Image-Files-Uparrow)\n",
    "    - [Show Inference](#Show-Inference-Uparrow)\n",
    "- [References](#References-Uparrow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2023.1.0\" \"monai>=0.9.1,<1.0.0\" \"torchmetrics>=0.11.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# On Windows, try to find the directory that contains x64 cl.exe and add it to the PATH to enable PyTorch\n",
    "# to find the required C++ tools. This code assumes that Visual Studio is installed in the default\n",
    "# directory. If you have a different C++ compiler, please add the correct path to os.environ[\"PATH\"]\n",
    "# directly. Note that the C++ Redistributable is not enough to run this notebook.\n",
    "\n",
    "# Adding the path to os.environ[\"LIB\"] is not always required - it depends on the system's configuration\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    import distutils.command.build_ext\n",
    "    import os\n",
    "    from pathlib import Path\n",
    "\n",
    "    if sys.getwindowsversion().build >= 20000:  # Windows 11\n",
    "        search_path = \"**/Hostx64/x64/cl.exe\"\n",
    "    else:\n",
    "        search_path = \"**/Hostx86/x64/cl.exe\"\n",
    "\n",
    "    VS_INSTALL_DIR_2019 = r\"C:/Program Files (x86)/Microsoft Visual Studio\"\n",
    "    VS_INSTALL_DIR_2022 = r\"C:/Program Files/Microsoft Visual Studio\"\n",
    "\n",
    "    cl_paths_2019 = sorted(list(Path(VS_INSTALL_DIR_2019).glob(search_path)))\n",
    "    cl_paths_2022 = sorted(list(Path(VS_INSTALL_DIR_2022).glob(search_path)))\n",
    "    cl_paths = cl_paths_2019 + cl_paths_2022\n",
    "\n",
    "    if len(cl_paths) == 0:\n",
    "        raise ValueError(\n",
    "            \"Cannot find Visual Studio. This notebook requires an x64 C++ compiler. If you installed \"\n",
    "            \"a C++ compiler, please add the directory that contains cl.exe to `os.environ['PATH']`.\"\n",
    "        )\n",
    "    else:\n",
    "        # If multiple versions of MSVC are installed, get the most recent version\n",
    "        cl_path = cl_paths[-1]\n",
    "        vs_dir = str(cl_path.parent)\n",
    "        os.environ[\"PATH\"] += f\"{os.pathsep}{vs_dir}\"\n",
    "        # Code for finding the library dirs from\n",
    "        # https://stackoverflow.com/questions/47423246/get-pythons-lib-path\n",
    "        d = distutils.core.Distribution()\n",
    "        b = distutils.command.build_ext.build_ext(d)\n",
    "        b.finalize_options()\n",
    "        os.environ[\"LIB\"] = os.pathsep.join(b.library_dirs)\n",
    "        print(f\"Added {vs_dir} to PATH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Union\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "import numpy as np\n",
    "import torch\n",
    "import nncf\n",
    "import openvino as ov\n",
    "from monai.transforms import LoadImage\n",
    "from nncf.common.logging.logger import set_log_level\n",
    "from torchmetrics import F1Score as F1\n",
    "\n",
    "set_log_level(logging.ERROR)  # Disables all NNCF info and warning messages\n",
    "\n",
    "from custom_segmentation import SegmentationModel\n",
    "from async_pipeline import show_live_inference\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "By default, this notebook will download one CT scan from the KITS19 dataset that will be used for quantization. To use the full dataset, set `BASEDIR` to the path of the dataset, as prepared according to the [Data Preparation](data-preparation-ct-scan.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "BASEDIR = Path(\"kits19_frames_1\")\n",
    "# Uncomment the line below to use the full dataset, as prepared in the data preparation notebook\n",
    "# BASEDIR = Path(\"~/kits19/kits19_frames\").expanduser()\n",
    "MODEL_DIR = Path(\"model\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load PyTorch Model [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "Download the pre-trained model weights, load the PyTorch model and the `state_dict` that was saved after training. The model used in this notebook is a [BasicUNet](https://docs.monai.io/en/stable/networks.html#basicunet) model from [MONAI](https://monai.io). We provide a pre-trained checkpoint. To see how this model performs, check out the [training notebook](pytorch-monai-training.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_dict_url = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/kidney-segmentation-kits19/unet_kits19_state_dict.pth\"\n",
    "state_dict_file = download_file(state_dict_url, directory=\"pretrained_model\")\n",
    "state_dict = torch.load(state_dict_file, map_location=torch.device(\"cpu\"))\n",
    "\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    new_key = k.replace(\"_model.\", \"\")\n",
    "    new_state_dict[new_key] = v\n",
    "new_state_dict.pop(\"loss_function.pos_weight\")\n",
    "\n",
    "model = monai.networks.nets.BasicUNet(spatial_dims=2, in_channels=1, out_channels=1).eval()\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download CT-scan Data [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The CT scan case number. For example: 2 for data from the case_00002 directory\n",
    "# Currently only 117 is supported\n",
    "CASE = 117\n",
    "if not (BASEDIR / f\"case_{CASE:05d}\").exists():\n",
    "    BASEDIR.mkdir(exist_ok=True)\n",
    "    filename = download_file(\n",
    "        f\"https://storage.openvinotoolkit.org/data/test_data/openvino_notebooks/kits19/case_{CASE:05d}.zip\"\n",
    "    )\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path=BASEDIR)\n",
    "    os.remove(filename)  # remove zipfile\n",
    "    print(f\"Downloaded and extracted data for case_{CASE:05d}\")\n",
    "else:\n",
    "    print(f\"Data for case_{CASE:05d} exists\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "The `KitsDataset` class in the next cell expects images and masks in the *`basedir`* directory, in a folder per patient. It is a simplified version of the Dataset class in the [training notebook](pytorch-monai-training.ipynb).\n",
    "\n",
    "Images are loaded with MONAI's [`LoadImage`](https://docs.monai.io/en/stable/transforms.html#loadimage), to align with the image loading method in the training notebook. This method rotates and flips the images. We define a `rotate_and_flip` method to display the images in the expected orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [],
    "test_replace": {
     "return len(self.dataset)": "return 30"
    }
   },
   "outputs": [],
   "source": [
    "def rotate_and_flip(image):\n",
    "    \"\"\"Rotate `image` by 90 degrees and flip horizontally\"\"\"\n",
    "    return cv2.flip(cv2.rotate(image, rotateCode=cv2.ROTATE_90_CLOCKWISE), flipCode=1)\n",
    "\n",
    "\n",
    "class KitsDataset:\n",
    "    def __init__(self, basedir: str):\n",
    "        \"\"\"\n",
    "        Dataset class for prepared Kits19 data, for binary segmentation (background/kidney)\n",
    "        Source data should exist in basedir, in subdirectories case_00000 until case_00210,\n",
    "        with each subdirectory containing directories imaging_frames, with jpg images, and\n",
    "        segmentation_frames with segmentation masks as png files.\n",
    "        See https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/110-ct-segmentation-quantize/data-preparation-ct-scan.ipynb\n",
    "\n",
    "        :param basedir: Directory that contains the prepared CT scans\n",
    "        \"\"\"\n",
    "        masks = sorted(BASEDIR.glob(\"case_*/segmentation_frames/*png\"))\n",
    "\n",
    "        self.basedir = basedir\n",
    "        self.dataset = masks\n",
    "        print(\n",
    "            f\"Created dataset with {len(self.dataset)} items. \"\n",
    "            f\"Base directory for data: {basedir}\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "\n",
    "        :return: (image, segmentation_mask)\n",
    "        \"\"\"\n",
    "        mask_path = self.dataset[index]\n",
    "        image_path = str(mask_path.with_suffix(\".jpg\")).replace(\n",
    "            \"segmentation_frames\", \"imaging_frames\"\n",
    "        )\n",
    "\n",
    "        # Load images with MONAI's LoadImage to match data loading in training notebook\n",
    "        mask = LoadImage(image_only=True, dtype=np.uint8)(str(mask_path)).numpy()\n",
    "        img = LoadImage(image_only=True, dtype=np.float32)(str(image_path)).numpy()\n",
    "\n",
    "        if img.shape[:2] != (512, 512):\n",
    "            img = cv2.resize(img.astype(np.uint8), (512, 512)).astype(np.float32)\n",
    "            mask = cv2.resize(mask, (512, 512))\n",
    "\n",
    "        input_image = np.expand_dims(img, axis=0)\n",
    "        return input_image, mask\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test whether the data loader returns the expected output, we show an image and a mask. The image and the mask are returned by the dataloader, after resizing and preprocessing. Since this dataset contains a lot of slices without kidneys, we select a slice that contains at least 5000 kidney pixels to verify that the annotations look correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = KitsDataset(BASEDIR)\n",
    "# Find a slice that contains kidney annotations\n",
    "# item[0] is the annotation: (id, annotation_data)\n",
    "image_data, mask = next(item for item in dataset if np.count_nonzero(item[1]) > 5000)\n",
    "# Remove extra image dimension and rotate and flip the image for visualization\n",
    "image = rotate_and_flip(image_data.squeeze())\n",
    "\n",
    "# The data loader returns annotations as (index, mask) and mask in shape (H,W)\n",
    "mask = rotate_and_flip(mask)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].imshow(image, cmap=\"gray\")\n",
    "ax[1].imshow(mask, cmap=\"gray\");"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metric [$\\Uparrow$](#Table-of-content:)\n",
    "Define a metric to determine the performance of the model.\n",
    "\n",
    "For this demo, we use the [F1 score](https://en.wikipedia.org/wiki/F-score), or Dice coefficient, from the [TorchMetrics](https://torchmetrics.readthedocs.io/en/stable/) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_f1(model: Union[torch.nn.Module, ov.CompiledModel], dataset: KitsDataset):\n",
    "    \"\"\"\n",
    "    Compute binary F1 score of `model` on `dataset`\n",
    "    F1 score metric is provided by the torchmetrics library\n",
    "    `model` is expected to be a binary segmentation model, images in the\n",
    "    dataset are expected in (N,C,H,W) format where N==C==1\n",
    "    \"\"\"\n",
    "    metric = F1(ignore_index=0, task=\"binary\", average=\"macro\")\n",
    "    with torch.no_grad():\n",
    "        for image, target in dataset:\n",
    "            input_image = torch.as_tensor(image).unsqueeze(0)\n",
    "            if isinstance(model, ov.CompiledModel):\n",
    "                output_layer = model.output(0)\n",
    "                output = model(input_image)[output_layer]\n",
    "                output = torch.from_numpy(output)\n",
    "            else:\n",
    "                output = model(input_image)\n",
    "            label = torch.as_tensor(target.squeeze()).long()\n",
    "            prediction = torch.sigmoid(output.squeeze()).round().long()\n",
    "            metric.update(label.flatten(), prediction.flatten())\n",
    "    return metric.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Quantization [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "Before quantizing the model, we compute the F1 score on the `FP32` model, for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fp32_f1 = compute_f1(model, dataset)\n",
    "print(f\"FP32 F1: {fp32_f1:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We convert the PyTorch model to OpenVINO IR and serialize it for comparing the performance of the `FP32` and `INT8` model later in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp32_ir_path = MODEL_DIR / Path('unet_kits19_fp32.xml')\n",
    "\n",
    "fp32_ir_model = ov.convert_model(model, example_input=torch.ones(1, 1, 512, 512, dtype=torch.float32))\n",
    "ov.save_model(fp32_ir_model, str(fp32_ir_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-24T14:05:30.690144Z",
     "iopub.status.busy": "2022-05-24T14:05:30.690006Z",
     "iopub.status.idle": "2022-05-24T14:05:30.692969Z",
     "shell.execute_reply": "2022-05-24T14:05:30.692533Z",
     "shell.execute_reply.started": "2022-05-24T14:05:30.690133Z"
    },
    "tags": []
   },
   "source": [
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "\n",
    "> **Note**: NNCF Post-training Quantization is available in OpenVINO 2023.0 release.\n",
    "\n",
    "Create a quantized model from the pre-trained `FP32` model and the calibration dataset. The optimization process contains the following steps:\n",
    "\n",
    "    1. Create a Dataset for quantization.\n",
    "    2. Run `nncf.quantize` for getting an optimized model.\n",
    "    3. Export the quantized model to ONNX and then convert to OpenVINO IR model.\n",
    "    4. Serialize the INT8 model using `ov.save_model` function for benchmarking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def transform_fn(data_item):\n",
    "    \"\"\"\n",
    "    Extract the model's input from the data item.\n",
    "    The data item here is the data item that is returned from the data source per iteration.\n",
    "    This function should be passed when the data item cannot be used as model's input.\n",
    "    \"\"\"\n",
    "    images, _ = data_item\n",
    "    return images\n",
    "\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset)\n",
    "calibration_dataset = nncf.Dataset(data_loader, transform_fn)\n",
    "quantized_model = nncf.quantize(\n",
    "    model,\n",
    "    calibration_dataset,\n",
    "    # Do not quantize LeakyReLU activations to allow the INT8 model to run on Intel GPU\n",
    "    ignored_scope=nncf.IgnoredScope(patterns=[\".*LeakyReLU.*\"])\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the quantized model to ONNX and then convert it to OpenVINO IR model and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 1, 512, 512)\n",
    "int8_onnx_path = MODEL_DIR / \"unet_kits19_int8.onnx\"\n",
    "int8_ir_path = Path(int8_onnx_path).with_suffix(\".xml\")\n",
    "torch.onnx.export(quantized_model, dummy_input, int8_onnx_path)\n",
    "int8_ir_model = ov.convert_model(int8_onnx_path)\n",
    "ov.save_model(int8_ir_model, str(int8_ir_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook demonstrates post-training quantization with NNCF.\n",
    "\n",
    "NNCF also supports quantization-aware training, and other algorithms than quantization. See the [NNCF documentation](https://github.com/openvinotoolkit/nncf/) in the NNCF repository for more information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Compare FP32 and INT8 Model [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "### Compare File Size [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fp32_ir_model_size = fp32_ir_path.with_suffix(\".bin\").stat().st_size / 1024\n",
    "quantized_model_size = int8_ir_path.with_suffix(\".bin\").stat().st_size / 1024\n",
    "\n",
    "print(f\"FP32 IR model size: {fp32_ir_model_size:.2f} KB\")\n",
    "print(f\"INT8 model size: {quantized_model_size:.2f} KB\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Metrics for the original model and the quantized model to be sure that there no degradation. [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "\n",
    "int8_compiled_model = core.compile_model(int8_ir_model)\n",
    "int8_f1 = compute_f1(int8_compiled_model, dataset)\n",
    "\n",
    "print(f\"FP32 F1: {fp32_f1:.3f}\")\n",
    "print(f\"INT8 F1: {int8_f1:.3f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Compare Performance of the FP32 IR Model and Quantized Models [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "To measure the inference performance of the `FP32` and `INT8` models, we use [Benchmark Tool](https://docs.openvino.ai/2023.0/openvino_inference_engine_tools_benchmark_tool_README.html) - OpenVINO's inference performance measurement tool. Benchmark tool is a command line application, part of OpenVINO development tools, that can be run in the notebook with `! benchmark_app` or `%sx benchmark_app`.\n",
    "\n",
    "> **NOTE**: For the most accurate performance estimation, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ! benchmark_app --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"CPU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [],
    "test_replace": {
     "-t 15": "-t 3"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark FP32 model\n",
    "! benchmark_app -m $fp32_ir_path -d $device -t 15 -api sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": [],
    "test_replace": {
     "-t 15": "-t 3"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark INT8 model\n",
    "! benchmark_app -m $int8_ir_path -d $device -t 15 -api sync"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visually Compare Inference Results [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "Visualize the results of the model on four slices of the validation set. Compare the results of the `FP32` IR model with the results of the quantized `INT8` model and the reference segmentation annotation.\n",
    "\n",
    "Medical imaging datasets tend to be very imbalanced: most of the slices in a CT scan do not contain kidney data. The segmentation model should be good at finding kidneys where they exist (in medical terms: have good sensitivity) but also not find spurious kidneys that do not exist (have good specificity). In the next cell, there are four slices: two slices that have no kidney data, and two slices that contain kidney data. For this example, a slice has kidney data if at least 50 pixels in the slices are annotated as kidney.\n",
    "\n",
    "Run this cell again to show results on a different subset. The random seed is displayed to enable reproducing specific runs of this cell.\n",
    "\n",
    "> **NOTE**: the images are shown after optional augmenting and resizing. In the Kits19 dataset all but one of the cases has the `(512, 512)` input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The sigmoid function is used to transform the result of the network\n",
    "# to binary segmentation masks\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))\n",
    "\n",
    "\n",
    "num_images = 4\n",
    "colormap = \"gray\"\n",
    "\n",
    "# Load FP32 and INT8 models\n",
    "core = ov.Core()\n",
    "fp_model = core.read_model(fp32_ir_path)\n",
    "int8_model = core.read_model(int8_ir_path)\n",
    "compiled_model_fp = core.compile_model(fp_model, device_name=\"CPU\")\n",
    "compiled_model_int8 = core.compile_model(int8_model, device_name=\"CPU\")\n",
    "output_layer_fp = compiled_model_fp.output(0)\n",
    "output_layer_int8 = compiled_model_int8.output(0)\n",
    "\n",
    "# Create subset of dataset\n",
    "background_slices = (item for item in dataset if np.count_nonzero(item[1]) == 0)\n",
    "kidney_slices = (item for item in dataset if np.count_nonzero(item[1]) > 50)\n",
    "data_subset = random.sample(list(background_slices), 2) + random.sample(list(kidney_slices), 2)\n",
    "\n",
    "# Set seed to current time. To reproduce specific results, copy the printed seed\n",
    "# and manually set `seed` to that value.\n",
    "seed = int(time.time())\n",
    "random.seed(seed)\n",
    "print(f\"Visualizing results with seed {seed}\")\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_images, ncols=4, figsize=(24, num_images * 4))\n",
    "for i, (image, mask) in enumerate(data_subset):\n",
    "    display_image = rotate_and_flip(image.squeeze())\n",
    "    target_mask = rotate_and_flip(mask).astype(np.uint8)\n",
    "    # Add batch dimension to image and do inference on FP and INT8 models\n",
    "    input_image = np.expand_dims(image, 0)\n",
    "    res_fp = compiled_model_fp([input_image])\n",
    "    res_int8 = compiled_model_int8([input_image])\n",
    "\n",
    "    # Process inference outputs and convert to binary segementation masks\n",
    "    result_mask_fp = sigmoid(res_fp[output_layer_fp]).squeeze().round().astype(np.uint8)\n",
    "    result_mask_int8 = sigmoid(res_int8[output_layer_int8]).squeeze().round().astype(np.uint8)\n",
    "    result_mask_fp = rotate_and_flip(result_mask_fp)\n",
    "    result_mask_int8 = rotate_and_flip(result_mask_int8)\n",
    "\n",
    "    # Display images, annotations, FP32 result and INT8 result\n",
    "    ax[i, 0].imshow(display_image, cmap=colormap)\n",
    "    ax[i, 1].imshow(target_mask, cmap=colormap)\n",
    "    ax[i, 2].imshow(result_mask_fp, cmap=colormap)\n",
    "    ax[i, 3].imshow(result_mask_int8, cmap=colormap)\n",
    "    ax[i, 2].set_title(\"Prediction on FP32 model\")\n",
    "    ax[i, 3].set_title(\"Prediction on INT8 model\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Show Live Inference [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "To show live inference on the model in the notebook, we will use the asynchronous processing feature of OpenVINO.\n",
    "\n",
    "We use the `show_live_inference` function from [Notebook Utils](../utils/notebook_utils.ipynb) to show live inference. This function uses [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)'s Async Pipeline and Model API to perform asynchronous inference. After inference on the specified CT scan has completed, the total time and throughput (fps), including preprocessing and displaying, will be printed.\n",
    "\n",
    "> **NOTE**: If you experience flickering on Firefox, consider using Chrome or Edge to run this notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model and List of Image Files [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "We load the segmentation model to OpenVINO Runtime with `SegmentationModel`, based on the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) Model API. This model implementation includes pre and post processing for the model. For `SegmentationModel`, this includes the code to create an overlay of the segmentation mask on the original image/frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "122wcKhzXn3z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "CASE = 117\n",
    "\n",
    "segmentation_model = SegmentationModel(\n",
    "    ie=core, model_path=int8_ir_path, sigmoid=True, rotate_and_flip=True\n",
    ")\n",
    "case_path = BASEDIR / f\"case_{CASE:05d}\"\n",
    "image_paths = sorted(case_path.glob(\"imaging_frames/*jpg\"))\n",
    "print(f\"{case_path.name}, {len(image_paths)} images\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Show Inference [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "In the next cell, we run the `show_live_inference` function, which loads the `segmentation_model` to the specified `device` (using caching for faster model loading on GPU devices), loads the images, performs inference, and displays the results on the frames loaded in `images` in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": [],
    "test_replace": {
     "=image_paths": "=image_paths[:5]"
    }
   },
   "outputs": [],
   "source": [
    "# Possible options for device include \"CPU\", \"GPU\", \"AUTO\", \"MULTI:CPU,GPU\"\n",
    "device = \"CPU\"\n",
    "reader = LoadImage(image_only=True, dtype=np.uint8)\n",
    "show_live_inference(\n",
    "    ie=core, image_paths=image_paths, model=segmentation_model, device=device, reader=reader\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "**OpenVINO**\n",
    "- [NNCF Repository](https://github.com/openvinotoolkit/nncf/)\n",
    "- [Neural Network Compression Framework for fast model inference](https://arxiv.org/abs/2002.08679)\n",
    "- [OpenVINO API Tutorial](../002-openvino-api/002-openvino-api.ipynb)\n",
    "- [OpenVINO PyPI (pip install openvino-dev)](https://pypi.org/project/openvino-dev/)\n",
    "\n",
    "**Kits19 Data**\n",
    "  - [Kits19 Challenge Homepage](https://kits19.grand-challenge.org/)\n",
    "  - [Kits19 GitHub Repository](https://github.com/neheller/kits19)\n",
    "  - [The KiTS19 Challenge Data: 300 Kidney Tumor Cases with Clinical Context, CT Semantic Segmentations, and Surgical Outcomes](https://arxiv.org/abs/1904.00445)\n",
    "  - [The state of the art in kidney and kidney tumor segmentation in contrast-enhanced CT imaging: Results of the KiTS19 challenge](https://www.sciencedirect.com/science/article/pii/S1361841520301857)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
