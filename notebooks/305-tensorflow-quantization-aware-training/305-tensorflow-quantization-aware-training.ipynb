{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f855a151",
   "metadata": {},
   "source": [
    "# Quantization Aware Training with NNCF, using TensorFlow framework\n",
    "\n",
    "The goal of this notebook to demonstrate how to use the Neural Network Compression Framework [NNCF](https://github.com/openvinotoolkit/nncf) 8-bit quantization to optimize a TensorFlow model for inference with OpenVINO Toolkit. The optimization process contains the following steps:\n",
    "* Fine-tuning of FP32 model\n",
    "* Transform the original FP32 model to INT8\n",
    "* Use fine-tuning to restore the accuracy\n",
    "* Export optimized and original models to Frozen Graph and then to OpenVINO\n",
    "* Measure and compare the performance of models\n",
    "\n",
    "For more advanced usage, please refer to these [examples](https://github.com/openvinotoolkit/nncf/tree/develop/examples).\n",
    "\n",
    "We selected the ResNet-18 model with Imagenette dataset. Imagenette is a subset of 10 easily classified classes from the Imagenet dataset. Using the smaller model and dataset will speed up training and download time. To see other Keras models, visit [tf.keras.applications](https://www.tensorflow.org/api_docs/python/tf/keras/applications#functions) module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd1b744",
   "metadata": {},
   "source": [
    "## Imports and Settings\n",
    "\n",
    "Import NNCF and all auxiliary packages from your Python* code.\n",
    "Set a name for the model, input image size, used batch size, and the learning rate. Also define paths where Frozen Graph and OpenVINO IR versions of the models will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e95247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\n",
    "from tensorflow.python.keras import backend\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras import models\n",
    "\n",
    "from nncf import NNCFConfig\n",
    "from nncf.tensorflow.helpers.model_creation import create_compressed_model\n",
    "from nncf.tensorflow.initialization import register_default_init_args\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "BASE_MODEL_NAME = \"ResNet-18\"\n",
    "\n",
    "fp32_h5_path = Path(MODEL_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".h5\")\n",
    "fp32_pb_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".pb\")\n",
    "fp32_pb_name = Path(BASE_MODEL_NAME + \"_fp32\").with_suffix(\".pb\")\n",
    "fp32_ir_path = fp32_pb_path.with_suffix(\".xml\")\n",
    "int8_pb_path = Path(OUTPUT_DIR / (BASE_MODEL_NAME + \"_int8\")).with_suffix(\".pb\")\n",
    "int8_pb_name = Path(BASE_MODEL_NAME + \"_int8\").with_suffix(\".pb\")\n",
    "int8_ir_path = int8_pb_path.with_suffix(\".xml\")\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = (224, 224)  # Default Imagenet image size\n",
    "NUM_CLASSES = 10  # For Imagenette dataset\n",
    "\n",
    "LR = 1e-5\n",
    "\n",
    "MEAN_RGB = (0.485 * 255, 0.456 * 255, 0.406 * 255)\n",
    "STDDEV_RGB = (0.229 * 255, 0.224 * 255, 0.225 * 255)\n",
    "\n",
    "fp32_pth_url = \"https://storage.openvinotoolkit.org/repositories/nncf/openvino_notebook_ckpts/305_resnet18_imagenette_fp32.h5\"\n",
    "_ = tf.keras.utils.get_file(fp32_h5_path.resolve(), fp32_pth_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f178c3c1",
   "metadata": {},
   "source": [
    "## Dataset preprocessing\n",
    "\n",
    "Download and prepare Imagenette 160px dataset.\n",
    "- Number of classes: 10\n",
    "- Download size: 94.18 MiB\n",
    "| Split        | Examples |\n",
    "|--------------|----------|\n",
    "| 'train'      | 12,894   |\n",
    "| 'validation' | 500      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3b1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets, datasets_info = tfds.load('imagenette/320px', shuffle_files=True, as_supervised=True, with_info=True)\n",
    "train_dataset, validation_dataset = datasets['train'], datasets['validation']\n",
    "fig = tfds.show_examples(train_dataset, datasets_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e18adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(image, label):\n",
    "    image = tf.image.resize(image, IMG_SIZE)\n",
    "    image = image - MEAN_RGB\n",
    "    image = image / STDDEV_RGB\n",
    "    label = tf.one_hot(label, NUM_CLASSES)\n",
    "    return image, label\n",
    "\n",
    "\n",
    "train_dataset = (train_dataset.map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)  # .shuffle(datasets_info.splits['train'].num_examples)\n",
    "                              .batch(BATCH_SIZE)\n",
    "                              .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "                )\n",
    "\n",
    "validation_dataset = (validation_dataset.map(preprocessing, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "                                        .batch(BATCH_SIZE)\n",
    "                                        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e0fdb",
   "metadata": {},
   "source": [
    "## Define a floating-point model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525431a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_block_names(stage, block):\n",
    "    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n",
    "    conv_name = name_base + 'conv'\n",
    "    bn_name = name_base + 'bn'\n",
    "    relu_name = name_base + 'relu'\n",
    "    sc_name = name_base + 'sc'\n",
    "    return conv_name, bn_name, relu_name, sc_name\n",
    "\n",
    "\n",
    "def get_conv_params(**params):\n",
    "    default_conv_params = {\n",
    "        'kernel_initializer': 'he_uniform',\n",
    "        'use_bias': False,\n",
    "        'padding': 'valid',\n",
    "    }\n",
    "    default_conv_params.update(params)\n",
    "    return default_conv_params\n",
    "\n",
    "\n",
    "def get_bn_params(**params):\n",
    "    axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    default_bn_params = {\n",
    "        'axis': axis,\n",
    "        'momentum': 0.99,\n",
    "        'epsilon': 2e-5,\n",
    "        'center': True,\n",
    "        'scale': True,\n",
    "    }\n",
    "    default_bn_params.update(params)\n",
    "    return default_bn_params\n",
    "\n",
    "\n",
    "def residual_conv_block(filters, stage, block, strides=(1, 1), attention=None, cut='pre'):\n",
    "    \"\"\"The identity block is the block that has no conv layer at shortcut.\n",
    "    # Arguments\n",
    "        input_tensor: input tensor\n",
    "        kernel_size: default 3, the kernel size of\n",
    "            middle conv layer at main path\n",
    "        filters: list of integers, the filters of 3 conv layer at main path\n",
    "        stage: integer, current stage label, used for generating layer names\n",
    "        block: 'a','b'..., current block label, used for generating layer names\n",
    "        cut: one of 'pre', 'post'. used to decide where skip connection is taken\n",
    "    # Returns\n",
    "        Output tensor for the block.\n",
    "    \"\"\"\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        # get params and names of layers\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = layers.BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = layers.Activation('relu', name=relu_name + '1')(x)\n",
    "\n",
    "        # defining shortcut connection\n",
    "        if cut == 'pre':\n",
    "            shortcut = input_tensor\n",
    "        elif cut == 'post':\n",
    "            shortcut = layers.Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(x)\n",
    "        else:\n",
    "            raise ValueError('Cut type not in [\"pre\", \"post\"]')\n",
    "\n",
    "        # continue with convolution layers\n",
    "        x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = layers.Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = layers.BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = layers.Activation('relu', name=relu_name + '2')(x)\n",
    "        x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = layers.Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        # use attention block if defined\n",
    "        if attention is not None:\n",
    "            x = attention(x)\n",
    "\n",
    "        # add residual connection\n",
    "        x = layers.Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def ResNet(repetitions, residual_block, attention, input_shape=None, include_top=True,\n",
    "           classes=1000):\n",
    "    \"\"\"Instantiates the ResNet architecture.\n",
    "    Args:\n",
    "        input_shape: shape of the input image.\n",
    "            It should have exactly 3 inputs channels.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True.\n",
    "    Returns:\n",
    "        A Keras model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    img_input = layers.Input(shape=input_shape, name='data')\n",
    "\n",
    "    # choose residual block type\n",
    "    ResidualBlock = residual_block\n",
    "    if attention:\n",
    "        Attention = attention(**kwargs)\n",
    "    else:\n",
    "        Attention = None\n",
    "\n",
    "    # get parameters for model layers\n",
    "    no_scale_bn_params = get_bn_params(scale=False)\n",
    "    bn_params = get_bn_params()\n",
    "    conv_params = get_conv_params()\n",
    "    init_filters = 64\n",
    "\n",
    "    # resnet bottom\n",
    "    x = layers.BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n",
    "    x = layers.ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n",
    "    x = layers.BatchNormalization(name='bn0', **bn_params)(x)\n",
    "    x = layers.Activation('relu', name='relu0')(x)\n",
    "    x = layers.ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = layers.MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n",
    "\n",
    "    # resnet body\n",
    "    for stage, rep in enumerate(repetitions):\n",
    "        for block in range(rep):\n",
    "\n",
    "            filters = init_filters * (2 ** stage)\n",
    "\n",
    "            # first block of first stage without strides because we have maxpooling before\n",
    "            if block == 0 and stage == 0:\n",
    "                x = ResidualBlock(filters, stage, block, strides=(1, 1),\n",
    "                                  cut='post', attention=Attention)(x)\n",
    "\n",
    "            elif block == 0:\n",
    "                x = ResidualBlock(filters, stage, block, strides=(2, 2),\n",
    "                                  cut='post', attention=Attention)(x)\n",
    "\n",
    "            else:\n",
    "                x = ResidualBlock(filters, stage, block, strides=(1, 1),\n",
    "                                  cut='pre', attention=Attention)(x)\n",
    "\n",
    "    x = layers.BatchNormalization(name='bn1', **bn_params)(x)\n",
    "    x = layers.Activation('relu', name='relu1')(x)\n",
    "\n",
    "    # resnet top\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='pool1')(x)\n",
    "        x = layers.Dense(classes, name='fc1')(x)\n",
    "        x = layers.Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(img_input, x)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def ResNet18(input_shape=None, classes=1000, include_top=True, **kwargs):\n",
    "    return ResNet(\n",
    "        repetitions=(2, 2, 2, 2),\n",
    "        residual_block=residual_conv_block,\n",
    "        attention=None,\n",
    "        input_shape=input_shape,\n",
    "        include_top=include_top,\n",
    "        classes=classes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a466089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "model = ResNet18(input_shape=IMG_SHAPE, classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fcd3fc",
   "metadata": {},
   "source": [
    "## Pre-train floating-point model\n",
    "\n",
    "Using NNCF for model compression assumes that the user has a pre-trained model and a training pipeline.\n",
    "\n",
    "Here we demonstrate one possible training pipeline: ResNet-18 model pre-trained on 1000 classes from ImageNet and fine-tuned with 10 classes from Imagenette.\n",
    "\n",
    "Subsequently, the training and validation functions will be reused for quantization-aware training.\n",
    "\n",
    "> **NOTE** By default we propose to use the pretrained model weights (the link is provided above). Otherwise, the model is not tuned till the final accuracy. For the sake of simplicity of the tutorial, we propose to tune for 10 epochs only and take the last model state for quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d92850f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if fp32_h5_path.exists():\n",
    "    model.load_weights(fp32_h5_path)\n",
    "    \n",
    "    # Compile the floating-point model\n",
    "    model.compile(metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc@1')])\n",
    "else:\n",
    "    # Compile the floating-point model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=LR),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "                  metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc@1')])\n",
    "\n",
    "    # Train the floating-point model\n",
    "    model.fit(train_dataset,\n",
    "              epochs=10,\n",
    "              validation_data=validation_dataset)\n",
    "\n",
    "# Validate the floating-point model\n",
    "test_loss, test_acc = model.evaluate(validation_dataset)\n",
    "print(f\"\\nAccuracy of FP32 model: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f67d6",
   "metadata": {},
   "source": [
    "Save the floating-point model to the frozen graph, which will be later used for conversion to OpenVINO IR and further performance measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450cbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_to_frozen_graph(model, output_dir, model_name):\n",
    "    # Convert Keras model to the frozen graph.\n",
    "    input_signature = []\n",
    "    for item in model.inputs:\n",
    "        input_signature.append(tf.TensorSpec(item.shape, item.dtype, item.name))\n",
    "    concrete_function = tf.function(model).get_concrete_function(input_signature)\n",
    "    frozen_func = convert_variables_to_constants_v2(concrete_function, lower_control_flow=False)\n",
    "    frozen_graph = frozen_func.graph.as_graph_def(add_shapes=True)\n",
    "    \n",
    "    tf.io.write_graph(frozen_graph, output_dir, model_name, as_text=False)\n",
    "\n",
    "\n",
    "export_model_to_frozen_graph(model, OUTPUT_DIR, fp32_pb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b81167",
   "metadata": {},
   "source": [
    "## Create and initialize quantization\n",
    "\n",
    "NNCF enables compression-aware training by integrating into regular training pipelines. The framework is designed so that modifications to your original training code are minor. Quantization is the simplest scenario and requires only 3 modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b74e77",
   "metadata": {},
   "source": [
    "1. Configure NNCF parameters to specify compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cafb190",
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf_config_dict = {\n",
    "    \"input_info\": {\"sample_size\": [1, 3] + list(IMG_SIZE)},\n",
    "    \"log_dir\": str(OUTPUT_DIR),  # log directory for NNCF-specific logging outputs\n",
    "    \"compression\": {\n",
    "        \"algorithm\": \"quantization\",  # specify the algorithm here\n",
    "        \"initializer\": {\n",
    "            \"batchnorm_adaptation\": {\n",
    "                \"num_bn_adaptation_samples\": 2048  # change the default number of samples for BatchNormalization adaptation\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "nncf_config = NNCFConfig.from_dict(nncf_config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb9e4d",
   "metadata": {},
   "source": [
    "2. Provide data loader to initialize the values of quantization ranges and determine which activation should be signed or unsigned from the collected statistics using a given number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d541d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf_config = register_default_init_args(nncf_config=nncf_config,\n",
    "                                         data_loader=train_dataset,\n",
    "                                         batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc64e3e",
   "metadata": {},
   "source": [
    "3. Create a wrapped model ready for compression fine-tuning from a pre-trained FP32 model and configuration object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c9021c",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_ctrl, model = create_compressed_model(model, nncf_config, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffadfd04",
   "metadata": {},
   "source": [
    "Evaluate the new model on the validation set after initialization of quantization. The accuracy should be not far from the accuracy of the floating-point FP32 model for a simple case like the one we are demonstrating now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aa6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the int8 model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=LR),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
    "              metrics=[tf.keras.metrics.CategoricalAccuracy(name='acc@1')])\n",
    "\n",
    "# Validate the int8 model\n",
    "test_loss, test_acc = model.evaluate(validation_dataset)\n",
    "print(f\"\\nAccuracy of INT8 model after initialization: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d0b8f8",
   "metadata": {},
   "source": [
    "## Fine-tune the compressed model\n",
    "\n",
    "At this step, a regular fine-tuning process is applied to restore accuracy drop. Normally, several epochs of tuning are required with a small learning rate, the same that is usually used at the end of the training of the original model. No other changes in the training pipeline are required. Here is a simple example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4047d88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the int8 model\n",
    "model.fit(train_dataset,\n",
    "          epochs=1)\n",
    "\n",
    "# Validate the int8 model\n",
    "test_loss, test_acc = model.evaluate(validation_dataset)\n",
    "print(f\"\\nAccuracy of INT8 model after fine-tuning: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af453ef",
   "metadata": {},
   "source": [
    "Save the INT8 model to the frozen graph, which will be later used for conversion to OpenVINO IR and further performance measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b208b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_model_to_frozen_graph(model, OUTPUT_DIR, int8_pb_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73345cd0",
   "metadata": {},
   "source": [
    "## Export Frozen Graph models to OpenVINO™ Intermediate Representation (IR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248a563",
   "metadata": {},
   "source": [
    "Call the OpenVINO Model Optimizer tool to convert the Frozen Graph model to OpenVINO IR. The models are saved to the current directory.\n",
    "\n",
    "See the [Model Optimizer Developer Guide](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information about Model Optimizer.\n",
    "\n",
    "Executing this command may take a while. There may be some errors or warnings in the output. Model Optimization successfully export to IR if the last lines of the output include: `[ SUCCESS ] Generated IR version 10 model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fda382",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fp32_ir_path.exists():\n",
    "    !mo  --framework=tf --reverse_input_channels --input_shape=[1,224,224,3] --input=Placeholder --input_model=$fp32_pb_path --output_dir=$OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adccc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not int8_ir_path.exists():\n",
    "    !mo  --framework=tf --reverse_input_channels --input_shape=[1,224,224,3] --input=Placeholder --input_model=$int8_pb_path --output_dir=$OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d3bbaf",
   "metadata": {},
   "source": [
    "## Benchmark model performance by computing inference time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f048b410",
   "metadata": {},
   "source": [
    "Finally, we will measure the inference performance of the FP32 and INT8 models. To do this, we use [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - OpenVINO's inference performance measurement tool. By default, Benchmark Tool runs inference for 60 seconds in asynchronous mode on CPU. It returns inference speed as latency (milliseconds per image) and throughput (frames per second) values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304a76c9",
   "metadata": {},
   "source": [
    "> **NOTE**: In this notebook we run benchmark_app for 15 seconds to give a quick indication of performance. For more accurate performance, we recommended running benchmark_app in a terminal/command prompt after closing other applications. Run benchmark_app -m model.xml -d CPU to benchmark async inference on CPU for one minute. Change CPU to GPU to benchmark on GPU. Run benchmark_app --help to see an overview of all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63355744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_benchmark_output(benchmark_output):\n",
    "    parsed_output = [line for line in benchmark_output if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "    print(*parsed_output, sep='\\n')\n",
    "\n",
    "\n",
    "print('Benchmark FP32 model (IR)')\n",
    "benchmark_output = ! benchmark_app -m $fp32_ir_path -d CPU -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)\n",
    "\n",
    "print('\\nBenchmark INT8 model (IR)')\n",
    "benchmark_output = ! benchmark_app -m $int8_ir_path -d CPU -api async -t 15\n",
    "parse_benchmark_output(benchmark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b19e9",
   "metadata": {},
   "source": [
    "Show CPU Information for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9c892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import cpuinfo\n",
    "\n",
    "    print(cpuinfo.get_cpu_info()[\"brand_raw\"])\n",
    "except Exception:\n",
    "    # OpenVINO installs cpuinfo, but if a different version is installed\n",
    "    # the command above may not work\n",
    "    print(platform.processor())"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Отсутствует",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
