{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iaTYXPizWvWj"
      },
      "source": [
        "#**Image colorization with OpenVINOâ„¢**\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "d1OcAsloYgib"
      },
      "source": [
        "Colorizer model is a type of artificial intelligence model that is designed to automatically add colors to black and white images. This technology uses deep learning algorithms to analyze the content of the image and infer the most likely colors for each pixel based on its surrounding context. By doing so, colorizer models can quickly and accurately colorize large volumes of images, which can be useful in a variety of applications such as digital restoration of old photographs\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "W6BZGXyEd1tM"
      },
      "source": [
        "A deep learning approach called \"Colorful Colorization Network\" (CCN), which takes a grayscale image as input and produces a colorized image as output. The CCN is trained using a large dataset of color images and their corresponding grayscale versions\n",
        "\n",
        "For more details on this can be found [paper](https://arxiv.org/pdf/1603.08511.pdf) and [repository](https://github.com/richzhang/colorization)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qRHKfkc-eZoU"
      },
      "source": [
        "This model is very usefull for Digital restoration of old photographs, Many old photographs only exist in grayscale, and colorization using traditional methods can be time-consuming and expensive. The deep learning approach proposed in this paper could provide a more efficient and cost-effective method for colorizing these photographs.\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_nid2x-oejeo"
      },
      "source": [
        "This tutorial demonstrates step by step instructions on how to run Colorizer with OpenVino\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6fpqpZjsy0rj"
      },
      "source": [
        "#1) Prerequisites\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bojbeEfhyuWO",
        "outputId": "f58788bb-0ab5-4454-f0dd-ebfcbc4793a4"
      },
      "outputs": [],
      "source": [
        "!pip install onnx\n",
        "!pip install onnxsim\n",
        "!pip install onnxruntime\n",
        "!pip install openvino\n",
        "!pip install openvino-dev[all]"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "IRXZsqFAixj-"
      },
      "source": [
        "##Imports and Settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9FFFnLqlf5H"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage import color\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from IPython import embed\n",
        "import colorizers\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from typing import Union\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4s-MA_GCnVO"
      },
      "outputs": [],
      "source": [
        "MODEL_OUTPUT_DIR=Path(\"/content/\")\n",
        "BASE_MODEL_NAME='colorizer_model'\n",
        "\n",
        "fp32_onnx_path = Path(MODEL_OUTPUT_DIR / (BASE_MODEL_NAME + \"_fp32\")).with_suffix(\".onnx\")\n",
        "fp32_ir_path = fp32_onnx_path.with_suffix(\".xml\")\n",
        "fp32_bin_path=fp32_onnx_path.with_suffix(\".bin\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RjXMk5L2g_i"
      },
      "source": [
        "#2) Check model inference\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "mL9rFDxz2l8a"
      },
      "source": [
        "##2.1) Get the Models\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7VAxHE0WflPu"
      },
      "source": [
        "**Each conv layer refers to a block of 2 or 3 repeated\n",
        "conv and ReLU layers, followed by a BatchNorm layer. The net has no pool layers.\n",
        "All changes in resolution are achieved through spatial downsampling or upsampling\n",
        "between conv blocks.**\n",
        "\n",
        "![model struct](http://richzhang.github.io/colorization/resources/images/net_diagram.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "0997c255be4c4a148a116af50be52129",
            "a56be011b2e84b62aa4059f91f63cb8e",
            "988670e07a4b46618840cb9a1c8694df",
            "586c07f0fc6b4ff2a4d092bfac72fe70",
            "f39478e9b33746f38c9ff3b84f03bf01",
            "0db5aadf2ffc434da41d7c7fb710810b",
            "c3465eb24e304c4f968bf2f804476630",
            "f28a82d1a6f84ab798951ac4af2717dc",
            "4b10237ef49248728a3b4483f614da85",
            "ec5285a0d50943c0b1ea7d06c263910f",
            "b1a3ba24c60c4ffc867358c56fa31b14",
            "48328e5a95eb400caf30105eb14fca58",
            "d131b6f65b724ac9b6abc66a6dc76b87",
            "f6a8ae609a4941a596853f327ff0785f",
            "cb137d9c611841919c3b0b90f719936d",
            "39c75dec5e8b401d8aeb96e22b86d7a6",
            "3e69f486d4fa4f89b3d28a3dd83a867e",
            "d2ea28994bdc42f78537d127b130d191",
            "968751d75e64484488d08de20f42f4f7",
            "71af891fa7ca440ea4ff06f304b0448c",
            "a0db83b8add648c4ad92c09179afdcf0",
            "4991b17a8d1243dc8cf21b79026aef65"
          ]
        },
        "id": "J0gryhexiwsI",
        "outputId": "9c70db43-d780-4615-cea2-e9a7aff20d2a"
      },
      "outputs": [],
      "source": [
        "COLORIZER_ECCV16=colorizers.eccv16().eval()\n",
        "COLORIZER_SIGGRAPH17 = colorizers.siggraph17().eval()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "G2ngOciI2yBS"
      },
      "source": [
        "##2.2) UTILS\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMjdqCLduXP"
      },
      "source": [
        "**Th `preprocess_img` function is used to convert the RGB image to the Lab color space using the function color.rgb2lab**\n",
        "\n",
        "The Lab color space, also known as CIELAB, is a color space that was designed to be more perceptually uniform than other color spaces such as RGB or CMYK. It is often used in color-related applications such as color correction, color matching, and image processing.\n",
        "\n",
        "The Lab color space is composed of three channels: L, a, and b.\n",
        "\n",
        "1. The L channel represents the lightness or brightness of the color, with values ranging from 0 (black) to 100 (white).\n",
        "2. The a and b channels represent the color opponent dimensions of red-green and blue-yellow, respectively.\n",
        "3. The a channel ranges from -128 (green) to +127 (red), and the b channel ranges from -128 (blue) to +127 (yellow).\n",
        "\n",
        "By separating the lightness and color information, the Lab color space allows for more accurate and consistent color reproduction across different devices and lighting conditions. It is often used as an intermediate color space in color-related workflows, and can be converted to and from other color spaces such as RGB, CMYK, and XYZ.\n",
        "\n",
        "---\n",
        "\n",
        "**The `postprocess_tens` function takes two PyTorch tensors as input**:\n",
        "_tens_orig_l_ and _out_ab_. _tens_orig_l_ is a tensor of size 1 x 1 x H*orig x W_orig containing the L channel of an original image. \\_out_ab* is a tensor of size 1 x 2 x H x W containing the predicted a and b color channels for the same image.\n",
        "\n",
        "The function first checks the dimensions of the two input tensors to see if resizing is needed. If the dimensions of tens_orig_l and out_ab are the same, then no resizing is necessary. Otherwise, the function uses the F.interpolate function from the PyTorch library to resize out_ab to the same dimensions as tens_orig_l.\n",
        "\n",
        "The function then concatenates tens_orig_l and out_ab along the channel dimension to create a 1 x 3 x H_orig x W_orig tensor in the Lab color space. The Lab tensor is converted to RGB using the color.lab2rgb function from the scikit-image package, and the resulting RGB image is returned as a numpy array of shape (H_orig, W_orig, 3).\n",
        "\n",
        "The mode parameter is optional and specifies the interpolation mode to be used if resizing is necessary. The default mode is 'bilinear', which performs bilinear interpolation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DeXg-OrtjALP"
      },
      "outputs": [],
      "source": [
        "def load_img(image_path: str) -> np.ndarray:\n",
        "    \"\"\"Load an image from a file path and return it as a numpy array.\n",
        "    \n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        \n",
        "    Returns:\n",
        "        numpy.ndarray: The image as a numpy array.\n",
        "    \n",
        "    Raises:\n",
        "        FileNotFoundError: If the file specified by image_path does not exist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with Image.open(image_path) as img:\n",
        "            out_np = np.asarray(img)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"File '{image_path}' not found.\")\n",
        "\t\t\n",
        "    if out_np.ndim ==2:\n",
        "        out_np = np.tile(out_np[:,:,None],3)\n",
        "    return out_np\n",
        "\n",
        "def resize_img(img: np.ndarray, HW: tuple[int, int] = (256, 256), resample: int = Image.BICUBIC) -> np.ndarray:\n",
        "    \"\"\"Resize an image to a specified height and width using a specified resampling method.\n",
        "    \n",
        "    Args:\n",
        "        img (numpy.ndarray): The image to be resized.\n",
        "        HW (tuple[int, int]): The desired height and width of the resized image. Defaults to (256, 256).\n",
        "        resample (int): The resampling method to use. Can be one of:\n",
        "            - Image.NEAREST (nearest-neighbor sampling)\n",
        "            - Image.BOX (box sampling)\n",
        "            - Image.BILINEAR (bilinear sampling)\n",
        "            - Image.HAMMING (hamming-windowed sinc interpolation)\n",
        "            - Image.BICUBIC (bicubic interpolation)\n",
        "            - Image.LANCZOS (Lanczos-windowed sinc interpolation)\n",
        "            Defaults to Image.BICUBIC.\n",
        "        \n",
        "    Returns:\n",
        "        numpy.ndarray: The resized image as a numpy array.\n",
        "    \"\"\"\n",
        "    pil_img = Image.fromarray(img)\n",
        "    resized_img = pil_img.resize((HW[1], HW[0]), resample=resample)\n",
        "    return np.asarray(resized_img)\n",
        "\n",
        "def preprocess_img(img_rgb_orig: np.ndarray, HW: tuple[int, int] = (256, 256), resample: int = 3) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "\t# return original size L and resized L as torch Tensors\n",
        "\t''' This function takes an input RGB image, resizes it to a specified height and width using the method resize_img, \n",
        "\tconverts the RGB image to the Lab color space using the function color.rgb2lab, \n",
        "\tand returns two PyTorch tensors: one tensor containing the L channel of the original image, \n",
        "\tand another tensor containing the L channel of the resized image.\n",
        "\t\n",
        "\tThe HW parameter specifies the desired height and width of the resized image\n",
        "\tThe resample parameter determines the resampling method used during the resizing process. \n",
        "\tThe default resampling method used is 3, which corresponds to the bicubic interpolation method. \n",
        "\n",
        "\t    Args:\n",
        "        img_rgb_orig (numpy.ndarray): The input RGB image.\n",
        "        HW (tuple[int, int]): The desired height and width of the resized image. Defaults to (256, 256).\n",
        "        resample (int): The resampling method to use during the resizing process. Can be one of:\n",
        "            - 0 (nearest-neighbor sampling)\n",
        "            - 1 (box sampling)\n",
        "            - 2 (bilinear sampling)\n",
        "            - 3 (bicubic interpolation)\n",
        "            - 4 (hamming-windowed sinc interpolation)\n",
        "            - 5 (Lanczos-windowed sinc interpolation)\n",
        "            Defaults to 3.\n",
        "        \n",
        "    Returns:\n",
        "        tuple[torch.Tensor, torch.Tensor]: A tuple containing two PyTorch tensors:\n",
        "            - The L channel of the original image as a tensor with shape (1, 1, height, width).\n",
        "            - The L channel of the resized image as a tensor with shape (1, 1, resized_height, resized_width).\n",
        "    \"\"\"'''\n",
        "\n",
        "\timg_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n",
        "\t\n",
        "\timg_lab_orig = color.rgb2lab(img_rgb_orig)\n",
        "\timg_lab_rs = color.rgb2lab(img_rgb_rs)\n",
        "\n",
        "\timg_l_orig = img_lab_orig[:,:,0]\n",
        "\timg_l_rs = img_lab_rs[:,:,0]\n",
        "\n",
        "\ttens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n",
        "\ttens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n",
        "\n",
        "\treturn (tens_orig_l, tens_rs_l)\n",
        "\n",
        "def postprocess_tens(tens_orig_l: torch.Tensor, out_ab: torch.Tensor, mode: str = 'bilinear') -> np.ndarray:\n",
        "\t\"\"\"Convert the output of a colorization model back to an RGB image.\n",
        "\n",
        "\tThe color.lab2rgb function maps the Lab color values to their \n",
        "\tcorresponding RGB color values using a transformation that takes into \n",
        "\taccount the nonlinear nature of the Lab color space.\n",
        "\t The output of color.lab2rgb is a numpy array of shape (H, W, 3) \n",
        "\t containing the RGB color image.\n",
        "\n",
        "    Args:\n",
        "        tens_orig_l (torch.Tensor): The L channel of the original image as a PyTorch tensor of shape (1, 1, height, width).\n",
        "        out_ab (torch.Tensor): The predicted a and b channels of the colorized image as a PyTorch tensor of shape (1, 2, height, width).\n",
        "        mode (str): The interpolation mode to use when resizing the predicted a and b channels to match the size of the original L channel. Can be one of:\n",
        "            - 'nearest' (nearest-neighbor interpolation)\n",
        "            - 'bilinear' (bilinear interpolation)\n",
        "            - 'bicubic' (bicubic interpolation)\n",
        "            - 'trilinear' (trilinear interpolation)\n",
        "            - 'area' (pixel area resampling)\n",
        "            Defaults to 'bilinear'.\n",
        "        \n",
        "    Returns:\n",
        "        numpy.ndarray: An RGB image as a numpy array of shape (height, width, 3).\"\"\"\n",
        "\tHW = out_ab.shape[2:]\n",
        "\tHW_orig = tens_orig_l.shape[2:]\n",
        "\n",
        "\tif(HW_orig[0]!=HW[0] or HW_orig[1]!=HW[1]):\n",
        "\t\tout_ab_orig = F.interpolate(out_ab, size=HW_orig, mode='bilinear')\n",
        "\telse:\n",
        "\t\tout_ab_orig = out_ab\n",
        "\n",
        "\tout_lab_orig = torch.cat((tens_orig_l, out_ab_orig), dim=1)\n",
        "\treturn color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))\n",
        " \n",
        "def show_image(image):\n",
        "\tplt.imshow(image)\n",
        "\treturn None"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "DWOnVct_nVhs"
      },
      "source": [
        "##3) Validate original model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "o85o8TEEnqRM"
      },
      "source": [
        "In order to showcase the remarkable capabilities of the algorithm for colorizing grayscale photos, we present a set of examples where the model performs exceptionally well in producing stunningly realistic colorizations.\n",
        "\n",
        "---\n",
        "\n",
        "These examples offer a glimpse into the impressive range of colors and details that the algorithm can bring to otherwise dull and lifeless images. However, if you are interested in exploring the full potential of the model and would like to experiment with the code yourself, we encourage you to visit the website at http://richzhang.github.io/colorization/, where you can find a wealth of additional results and resources to help you get started.![](http://richzhang.github.io/colorization/resources/images/teaser3.jpg)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mxCKz0ROtY_"
      },
      "outputs": [],
      "source": [
        "sys.path.append('/content/drive/MyDrive/GSOC/openvino_notebooks-main/notebooks/utils')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_b4s8NzakgLA"
      },
      "outputs": [],
      "source": [
        "IMAGE_PATH='/content/drive/MyDrive/GSOC/colorization-master/imgs/ansel_adams.jpg'\n",
        "IMAGE = load_img(IMAGE_PATH)\n",
        "(tens_l_orig, tens_l_rs) = preprocess_img(IMAGE, HW=(256,256))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XjCa0xDtcYre"
      },
      "source": [
        "here it takes a grayscale image and applies the two pre-trained deep learning models, COLORIZER_ECCV16 and COLORIZER_SIGGRAPH17, to produce two colorized versions of the image, stored as numpy arrays in out_img_eccv16 and out_img_siggraph17. The postprocess_tens function is used to convert the model output to RGB color images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_RJJkMQJksTv"
      },
      "outputs": [],
      "source": [
        "img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n",
        "out_img_eccv16 = postprocess_tens(tens_l_orig, COLORIZER_ECCV16(tens_l_rs).cpu())\n",
        "out_img_siggraph17 = postprocess_tens(tens_l_orig, COLORIZER_SIGGRAPH17(tens_l_rs).cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ar7xjfadlIm8",
        "outputId": "40b076af-a08f-44bd-b4c4-bbb301f5f819"
      },
      "outputs": [],
      "source": [
        "show_image(out_img_eccv16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "n1sinZ5RlW2G",
        "outputId": "cddad060-05e2-4208-bbec-7a30dbd7fb7b"
      },
      "outputs": [],
      "source": [
        "show_image(out_img_siggraph17)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CgXxuvTA2_qT"
      },
      "source": [
        "#4) Convert PyTorch model to ONNX\n",
        "\n",
        "Converting a PyTorch model to the ONNX format enables the use of the model in a variety of production environments, such as web services and mobile applications. ONNX is an open format that allows interoperability between deep learning frameworks, making it a valuable tool for model portability. The convert_to_onnx function simplifies the process of converting PyTorch models to ONNX by creating a dummy input tensor, exporting the model using torch.onnx.export, and simplifying the resulting model using onnxsim. The resulting ONNX model can be used in a wide range of environments and provides a standardized way to represent deep learning models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hg0C75TAo4mG"
      },
      "outputs": [],
      "source": [
        "import onnxsim\n",
        "import onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8sCveCwkxX3"
      },
      "outputs": [],
      "source": [
        "def convert_to_onnx(model: torch.nn.Module, output_path: str, input_shape: tuple = (256,256)):\n",
        "    '''This function takes a PyTorch model and converts it to the ONNX format. \n",
        "    The resulting ONNX model is saved to the file specified by output_path.\n",
        "    The input shape of the model is specified by input_shape, which is a tuple containing the height and width of the input image. \n",
        "    A dummy input tensor of size 1 x 1 x H x W is created using torch.randn.\n",
        "\n",
        "    The ONNX conversion is performed using torch.onnx.export. The function takes the following arguments:\n",
        "\n",
        "    model: the PyTorch model to convert\n",
        "    dummy_input: a dummy input tensor used to trace the model\n",
        "    output_path: the path where the resulting ONNX model will be saved\n",
        "    verbose: a flag indicating whether to print verbose output during conversion\n",
        "    keep_initializers_as_inputs: a flag indicating whether to treat initializer tensors as inputs\n",
        "    opset_version: the ONNX opset version to use\n",
        "    input_names: a list of input names for the model\n",
        "    output_names: a list of output names for the model\n",
        "    After the ONNX model is created, the function uses the onnxsim package to simplify the model. \n",
        "    The simplified model is then saved to output_path. If the simplification process fails, an error message is printed.\n",
        "\n",
        "        Args:\n",
        "        model (torch.nn.Module): The PyTorch model to convert.\n",
        "        output_path (str): The path to save the resulting ONNX model.\n",
        "        input_shape (tuple, optional): A tuple containing the height and width of the input image. \n",
        "            Defaults to (256, 256).\n",
        "\n",
        "    Raises:\n",
        "        Exception: If the ONNX simplification process fails.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    '''\n",
        "\n",
        "    dummy_input = torch.autograd.Variable(\n",
        "        torch.randn(1, 1, input_shape[0], input_shape[1])\n",
        "    )\n",
        "\n",
        "    torch.onnx.export(\n",
        "        model,\n",
        "        dummy_input,\n",
        "        output_path,\n",
        "        verbose=True,\n",
        "        keep_initializers_as_inputs=True,\n",
        "        opset_version=11,\n",
        "        input_names=[\"data\"],\n",
        "        output_names=[\"output\"],\n",
        "    )\n",
        "    input_data = {\"data\": dummy_input.detach().cpu().numpy()}\n",
        "    model_sim, flag = onnxsim.simplify(output_path, input_data=input_data)\n",
        "    if flag:\n",
        "        onnx.save(model_sim, output_path)\n",
        "    else:\n",
        "        print(\"error\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQEX_H59oEae"
      },
      "outputs": [],
      "source": [
        "convert_to_onnx(COLORIZER_SIGGRAPH17, str(fp32_onnx_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dx-yR2pn3i15"
      },
      "source": [
        "#5) Convert ONNX Model to OpenVINO Intermediate Representation (IR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "vY99Z7uCjY8F"
      },
      "source": [
        "The OpenVINO toolkit allows developers to optimize and deploy deep learning models on a variety of Intel hardware. To use an ONNX model with OpenVINO, the ONNX model must be converted to OpenVINO Intermediate Representation (IR) format. This process involves using the mo.py script provided by OpenVINO, which takes the ONNX model as input and generates an IR representation that can be executed on Intel hardware. The mo.py script performs optimizations such as weight quantization and layout transformations to improve model performance on Intel hardware. Once the ONNX model has been converted to IR format, it can be executed on a range of Intel devices, including CPUs, GPUs, and VPUs, using the OpenVINO inference engine. The ability to convert ONNX models to OpenVINO IR format provides a simple and effective way to optimize deep learning models for deployment on Intel hardware.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVxIcXsvrMq1"
      },
      "outputs": [],
      "source": [
        "from openvino.tools import mo\n",
        "from openvino.runtime import serialize\n",
        "from openvino.runtime import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUFSO9fnrXIe"
      },
      "outputs": [],
      "source": [
        "def create_bin_and_xml_files(onnx_model_path: Union[str, Path], ir_path: Union[str, Path]) -> None:\n",
        "    '''\n",
        "    Converts an ONNX model to the OpenVINO format and saves it as .xml and .bin files.\n",
        "\n",
        "    Args:\n",
        "        - onnx_model_path (Union[str, Path]): path to the ONNX model\n",
        "        - ir_path (Union[str, Path]): path where the .xml and .bin files will be saved\n",
        "\n",
        "    Returns: None\n",
        "    '''\n",
        "    \n",
        "    onnx_model = mo.convert_model(str(onnx_model_path))\n",
        "    serialize(model=onnx_model, xml_path=str(ir_path))\n",
        "\n",
        "create_bin_and_xml_files(fp32_onnx_path, fp32_ir_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24G0szt5rZQN"
      },
      "outputs": [],
      "source": [
        "from openvino.runtime import Core\n",
        "core = Core()\n",
        "model = core.read_model(fp32_ir_path)\n",
        "compiled_model = core.compile_model(model, 'CPU')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiExPmqfwQ-6"
      },
      "outputs": [],
      "source": [
        "output_blob = compiled_model.output(0)\n",
        "preds = (compiled_model(tens_l_rs)[output_blob])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7wF2qTy-pJ6k"
      },
      "source": [
        "#6) Validate converted model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR2tPYZUvqPU"
      },
      "outputs": [],
      "source": [
        "img = load_img(IMAGE_PATH)\n",
        "(tens_l_orig, tens_l_rs) = preprocess_img(img, HW=(256,256))\n",
        "img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n",
        "\n",
        "output_blob = compiled_model.output(0)\n",
        "preds = (compiled_model(tens_l_rs)[output_blob])\n",
        "\n",
        "out_img_eccv16 = postprocess_tens(tens_l_orig, torch.from_numpy(preds))\n",
        "# out_img_siggraph17 = postprocess_tens(tens_l_orig, colorizer_siggraph17(tens_l_rs).cpu())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "fX80y84OweQc",
        "outputId": "4151e2ef-2b1b-4246-cb06-b8de7efced53"
      },
      "outputs": [],
      "source": [
        "show_image(out_img_eccv16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYEL2GgvQOiW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.1"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0997c255be4c4a148a116af50be52129": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a56be011b2e84b62aa4059f91f63cb8e",
              "IPY_MODEL_988670e07a4b46618840cb9a1c8694df",
              "IPY_MODEL_586c07f0fc6b4ff2a4d092bfac72fe70"
            ],
            "layout": "IPY_MODEL_f39478e9b33746f38c9ff3b84f03bf01"
          }
        },
        "0db5aadf2ffc434da41d7c7fb710810b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39c75dec5e8b401d8aeb96e22b86d7a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e69f486d4fa4f89b3d28a3dd83a867e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48328e5a95eb400caf30105eb14fca58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d131b6f65b724ac9b6abc66a6dc76b87",
              "IPY_MODEL_f6a8ae609a4941a596853f327ff0785f",
              "IPY_MODEL_cb137d9c611841919c3b0b90f719936d"
            ],
            "layout": "IPY_MODEL_39c75dec5e8b401d8aeb96e22b86d7a6"
          }
        },
        "4991b17a8d1243dc8cf21b79026aef65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b10237ef49248728a3b4483f614da85": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "586c07f0fc6b4ff2a4d092bfac72fe70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec5285a0d50943c0b1ea7d06c263910f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_b1a3ba24c60c4ffc867358c56fa31b14",
            "value": " 123M/123M [00:02&lt;00:00, 66.4MB/s]"
          }
        },
        "71af891fa7ca440ea4ff06f304b0448c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "968751d75e64484488d08de20f42f4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "988670e07a4b46618840cb9a1c8694df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f28a82d1a6f84ab798951ac4af2717dc",
            "max": 128976165,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b10237ef49248728a3b4483f614da85",
            "value": 128976165
          }
        },
        "a0db83b8add648c4ad92c09179afdcf0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a56be011b2e84b62aa4059f91f63cb8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0db5aadf2ffc434da41d7c7fb710810b",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c3465eb24e304c4f968bf2f804476630",
            "value": "100%"
          }
        },
        "b1a3ba24c60c4ffc867358c56fa31b14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3465eb24e304c4f968bf2f804476630": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb137d9c611841919c3b0b90f719936d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0db83b8add648c4ad92c09179afdcf0",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4991b17a8d1243dc8cf21b79026aef65",
            "value": " 130M/130M [00:02&lt;00:00, 52.7MB/s]"
          }
        },
        "d131b6f65b724ac9b6abc66a6dc76b87": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e69f486d4fa4f89b3d28a3dd83a867e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d2ea28994bdc42f78537d127b130d191",
            "value": "100%"
          }
        },
        "d2ea28994bdc42f78537d127b130d191": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec5285a0d50943c0b1ea7d06c263910f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f28a82d1a6f84ab798951ac4af2717dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f39478e9b33746f38c9ff3b84f03bf01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a8ae609a4941a596853f327ff0785f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_968751d75e64484488d08de20f42f4f7",
            "max": 136787426,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_71af891fa7ca440ea4ff06f304b0448c",
            "value": 136787426
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
