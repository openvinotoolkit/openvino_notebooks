{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "804e0e25-64e0-4292-8540-a20b4206ad38",
   "metadata": {},
   "source": [
    "# Pedestrian counting with OpenVINOâ„¢\n",
    "\n",
    "This demo shows how to run pedestrian counting inference with OpenVINO, using [PP-YOLOE](https://gitee.com/paddlepaddle/PaddleDetection/blob/release/2.5/configs/ppyoloe) and [OC-SORT](https://gitee.com/paddlepaddle/PaddleDetection/blob/release/2.5/configs/mot/ocsort) from [PaddleDetection Github](https://github.com/PaddlePaddle/PaddleDetection) or [PaddleDetection Gitee](https://gitee.com/paddlepaddle/PaddleDetection). \n",
    "The PP-YOLOE pre-trained model used in the demo refers to the \"mot_ppyoloe_s_36e_pipeline(27M)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265d63d-40e3-4b8c-826e-126f856d6d4f",
   "metadata": {},
   "source": [
    "## OC-Sort Tracker requirments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a745bb-4d39-404f-bd09-6470e6e064fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install filterpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59712367-9f3d-4678-bf3a-812563a3c555",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ddb6dc-3b58-4796-a23b-b6e6f4861dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from collections import defaultdict, deque\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import openvino.runtime as ov\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68dfaef-0fb5-42cc-8ca2-5fb48943e89b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Models for Pedestrian counting\n",
    "\n",
    "Pedestrian counting requires \"mot_ppyoloe_s_36e_pipeline\" model(27M). Pre-trained models used in the demo are downloaded and stored in the \"model\" folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7e127-7ea2-4149-b860-c2fc22a79651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to download mot_ppyoloe_s_36e_pipeline model from PaddleDetection resources.\n",
    "\n",
    "\n",
    "def run_model_download(model_url, model_file_path):\n",
    "    \"\"\"\n",
    "    Download pre-trained models from PaddleDetection resources\n",
    "\n",
    "    Parameters:\n",
    "        model_url: url link to pre-trained models\n",
    "        model_file_path: file path to store the downloaded model\n",
    "    \"\"\"\n",
    "    model_name = model_url.split(\"/\")[-1]\n",
    "\n",
    "    if model_file_path.is_file():\n",
    "        print(\"Model already exists\")\n",
    "    else:\n",
    "        # Download the model from the server, and untar it.\n",
    "        print(\"Downloading the pre-trained model... May take a while...\")\n",
    "\n",
    "        # Create a directory.\n",
    "        os.makedirs(\"model\", exist_ok=True)\n",
    "        urllib.request.urlretrieve(model_url, f\"model/{model_name} \")\n",
    "        print(\"Model Downloaded\")\n",
    "        print(f\"model/{model_name} \")\n",
    "        file = zipfile.ZipFile(f\"model/{model_name} \")\n",
    "        res = file.extractall(\"model\")\n",
    "        file.close()\n",
    "        if not res:\n",
    "            print(f\"Model Extracted to {model_file_path}.\")\n",
    "        else:\n",
    "            print(\"Error Extracting the model. Please check the network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9149e74-daf5-4786-a163-998990e24365",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Download the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75312f82-47e0-47ac-bad4-f702deea025d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A directory where the model will be downloaded.\n",
    "\n",
    "model_url = (\n",
    "    \"https://bj.bcebos.com/v1/paddledet/models/pipeline/mot_ppyoloe_s_36e_pipeline.zip\"\n",
    ")\n",
    "model_file_path = Path(\"model/mot_ppyoloe_s_36e_pipeline/model.pdmodel\")\n",
    "\n",
    "run_model_download(model_url, model_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e11303f-8fbc-4d03-8b9e-b41ba1265ec2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preprocessing Functions for Pedestrian counting\n",
    "\n",
    "### load preprocess config \n",
    "\n",
    "All PaddlePaddle Pre-trained model downloaded from PaddleDetection contains `infer_cfg.yml` file which All image preprocessing configurations are saved. \n",
    "\n",
    "Using the code given by PaddleDetection, we can load `infer_cfg.yml` file and complete the preprocess easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d536c52-a99d-42f4-991e-14a9d0fd6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ocsort_utils.ocsort_tracker import OCSORTTracker\n",
    "from preprocess import Compose\n",
    "\n",
    "# Global dictionary\n",
    "SUPPORT_MODELS = {\n",
    "    \"YOLO\",\n",
    "    \"RCNN\",\n",
    "    \"SSD\",\n",
    "    \"Face\",\n",
    "    \"FCOS\",\n",
    "    \"SOLOv2\",\n",
    "    \"TTFNet\",\n",
    "    \"S2ANet\",\n",
    "    \"JDE\",\n",
    "    \"FairMOT\",\n",
    "    \"DeepSORT\",\n",
    "    \"GFL\",\n",
    "    \"PicoDet\",\n",
    "    \"CenterNet\",\n",
    "    \"TOOD\",\n",
    "    \"RetinaNet\",\n",
    "    \"StrongBaseline\",\n",
    "    \"STGCN\",\n",
    "    \"YOLOX\",\n",
    "    \"HRNet\",\n",
    "}\n",
    "\n",
    "\n",
    "class PredictConfig(object):\n",
    "    \"\"\"set config of preprocess, postprocess and visualize\n",
    "    Args:\n",
    "        infer_config (str): path of infer_cfg.yml\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, infer_config):\n",
    "        # parsing Yaml config for Preprocess\n",
    "        with open(infer_config) as f:\n",
    "            yml_conf = yaml.safe_load(f)\n",
    "        self.check_model(yml_conf)\n",
    "        self.arch = yml_conf[\"arch\"]\n",
    "        self.preprocess_infos = yml_conf[\"Preprocess\"]\n",
    "        self.min_subgraph_size = yml_conf[\"min_subgraph_size\"]\n",
    "        self.label_list = yml_conf[\"label_list\"]\n",
    "        self.use_dynamic_shape = yml_conf[\"use_dynamic_shape\"]\n",
    "        self.draw_threshold = yml_conf.get(\"draw_threshold\", 0.5)\n",
    "        self.mask = yml_conf.get(\"mask\", False)\n",
    "        self.tracker = yml_conf.get(\"tracker\", None)\n",
    "        self.nms = yml_conf.get(\"NMS\", None)\n",
    "        self.fpn_stride = yml_conf.get(\"fpn_stride\", None)\n",
    "        if self.arch == \"RCNN\" and yml_conf.get(\"export_onnx\", False):\n",
    "            print(\n",
    "                \"The RCNN export model is used for ONNX and it only supports batch_size = 1\"\n",
    "            )\n",
    "        self.print_config()\n",
    "\n",
    "    def check_model(self, yml_conf):\n",
    "        \"\"\"\n",
    "        Raises:\n",
    "            ValueError: loaded model not in supported model type\n",
    "        \"\"\"\n",
    "        for support_model in SUPPORT_MODELS:\n",
    "            if support_model in yml_conf[\"arch\"]:\n",
    "                return True\n",
    "        raise ValueError(\n",
    "            \"Unsupported arch: {}, expect {}\".format(yml_conf[\"arch\"], SUPPORT_MODELS)\n",
    "        )\n",
    "\n",
    "    def print_config(self):\n",
    "        print(\"-----------  Model Configuration -----------\")\n",
    "        print(\"%s: %s\" % (\"Model Arch\", self.arch))\n",
    "        print(\"%s: \" % (\"Transform Order\"))\n",
    "        for op_info in self.preprocess_infos:\n",
    "            print(\"--%s: %s\" % (\"transform op\", op_info[\"type\"]))\n",
    "        print(\"--------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42dd8dae-e611-49ba-916b-13b0405679a0",
   "metadata": {},
   "source": [
    "## Initialize Openvino predictor for PP-YOLOE and OC-Sort\n",
    "1. initialize the runtime for inference. Then, read the network architecture and model weights from the `.pdmodel` and `.pdiparams` files to load to CPU.\n",
    "2. initialize the OC-Sort Tracker with default configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f57a2-788c-430c-b746-d0541f125c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class label(object):\n",
    "    def __init__(self):\n",
    "        self.labels = [\"pedestrian\"]\n",
    "\n",
    "\n",
    "class OpenvineDetector(object):\n",
    "    def __init__(self, paddlefile, infer_cfg):\n",
    "        self.paddle_file = paddlefile\n",
    "        self.infer_cfg = infer_cfg\n",
    "        self.infer_config = PredictConfig(infer_cfg)\n",
    "        self.pred_config = label()\n",
    "        det_thresh = 0.4\n",
    "        max_age = 30\n",
    "        min_hits = 3\n",
    "        iou_threshold = 0.3\n",
    "        delta_t = 3\n",
    "        inertia = 0.2\n",
    "        min_box_area = 0\n",
    "        vertical_ratio = 0\n",
    "        use_byte = False\n",
    "\n",
    "        # initialize OCSORTTracker, which is not a deep learning model and does not require the use of OpenVino inference\n",
    "        self.tracker = OCSORTTracker(\n",
    "            det_thresh=det_thresh,\n",
    "            max_age=max_age,\n",
    "            min_hits=min_hits,\n",
    "            iou_threshold=iou_threshold,\n",
    "            delta_t=delta_t,\n",
    "            inertia=inertia,\n",
    "            min_box_area=min_box_area,\n",
    "            vertical_ratio=vertical_ratio,\n",
    "            use_byte=use_byte,\n",
    "        )\n",
    "        # create runtime.Core\n",
    "        self.core = ov.Core()\n",
    "        print(\"OpenVINO Runtime Core Created!\")\n",
    "        # directly load paddlemodel, using model.pdmodel file\n",
    "        # Openvino will automatically search for model.pdiparams file and load model parameters\n",
    "        # Notice whether the file you downloaded contains model.pdmodel and model.pdiparams\n",
    "        self.model = self.core.read_model(paddlefile)\n",
    "        # compile model into cpu\n",
    "        self.compiled_model = self.core.compile_model(self.model, \"CPU\")\n",
    "        # create infer request for the model\n",
    "        self.predictor = self.compiled_model.create_infer_request()\n",
    "        print(\"[OpenVINO]%s infer request created\" % paddlefile)\n",
    "\n",
    "    # PP-YOLOe detector predict function\n",
    "    def det_predict(self, img_list):\n",
    "        # load preprocess transforms\n",
    "        transforms = Compose(self.infer_config.preprocess_infos)\n",
    "        # predict image\n",
    "        for img_path in img_list:\n",
    "            inputs = transforms(img_path)\n",
    "            # Create tensor from external memory\n",
    "            np_image = np.array([inputs[\"image\"]], dtype=\"float32\")\n",
    "            np_scale_factor = np.array([inputs[\"scale_factor\"]])\n",
    "            # Create Openvino Tensor from np.array\n",
    "            image_tensor = ov.Tensor(array=np_image, shared_memory=True)\n",
    "            scale_factor_tensor = ov.Tensor(array=np_scale_factor, shared_memory=True)\n",
    "            # Set input tensor for model with two input: image and scale_factor\n",
    "            self.predictor.set_input_tensor(1, image_tensor)\n",
    "            self.predictor.set_input_tensor(0, scale_factor_tensor)\n",
    "            self.predictor.start_async()\n",
    "            self.predictor.wait()\n",
    "            # Get output tensor for model with one output\n",
    "            output_0 = self.predictor.get_output_tensor(0)\n",
    "            output_1 = self.predictor.get_output_tensor(1)\n",
    "            outputs = [output_0.data, output_1.data]\n",
    "            # outputs = self.predictor.infer({'image': inputs['image'], 'scale_factor': inputs['scale_factor']})\n",
    "            # print(\"Openvino predict: \")\n",
    "            if self.infer_config.arch in [\"HRNet\"]:\n",
    "                print(np.array(outputs[0]))\n",
    "            else:\n",
    "                # bboxes = np.array(outputs[\"multiclass_nms3_0.tmp_0\"])\n",
    "                bboxes = np.array(outputs[0])\n",
    "                return bboxes\n",
    "\n",
    "    # OCSORT predict function\n",
    "    def track(self, det_result):\n",
    "        pred_embs = None\n",
    "        pred_dets = det_result\n",
    "        online_targets = self.tracker.update(pred_dets, pred_embs)\n",
    "        online_tlwhs = list()\n",
    "        online_scores = list()\n",
    "        online_ids = list()\n",
    "        for t in online_targets:\n",
    "            tlwh = [t[0], t[1], t[2] - t[0], t[3] - t[1]]\n",
    "            tscore = float(t[4])\n",
    "            tid = int(t[5])\n",
    "            if tlwh[2] * tlwh[3] <= self.tracker.min_box_area:\n",
    "                continue\n",
    "            if (\n",
    "                self.tracker.vertical_ratio > 0\n",
    "                and tlwh[2] / tlwh[3] > self.tracker.vertical_ratio\n",
    "            ):\n",
    "                continue\n",
    "            if tlwh[2] * tlwh[3] > 0:\n",
    "                online_tlwhs.append(tlwh)\n",
    "                online_ids.append(tid)\n",
    "                online_scores.append(tscore)\n",
    "        tracking_outs = {\n",
    "            \"online_tlwhs\": online_tlwhs,\n",
    "            \"online_scores\": online_scores,\n",
    "            \"online_ids\": online_ids,\n",
    "        }\n",
    "        return tracking_outs\n",
    "\n",
    "    # pipeline of PP-YOLOe and OCSORTTracker\n",
    "    def predict_image(self, im, visual=False):\n",
    "        # np.save(\"mot_input.npy\", im)\n",
    "        bboxes = self.det_predict([im])\n",
    "        tracking_outs = self.track(bboxes)\n",
    "        online_tlwhs = [tracking_outs[\"online_tlwhs\"]]\n",
    "        online_scores = [tracking_outs[\"online_scores\"]]\n",
    "        online_ids = [tracking_outs[\"online_ids\"]]\n",
    "        return [[online_tlwhs, online_scores, online_ids]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd5161b-0e0f-459b-895f-bc379d7c574d",
   "metadata": {},
   "source": [
    "## Postprocessing\n",
    "1. format model's output (parse_mot_res)\n",
    "2. Pedestrian in or out count (flow_statistic)\n",
    "3. Boxes visulize (plot_tracking_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3f91dd-043d-488b-bcbb-611be7887a62",
   "metadata": {},
   "source": [
    "### Pedestrian entrance counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679d8232-5401-4094-96e4-b748f8c3faaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the tracking input\n",
    "def parse_mot_res(input):\n",
    "    mot_res = []\n",
    "    boxes, scores, ids = input[0]\n",
    "    for box, score, i in zip(boxes[0], scores[0], ids[0]):\n",
    "        xmin, ymin, w, h = box\n",
    "        res = [i, 0, score, xmin, ymin, xmin + w, ymin + h]\n",
    "        mot_res.append(res)\n",
    "    return {\"boxes\": np.array(mot_res)}\n",
    "\n",
    "\n",
    "# pedestrian in or out count function\n",
    "# returns in or out id list\n",
    "def flow_statistic(\n",
    "    result,\n",
    "    secs_interval,\n",
    "    do_entrance_counting,\n",
    "    do_break_in_counting,\n",
    "    region_type,\n",
    "    video_fps,\n",
    "    entrance,\n",
    "    id_set,\n",
    "    interval_id_set,\n",
    "    in_id_list,\n",
    "    out_id_list,\n",
    "    prev_center,\n",
    "    records,\n",
    "    data_type=\"mot\",\n",
    "    num_classes=1,\n",
    "):\n",
    "    # Count in/out number:\n",
    "    # Note that 'region_type' should be one of ['horizontal', 'vertical'],\n",
    "    # 'horizontal' and 'vertical' means entrance is the center line as the entrance when do_entrance_counting\n",
    "\n",
    "    if do_entrance_counting:\n",
    "        assert region_type in [\n",
    "            \"horizontal\",\n",
    "            \"vertical\",\n",
    "        ], \"region_type should be 'horizontal' or 'vertical' when do entrance counting.\"\n",
    "        entrance_x, entrance_y = entrance[0], entrance[1]\n",
    "        frame_id, tlwhs, tscores, track_ids = result\n",
    "        for tlwh, score, track_id in zip(tlwhs, tscores, track_ids):\n",
    "            if track_id < 0:\n",
    "                continue\n",
    "            if data_type == \"kitti\":\n",
    "                frame_id -= 1\n",
    "            x1, y1, w, h = tlwh\n",
    "            center_x = x1 + w / 2.0\n",
    "            center_y = y1 + h / 2.0\n",
    "            if track_id in prev_center:\n",
    "                if region_type == \"horizontal\":\n",
    "                    # horizontal center line\n",
    "                    if prev_center[track_id][1] <= entrance_y and center_y > entrance_y:\n",
    "                        in_id_list.append(track_id)\n",
    "                    if prev_center[track_id][1] >= entrance_y and center_y < entrance_y:\n",
    "                        out_id_list.append(track_id)\n",
    "                else:\n",
    "                    # vertical center line\n",
    "                    if prev_center[track_id][0] <= entrance_x and center_x > entrance_x:\n",
    "                        in_id_list.append(track_id)\n",
    "                    if prev_center[track_id][0] >= entrance_x and center_x < entrance_x:\n",
    "                        out_id_list.append(track_id)\n",
    "                prev_center[track_id][0] = center_x\n",
    "                prev_center[track_id][1] = center_y\n",
    "            else:\n",
    "                prev_center[track_id] = [center_x, center_y]\n",
    "\n",
    "    if do_break_in_counting:\n",
    "        assert region_type in [\n",
    "            \"custom\"\n",
    "        ], \"region_type should be 'custom' when do break_in counting.\"\n",
    "        assert (\n",
    "            len(entrance) >= 4\n",
    "        ), \"entrance should be at least 3 points and (w,h) of image when do break_in counting.\"\n",
    "        im_w, im_h = entrance[-1][:]\n",
    "        entrance = np.array(entrance[:-1])\n",
    "\n",
    "        frame_id, tlwhs, tscores, track_ids = result\n",
    "        for tlwh, score, track_id in zip(tlwhs, tscores, track_ids):\n",
    "            if track_id < 0:\n",
    "                continue\n",
    "            if data_type == \"kitti\":\n",
    "                frame_id -= 1\n",
    "            x1, y1, w, h = tlwh\n",
    "            center_x = min(x1 + w / 2.0, im_w - 1)\n",
    "            center_down_y = min(y1 + h, im_h - 1)\n",
    "\n",
    "            # counting objects in region of the first frame\n",
    "            if frame_id == 1:\n",
    "                if in_quadrangle([center_x, center_down_y], entrance, im_h, im_w):\n",
    "                    in_id_list.append(-1)\n",
    "                else:\n",
    "                    prev_center[track_id] = [center_x, center_down_y]\n",
    "            else:\n",
    "                if track_id in prev_center:\n",
    "                    if not in_quadrangle(\n",
    "                        prev_center[track_id], entrance, im_h, im_w\n",
    "                    ) and in_quadrangle(\n",
    "                        [center_x, center_down_y], entrance, im_h, im_w\n",
    "                    ):\n",
    "                        in_id_list.append(track_id)\n",
    "                    prev_center[track_id] = [center_x, center_down_y]\n",
    "                else:\n",
    "                    prev_center[track_id] = [center_x, center_down_y]\n",
    "\n",
    "    # Count totol number, number at a manual-setting interval\n",
    "    frame_id, tlwhs, tscores, track_ids = result\n",
    "    for tlwh, score, track_id in zip(tlwhs, tscores, track_ids):\n",
    "        if track_id < 0:\n",
    "            continue\n",
    "        id_set.add(track_id)\n",
    "        interval_id_set.add(track_id)\n",
    "\n",
    "    # Reset counting at the interval beginning\n",
    "    if frame_id % video_fps == 0 and frame_id / video_fps % secs_interval == 0:\n",
    "        curr_interval_count = len(interval_id_set)\n",
    "        interval_id_set.clear()\n",
    "    info = \"Frame id: {}, Total count: {}\".format(frame_id, len(id_set))\n",
    "    if do_entrance_counting:\n",
    "        info += \", In count: {}, Out count: {}\".format(\n",
    "            len(in_id_list), len(out_id_list)\n",
    "        )\n",
    "    if do_break_in_counting:\n",
    "        info += \", Break_in count: {}\".format(len(in_id_list))\n",
    "    if frame_id % video_fps == 0 and frame_id / video_fps % secs_interval == 0:\n",
    "        info += \", Count during {} secs: {}\".format(secs_interval, curr_interval_count)\n",
    "        interval_id_set.clear()\n",
    "    print(info)\n",
    "    info += \"\\n\"\n",
    "    records.append(info)\n",
    "\n",
    "    return {\n",
    "        \"id_set\": id_set,\n",
    "        \"interval_id_set\": interval_id_set,\n",
    "        \"in_id_list\": in_id_list,\n",
    "        \"out_id_list\": out_id_list,\n",
    "        \"prev_center\": prev_center,\n",
    "        \"records\": records,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5482c9-f825-41ab-96f1-4a46c4e8941f",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f630b53-2710-45f2-b5bf-ee4af27b9a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_color(idx):\n",
    "    idx = idx * 3\n",
    "    color = ((37 * idx) % 255, (17 * idx) % 255, (29 * idx) % 255)\n",
    "    return color\n",
    "\n",
    "\n",
    "# visualize function visualize output to the video\n",
    "def plot_tracking_dict(\n",
    "    image,\n",
    "    num_classes,\n",
    "    tlwhs_dict,\n",
    "    obj_ids_dict,\n",
    "    scores_dict,\n",
    "    frame_id=0,\n",
    "    fps=0.0,\n",
    "    ids2names=[],\n",
    "    do_entrance_counting=False,\n",
    "    do_break_in_counting=False,\n",
    "    entrance=None,\n",
    "    records=None,\n",
    "    center_traj=None,\n",
    "):\n",
    "    im = np.ascontiguousarray(np.copy(image))\n",
    "    im_h, im_w = im.shape[:2]\n",
    "    if do_break_in_counting:\n",
    "        entrance = np.array(entrance[:-1])  # last pair is [im_w, im_h]\n",
    "\n",
    "    text_scale = max(0.5, image.shape[1] / 3000.0)\n",
    "    text_thickness = 2\n",
    "    line_thickness = max(1, int(image.shape[1] / 500.0))\n",
    "\n",
    "    if num_classes == 1:\n",
    "        if records is not None:\n",
    "            start = records[-1].find(\"Total\")\n",
    "            end = records[-1].find(\"In\")\n",
    "            cv2.putText(\n",
    "                im,\n",
    "                records[-1][start:end],\n",
    "                (0, int(40 * text_scale) + 10),\n",
    "                cv2.FONT_ITALIC,\n",
    "                text_scale,\n",
    "                (0, 0, 255),\n",
    "                thickness=text_thickness,\n",
    "            )\n",
    "\n",
    "    if num_classes == 1 and do_entrance_counting:\n",
    "        entrance_line = tuple(map(int, entrance))\n",
    "        cv2.rectangle(\n",
    "            im,\n",
    "            entrance_line[0:2],\n",
    "            entrance_line[2:4],\n",
    "            color=(0, 255, 255),\n",
    "            thickness=line_thickness,\n",
    "        )\n",
    "        # find start location for entrance counting data\n",
    "        start = records[-1].find(\"In\")\n",
    "        cv2.putText(\n",
    "            im,\n",
    "            records[-1][start:-1],\n",
    "            (0, int(60 * text_scale) + 10),\n",
    "            cv2.FONT_ITALIC,\n",
    "            text_scale,\n",
    "            (0, 0, 255),\n",
    "            thickness=text_thickness,\n",
    "        )\n",
    "\n",
    "    if num_classes == 1 and do_break_in_counting:\n",
    "        np_masks = np.zeros((im_h, im_w, 1), np.uint8)\n",
    "        cv2.fillPoly(np_masks, [entrance], 255)\n",
    "\n",
    "        # Draw region mask\n",
    "        alpha = 0.3\n",
    "        im = np.array(im).astype(\"float32\")\n",
    "        mask = np_masks[:, :, 0]\n",
    "        color_mask = [0, 0, 255]\n",
    "        idx = np.nonzero(mask)\n",
    "        color_mask = np.array(color_mask)\n",
    "        im[idx[0], idx[1], :] *= 1.0 - alpha\n",
    "        im[idx[0], idx[1], :] += alpha * color_mask\n",
    "        im = np.array(im).astype(\"uint8\")\n",
    "\n",
    "        # find start location for break in counting data\n",
    "        start = records[-1].find(\"Break_in\")\n",
    "        cv2.putText(\n",
    "            im,\n",
    "            records[-1][start:-1],\n",
    "            (entrance[0][0] - 10, entrance[0][1] - 10),\n",
    "            cv2.FONT_ITALIC,\n",
    "            text_scale,\n",
    "            (0, 0, 255),\n",
    "            thickness=text_thickness,\n",
    "        )\n",
    "\n",
    "    for cls_id in range(num_classes):\n",
    "        tlwhs = tlwhs_dict[cls_id]\n",
    "        obj_ids = obj_ids_dict[cls_id]\n",
    "        scores = scores_dict[cls_id]\n",
    "        cv2.putText(\n",
    "            im,\n",
    "            \"frame: %d fps: %.2f num: %d\" % (frame_id, fps, len(tlwhs)),\n",
    "            (0, int(15 * text_scale) + 5),\n",
    "            cv2.FONT_ITALIC,\n",
    "            text_scale,\n",
    "            (0, 0, 255),\n",
    "            thickness=text_thickness,\n",
    "        )\n",
    "\n",
    "        record_id = set()\n",
    "        for i, tlwh in enumerate(tlwhs):\n",
    "            x1, y1, w, h = tlwh\n",
    "            intbox = tuple(map(int, (x1, y1, x1 + w, y1 + h)))\n",
    "            center = tuple(map(int, (x1 + w / 2.0, y1 + h / 2.0)))\n",
    "            obj_id = int(obj_ids[i])\n",
    "            if center_traj is not None:\n",
    "                record_id.add(obj_id)\n",
    "                if obj_id not in center_traj[cls_id]:\n",
    "                    center_traj[cls_id][obj_id] = deque(maxlen=30)\n",
    "                center_traj[cls_id][obj_id].append(center)\n",
    "\n",
    "            id_text = \"{}\".format(int(obj_id))\n",
    "            if ids2names != []:\n",
    "                id_text = \"{}_{}\".format(ids2names[cls_id], id_text)\n",
    "            else:\n",
    "                id_text = \"class{}_{}\".format(cls_id, id_text)\n",
    "            in_region = False\n",
    "            if do_break_in_counting:\n",
    "                center_x = min(x1 + w / 2.0, im_w - 1)\n",
    "                center_down_y = min(y1 + h, im_h - 1)\n",
    "                if in_quadrangle([center_x, center_down_y], entrance, im_h, im_w):\n",
    "                    in_region = True\n",
    "\n",
    "            color = get_color(abs(obj_id)) if not in_region else (0, 0, 255)\n",
    "            cv2.rectangle(\n",
    "                im, intbox[0:2], intbox[2:4], color=color, thickness=line_thickness\n",
    "            )\n",
    "            cv2.putText(\n",
    "                im,\n",
    "                id_text,\n",
    "                (intbox[0], intbox[1] - 25),\n",
    "                cv2.FONT_ITALIC,\n",
    "                text_scale,\n",
    "                color,\n",
    "                thickness=text_thickness,\n",
    "            )\n",
    "\n",
    "            if do_break_in_counting and in_region:\n",
    "                cv2.putText(\n",
    "                    im,\n",
    "                    \"Break in now.\",\n",
    "                    (intbox[0], intbox[1] - 50),\n",
    "                    cv2.FONT_ITALIC,\n",
    "                    text_scale,\n",
    "                    (0, 0, 255),\n",
    "                    thickness=text_thickness,\n",
    "                )\n",
    "\n",
    "            if scores is not None:\n",
    "                text = \"score: {:.2f}\".format(float(scores[i]))\n",
    "                cv2.putText(\n",
    "                    im,\n",
    "                    text,\n",
    "                    (intbox[0], intbox[1] - 6),\n",
    "                    cv2.FONT_ITALIC,\n",
    "                    text_scale,\n",
    "                    color,\n",
    "                    thickness=text_thickness,\n",
    "                )\n",
    "        if center_traj is not None:\n",
    "            for traj in center_traj:\n",
    "                for i in traj.keys():\n",
    "                    if i not in record_id:\n",
    "                        continue\n",
    "                    for point in traj[i]:\n",
    "                        cv2.circle(im, point, 3, (0, 0, 255), -1)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb32bf7d-7e32-444b-8a3c-0a5cf6821451",
   "metadata": {},
   "source": [
    "## Main Processing Function for Pedestrian counting\n",
    "1. Create a videocapture to load frames\n",
    "2. Prepare a set of frames for mot_predictor\n",
    "3. Run AI inference for both PP-YOLOE and OC-Sort.\n",
    "4. Save the results as `output.mp4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4c84d4-a930-46ce-9736-1bdc465439e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(video_file, region_type, model_dir):\n",
    "    # Initialize the OpenvineDetector\n",
    "    mot_predictor = OpenvineDetector(\n",
    "        os.path.join(model_dir, \"model.pdmodel\"),\n",
    "        os.path.join(model_dir, \"infer_cfg.yml\"),\n",
    "    )\n",
    "    # Load the video\n",
    "    capture = cv2.VideoCapture(video_file)\n",
    "    out_path = \"output.mp4\"\n",
    "    # Get Video info : resolution, fps, frame count\n",
    "    width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "    frame_count = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"video fps: %d, frame_count: %d\" % (fps, frame_count))\n",
    "    # Create video writer\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    writer = cv2.VideoWriter(out_path, fourcc, fps, (width, height))\n",
    "    frame_id = 0\n",
    "    entrance, records, center_traj = None, None, None\n",
    "    center_traj = [{}]\n",
    "    id_set = set()\n",
    "    interval_id_set = set()\n",
    "    in_id_list = list()\n",
    "    out_id_list = list()\n",
    "    prev_center = dict()\n",
    "    records = list()\n",
    "    if region_type == \"horizontal\":\n",
    "        entrance = [0, height / 2.0, width, height / 2.0]\n",
    "    elif region_type == \"vertical\":\n",
    "        entrance = [width / 2, 0.0, width / 2, height]\n",
    "\n",
    "    while 1:\n",
    "        if frame_id % 10 == 0:\n",
    "            print(\"frame id: \", frame_id)\n",
    "\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        res = mot_predictor.predict_image([copy.deepcopy(frame_rgb)], visual=False)\n",
    "        # mot output format: id, class, score, xmin, ymin, xmax, ymax\n",
    "        mot_res = parse_mot_res(res)\n",
    "        # flow_statistic only support single class MOT\n",
    "        boxes, scores, ids = res[0]  # batch size = 1 in MOT\n",
    "        mot_result = (frame_id + 1, boxes[0], scores[0], ids[0])  # single class\n",
    "        statistic = flow_statistic(\n",
    "            mot_result,\n",
    "            10,\n",
    "            True,\n",
    "            False,\n",
    "            region_type,\n",
    "            fps,\n",
    "            entrance,\n",
    "            id_set,\n",
    "            interval_id_set,\n",
    "            in_id_list,\n",
    "            out_id_list,\n",
    "            prev_center,\n",
    "            records,\n",
    "        )\n",
    "        records = statistic[\"records\"]\n",
    "        if mot_res is not None:\n",
    "            ids = mot_res[\"boxes\"][:, 0]\n",
    "            scores = mot_res[\"boxes\"][:, 2]\n",
    "            boxes = mot_res[\"boxes\"][:, 3:]\n",
    "            boxes[:, 2] = boxes[:, 2] - boxes[:, 0]\n",
    "            boxes[:, 3] = boxes[:, 3] - boxes[:, 1]\n",
    "        else:\n",
    "            boxes = np.zeros([0, 4])\n",
    "            ids = np.zeros([0])\n",
    "            scores = np.zeros([0])\n",
    "\n",
    "        # single class, still need to be defaultdict type for ploting\n",
    "        num_classes = 1\n",
    "        online_tlwhs = defaultdict(list)\n",
    "        online_scores = defaultdict(list)\n",
    "        online_ids = defaultdict(list)\n",
    "        online_tlwhs[0] = boxes\n",
    "        online_scores[0] = scores\n",
    "        online_ids[0] = ids\n",
    "\n",
    "        if mot_res is not None:\n",
    "            image = plot_tracking_dict(\n",
    "                frame,\n",
    "                num_classes,\n",
    "                online_tlwhs,\n",
    "                online_ids,\n",
    "                online_scores,\n",
    "                frame_id=frame_id,\n",
    "                fps=fps,\n",
    "                ids2names=mot_predictor.pred_config.labels,\n",
    "                do_entrance_counting=True,\n",
    "                entrance=entrance,\n",
    "                records=records,\n",
    "                center_traj=center_traj,\n",
    "            )\n",
    "        writer.write(image)\n",
    "        frame_id += 1\n",
    "    writer.release()\n",
    "    print(\"save result to {}\".format(out_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c28ef-7d04-4ee3-8b3b-6780ea39631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that 'region_type' should be one of ['horizontal', 'vertical']\n",
    "predict_video(\n",
    "    video_file=\"data/test.mp4\",\n",
    "    region_type=\"vertical\",\n",
    "    model_dir=\"model/mot_ppyoloe_s_36e_pipeline\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c287e517-6766-4469-ac61-7e8ba1698486",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
