{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e47ebc64-c831-494f-9b57-be3e0baeeb47",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 224 Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e506571-9c93-407c-8967-565ffab5fb9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "from pytube import YouTube\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c95c07-1776-4572-b060-65e3c6be94de",
   "metadata": {},
   "source": [
    "## Download Inference Video\n",
    "Pytube is used to download a video from youtube. Feel free to change the link to whichever video you like. Keep in mind long videos may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918c45d-014b-44a0-b2a7-321ef2f7fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.youtube.com/watch?v=3kO21UGpCNw'\n",
    "\n",
    "video = YouTube(link)\n",
    "\n",
    "path = os.path.join(os.getcwd(), 'utils')\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "existing_videos = [title[:-4] for title in os.listdir(path)]\n",
    "if not video.title in existing_videos:\n",
    "    video.streams \\\n",
    "        .filter(file_extension='mp4') \\\n",
    "        .first().download(output_path=path)\n",
    "video_file = f'{video.title}.mp4'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c686cd-7336-496d-a20e-bb4054874183",
   "metadata": {},
   "source": [
    "## Download Model\n",
    "Omz downloader is a tool in openvino toolkit to download premade models. <br>\n",
    "You can select either yolo v4 tiny as 'yolo-v4-tiny-tf' or yolo v4 as 'yolo-v4-tf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb921db-1baa-497c-bc28-ada2036aecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'yolo-v4-tiny-tf'\n",
    "#model_name = 'yolo-v4-tf'\n",
    "\n",
    "download_command = f'omz_downloader ' \\\n",
    "                   f'--name {model_name} ' \\\n",
    "                   f'--output_dir {path} ' \\\n",
    "                   f'--cache_dir {path}'\n",
    "\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c204e881-d825-4acb-bd03-8738a4fab03b",
   "metadata": {},
   "source": [
    "## Convert Model\n",
    "Here we convert to FP16. You can also use FP32 but will have increased inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d6d7c4-b568-4c70-b629-08492106e49b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "precision = 'FP16'\n",
    "\n",
    "# output path for the conversion\n",
    "converted_model_path = f'public/{model_name}/{precision}/{model_name}.xml'\n",
    "full_converted_path = os.path.join(path, converted_model_path)\n",
    "\n",
    "if not os.path.exists(full_converted_path):\n",
    "    convert_command = f'omz_converter ' \\\n",
    "                      f'--name {model_name} ' \\\n",
    "                      f'--download_dir {path} ' \\\n",
    "                      f'--precisions {precision}'\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493889-9e16-4ff4-8789-191e9e144082",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890f107c-dbaf-410d-b2c1-f0460ab92b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "model = ie.read_model(model=full_converted_path)\n",
    "compiled_model = ie.compile_model(model=model, device_name='CPU')\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "\n",
    "#Yolov4 uses PAnet, resulting in multiple detection layers\n",
    "#make sure to use .output(layer) because .outputs\n",
    "#doesn't maintain order\n",
    "outputs = []\n",
    "for layer in range(len(compiled_model.outputs)):\n",
    "    outputs.append(compiled_model.output(layer))\n",
    "\n",
    "#height and width of inputs\n",
    "height = compiled_model.inputs[0].shape[1]\n",
    "width = compiled_model.inputs[0].shape[2]\n",
    "\n",
    "#incorrect layer output order work around\n",
    "if model_name == 'yolo-v4-tf':\n",
    "    #orig layer size order 36, 19, 76\n",
    "    #change to order 19, 36, 76 to fit anchors order\n",
    "    temp = outputs[0]\n",
    "    outputs[0] = outputs[1]\n",
    "    outputs[1] = temp\n",
    "    \n",
    "#view outputs shapes\n",
    "for layer in outputs:\n",
    "    print(layer.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ef2481-517b-4121-9097-0a9796f7c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors_dict = {'yolo-v4-tiny-tf' : [[[81,82], [135,169], [344,319]],\n",
    "                                     [[23,27], [37,58], [81,82]]],\n",
    "                'yolo-v4-tf' : [[[142,110], [192,243], [459,401]],\n",
    "                                [[36,75], [76,55], [72,146]],\n",
    "                                [[12,16], [19,36], [40,28]]]\n",
    "               }\n",
    "anchors = anchors_dict[model_name]\n",
    "\n",
    "# https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
    "classes = [\n",
    "    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', \n",
    "    'boat', 'trafficlight', 'firehydrant', 'stopsign', 'parkingmeter', 'bench', 'bird', \n",
    "    'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', \n",
    "    'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', \n",
    "    'sportsball', 'kite', 'baseballbat', 'baseballglove', 'skateboard', 'surfboard', \n",
    "    'tennisracket', 'bottle', 'wineglass', 'cup', 'fork', 'knife', 'spoon', 'bowl', \n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hotdog', 'pizza', \n",
    "    'donut', 'cake', 'chair', 'couch', 'pottedplant', 'bed', 'diningtable', 'toilet', \n",
    "    'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cellphone', 'microwave', 'oven', \n",
    "    'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', \n",
    "    'teddybear', 'hairdrier', 'toothbrush'\n",
    "]\n",
    "\n",
    "# colors for above classes (Rainbow Color Map)\n",
    "colors = cv2.applyColorMap(\n",
    "    src=np.arange(0, 255, 255 / len(classes), dtype=np.float32).astype(np.uint8),\n",
    "    colormap=cv2.COLORMAP_RAINBOW\n",
    ").squeeze()\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def process_results(results, thresh=0.5):\n",
    "    # size of the original frame\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    #results is a list of 2 tensors\n",
    "    for layer, output in enumerate(results):\n",
    "        for cell_y, col in enumerate(output):\n",
    "            for cell_x, grid in enumerate(col):\n",
    "                for idx, base_idx in enumerate(range(0, 255, 85)):\n",
    "                    if sigmoid(grid[base_idx+4]) > thresh:\n",
    "                        offset_x = sigmoid(grid[base_idx])\n",
    "                        offset_y = sigmoid(grid[base_idx+1])\n",
    "                        center_x = cell_x + offset_x\n",
    "                        center_x *= width / output.shape[1]\n",
    "                        center_y = cell_y + offset_y\n",
    "                        center_y *= height / output.shape[0] \n",
    "                        box_width = np.exp(grid[base_idx+2])\n",
    "                        box_width *= anchors[layer][idx][0]\n",
    "                        box_height = np.exp(grid[base_idx+3])\n",
    "                        box_height *= anchors[layer][idx][1]\n",
    "                        left = center_x - box_width / 2\n",
    "                        top = center_y - box_height / 2\n",
    "                        right = left + box_width\n",
    "                        bot = top + box_height\n",
    "                        boxes.append((left, top, right, bot))\n",
    "                        scores.append(sigmoid(grid[base_idx+4]))\n",
    "                        labels.append(np.argmax(grid[base_idx+5:base_idx+85]))\n",
    "\n",
    "    # apply non-maximum suppression to get rid of many overlapping entities\n",
    "    # see https://paperswithcode.com/method/non-maximum-suppression\n",
    "    # this algorithm returns indices of objects to keep\n",
    "    indices = cv2.dnn.NMSBoxes(bboxes=boxes, scores=scores, score_threshold=thresh, nms_threshold=0.6)\n",
    "\n",
    "    # if there are no boxes\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "\n",
    "    # filter detected objects\n",
    "    return [(labels[idx], scores[idx], boxes[idx]) for idx in indices.flatten()]\n",
    "\n",
    "def draw_boxes(frame, boxes, dsize):\n",
    "    frame_h, frame_w = frame.shape[0:2]\n",
    "    for label, score, box in boxes:\n",
    "        # choose color for the label\n",
    "        color = tuple(map(int, colors[label]))\n",
    "        #establish xy coords for box corners\n",
    "        #divide by the factor to reverse the scaling\n",
    "        factor = min(dsize[0] / frame.shape[0],\n",
    "                     dsize[1] / frame.shape[1])\n",
    "        x1 = int(box[0] / factor)\n",
    "        y1 = int(box[1] / factor)\n",
    "        x2 = int(box[2] / factor)\n",
    "        y2 = int(box[3] / factor)\n",
    "        cv2.rectangle(img=frame, pt1=(x1,y1), pt2=(x2,y2), color=color, thickness=3)\n",
    "        # draw label name inside the box\n",
    "        cv2.putText(img=frame, text=f\"{classes[label]} {score:.2f}\", org=(x1, y1 - 3),\n",
    "                    fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=frame_w / 1500, color=color,\n",
    "                    thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    return frame\n",
    "\n",
    "# modded from:\n",
    "# https://stackoverflow.com/questions/44650888/resize-an-image-without-distortion-opencv\n",
    "# https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/\n",
    "def preserved_resize(frame, dsize, interpolation=cv2.INTER_AREA):\n",
    "    #find the smallest resize ratio. this is how much we need to scale the image. \n",
    "    factor = min(dsize[0] / frame.shape[0],\n",
    "                 dsize[1] / frame.shape[1])\n",
    "    height = int(frame.shape[0] * factor)\n",
    "    width = int(frame.shape[1] * factor)\n",
    "    frame = cv2.resize(src=frame, dsize=(width,height), interpolation=interpolation)\n",
    "    \n",
    "    #pad image\n",
    "    dh = dsize[0] - height\n",
    "    dw = dsize[1] - width\n",
    "    \n",
    "    color = [127,127,127] #grey pad\n",
    "    #only pad the right and bottom so bounding box is easier to scale\n",
    "    frame = cv2.copyMakeBorder(frame, 0, dh, 0, dw, cv2.BORDER_CONSTANT, value=color)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c593d4d-87fc-4fb6-81dd-31af54139970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main processing function to run object detection\n",
    "def run_object_detection(use_popup=False, save=False):\n",
    "    player = None\n",
    "    try:\n",
    "        # create video player to play with target fps\n",
    "        player = cv2.VideoCapture(os.path.join(path, video_file))\n",
    "        # start capturing\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "        elif save:\n",
    "            w  = int(player.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "            h = int(player.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "            writer = cv2.VideoWriter(save, fourcc, 30, (w, h))\n",
    "\n",
    "        processing_times = deque()\n",
    "        while True:\n",
    "            #youtube is 30fps begin counting time.\n",
    "            frame_start = time.time()\n",
    "            \n",
    "            # grab the frame\n",
    "            ret, frame = player.read()\n",
    "            if not ret:\n",
    "                print(\"Source ended or error finding video\")\n",
    "                break\n",
    "            \n",
    "\n",
    "            # resize image and change dims to fit neural network input\n",
    "            input_img = preserved_resize(frame, dsize=(height, width), interpolation=cv2.INTER_AREA)\n",
    "            # create batch of images (size = 1)\n",
    "            input_img = input_img[np.newaxis, ...]\n",
    "\n",
    "            # measure processing time\n",
    "            start_time = time.time()\n",
    "            # get results. OpenVINO, as with cv2, uses BGR not RGB\n",
    "            results = compiled_model([input_img])\n",
    "            #change results from dict to list which each output being the index\n",
    "            #we can squeeze because batch size is only 1\n",
    "            results = [results[layer].squeeze() for layer in outputs]\n",
    "            stop_time = time.time()\n",
    "            # get poses from network results\n",
    "            boxes = process_results(results=results)\n",
    "\n",
    "            # draw boxes on a frame\n",
    "            frame = draw_boxes(frame=frame, boxes=boxes, dsize=(width, height))\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # use processing times from last 200 frames\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # mean processing time [ms]\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(img=frame, text=f\"Inference time: {processing_time:.1f}ms\", org=(20, 40),\n",
    "                        fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=f_width / 1000,\n",
    "                        color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "            # use this workaround if there is flickering\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            elif save:\n",
    "                writer.write(frame)\n",
    "            else:\n",
    "                # encode numpy array to jpg\n",
    "                _, encoded_img = cv2.imencode(ext=\".jpg\", img=frame,\n",
    "                                              params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                # create IPython image\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # display the image in this notebook\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "            \n",
    "            #finish counting the time.\n",
    "            #sleep till 30fps\n",
    "            dt = time.time() - frame_start\n",
    "            while dt < 1/30 and not save:\n",
    "                dt = time.time() - frame_start\n",
    "            \n",
    "            \n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # stop capturing\n",
    "            player.release()\n",
    "        if save:\n",
    "            writer.release()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba572e6-f28f-4e6c-b18a-6abc1a283d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_object_detection(use_popup=False, save='testrgb.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73c4d45-f53f-4e3a-a8ea-3b266d35608b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Refrences\n",
    "\n",
    "1. [SSDLite MobileNetV2](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/ssdlite_mobilenet_v2)\n",
    "2. [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)\n",
    "3. [Non-Maximum Suppression](https://paperswithcode.com/method/non-maximum-suppression)\n",
    "4. 401-object-detection (/../401-object-detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5a9f60-3326-4039-9cb2-c1fdbf25113f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0c9f53-c664-4317-8d73-25e3829ecd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6612689d-2655-4aec-a0d4-073806f67ffc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
