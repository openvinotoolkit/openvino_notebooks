# GPT-2 Text Prediction with OpenVINO
Text generation is a type of natural language processing that uses computational linguistics and artificial intelligence to automatically produce text that can meet specific communicative needs.
In this demo we use the **Generative Pre-trained Transformer 2 ([GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md))** model for text prediction.

The complete pipeline of this demo's notebook is shown below.

![image2](https://user-images.githubusercontent.com/91228207/163990722-d2713ede-921e-4594-8b00-8b5c1a4d73b5.jpeg)

This is a demonstration in which the user can type the beginning of the text and the network will generate a further. This procedure can be repeated as many times as the user desires.

The following image show an example of the input sequence and corresponding predicted sequence.

![image](https://user-images.githubusercontent.com/91228207/185103977-54b1671a-f02c-4f4b-9722-5c4e8b119fc7.png)
## Notebook Contents

This notebook demonstrates text prediction with OpenVINO using the [gpt-2](https://huggingface.co/gpt2) model from HuggingFace Transformers.

## Installation Instructions

If you have not done so already, please follow the [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/README.md) to install all required dependencies.
