# GPT-2 Text Prediction with OpenVINO
Text generation is a type of natural language processing that uses computational linguistics and artificial intelligence to automatically produce text that can meet specific communicative needs.
This demo uses the **Generative Pre-trained Transformer 2 ([GPT-2](https://github.com/openai/gpt-2/blob/master/model_card.md))** model for text prediction.

The complete pipeline of this demo's notebook is shown below.

![image2](https://user-images.githubusercontent.com/91228207/163990722-d2713ede-921e-4594-8b00-8b5c1a4d73b5.jpeg)

This is a demonstration in which you can type the beginning of the text and the network will generate a further. This procedure can be repeated as many times as you desire.

The following image show an example of the input sequence and corresponding predicted sequence.

![image](https://user-images.githubusercontent.com/91228207/185103977-54b1671a-f02c-4f4b-9722-5c4e8b119fc7.png)
## Notebook Contents

This notebook demonstrates text prediction with OpenVINO, using the [gpt-2](https://huggingface.co/gpt2) model from HuggingFace Transformers.

## Installation Instructions

If you have not installed all required dependencies, follow the [Installation Guide](../../README.md).
