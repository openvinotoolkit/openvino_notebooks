{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# GPT-2 Text Prediction with OpenVINO\n",
    "\n",
    "This notebook shows a text prediction with OpenVINO. We use the [GPT-2](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/gpt-2) model, which is a part of the Generative Pre-trained Transformer (GPT) family. GPT-2 is pre-trained on a large corpus of English text using unsupervised training. The model is available from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/), which we will use to download and convert the model to OpenVINO IR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "sys.path.append(\"../utils\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# directory where the model will be downloaded.\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# name of the model\n",
    "model_name = 'gpt-2'\n",
    "\n",
    "# desired precision\n",
    "precision = \"FP16\"\n",
    "\n",
    "model_path = f\"model/public/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/public/{model_name}/{precision}/{model_name}.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download GPT-2 from Open Model Zoo\n",
    "\n",
    "We use `omz_downloader`, which is a command-line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. Skip this step if the model is already downloaded. For this demo, we have to download and use `gpt-2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {model_name} \" \\\n",
    "                   f\"--output_dir {base_model_dir} \" \\\n",
    "                   f\"--cache_dir {base_model_dir}\"\n",
    "\n",
    "display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "display(Markdown(f\"Downloading {model_name}... (This may take a few minutes depending on your connection.)\"))\n",
    "\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GPT-2 to OpenVINO IR\n",
    "Since the downloaded GPT-2 model is not yet in OpenVINO IR format, we to perform an additional step to convert it. Use following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(model_path).exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter --name {model_name} --precisions {precision}\"\n",
    "        f\" --download_dir {base_model_dir} --output_dir {base_model_dir}\"\n",
    "    )\n",
    "    display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "    display(Markdown(f\"Converting {model_name}\"))\n",
    "\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the model\n",
    "\n",
    "Converted models are located in a fixed directory structure, which indicates source, model name and precision. We start by building an Inference Engine object. Then we read the network architecture and model weights from the .xml and .bin files, respectively. Finally, we compile the model for the desired device. Because we use the dynamic shapes feature, which is only available on CPU, we must use `CPU` for the device. Dynamic shapes support on GPU is coming soon.\n",
    "\n",
    "Since the text recognition model has a dynamic input shape, you cannot directly switch device to `GPU` for inference on integrated or discrete Intel GPUs. In order to run inference on iGPU or dGPU with this model, you will need to resize the inputs to this model to use a fixed size and then try running the inference on `GPU` device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "ie_core = Core()\n",
    "\n",
    "# read the model and corresponding weights from file\n",
    "model = ie_core.read_model(model=model_path, weights=model_weights_path)\n",
    "\n",
    "# assign dynamic shapes to every input layer\n",
    "for input_layer in model.inputs:\n",
    "    input_shape = input_layer.partial_shape\n",
    "    input_shape[0] = -1\n",
    "    input_shape[1] = -1\n",
    "    model.reshape({input_layer: input_shape})\n",
    "\n",
    "# compile the model for CPU devices\n",
    "compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_keys = next(iter(compiled_model.inputs))\n",
    "output_keys = next(iter(compiled_model.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Input keys are the names of the input nodes and output keys contain names of the output nodes of the network. In the case of GPT-2, we have `batch size` and `sequence length` as inputs and `batch size`, `sequence length` and `vocab size` as outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pre-Processing\n",
    "\n",
    "NLP models often take a list of tokens as a standard input. A token is a single word mapped to an integer. To provide the proper input, we use a vocabulary file to handle the mapping. So first let's load the vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_file(vocab_file_path):\n",
    "    with open(vocab_file_path, \"r\", encoding=\"utf-8\") as content:\n",
    "        return json.load(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_file_path = f\"model/public/{model_name}/gpt2/vocab.json\"\n",
    "vocab = load_vocab_file(vocal_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts text to tokens\n",
    "def tokenize(text):\n",
    "    input_ids = tokenizer(text)['input_ids']\n",
    "    input_ids = np.array(input_ids).reshape(1, -1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last token in the vocabulary list is an `endoftext` token. We store the index of this token in order to use this index as padding at later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = len(vocab) - 1\n",
    "tokenizer._convert_id_to_token(len(vocab) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Softmax layer\n",
    "A softmax function is used to convert top-k logits into a probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    summation = e_x.sum(axis=-1, keepdims=True)\n",
    "    return e_x / summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the minimum sequence length  \n",
    "If the minimum sequence length is not reached, the following code will reduce the probability of the `eos` token occurring. This continues the process of generating the next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logits(input_ids, scores, eos_token_id, min_length=0):\n",
    "    cur_length = input_ids.shape[-1]\n",
    "    if cur_length < min_length:\n",
    "        scores[:, eos_token_id] = -float(\"inf\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K sampling\n",
    "In Top-K sampling, we filter the K most likely next words and redistribute the probability mass among only those K next words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_logits(scores, top_k):\n",
    "    filter_value = -float(\"inf\")\n",
    "    top_k = min(max(top_k, 1), scores.shape[-1])\n",
    "    top_k_scores = -np.sort(-scores)[:, :top_k]\n",
    "    indices_to_remove = scores < np.min(top_k_scores)\n",
    "    filtred_scores = np.ma.array(scores, mask=indices_to_remove,\n",
    "                                 fill_value=filter_value).filled()\n",
    "    return filtred_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Function\n",
    "Generating the predicted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(input_ids, max_sequence_length=128,\n",
    "                      eos_token_id=eos_token_id):\n",
    "    while True:\n",
    "        cur_input_len = len(input_ids[0])\n",
    "        pad_len = max_sequence_length - cur_input_len\n",
    "        model_input = np.concatenate((input_ids,\n",
    "                                      [[eos_token_id] * pad_len]), axis=-1)\n",
    "        # passing the padded sequnce into the model\n",
    "        outputs = compiled_model(inputs=[model_input])[output_keys]\n",
    "        next_token_logits = outputs[:, cur_input_len - 1, :]\n",
    "        # pre-process distribution\n",
    "        next_token_scores = process_logits(input_ids,\n",
    "                                           next_token_logits, eos_token_id)\n",
    "        top_k = 20\n",
    "        next_token_scores = get_top_k_logits(next_token_scores, top_k)\n",
    "        # get next token id\n",
    "        probs = softmax(next_token_scores)\n",
    "        next_tokens = np.random.choice(probs.shape[-1], 1,\n",
    "                                       p=probs[0], replace=True)\n",
    "        # break the loop if max length or end of text token is reached\n",
    "        if cur_input_len == max_sequence_length or next_tokens == eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            input_ids = np.concatenate((input_ids, [next_tokens]), axis=-1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "The `text` variable below is the input used to generate a predicted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Deep learning is a type of machine learning that uses neural networks\"\n",
    "input_ids = tokenize(text)\n",
    "output_ids = generate_sequence(input_ids)\n",
    "S = \" \"\n",
    "# Convert IDs to words and make the sentence from it\n",
    "for i in output_ids[0]:\n",
    "    S += tokenizer.convert_tokens_to_string(tokenizer._convert_id_to_token(i))\n",
    "print(\"Input Text: \", text)\n",
    "print()\n",
    "print(f\"Predicted Sequence:{S}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "033257a69bf603b2de0dc0c42b5465d421ac707c57e304e82520be1d43cc042f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
