{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7bf10c4",
   "metadata": {},
   "source": [
    "## AI photo-realistic Synthesis with OpenVINO™\n",
    "\n",
    "This notebook demonstrates the synthesis of AI photo-realistic image based on an exemplar image sketch semantic using CocosNet and OpenVINO. We utilize CoCosNet model from Open Model Zoo. At the end of the notebook you should see the demo where users can draw sketch using interactive canvas and get a realistic photo based on provided semantic drawings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c239db",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d031a61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "import logging as log\n",
    "from openvino.runtime import Core\n",
    "\n",
    "from utils.models import CocosnetModel, SegmentationModel\n",
    "from utils.preprocessing import preprocess_for_seg_model, preprocess_image, preprocess_semantics\n",
    "from utils.postprocessing import postprocess\n",
    "\n",
    "log.basicConfig(format='[ %(levelname)s ] %(message)s', level=log.DEBUG, stream=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e70a87",
   "metadata": {},
   "source": [
    "## The Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9842e4",
   "metadata": {},
   "source": [
    "### Download the Model\n",
    "\n",
    "The cococsnet model, will be downloaded to the `base_model_dir`. if you have not already downloaded it. This notebook will also show how you can use the Model downloader to get OpenVINO Intermediate Representation (IR) with FP16 precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adafe30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "translation_model_name = \"cocosnet\"\n",
    "segmentation_model_name = \"hrnet-v2-c1-segmentation\"\n",
    "# selected precision (FP32, FP16)\n",
    "precision = \"FP16\"\n",
    "\n",
    "TRANSLATION_MODEL = f\"{base_model_dir}/public/{translation_model_name}/{precision}/{translation_model_name}\"\n",
    "SEGMENTATION_MODEL = f\"{base_model_dir}/public/{segmentation_model_name}/{precision}/{segmentation_model_name}\"\n",
    "\n",
    "# Path to the model path\n",
    "translation_model_path = Path(TRANSLATION_MODEL).with_suffix(\".xml\")\n",
    "segmentation_model_path = Path(SEGMENTATION_MODEL).with_suffix(\".xml\")\n",
    "\n",
    "if not translation_model_path.exists() :\n",
    "    download_command = (\n",
    "        f\"omz_downloader \" f\"--name {translation_model_name} \" f\"--output_dir {base_model_dir}\"\n",
    "    )\n",
    "    ! $download_command\n",
    "if not segmentation_model_path.exists() :\n",
    "    download_command = (\n",
    "        f\"omz_downloader \" f\"--name {segmentation_model_name} \" f\"--output_dir {base_model_dir}\"\n",
    "    )\n",
    "    ! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f753fb8b",
   "metadata": {},
   "source": [
    "### Convert Model to OpenVINO IR format\n",
    "The selected model comes from the public directory, which means it must be converted into OpenVINO Intermediate Representation (OpenVINO IR). We use `omz_converter` to convert the ONNX format model to the OpenVINO IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb825062",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSLATION_ONNX = f\"{base_model_dir}/public/{translation_model_name}/{translation_model_name}\"\n",
    "SEGMENTATION_ONNX = f\"{base_model_dir}/public/{segmentation_model_name}/{segmentation_model_name}\"\n",
    "translation_onnx_path = Path(TRANSLATION_ONNX).with_suffix(\".onnx\")\n",
    "segmentation_onnx_path = Path(SEGMENTATION_ONNX).with_suffix(\".onnx\")\n",
    "\n",
    "if not translation_onnx_path.exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter \"\n",
    "        f\"--name {translation_model_name} \"\n",
    "        f\"--precisions {precision} \"\n",
    "        f\"--download_dir {base_model_dir} \"\n",
    "        f\"--output_dir {base_model_dir}\"\n",
    "    )\n",
    "    ! $convert_command\n",
    "\n",
    "if not segmentation_onnx_path.exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter \"\n",
    "        f\"--name {segmentation_model_name} \"\n",
    "        f\"--precisions {precision} \"\n",
    "        f\"--download_dir {base_model_dir} \"\n",
    "        f\"--output_dir {base_model_dir}\"\n",
    "    )\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aba207e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SegmentationVisualizer:\n",
    "    def __init__(self, colors_path=None):\n",
    "        if colors_path:\n",
    "            self.color_palette = self.get_palette_from_file(colors_path)\n",
    "            log.debug('The palette is loaded from {}'.format(colors_path))\n",
    "        else:\n",
    "            pascal_palette_path = Path(__file__).resolve().parents[3] /\\\n",
    "                'data/palettes/pascal_voc_21cl_colors.txt'\n",
    "            self.color_palette = self.get_palette_from_file(pascal_palette_path)\n",
    "            log.debug('The PASCAL VOC palette is used')\n",
    "        log.debug('Get {} colors'.format(len(self.color_palette)))\n",
    "        self.color_map = self.create_color_map()\n",
    "\n",
    "    def get_palette_from_file(self, colors_path):\n",
    "        with open(colors_path, 'r') as file:\n",
    "            colors = []\n",
    "            for line in file.readlines():\n",
    "                values = line[line.index('(') + 1:line.index(')')].split(',')\n",
    "                colors.append([int(v.strip()) for v in values])\n",
    "            return colors\n",
    "\n",
    "    def create_color_map(self):\n",
    "        classes = np.array(self.color_palette, dtype=np.uint8)[:, ::-1]  # RGB to BGR\n",
    "        color_map = np.zeros((256, 1, 3), dtype=np.uint8)\n",
    "        classes_num = len(classes)\n",
    "        color_map[:classes_num, 0, :] = classes\n",
    "        color_map[classes_num:, 0, :] = np.random.uniform(0, 255, size=(256 - classes_num, 3))\n",
    "        return color_map\n",
    "\n",
    "    def apply_color_map(self, input):\n",
    "        input_3d = cv2.merge([input, input, input])\n",
    "        return cv2.LUT(input_3d, self.color_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3389bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_segmentation(frame, masks, visualiser, only_masks=False):\n",
    "    output = visualiser.apply_color_map(masks)\n",
    "    if not only_masks:\n",
    "        output = cv2.addWeighted(frame, 0.5, output, 0.5, 0)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f21b0217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ DEBUG ] The palette is loaded from ../colors.txt\n",
      "[ DEBUG ] Get 150 colors\n"
     ]
    }
   ],
   "source": [
    "visualizer = SegmentationVisualizer(\"./data/colors.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ed687c",
   "metadata": {},
   "source": [
    "## Model Initialization\n",
    "\n",
    "We are loading Cocosnet and Segmentation model for image translation.\n",
    "\n",
    "Converted models are located in a fixed structure, which indicates vendor, model name and precision.\n",
    "First, initialize the inference engine, OpenVINO Runtime. Then, read the network architecture and model weights from the .bin and .xml files to compile for the desired device. An inference request is then created to infer the compiled model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1612742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime.\n",
    "ie_core = Core()\n",
    "device = \"CPU\"\n",
    "\n",
    "# Initialize CocosnetModel\n",
    "gan_model = CocosnetModel(ie_core, translation_model_path, device)\n",
    "\n",
    "# Initialize SegmentationModel\n",
    "seg_model = SegmentationModel(ie_core, segmentation_model_path,\n",
    "                              device) if segmentation_model_path else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd6646d",
   "metadata": {},
   "source": [
    "## Input preprocessing and Model inferencing\n",
    "\n",
    "In this section, we are mainly preprocessing the input by using masks from segmentation model to generate input and reference semantics.\n",
    "\n",
    "Model Inference is done with the GAN model, by providing input and reference semantics repectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92f12b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method to get mask from image\n",
    "def get_mask_from_image(image, model):\n",
    "    image = preprocess_for_seg_model(image, input_size=model.input_size)\n",
    "    res = model.infer(image)\n",
    "    print(res.shape)\n",
    "    mask = np.argmax(res, axis=1)\n",
    "    mask = np.squeeze(mask, 0)\n",
    "    return mask\n",
    "\n",
    "\n",
    "# Process the input and reference image\n",
    "def gradioProcessing(input_image=None, reference_image=None):\n",
    "    print(\"input image\",input_image.shape)\n",
    "    print(\"reference image\",reference_image.shape)\n",
    "    \n",
    "    if input_image is None:\n",
    "        raise IOError('Image {} cannot be read'.format(input_image))\n",
    "    input_semantic = get_mask_from_image(input_image, seg_model)\n",
    "    if reference_image is None:\n",
    "        raise IOError('Image {} cannot be read'.format(reference_image))\n",
    "    reference_semantic = get_mask_from_image(reference_image, seg_model)\n",
    "    input_semantic = preprocess_semantics(input_semantic, input_size=gan_model.input_semantic_size)\n",
    "    \n",
    "    if reference_image is None:\n",
    "        raise IOError('Image {} cannot be read'.format(reference_image))\n",
    "    reference_image = preprocess_image(reference_image, input_size=gan_model.input_image_size)\n",
    "    reference_semantic = preprocess_semantics(reference_semantic, input_size=gan_model.input_semantic_size)\n",
    "    \n",
    "    # Model Inference\n",
    "    result = postprocess(gan_model.infer(input_semantic, reference_image,reference_semantic))\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562eee2b",
   "metadata": {},
   "source": [
    "## Gradio interface \n",
    "\n",
    "A web-based GUI to synthesize an image based on the drawing input on the Gradio canvas and the uploaded reference image. \n",
    "User can upload the reference image and draw on the gradio canvas using different colors representing different objects.\n",
    "Click Generate button to synthesize the AI painting.\n",
    "\n",
    "Path to reference images: [data](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/data/image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42ff73b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ DEBUG ] Starting new HTTPS connection (1): api.gradio.app:443\n",
      "[ DEBUG ] Using selector: EpollSelector\n",
      "[ DEBUG ] Starting new HTTP connection (1): 127.0.0.1:7911\n",
      "[ DEBUG ] http://127.0.0.1:7911 \"GET /startup-events HTTP/1.1\" 200 5\n",
      "[ DEBUG ] Starting new HTTP connection (1): 127.0.0.1:7911\n",
      "[ DEBUG ] http://127.0.0.1:7911 \"HEAD / HTTP/1.1\" 200 0\n",
      "Running on local URL:  http://127.0.0.1:7911\n",
      "[ DEBUG ] Starting new HTTPS connection (1): api.gradio.app:443\n",
      "[ DEBUG ] https://api.gradio.app:443 \"POST /gradio-initiated-analytics/ HTTP/1.1\" 200 None\n",
      "[ DEBUG ] https://api.gradio.app:443 \"GET /v2/tunnel-request HTTP/1.1\" 200 None\n",
      "Running on public URL: https://7cc4ab4000f93df6ad.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
      "[ DEBUG ] Starting new HTTPS connection (1): 7cc4ab4000f93df6ad.gradio.live:443\n",
      "[ DEBUG ] https://7cc4ab4000f93df6ad.gradio.live:443 \"HEAD / HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://7cc4ab4000f93df6ad.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ DEBUG ] Starting new HTTPS connection (1): api.gradio.app:443\n",
      "[ DEBUG ] Starting new HTTPS connection (1): api.gradio.app:443\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ DEBUG ] https://api.gradio.app:443 \"POST /gradio-launched-analytics/ HTTP/1.1\" 200 None\n",
      "[ DEBUG ] https://api.gradio.app:443 \"POST /gradio-launched-telemetry/ HTTP/1.1\" 200 None\n",
      "(1, 150, 320, 320)\n",
      "[ DEBUG ] STREAM b'IHDR' 16 13\n",
      "[ DEBUG ] STREAM b'sRGB' 41 1\n",
      "[ DEBUG ] STREAM b'IDAT' 54 5095\n",
      "input image (320, 320, 3)\n",
      "reference image (429, 640, 3)\n",
      "(1, 150, 320, 320)\n",
      "(1, 150, 320, 320)\n",
      "[ DEBUG ] STREAM b'IHDR' 16 13\n",
      "[ DEBUG ] STREAM b'sRGB' 41 1\n",
      "[ DEBUG ] STREAM b'IDAT' 54 6409\n",
      "input image (320, 320, 3)\n",
      "reference image (429, 640, 3)\n",
      "(1, 150, 320, 320)\n",
      "(1, 150, 320, 320)\n",
      "(1, 150, 320, 320)\n",
      "[ DEBUG ] STREAM b'IHDR' 16 13\n",
      "[ DEBUG ] STREAM b'sRGB' 41 1\n",
      "[ DEBUG ] STREAM b'IDAT' 54 5101\n",
      "input image (320, 320, 3)\n",
      "reference image (429, 640, 3)\n",
      "(1, 150, 320, 320)\n",
      "(1, 150, 320, 320)\n",
      "(1, 150, 320, 320)\n",
      "[ DEBUG ] STREAM b'IHDR' 16 13\n",
      "[ DEBUG ] STREAM b'sRGB' 41 1\n",
      "[ DEBUG ] STREAM b'IDAT' 54 5095\n",
      "input image (320, 320, 3)\n",
      "reference image (429, 640, 3)\n",
      "(1, 150, 320, 320)\n",
      "(1, 150, 320, 320)\n"
     ]
    }
   ],
   "source": [
    "# gradio method to fetch user input and reference image\n",
    "def generate(image_input, ref_image):\n",
    "    result = gradioProcessing(input_image=image_input, reference_image=ref_image)\n",
    "    return result\n",
    "\n",
    "def createCanvas(reference_image=None):\n",
    "    canvas = get_mask_from_image(reference_image, seg_model)\n",
    "    canvas = canvas.astype(np.uint8)      \n",
    "    canvas = render_segmentation(reference_image, canvas, visualizer, only_masks=True)\n",
    "    return canvas\n",
    "\n",
    "# Initialize gradio canvas\n",
    "with gr.Blocks(css=\"#small-b {width: 24px}\") as demo:\n",
    "    with gr.Row().style(equal_height=True):\n",
    "        with gr.Column():\n",
    "            canvas_input = gr.ImagePaint(source=\"upload\")\n",
    "            \n",
    "            load_mask = gr.Button(\" Load Painting \")\n",
    "            submit = gr.Button(\"Translate\")\n",
    "        output_image = gr.Image(label='Synthesis')\n",
    "        ref_image = gr.Image(label='Reference')\n",
    "        load_mask.click(createCanvas,inputs=[ref_image],outputs=[canvas_input])\n",
    "        submit.click(generate, inputs=[canvas_input, ref_image], outputs=output_image)\n",
    "\n",
    "# Start the gradio interactive canvas\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a4ee87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
