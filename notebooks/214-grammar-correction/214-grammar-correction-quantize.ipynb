{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b677ff-3399-4f27-ac0f-782bfe25f151",
   "metadata": {},
   "source": [
    "# Post-Training Quantization of Grammar Error Correction model with NNCF\n",
    "\n",
    "The goal of this tutorial is to demonstrate how to speed up the model by applying 8-bit post-training quantization from [NNCF](https://github.com/openvinotoolkit/nncf/) (Neural Network Compression Framework) and infer quantized model via OpenVINO™ Toolkit. The optimization process contains the following steps:\n",
    "\n",
    "1. Quantize the converted OpenVINO model from [214-grammar-correction-convert notebook](214-grammar-correction-convert.ipynb) with NNCF.\n",
    "2. Check model result for the sample text.\n",
    "3. Compare model size and performance of FP32 and quantized INT8 models.\n",
    "\n",
    "> **NOTE**: you should run [214-grammar-correction-convert](214-grammar-correction-convert.ipynb) notebook first to generate OpenVINO IR model that is used for quantization.\n",
    "\n",
    "Table of content:\n",
    "- [Prerequisites](#Prerequisites-$\\Uparrow$)\n",
    "- [Create and initialize quantization](#Create-and-initialize-quantization-$\\Uparrow$)\n",
    "    - [Prepare calibration datasets](#Prepare-calibration-datasets-$\\Uparrow$)\n",
    "    - [Quantize Whisper encoder and decoder models](#Quantize-Whisper-encoder-and-decoder-models-$\\Uparrow$)\n",
    "- [Transcribe video with quantized OpenVINO model](#Transcribe-video-with-quantized-OpenVINO-model-$\\Uparrow$)\n",
    "- [Compare performance and accuracy of the FP32 and INT8 IRs](#Compare-performance-and-accuracy-of-the-FP32-and-INT8-IRs-$\\Uparrow$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9a760a-aaf7-41f6-ab0d-da993e486336",
   "metadata": {},
   "source": [
    "## Prerequisites [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "First we define the prerequisites and load models same as in [214-grammar-correction-convert](214-grammar-correction-convert.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# %pip install -q \"nncf>=2.6.0\"\n",
    "# %pip install -q datasets\n",
    "# %pip install evaluate jiwer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T16:33:01.273350300Z",
     "start_time": "2023-09-18T16:33:01.271349400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "833e0871-c828-4104-a986-230a27c913a5",
   "metadata": {},
   "source": [
    "Select inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "053b4f68-a329-43ac-920c-9d86949edc05",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T16:33:01.927072600Z",
     "start_time": "2023-09-18T16:33:01.273350300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Dropdown(description='Device:', index=4, options=('CPU', 'GPU.0', 'GPU.1', 'GPU.2', 'AUTO'), value='AUTO')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f6126d4d7b3e48b1ac6b02671a781619"
      }
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6131a0ec-654e-435e-a668-55ad33cff74b",
   "metadata": {},
   "source": [
    "Load Grammar Checker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47af0ecf-99ff-4852-bfaa-6692caeaca21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T16:33:03.347739100Z",
     "start_time": "2023-09-18T16:33:01.931140100Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "Set CACHE_DIR to roberta-base-cola/model_cache\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForSequenceClassification\n",
    "\n",
    "grammar_checker_model_id = \"textattack/roberta-base-CoLA\"\n",
    "grammar_checker_dir = Path(\"roberta-base-cola\")\n",
    "grammar_checker_tokenizer = AutoTokenizer.from_pretrained(grammar_checker_model_id)\n",
    "\n",
    "if grammar_checker_dir.exists():\n",
    "    grammar_checker_model = OVModelForSequenceClassification.from_pretrained(grammar_checker_dir, device=device.value)\n",
    "else:\n",
    "    grammar_checker_model = OVModelForSequenceClassification.from_pretrained(grammar_checker_model_id, export=True, device=device.value)\n",
    "    grammar_checker_model.save_pretrained(grammar_checker_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdba3c17-9f94-4d1c-afae-39c857caf5af",
   "metadata": {},
   "source": [
    "Load Grammar Corrector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the encoder...\n",
      "Compiling the decoder...\n",
      "Compiling the decoder...\n"
     ]
    }
   ],
   "source": [
    "from optimum.intel.openvino import OVModelForSeq2SeqLM\n",
    "\n",
    "grammar_corrector_model_id = \"pszemraj/flan-t5-large-grammar-synthesis\"\n",
    "grammar_corrector_dir = Path(\"flan-t5-large-grammar-synthesis\")\n",
    "grammar_corrector_tokenizer = AutoTokenizer.from_pretrained(grammar_corrector_model_id)\n",
    "\n",
    "if grammar_corrector_dir.exists():\n",
    "    grammar_corrector_model_fp32 = OVModelForSeq2SeqLM.from_pretrained(grammar_corrector_dir, device=device.value)\n",
    "else:\n",
    "    grammar_corrector_model_fp32 = OVModelForSeq2SeqLM.from_pretrained(grammar_corrector_model_id, export=True, device=device.value)\n",
    "    grammar_corrector_model_fp32.save_pretrained(grammar_corrector_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T16:33:12.888488700Z",
     "start_time": "2023-09-18T16:33:03.356739400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create grammar checker and corrector pipelines"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf3d0d24-c94a-42c7-b603-499bd9d251d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T16:33:13.571733800Z",
     "start_time": "2023-09-18T16:33:13.571733800Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "grammar_checker_pipe = pipeline(\"text-classification\", model=grammar_checker_model, tokenizer=grammar_checker_tokenizer)\n",
    "grammar_corrector_pipe_fp32 = pipeline(\"text2text-generation\", model=grammar_corrector_model_fp32, tokenizer=grammar_corrector_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69faa673-45fd-481d-9573-4f54ea17fb77",
   "metadata": {},
   "source": [
    "## Quantize Grammar Corrector [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "TODO: description"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Collect calibration dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c2cb9ec-8886-4ed5-b2fc-f9733ba93ea9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T16:33:31.030244800Z",
     "start_time": "2023-09-18T16:33:13.580008200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Collecting calibration data:   0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08d90dee538149159a1fc5bb9ecee61e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsavel/venvs/ov_notebooks/lib/python3.8/site-packages/optimum/intel/openvino/modeling_seq2seq.py:339: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  last_hidden_state = torch.from_numpy(self.request(inputs, shared_memory=True)[\"last_hidden_state\"]).to(\n",
      "/home/nsavel/venvs/ov_notebooks/lib/python3.8/site-packages/optimum/intel/openvino/modeling_seq2seq.py:417: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n",
      "/tmp/ipykernel_4027678/2901987538.py:13: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  return original_fn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "CALIBRATION_DATASET_SIZE = 10\n",
    "calibration_data = []\n",
    "ov_decoder = grammar_corrector_pipe_fp32.model.decoder_with_past\n",
    "\n",
    "def wrap_for_data_collection():\n",
    "    original_fn = ov_decoder.request.start_async\n",
    "    def wrapper(*args, **kwargs):\n",
    "        inputs = kwargs.get(\"inputs\", args[0])\n",
    "        calibration_data.append(inputs)\n",
    "        return original_fn(*args, **kwargs)\n",
    "    ov_decoder.request.start_async = wrapper\n",
    "\n",
    "wrap_for_data_collection()\n",
    "\n",
    "calibration_dataset = datasets.load_dataset(\"jfleg\", split=f\"validation[:{CALIBRATION_DATASET_SIZE}]\")\n",
    "\n",
    "for data_item in tqdm(calibration_dataset, total=CALIBRATION_DATASET_SIZE, desc=\"Collecting calibration data\"):\n",
    "    grammar_corrector_pipe_fp32(data_item[\"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Quantize"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics collection: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 279/279 [00:17<00:00, 15.73it/s]\n",
      "Applying Smooth Quant: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 145/145 [00:05<00:00, 24.97it/s]\n",
      "Statistics collection: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 279/279 [00:57<00:00,  4.85it/s]\n",
      "Applying Fast Bias correction: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "import openvino.runtime as ov\n",
    "import nncf\n",
    "from nncf.quantization.range_estimator import RangeEstimatorParameters, StatisticsCollectorParameters, StatisticsType, AggregatorType\n",
    "\n",
    "quantized_model_path = Path(\"quantized_decoder_with_past\") / \"openvino_model.xml\"\n",
    "\n",
    "quantized_model = nncf.quantize(\n",
    "    ov_decoder.model,\n",
    "    calibration_dataset=nncf.Dataset(calibration_data),\n",
    "    subset_size=len(calibration_data),\n",
    "    model_type=nncf.ModelType.TRANSFORMER,\n",
    "    advanced_parameters=nncf.AdvancedQuantizationParameters(\n",
    "        smooth_quant_alpha=0.95,\n",
    "        activations_range_estimator_params=RangeEstimatorParameters(\n",
    "            max=StatisticsCollectorParameters(statistics_type=StatisticsType.QUANTILE, aggregator_type=AggregatorType.MEAN)\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "if not quantized_model_path.parent.exists():\n",
    "    quantized_model_path.parent.mkdir(parents=True)\n",
    "ov.serialize(quantized_model, quantized_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T16:37:51.937019600Z",
     "start_time": "2023-09-18T16:33:31.070635600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create quantized grammar corrector pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the encoder...\n",
      "Compiling the decoder...\n",
      "Compiling the decoder...\n",
      "Compiling the decoder...\n"
     ]
    }
   ],
   "source": [
    "grammar_corrector_model_int8 = OVModelForSeq2SeqLM.from_pretrained(grammar_corrector_dir, device=device.value)\n",
    "grammar_corrector_model_int8.decoder_with_past.model = quantized_model\n",
    "grammar_corrector_model_int8.decoder_with_past.request = None\n",
    "grammar_corrector_model_int8.decoder_with_past._compile()\n",
    "grammar_corrector_pipe_int8 = pipeline(\"text2text-generation\", model=grammar_corrector_model_int8, tokenizer=grammar_corrector_tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T16:47:46.780952700Z",
     "start_time": "2023-09-18T16:47:37.720926900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "26d3d759-3cb2-418d-82f8-3be2e445916a",
   "metadata": {},
   "source": [
    "Let us see it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee397f5-12cb-460b-8824-327f19af8e5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-18T16:38:06.852946400Z",
     "start_time": "2023-09-18T16:38:02.108949700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "correcting text..:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9843e1071d2543ec8f2e2b43fa078dc5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import correct_text\n",
    "\n",
    "default_text = (\n",
    "    \"Most of the course is about semantic or  content of language but there are also interesting\"\n",
    "    \" topics to be learned from the servicefeatures except statistics in characters in documents.At\"\n",
    "    \" this point, He introduces herself as his native English speaker and goes on to say that if\"\n",
    "    \" you contine to work on social scnce\"\n",
    ")\n",
    "\n",
    "corrected_text = correct_text(default_text, grammar_checker_pipe, grammar_corrector_pipe_int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5862ec36-8d77-418f-9295-5dc644b50068",
   "metadata": {
    "tags": [],
    "ExecuteTime": {
     "end_time": "2023-09-18T16:38:06.875212200Z",
     "start_time": "2023-09-18T16:38:06.852946400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input text:     Most of the course is about semantic or  content of language but there are also interesting topics to be learned from the servicefeatures except statistics in characters in documents.At this point, He introduces herself as his native English speaker and goes on to say that if you contine to work on social scnce\n",
      "\n",
      "generated text: Most of the course is about the semantic content of language but there are also interesting topics to be learned from the service features except statistics in characters in documents. At this point, she introduces herself as a native English speaker and goes on to say that if you continue to work on social science, you will continue to be successful.\n"
     ]
    }
   ],
   "source": [
    "print(f\"input text:     {default_text}\\n\") \n",
    "print(f'generated text: {corrected_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Compare model size and performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: openvino_decoder_with_past_model\n",
      "    * FP32 IR model size: 1658150.16 KB\n",
      "    * INT8 IR model size: 513467.73 KB\n",
      "    * Model compression rate: 3.229\n"
     ]
    }
   ],
   "source": [
    "def calculate_compression_rate(model_path_ov, model_path_ov_int8):\n",
    "    model_size_fp32 = model_path_ov.with_suffix(\".bin\").stat().st_size / 1024\n",
    "    model_size_int8 = model_path_ov_int8.with_suffix(\".bin\").stat().st_size / 1024\n",
    "    print(f\"Model: {model_path_ov.stem}\")\n",
    "    print(f\"    * FP32 IR model size: {model_size_fp32:.2f} KB\")\n",
    "    print(f\"    * INT8 IR model size: {model_size_int8:.2f} KB\")\n",
    "    print(f\"    * Model compression rate: {model_size_fp32 / model_size_int8:.3f}\")\n",
    "\n",
    "calculate_compression_rate(grammar_corrector_dir / \"openvino_decoder_with_past_model.xml\", quantized_model_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T16:38:06.904673600Z",
     "start_time": "2023-09-18T16:38:06.875047200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Evaluation:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "42f776ddbed54345b1abb3ff7b9c9ded"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4027678/2901987538.py:13: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  return original_fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Evaluation:   0%|          | 0/50 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ed1496facb7a47f8a8a33c154adf648c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar correction performance speedup: 1.217\n",
      "Grammar correction word accuracy. FP32: 68.55%. INT8: 68.82%. Accuracy drop :-0.27%.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from jiwer import wer, wer_standardize\n",
    "\n",
    "TEST_DATASET_SIZE = 50\n",
    "test_dataset = datasets.load_dataset(\"jfleg\", split=f\"test[:{TEST_DATASET_SIZE}]\")\n",
    "\n",
    "def calculate_inference_time_and_accuracy(grammar_corrector_pipe):\n",
    "    ground_truths = []\n",
    "    predictions = []\n",
    "    inference_time = []\n",
    "    for data_item in tqdm(test_dataset, total=TEST_DATASET_SIZE, desc=\"Evaluation\"):\n",
    "        input_text = data_item[\"sentence\"]\n",
    "        reference = data_item[\"corrections\"][0]\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        corrected_text = correct_text(input_text, grammar_checker_pipe, grammar_corrector_pipe, disable_tqdm=True)\n",
    "        end_time = time.perf_counter()\n",
    "        delta_time = end_time - start_time\n",
    "\n",
    "        ground_truths.append(reference)\n",
    "        predictions.append(corrected_text)\n",
    "        inference_time.append(delta_time)\n",
    "\n",
    "    word_accuracy = (1 - wer(ground_truths, predictions, reference_transform=wer_standardize, hypothesis_transform=wer_standardize)) * 100\n",
    "    mean_inference_time = np.mean(inference_time)\n",
    "    return mean_inference_time, word_accuracy\n",
    "\n",
    "inference_time_fp32, accuracy_fp32 = calculate_inference_time_and_accuracy(grammar_corrector_pipe_fp32)\n",
    "inference_time_int8, accuracy_int8 = calculate_inference_time_and_accuracy(grammar_corrector_pipe_int8)\n",
    "print(f\"Grammar correction performance speedup: {inference_time_fp32 / inference_time_int8:.3f}\")\n",
    "print(f\"Grammar correction word accuracy. FP32: {accuracy_fp32:.2f}%. INT8: {accuracy_int8:.2f}%. Accuracy drop :{accuracy_fp32 - accuracy_int8:.2f}%.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-18T16:49:31.470488800Z",
     "start_time": "2023-09-18T16:47:46.780952700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "a6bebbeb-176a-43f0-99d1-e96e6db60ccf",
   "metadata": {},
   "source": [
    "## Interactive demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "87cfc164-46f4-4310-b088-7504f3f42da1",
   "metadata": {
    "test_replace": {
     "    demo.queue().launch(debug=True)": "    demo.queue().launch()",
     "    demo.queue().launch(share=True, debug=True)": "    demo.queue().launch(share=True)"
    },
    "ExecuteTime": {
     "end_time": "2023-09-18T16:40:00.722290500Z",
     "start_time": "2023-09-18T16:40:00.714247Z"
    }
   },
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "#\n",
    "#\n",
    "# def correct(text, _=gr.Progress(track_tqdm=True)):\n",
    "#     return correct_text(text, grammar_checker_pipe, grammar_corrector_pipe_int8)\n",
    "#\n",
    "#\n",
    "# demo = gr.Interface(\n",
    "#     correct,\n",
    "#     gr.Textbox(label=\"Text\"),\n",
    "#     gr.Textbox(label=\"Correction\"),\n",
    "#     examples=[default_text],\n",
    "#     allow_flagging=\"never\",\n",
    "# )\n",
    "# try:\n",
    "#     demo.queue().launch(debug=True, server_port=7860)\n",
    "# except Exception:\n",
    "#     demo.queue().launch(share=True, debug=True)\n",
    "# # if you are launching remotely, specify server_name and server_port\n",
    "# # demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# # Read more in the docs: https://gradio.app/docs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04987ecd2c7a48c48a599fd1f23ea586": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "2cf6aa309bfe43ae9f694ee2e26b27eb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_8865247c89cc4a60b22380b09547f508",
       "style": "IPY_MODEL_7bd41bb8c1ad4bf6a2ee29a96900eea6",
       "value": "correcting text..: 100%"
      }
     },
     "40d0eca7cba048a79102b84d1e14a802": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "44b1c27db36641e58268ac097c32240c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextareaModel",
      "state": {
       "description": "your text",
       "layout": "IPY_MODEL_a2d5a746e06e4e81aa4d2c4faadd9ed7",
       "style": "IPY_MODEL_04987ecd2c7a48c48a599fd1f23ea586",
       "value": "Most of the course is about semantic or  content of language but there are also interesting topics to be learned from the servicefeatures except statistics in characters in documents.At this point, He introduces herself as his native English speaker and goes on to say that if you contine to work on social scnce"
      }
     },
     "62b441aad45e407c9b108778bb7819b7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_40d0eca7cba048a79102b84d1e14a802",
       "max": 1,
       "style": "IPY_MODEL_63a5bb6794e14161b6a507fa233c7ff6",
       "value": 1
      }
     },
     "63a5bb6794e14161b6a507fa233c7ff6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "6f75e3b03db54b3aaf74c8039a46c904": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_fd18874977234332aba5ea97af20e016",
       "style": "IPY_MODEL_ebec355dbe87452fbceb2bde75fa716c",
       "value": " 1/1 [00:04&lt;00:00,  4.42s/it]"
      }
     },
     "7bd41bb8c1ad4bf6a2ee29a96900eea6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "873341cd0b374e6abd68ead9e0fb5eea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_2cf6aa309bfe43ae9f694ee2e26b27eb",
        "IPY_MODEL_62b441aad45e407c9b108778bb7819b7",
        "IPY_MODEL_6f75e3b03db54b3aaf74c8039a46c904"
       ],
       "layout": "IPY_MODEL_964b79b8f6c84659861916b5098d44b4"
      }
     },
     "8865247c89cc4a60b22380b09547f508": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "964b79b8f6c84659861916b5098d44b4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e896b2be776416b906d41bca2cc56a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "a2d5a746e06e4e81aa4d2c4faadd9ed7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "width": "auto"
      }
     },
     "a6cc5a5b6a764c07883ab0d2aa6b7ae9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "GPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 2,
       "layout": "IPY_MODEL_ca698f18cc0c48abac11c1d6d4075e91",
       "style": "IPY_MODEL_9e896b2be776416b906d41bca2cc56a6"
      }
     },
     "ca698f18cc0c48abac11c1d6d4075e91": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ebec355dbe87452fbceb2bde75fa716c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fd18874977234332aba5ea97af20e016": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
