{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc41ac0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe80a355",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\arunimac\\AppData\\Roaming\\Python\\Python38\\site-packages\\onnx\\mapping.py:27: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  int(TensorProto.STRING): np.dtype(np.object)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from openvino.inference_engine import IECore\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import onnx\n",
    "import os\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde5b43",
   "metadata": {},
   "source": [
    "# Transformers Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742fd4b",
   "metadata": {},
   "source": [
    "Check out this link [https://huggingface.co/docs/transformers/serialization] to learn how to convert transformers to onnx format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db803ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: Hugging Face ONNX Exporter tool [-h] -m MODEL\n",
      "                                       [--feature {causal-lm,causal-lm-with-past,default,default-with-past,masked-lm,question-answering,seq2seq-lm,seq2seq-lm-with-past,sequence-classification,token-classification}]\n",
      "                                       [--opset OPSET] [--atol ATOL]\n",
      "                                       output\n",
      "\n",
      "positional arguments:\n",
      "  output                Path indicating where to store generated ONNX model.\n",
      "\n",
      "optional arguments:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 16:12:39.295443: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-11-21 16:12:39.295541: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  -h, --help            show this help message and exit\n",
      "  -m MODEL, --model MODEL\n",
      "                        Model's name of path on disk to load.\n",
      "  --feature {causal-lm,causal-lm-with-past,default,default-with-past,masked-lm,question-answering,seq2seq-lm,seq2seq-lm-with-past,sequence-classification,token-classification}\n",
      "                        Export the model with some additional feature.\n",
      "  --opset OPSET         ONNX opset version to export the model with (default\n",
      "                        12).\n",
      "  --atol ATOL           Absolute difference tolerence when validating the\n",
      "                        model.\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4794f066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 16:12:49.614851: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2022-11-21 16:12:49.614940: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "Using framework PyTorch: 1.9.1+cpu\n",
      "Validating ONNX model...\n",
      "\t-[\\u2713] ONNX model outputs' name match reference model ({'logits'})\n",
      "\t- Validating ONNX Model output \"logits\":\n",
      "\t\t-[\\u2713] (2, 2) matches (2, 2)\n",
      "\t\t-[\\u2713] all values close (atol: 1e-05)\n",
      "All good, model saved at: model/model.onnx\n"
     ]
    }
   ],
   "source": [
    "!python -m transformers.onnx -m distilbert-base-uncased-finetuned-sst-2-english --feature sequence-classification model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782bbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff79fc",
   "metadata": {},
   "source": [
    "# Model Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f48c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \tC:\\Users\\arunimac\\OneDrive - Intel Corporation\\Documents\\Projects\\OpenVINO Contrib\\224-distilbert-Sequence-Classification\\model\\model.onnx\n",
      "\t- Path for generated IR: \tC:\\Users\\arunimac\\OneDrive - Intel Corporation\\Documents\\Projects\\OpenVINO Contrib\\224-distilbert-Sequence-Classification\\model/\n",
      "\t- IR output name: \tdistilbert-base-uncased-finetuned-sst-2-english\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tinput_ids,attention_mask\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \t[1,512],[1,512]\n",
      "\t- Source layout: \tNot specified\n",
      "\t- Target layout: \tNot specified\n",
      "\t- Layout: \tNot specified\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- User transformations: \tNot specified\n",
      "\t- Reverse input channels: \tFalse\n",
      "\t- Enable IR generation for fixed input shape: \tFalse\n",
      "\t- Use the transformations config file: \tNone\n",
      "Advanced parameters:\n",
      "\t- Force the usage of legacy Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "\t- Force the usage of new Frontend of Model Optimizer for model conversion into IR: \tFalse\n",
      "OpenVINO runtime found in: \tc:\\users\\arunimac\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\openvino\n",
      "OpenVINO runtime version: \t2022.2.0-7690-940e927a22b-refs/pull/1296/head\n",
      "Model Optimizer version: \t2022.2.0-7690-940e927a22b-refs/pull/1296/head\n",
      "[ SUCCESS ] Generated IR version 11 model.\n",
      "[ SUCCESS ] XML file: C:\\Users\\arunimac\\OneDrive - Intel Corporation\\Documents\\Projects\\OpenVINO Contrib\\224-distilbert-Sequence-Classification\\model\\distilbert-base-uncased-finetuned-sst-2-english.xml\n",
      "[ SUCCESS ] BIN file: C:\\Users\\arunimac\\OneDrive - Intel Corporation\\Documents\\Projects\\OpenVINO Contrib\\224-distilbert-Sequence-Classification\\model\\distilbert-base-uncased-finetuned-sst-2-english.bin\n",
      "[ SUCCESS ] Total execution time: 4.55 seconds. \n",
      "[ INFO ] The model was converted to IR v11, the latest model format that corresponds to the source DL framework input/output format. While IR v11 is backwards compatible with OpenVINO Inference Engine API v1.0, please use API v2.0 (as of 2022.1) to take advantage of the latest improvements in IR v11.\n",
      "Find more information about API v2.0 and IR v11 at https://docs.openvino.ai\n"
     ]
    }
   ],
   "source": [
    "onnx_model_path = 'model.onnx'\n",
    "MODEL_DIR = f\"model/\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "onnx_model_path = Path(MODEL_DIR)/onnx_model_path\n",
    "\n",
    "!mo --input_model $onnx_model_path --output_dir $MODEL_DIR --model_name $checkpoint --input input_ids,attention_mask --input_shape [1,512],[1,512] --data_type FP32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878fbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_model_xml = (Path(MODEL_DIR)/checkpoint).with_suffix(\".xml\")\n",
    "ir_model_bin = (Path(MODEL_DIR)/checkpoint).with_suffix(\".bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc074e",
   "metadata": {},
   "source": [
    "Creating the input and output features for the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "026c9eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "ie = IECore()\n",
    "net_onnx = ie.read_network(model=ir_model_xml, weights=ir_model_bin)\n",
    "exec_net_onnx = ie.load_network(network=net_onnx, device_name=\"CPU\")\n",
    "\n",
    "output_layer_onnx = next(iter(exec_net_onnx.outputs))\n",
    "input_layer_onnx = next(iter(exec_net_onnx.input_info))\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_keys = list(exec_net_onnx.input_info)\n",
    "output_keys = list(exec_net_onnx.outputs.keys())\n",
    "\n",
    "# get network input size\n",
    "input_size = exec_net_onnx.input_info[input_keys[0]].input_data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb25e2",
   "metadata": {},
   "source": [
    "Defining a softmax function to extract the prediction from the output of the IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de01fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e778507",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd429d55",
   "metadata": {},
   "source": [
    "Creating a generic inference function to read the input and infer the result into 2 classes: Positive or Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc0c91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input_text):\n",
    "    input_text = tokenizer(input_text, padding='max_length', max_length = 512, truncation=True, return_tensors=\"pt\")\n",
    "    inputs = {'input_ids': input_text['input_ids'], 'attention_mask': input_text['attention_mask']}\n",
    "    result = exec_net_onnx.infer(inputs= inputs)\n",
    "    result_ir = result[output_layer_onnx]\n",
    "    probability = np.argmax(softmax(result_ir))\n",
    "    print(\"Label:\")\n",
    "    label = {0: 'NEGATIVE', 1: 'POSITIVE'}     \n",
    "    return label[probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e79fd",
   "metadata": {},
   "source": [
    "For a single input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf976f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input:\n",
      "Have a great day\n",
      "Label:\n",
      "POSITIVE\n"
     ]
    }
   ],
   "source": [
    "print(\"User Input:\")\n",
    "input_text = input()\n",
    "result = infer(input_text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d013",
   "metadata": {},
   "source": [
    "Read from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63f57d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Input:\n",
      " The food was horrible.\n",
      "Label:\n",
      "NEGATIVE\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/sample.txt\",\"r\") as f:\n",
    "    input_text = f.readlines()\n",
    "print(\"User Input:\\n\",input_text[0])\n",
    "result = infer(input_text)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
