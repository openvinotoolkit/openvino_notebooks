{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize Open Model Zoo object detection models\n",
    "Quantizing a model accelerates a trained model by reducing the precision necessary for its calculations.  Acceleration comes from lower-precision calculations being faster as well as less memory needed and less data to transfer since the data type itself is smaller along with the model weights data.  Though lower-precision may reduce model accuracy, typically a model using 32-bit floating-point precision (FP32) can be quantized to use lower-precision 8-bit integers (INT8) giving good results that are worth the trade off between accuracy and speed.  To see how quantization can accelerate models, see [INT8 vs FP32 Comparison on Select Networks and Platforms](https://docs.openvino.ai/latest/openvino_docs_performance_int8_vs_fp32.html#doxid-openvino-docs-performance-int8-vs-fp32) for some benchmarking results.\n",
    "\n",
    "[Intel Distribution of OpenVINO toolkit](https://software.intel.com/openvino-toolkit) includes the [Post-Training Optimization Tool (POT)](https://docs.openvino.ai/latest/pot_README.html) to automate quantization.  For models available from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), the [`omz_quantizer`](../104-model-tools/104-model-tools.ipynb) tool is available to automate running POT using its [DefaultQuantization](https://docs.openvino.ai/latest/pot_compression_algorithms_quantization_default_README.html#doxid-pot-compression-algorithms-quantization-default-r-e-a-d-m-e) 8-bit quantization algorithm to quantize models down to INT8 precision.\n",
    "\n",
    "This Jupyter* Notebook will go step-by-step through the workflow of downloading either the [ssd_mobilenet_v1_coco](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/ssd_mobilenet_v1_coco) or the [yolo-v4-tf](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/yolo-v4-tf) model from the Open Model Zoo through quantization and then checking and benchmarking the results.  The workflow consists of following these steps:\n",
    "1. Download and set up the the [Common Objects in Context (COCO)](https://cocodataset.org/) validation dataset to be used by omz_quantize\n",
    "2. Download model from the Open Model Zoo\n",
    "3. Convert model to FP32 IR files\n",
    "4. Quantize FP32 model to create INT8 IR files\n",
    "5. Run inference on original and quantized model\n",
    "6. Check accuracy before and after quantization\n",
    "7. Benchmark before and after quantization\n",
    "\n",
    "While performing the steps above, the following [OpenVINO tools](../104-model-tools/104-model-tools.ipynb) will be used to download, convert, quantize, check accuracy, and benchmark the model:\n",
    "- `omz_downloader` - Download model from the Open Model Zoo\n",
    "- `omz_converter` - Convert an Open Model Zoo model\n",
    "- `omz_quantizer` - Quantize an Open Model Zoo model\n",
    "- `accuracy_check` - Check the accuracy of models using a validation dataset\n",
    "- `benchmark_app` - Benchmark models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the models\n",
    "This notebook is configurable to work with either of the two Open Model Zoo object detection models: ssd_mobilenet_v1_coco (the default) or yolo-v4-tf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the ssd_mobilenet_v1_coco model\n",
    "The ssd_mobilenet_v1_coco model is a [Single-Shot multi-box Detection (SSD) network](https://arxiv.org/abs/1801.04381) that has been trained on the COCO dataset to perform object detection.  \n",
    "The input to the converted model is a 300x300 BGR image.  The output of the model is an array of detection information for up to 100 objects giving the:\n",
    "- image_id: image identifier of the image within the batch\n",
    "- label: class identifier in the range of 1-91 for each class, plus one for background\n",
    "- confidence: the prediction probability in the range of 0.0-1.0 for label\n",
    "- (x_min, y_min): coordinates in normalized format (range 0.0-1.0) of the top-left of the bounding box\n",
    "- (x_max, y_max): coordinates in normalized format (range 0.0-1.0) of the bottom-right of the bounding box\n",
    "\n",
    "For details more details on the ssd_mobilenet_v1_coco model, see the Open Model Zoo [model](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/ssd_mobilenet_v1_coco)  and the [paper](https://arxiv.org/abs/1801.04381)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the yolo-v4-tf model\n",
    "The yolo-v4-tf model is a YOLO v4 real-time object detection model that was implemented in a Keras* framework and converted to a TensorFlow* framework.  The model was trained on the [Common Objects in Context (COCO)](https://cocodataset.org/#home) dataset with 80 classes.  The input to the converted model is a 608x608 BGR image.  The output of the model are arrays of detection boxes contained in the three output layers:\n",
    "- StatefulPartitionedCall/model/conv2d_93/BiasAdd/Add: 76x76 \n",
    "- StatefulPartitionedCall/model/conv2d_101/BiasAdd/Add: 38x38\n",
    "- StatefulPartitionedCall/model/conv2d_109/BiasAdd/Add: 19x19\n",
    "\n",
    "Each output layer contains an NxN array for different sized detection boxes within the original image.  Each detection box contains the following information:\n",
    "- (x, y) - raw coordinates of box center, must apply [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) to get relative to the cell coordinates\n",
    "- h, w - raw height and width of box, must apply [exponential function](https://en.wikipedia.org/wiki/Exponential_function) and multiply by corresponding anchors to get absolute height and width values\n",
    "- box_score - confidence of detection box, must apply [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) to get confidence in 0.0-1.0 range\n",
    "- class_no[80] - array of probability distribution over the 80 classes in logits format, must apply [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) and multiply by obtained confidence value to get confidence for each class\n",
    "\n",
    "To reduce the results from the three output layers into distinct objects within the original image, the \"intersection over union\" (also known as the [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)) algorithm is typically used to combine overlapping detection boxes with the same class into a single box containing the detected object.\n",
    "\n",
    "For details more details on the yolo-v4-tf model, see the Open Model Zoo [model](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/yolo-v4-tf), the paper [\"YOLOv4: Optimal Speed and Accuracy of Object Detection\"](https://arxiv.org/abs/2004.10934), and the [repository](https://github.com/david8862/keras-YOLOv3-model-set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import shutil\n",
    "import sys\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from subprocess import PIPE, STDOUT, Popen\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as nbutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "By default, this notebook will run using the ssd_mobilenet_v1_coco object detection model.  The `USE_YOLOV4_MODEL` variable may be set to select the model to use as follows:\n",
    "* `OMZ_MODEL_NAME`: Set to `OMZ_MODEL_NAME_YOLO` to use the yolo-v4-tf model or set to `OMZ_MODEL_NAME_SSD_MOBILNET` to use the ssd_mobilenet_v1_coco model\n",
    "\n",
    "The variable `USING_YOLOV4_MODEL` is set according to the value of `OMZ_MODEL_NAME` and is used later in the code where the two models require different processing (e.g. post-processing inference results).\n",
    "\n",
    "By default, this notebook downloads the model, dataset, etc. to subdirectories where this notebook is located.  The following variables may be used to set file locations:\n",
    "* `OMZ_MODEL_NAME`: Model name as it appears on the Open Model Zoo\n",
    "* `DATA_DIR`: Directory where dataset will be downloaded and set up\n",
    "* `MODEL_DIR`: Models will be downloaded into the `intel` and `public` folders in this directory\n",
    "* `OUTPUT_DIR`: Directory used to store any output and other downloaded files (e.g. configuration files for running accuracy_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base settings\n",
    "OMZ_MODEL_NAME_YOLO = \"yolo-v4-tf\"\n",
    "OMZ_MODEL_NAME_SSD_MOBILNET = \"ssd_mobilenet_v1_coco\"\n",
    "\n",
    "OMZ_MODEL_NAME = OMZ_MODEL_NAME_SSD_MOBILNET\n",
    "# OMZ_MODEL_NAME = OMZ_MODEL_NAME_YOLO\n",
    "\n",
    "USING_YOLOV4_MODEL = bool(OMZ_MODEL_NAME == OMZ_MODEL_NAME_YOLO)\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATASET_DIR = DATA_DIR / \"coco\"\n",
    "\n",
    "if USING_YOLOV4_MODEL:\n",
    "    LABELS_PATH = DATASET_DIR / \"coco_80cl.txt\"\n",
    "else:\n",
    "    LABELS_PATH = DATASET_DIR / \"coco_91cl_bkgr.txt\"\n",
    "\n",
    "TEST_INPUT_IMAGE = DATASET_DIR / \"val2017/000000005477.jpg\"  # airplane\n",
    "# TEST_INPUT_IMAGE = DATASET_DIR / \"val2017/000000000285.jpg\"  # bear\n",
    "# TEST_INPUT_IMAGE = DATASET_DIR / \"val2017/000000007108.jpg\"   # elephants\n",
    "\n",
    "# different model precisions location\n",
    "MODEL_PUBLIC_DIR = MODEL_DIR / \"public\" / OMZ_MODEL_NAME\n",
    "MODEL_FP32_DIR = MODEL_PUBLIC_DIR / \"FP32\"\n",
    "MODEL_FP32INT8_DIR = MODEL_PUBLIC_DIR / \"FP32-INT8\"\n",
    "\n",
    "# create directories if they do not already exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The `run_command_line()` helper function is provided to aid filtering the output of some of the commands that will be run.  The two functions, `parse_yolo_region()` and `filter_yolo_detections()` are used during post-processing of the yolo-v4-tf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command_line(cmd: str, filter=None):\n",
    "    \"\"\"\n",
    "    runs the given command-line outputting lines as they become available to show progress in ~realtime.\n",
    "    If a filter is provided, it will be called with each line before printing the result from calling the filter\n",
    "    :param cmd: String containing complete command-line to run\n",
    "    :param filter: Optional filter called per-line before printing\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    proc = Popen(cmd.split(), stdout=PIPE, stderr=STDOUT, universal_newlines=True)\n",
    "    while proc.poll() is None:\n",
    "        line = proc.stdout.readline()\n",
    "        if filter is not None:\n",
    "            line = filter(line)\n",
    "        if line is not None:\n",
    "            sys.stdout.write(\"%s\" % (line))\n",
    "\n",
    "\n",
    "def parse_yolo_region(\n",
    "    output_blob,\n",
    "    layer_name,\n",
    "    input_width,\n",
    "    input_height,\n",
    "    orig_width,\n",
    "    orig_height,\n",
    "    threshold,\n",
    "):\n",
    "    \"\"\"\n",
    "    Parse the yolo inference output for a layer/region returning a list of detections \n",
    "     with coordinates within the original imput image\n",
    "    :param output_blob: Output blob containing inference layer's results\n",
    "    :param layer_name: Name of output layer\n",
    "    :param input_width: Inference input's width\n",
    "    :param input_height: Inference input's height\n",
    "    :param orig_width: Original input image's width\n",
    "    :param orig_height: Original input image's height\n",
    "    :param threshold: Confidence threshold for determining an object detection\n",
    "    return: List of all dections for the layer's output_blob\n",
    "    \"\"\"\n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    # YOLOV4 parameters\n",
    "    num_channels = 3\n",
    "    num_coords = 4\n",
    "    num_classes = 80\n",
    "    # anchors needed for all the output layers\n",
    "    anchors = {\n",
    "        \"conv2d_93/BiasAdd\": [12.0, 16.0,19.0, 36.0,40.0, 28.0],  # layer conv2d_93/BiasAdd\n",
    "        \"conv2d_101/BiasAdd\": [36.0, 75.0, 76.0, 55.0, 72.0, 146.0],  # layer conv2d_101/BiasAdd\n",
    "        \"conv2d_109/BiasAdd\": [142.0, 110.0, 192.0, 243.0, 459.0, 401.0],  # layer conv2d_109/BiasAdd\n",
    "    }\n",
    "    anchor_key = [key for key in anchors.keys() if key in layer_name]\n",
    "    layer_anchors = []\n",
    "    if len(anchor_key) > 0:\n",
    "        layer_anchors = anchors[anchor_key[0]]\n",
    "    else:\n",
    "        return []\n",
    "    # ------------------------------------------ Extracting layer parameters ---------------------------------------\n",
    "    objects = []\n",
    "    bbox_size = num_coords + 1 + num_classes\n",
    "    output_width = output_blob.shape[2]\n",
    "    output_height = output_blob.shape[3]\n",
    "    # ------------------------------------------- Parsing YOLO Region output ---------------------------------------\n",
    "    for row, col, n in np.ndindex(output_height, output_width, num_channels):\n",
    "        # Getting raw values for each detection bounding bbox\n",
    "        bbox = output_blob[0, n * bbox_size : (n + 1) * bbox_size, row, col]\n",
    "        x, y = sigmoid(bbox[:2])\n",
    "        width, height = bbox[2:4]\n",
    "        object_probability = sigmoid(bbox[4])\n",
    "        class_probabilities = sigmoid(bbox[5:])\n",
    "        if object_probability < threshold:\n",
    "            continue\n",
    "        # Process raw value\n",
    "        x = (col + x) / output_width\n",
    "        y = (row + y) / output_height\n",
    "        # Value for exp is very big number in some cases so following construction is using here\n",
    "        try:\n",
    "            width = np.exp(width)\n",
    "            height = np.exp(height)\n",
    "        except OverflowError:\n",
    "            continue\n",
    "        width = width * layer_anchors[2 * n] / input_width\n",
    "        height = height * layer_anchors[2 * n + 1] / input_height\n",
    "\n",
    "        class_id = np.argmax(class_probabilities)\n",
    "        confidence = class_probabilities[class_id] * object_probability\n",
    "        if confidence < threshold:\n",
    "            continue\n",
    "\n",
    "        # translate coordinates within original image\n",
    "        xmin = max(int((x - width / 2) * (orig_width)), 0)\n",
    "        ymin = max(int((y - height / 2) * (orig_height)), 0)\n",
    "        xmax = min(int((x + width / 2) * (orig_width)), orig_width)\n",
    "        ymax = min(int((y + height / 2) * (orig_height)), orig_height)\n",
    "\n",
    "        objects.append(dict(xmin=xmin, ymin=ymin, xmax=xmax, ymax=ymax,\n",
    "                            confidence=confidence.item(),\n",
    "                            class_id=int(class_id.item()),\n",
    "            )\n",
    "        )\n",
    "    return objects\n",
    "\n",
    "\n",
    "def filter_yolo_detections(detections, iou_threshold):\n",
    "    \"\"\"\n",
    "    Filters the object detections using intersection over union to identify unique detections\n",
    "    :param detections: List of detections\n",
    "    :param iou_threshold: Threshold for when performing intersection over union algorithm\n",
    "    :return: annotated image\n",
    "\n",
    "    \"\"\"\n",
    "    def iou(box_1, box_2):\n",
    "        width_of_overlap_area = min(box_1[\"xmax\"], box_2[\"xmax\"]) - max(\n",
    "            box_1[\"xmin\"], box_2[\"xmin\"]\n",
    "        )\n",
    "        height_of_overlap_area = min(box_1[\"ymax\"], box_2[\"ymax\"]) - max(\n",
    "            box_1[\"ymin\"], box_2[\"ymin\"]\n",
    "        )\n",
    "        if width_of_overlap_area < 0 or height_of_overlap_area < 0:\n",
    "            area_of_overlap = 0\n",
    "        else:\n",
    "            area_of_overlap = width_of_overlap_area * height_of_overlap_area\n",
    "        box_1_area = (box_1[\"ymax\"] - box_1[\"ymin\"]) * (box_1[\"xmax\"] - box_1[\"xmin\"])\n",
    "        box_2_area = (box_2[\"ymax\"] - box_2[\"ymin\"]) * (box_2[\"xmax\"] - box_2[\"xmin\"])\n",
    "        area_of_union = box_1_area + box_2_area - area_of_overlap\n",
    "        if area_of_union == 0:\n",
    "            return 0\n",
    "        return area_of_overlap / area_of_union\n",
    "\n",
    "    detections = sorted(detections, key=lambda obj: obj[\"confidence\"], reverse=True)\n",
    "    for i in range(len(detections)):\n",
    "        if detections[i][\"confidence\"] == 0:\n",
    "            continue\n",
    "        for j in range(i + 1, len(detections)):\n",
    "            # We perform IOU only on objects of same class\n",
    "            if detections[i][\"class_id\"] != detections[j][\"class_id\"]:\n",
    "                continue\n",
    "\n",
    "            if iou(detections[i], detections[j]) > iou_threshold:\n",
    "                detections[j][\"confidence\"] = 0\n",
    "\n",
    "    return [det for det in detections if det[\"confidence\"] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and set up the validation dataset\n",
    "The [Common Objects in Context (COCO)](https://cocodataset.org/#home) dataset will be downloaded to be used by the `omz_quantizer` and `accuracy_check` tools.  The COCO dataset must be set up as described on the Open Model Zoo [dataset.md:COCO](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/datasets.md#common-objects-in-context-coco) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_coco_dataset(output_dir):\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # download zip files\n",
    "    data_zipname = \"val2017.zip\"\n",
    "    data_url = f\"http://images.cocodataset.org/zips/{data_zipname}\"\n",
    "    data_zippath = nbutils.download_file(data_url, data_zipname, output_dir)\n",
    "\n",
    "    annotations_zipname = \"annotations_trainval2017.zip\"\n",
    "    annotations_url = f\"http://images.cocodataset.org/annotations/{annotations_zipname}\"\n",
    "    annotations_zippath = nbutils.download_file(\n",
    "        annotations_url, annotations_zipname, output_dir\n",
    "    )\n",
    "\n",
    "    # unzip zip files\n",
    "    zip_ref = zipfile.ZipFile(data_zippath, \"r\")\n",
    "    zip_ref.extractall(path=output_dir)\n",
    "    zip_ref.close()\n",
    "\n",
    "    zip_ref = zipfile.ZipFile(annotations_zippath, \"r\")\n",
    "    required_files = [\"instances_val2017.json\", \"person_keypoints_val2017.json\"]\n",
    "    for zip_info in zip_ref.infolist():\n",
    "        if any(fn in zip_info.filename for fn in required_files):\n",
    "            with zip_ref.open(zip_info) as zipped_file, open(\n",
    "                output_dir / Path(zip_info.filename).name, \"wb\"\n",
    "            ) as disk_file:\n",
    "                shutil.copyfileobj(zipped_file, disk_file)\n",
    "    zip_ref.close()\n",
    "\n",
    "    # download the class labels\n",
    "    labels_url = f\"https://github.com/openvinotoolkit/open_model_zoo/raw/master/data/dataset_classes/{LABELS_PATH.name}\"\n",
    "    nbutils.download_file(labels_url, LABELS_PATH.name, output_dir)\n",
    "\n",
    "\n",
    "if not LABELS_PATH.exists():\n",
    "    set_up_coco_dataset(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "The OpenVINO tool [`omz_downloader`](../104-model-tools/104-model-tools.ipynb) is used to automatically download files from the Open Model Zoo.\n",
    "\n",
    "> **NOTE**: If model IR files are available from the Open Model Zoo, then the downloaded models will appear in the `intel` subdirectory.  If no model IR files are available, then the downloaded models will appear in the `public` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_downloader --name $OMZ_MODEL_NAME --output $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to IR files\n",
    "\n",
    "The public models from the Open Model Zoo are made available in their native framework file format and must be converted to OpenVINO Intermediate Representation (IR) files before running inference.  The OpenVINO tool [`omz_convert`](../104-model-tools/104-model-tools.ipynb) is used to convert Open Model Zoo models to the IR files necessary to run inference.\n",
    "\n",
    "> **NOTE**: For models that are downloaded from the Open Model Zoo already as IR files, the converter utility will not do any conversion and will output the message \"Skipping <model_name> (no conversions defined)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_converter --name $OMZ_MODEL_NAME --precisions FP32 --download_dir $MODEL_DIR  --output $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model to INT8\n",
    "For models downloaded from the Open Model Zoo, the [`omz_quantizer`](../104-model-tools/104-model-tools.ipynb) tool is used to quantize the model to a lower precision (e.g. quantize FP32 to INT8 precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_omz_quantizer_output(line):\n",
    "    if (line.startswith(\"Quantization command\") \n",
    "            or line.startswith(\"Moving\") \n",
    "            or line.startswith(\"INFO\")):\n",
    "        return line\n",
    "    return None\n",
    "\n",
    "\n",
    "cmd = f\"omz_quantizer --name {OMZ_MODEL_NAME} --model_dir {MODEL_DIR}  --output {MODEL_DIR}  --dataset_dir {DATASET_DIR} --precisions FP32-INT8\"\n",
    "run_command_line(cmd, filter_omz_quantizer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "Now that the model has been quantized, we will run inference using both the original FP32 model and the new INT8 quantized model to see their results.  First we will run the FP32 model.\n",
    "\n",
    "> **NOTE**: Post-processing the inference results from the [yolo-v4-tf](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/yolo-v4-tf) model requires more than one step to gather detection results from all the output layers and then filter them into individual detections.  The code here performing the necessary steps was derived from the [YOLOV4 code](https://github.com/openvinotoolkit/open_model_zoo/blob/master/demos/common/python/models/yolo.py) used by the Open Model Zoo [object_detection_demo](https://github.com/openvinotoolkit/open_model_zoo/tree/master/demos/object_detection_demo/python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image(image, detections):\n",
    "    \"\"\"\n",
    "    Annotate image with detections by adding a label and box around each detection\n",
    "    :param image: Input image to add annotations to\n",
    "    :param detections: List of detections\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    # Convert the inference result to a class name using the labels file\n",
    "    with open(LABELS_PATH) as f:\n",
    "        labels = [line.rstrip() for line in f]\n",
    "\n",
    "    # Draw boxes around each detected object with labels\n",
    "    for det in detections:\n",
    "        class_id = det[\"class_id\"]\n",
    "        conf = det[\"confidence\"]\n",
    "        det_label = labels[class_id]\n",
    "        color = (\n",
    "            min(class_id * 12.5, 255),\n",
    "            min(class_id * 7, 255),\n",
    "            min(class_id * 5, 255),\n",
    "        )\n",
    "        cv2.rectangle(\n",
    "            image, (det[\"xmin\"], det[\"ymin\"]), (det[\"xmax\"], det[\"ymax\"]), (color), 2\n",
    "        )\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            \"#\" + det_label + \" \" + str(round(conf * 100, 1)) + \" %\",\n",
    "            (det[\"xmin\"], det[\"ymin\"] - 7),\n",
    "            cv2.FONT_HERSHEY_COMPLEX,\n",
    "            1,\n",
    "            color,\n",
    "            2,\n",
    "        )\n",
    "\n",
    "\n",
    "def yolo_apply_results_to_image(input_image, results, image):\n",
    "    \"\"\"\n",
    "    Post-process YOLO inference results and apply to the original input image\n",
    "    :param input_image: Input image\n",
    "    :param results: Inference results\n",
    "    :param image: Original image to annotate\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    N, C, input_H, input_W = input_image.shape\n",
    "\n",
    "    orig_H = image.shape[0]\n",
    "    orig_W = image.shape[1]\n",
    "\n",
    "    # Parse inference results\n",
    "    detections = []\n",
    "    for layer_name, out_blob in results.items():\n",
    "        detections += parse_yolo_region(\n",
    "            out_blob, layer_name, input_W, input_H, orig_W, orig_H, 0.5\n",
    "        )\n",
    "\n",
    "    filtered = filter_yolo_detections(detections, 0.4)\n",
    "\n",
    "    # Annotate image with results\n",
    "    annotate_image(image, filtered)\n",
    "\n",
    "\n",
    "def ssd_mobilenet_apply_results_to_image(input_image, results, image):\n",
    "    \"\"\"\n",
    "    Post-process SSD MobileNet inference results and apply to the original input image\n",
    "    :param input_image: Input image\n",
    "    :param results: Inference results\n",
    "    :param image: Original image to annotate\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "\n",
    "    # Process results into detections used for annotating the image\n",
    "    # inference result layer [1,1,N,7] for N detections with 7 parameters [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "    img_H = image.shape[0]\n",
    "    img_W = image.shape[1]\n",
    "    detections = []\n",
    "    for res in next(iter(results.values()))[0][0]:\n",
    "        image_id = res[0]\n",
    "        if image_id >= 0:\n",
    "            class_id = int(res[1])\n",
    "            conf = res[2]\n",
    "            xmin = int(res[3] * img_W)\n",
    "            ymin = int(res[4] * img_H)\n",
    "            xmax = int(res[5] * img_W)\n",
    "            ymax = int(res[6] * img_H)\n",
    "            detections.append(dict(xmin=xmin, ymin=ymin, xmax=xmax, ymax=ymax,\n",
    "                                   confidence=conf, class_id=class_id\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Annotate image with results\n",
    "    annotate_image(image, detections)\n",
    "\n",
    "\n",
    "def run_inference(model_base_path, image_path):\n",
    "    \"\"\"\n",
    "    runs inferrence on an image using the given model and then displays the results\n",
    "    :param model_base_path: String containing path and file name of model excluding the extension (i.e. \".xml\")\n",
    "    :param image_path: String containing full path to the input image\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    ie = IECore()\n",
    "\n",
    "    # Create the network from the model\n",
    "    net = ie.read_network(\n",
    "        model=f\"{model_base_path}.xml\", weights=f\"{model_base_path}.bin\"\n",
    "    )\n",
    "    exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "    input_key = next(iter(exec_net.input_info))\n",
    "\n",
    "    # Load image\n",
    "    image = nbutils.load_image(image_path)\n",
    "    # N,C,H,W = batch size, number of channels, height, width\n",
    "    N, C, H, W = exec_net.input_info[input_key].tensor_desc.dims\n",
    "    # The network expects images in BGR format, same as OpenCV so just resize\n",
    "    input_image = cv2.resize(src=image, dsize=(W, H))\n",
    "    # reshape image to network input shape ([W,H,C]->[B,C,H,W])\n",
    "    input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)\n",
    "\n",
    "    # Run inference, result = [1,1,N,7] for N detections with 7 parameters [image_id, label, conf, x_min, y_min, x_max, y_max]\n",
    "    results = exec_net.infer(inputs={input_key: input_image})\n",
    "\n",
    "    # Annotate image with results\n",
    "    if USING_YOLOV4_MODEL:\n",
    "        yolo_apply_results_to_image(input_image, results, image)\n",
    "    else:\n",
    "        ssd_mobilenet_apply_results_to_image(input_image, results, image)\n",
    "\n",
    "    # Display annotated image (imshow requires RGB format, so convert BGR->RGB)\n",
    "    plt.imshow(nbutils.to_rgb(image))\n",
    "\n",
    "\n",
    "run_inference(f\"{MODEL_FP32_DIR}/{OMZ_MODEL_NAME}\", str(TEST_INPUT_IMAGE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the INT8 model and can compare the results to the FP32 results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(f\"{MODEL_FP32INT8_DIR}/{OMZ_MODEL_NAME}\", str(TEST_INPUT_IMAGE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up to run accuracy_check\n",
    "We will check the accuracy of the two FP32 and INT8 models using  [OpenVINO's Accuracy Checker Tool](https://docs.openvino.ai/latest/omz_tools_accuracy_checker.html), [`accuracy_check`](../104-model-tools/104-model-tools.ipynb).  For each model, The Open Model Zoo includes the necessary `accuracy-check.yml` configuration and the global [`dataset_definitions.yml`](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/dataset_definitions.yml) files needed to run the `accuracy_check` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve files needed by accuracy_check\n",
    "OMZ_GITHUB_URL = \"https://github.com/openvinotoolkit/open_model_zoo/raw/master\"\n",
    "dataset_def_yml = \"dataset_definitions.yml\"\n",
    "dataset_def_yml_url = f\"{OMZ_GITHUB_URL}/data/{dataset_def_yml}\"\n",
    "model_acheck_yml = \"accuracy-check.yml\"\n",
    "model_acheck_yml_url = (\n",
    "    f\"{OMZ_GITHUB_URL}/models/public/{OMZ_MODEL_NAME}/{model_acheck_yml}\"\n",
    ")\n",
    "\n",
    "model_acheck_yml_path = nbutils.download_file(\n",
    "    model_acheck_yml_url, model_acheck_yml, OUTPUT_DIR\n",
    ")\n",
    "\n",
    "dataset_def_yml_path = nbutils.download_file(\n",
    "    dataset_def_yml_url, dataset_def_yml, OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy of the model before and after quantization\n",
    "Now we will run `accuracy_check` for both the original FP32 and the new quantized INT8 models to compare accuracies.  First we will check the accuracy of the FP32 model.\n",
    "\n",
    "> **NOTE**: In this notebook, we run accuracy_check on a subset of the images in the dataset which takes less time.  For a more accurate check, all images should be used which may be done by not specifying the \"-ss <number>\" command line argument.\n",
    "\n",
    "> **NOTE**: The higher the percentage reported by `accuracy_check` the better, however most models are not 100% accurate.  For reference on what to expect form the model, the details for [ssd_mobilenet_v1_coco](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/ssd_mobilenet_v1_coco) on the Open Model Zoo include the accuracy of the original trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to '-ss <number>' to use only <number> of images, or set '' to use all images\n",
    "num_subsamples = \"-ss 300\"\n",
    "\n",
    "cmd = f\"accuracy_check -tf dlsdk -td CPU -s {DATASET_DIR} -d {dataset_def_yml_path} -c {model_acheck_yml_path} -m {MODEL_FP32_DIR} {num_subsamples}\"\n",
    "run_command_line(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check the accuracy of the INT8 model and can compare the results to the FP32 results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"accuracy_check -tf dlsdk -td CPU -s {DATASET_DIR} -d {dataset_def_yml_path} -c {model_acheck_yml_path} -m {MODEL_FP32INT8_DIR} {num_subsamples}\"\n",
    "run_command_line(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark the model before and after quantization\n",
    "Finally, we will measure the inference performance of the FP32 and INT8 models using  [OpenVINO's Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html), [`benchmark_app`](../104-model-tools/104-model-tools.ipynb)\n",
    "  \n",
    "> **NOTE**: In this notebook, we run benchmark_app for 15 seconds (\"-t <time_seconds>\" argument) to give a quick indication of performance. For more accurate performance, we recommended running benchmark_app for 60 seconds in a terminal/command prompt after closing other applications.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_benchmark_output(line):\n",
    "    if not (line.startswith(r\"[\") or line.startswith(\"  \") or len(line.rstrip()) < 1):\n",
    "        return line\n",
    "    return None\n",
    "\n",
    "\n",
    "# time to run benchmark\n",
    "time_secs = 15\n",
    "\n",
    "cmd = f\"benchmark_app -m {MODEL_FP32_DIR}/{OMZ_MODEL_NAME}.xml -d CPU -api async -t {time_secs}\"\n",
    "run_command_line(cmd, filter_benchmark_output)\n",
    "print()\n",
    "cmd = f\"benchmark_app -m {MODEL_FP32INT8_DIR}/{OMZ_MODEL_NAME}.xml -d CPU -api async -t {time_secs}\"\n",
    "run_command_line(cmd, filter_benchmark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Optionally, all the downloaded and generated files may be removed by setting `do_cleanup` to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cleanup = False\n",
    "if do_cleanup:\n",
    "    shutil.rmtree(DATASET_DIR)\n",
    "    shutil.rmtree(MODEL_DIR)\n",
    "    shutil.rmtree(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
