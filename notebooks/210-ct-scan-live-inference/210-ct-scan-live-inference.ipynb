{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Inference and Benchmark CT-scan Data with OpenVINO\n",
    "\n",
    "## Kidney Segmentation with PyTorch Lightning and OpenVINOâ„¢ - Part 4\n",
    "\n",
    "This tutorial is part of a series on how to train, optimize, quantize and show live inference on a medical segmentation model. The goal is to accelerate inference on a kidney segmentation model. The [UNet](https://arxiv.org/abs/1505.04597) model is trained from scratch; the data is from [Kits19](https://github.com/neheller/kits19).\n",
    "\n",
    "This tutorial shows how to \n",
    "\n",
    "- Visually compare inference results of an FP16 and INT8 OpenVINO IR model\n",
    "- Benchmark performance of the original model and the quantized model\n",
    "- Show live inference with OpenVINO's async API and MULTI plugin\n",
    "\n",
    "To learn how this model was quantized, please see the [Convert and Quantize a UNet Model and Show Live Inference](../110-ct-segmentation-quantize/110-ct-segmentation-quantize.ipynb) tutorial. The content of the current tutorial partly overlaps with that. It demonstrates how to visualize the results and show benchmark information when you already have a quantized model. \n",
    "\n",
    "All notebooks in this series:\n",
    "\n",
    "- [Data Preparation for 2D Segmentation of 3D Medical Data](../110-ct-segmentation-quantize/data-preparation-ct-scan.ipynb)\n",
    "- Train a 2D-UNet Medical Imaging Model with PyTorch Lightning (will be published soon)\n",
    "- [Convert and Quantize a UNet Model and Show Live Inference](../110-ct-segmentation-quantize/110-ct-segmentation-quantize.ipynb)\n",
    "- Live Inference and Benchmark CT-scan data (this notebook)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook needs a quantized OpenVINO IR model. We provide a pretrained model trained for 20 epochs with the full [Kits-19](https://github.com/neheller/kits19) frames dataset, which has an F1 score on the validation set of 0.9. The training code will be made available soon. It also needs images from the Kits19 dataset, converted to 2D images. For demonstration purposes, this tutorial will download one converted CT scan to use for inference.\n",
    "\n",
    "To install the requirements for running this notebook, please follow the instructions in the README."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from async_inference import CTAsyncPipeline, SegModel\n",
    "from IPython.display import Image, display\n",
    "from omz_python.models import model as omz_model\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import benchmark_model, download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "To use the pretrained models, set `IR_PATH` to `\"pretrained_model/unet44.xml\"` and `COMPRESSED_MODEL_PATH` to `\"pretrained_model/quantized_unet44.xml\"`. To use a model that you trained or optimized yourself, adjust the model paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory that contains the CT scan data. This directory should contain subdirectories\n",
    "# case_00XXX where XXX is between 000 and 299\n",
    "BASEDIR = \"kits19_frames_1\"\n",
    "# The directory that contains the IR model files. Should contain unet44.xml and bin\n",
    "# and quantized_unet44.xml and bin.\n",
    "IR_PATH = \"pretrained_model/unet44.xml\"\n",
    "COMPRESSED_MODEL_PATH = \"pretrained_model/quantized_unet44.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Data\n",
    "\n",
    "Download one validation video for live inference. We reuse the KitsDataset class that was also used in the training and quantization notebook that will be released later.\n",
    "\n",
    "Data is expected in `BASEDIR` defined in the cell above. `BASEDIR` should contain directories named `case_00000` to `case_00299`. If data for the case specified above does not exist yet, it will be downloaded and extracted in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CT scan case number. For example: 16 for data from the case_00016 directory\n",
    "# Currently only 16 is supported\n",
    "case = 16\n",
    "\n",
    "if not Path(f\"{BASEDIR}/case_{case:05d}\").exists():\n",
    "    filename = download_file(\n",
    "        f\"https://s3.us-west-1.amazonaws.com/openvino.notebooks/case_{case:05d}.zip\"\n",
    "    )\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(path=BASEDIR)\n",
    "    os.remove(filename)  # remove zipfile\n",
    "    print(f\"Downloaded and extracted data for case_{case:05d}\")\n",
    "else:\n",
    "    print(f\"Data for case_{case:05d} exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KitsDataset(object):\n",
    "    def __init__(self, basedir: str, dataset_type: str, transforms=None):\n",
    "        \"\"\"\n",
    "        Dataset class for prepared Kits19 data, for binary segmentation (background/kidney)\n",
    "\n",
    "        :param basedir: Directory that contains the prepared CT scans, in subdirectories\n",
    "                        case_00000 until case_00210\n",
    "        :param dataset_type: either \"train\" or \"val\"\n",
    "        :param transforms: Compose object with augmentations\n",
    "        \"\"\"\n",
    "        allmasks = sorted(glob.glob(f\"{basedir}/case_*/segmentation_frames/*png\"))\n",
    "\n",
    "        if len(allmasks) == 0:\n",
    "            raise ValueError(\n",
    "                f\"basedir: '{basedir}' does not contain data for type '{dataset_type}'\"\n",
    "            )\n",
    "        self.valpatients = [11, 15, 16, 49, 50, 79, 81, 89, 106, 108, 112, 126, 129, 133,\n",
    "                            141, 166, 169, 170, 192, 202, 204]  # fmt: skip\n",
    "        valcases = [f\"case_{i:05d}\" for i in self.valpatients]\n",
    "        if dataset_type == \"train\":\n",
    "            masks = [mask for mask in allmasks if Path(mask).parents[1].name not in valcases]\n",
    "        elif dataset_type == \"val\":\n",
    "            masks = [mask for mask in allmasks if Path(mask).parents[1].name in valcases]\n",
    "        else:\n",
    "            raise ValueError(\"Please choose train or val dataset split\")\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            random.shuffle(masks)\n",
    "        self.basedir = basedir\n",
    "        self.dataset_type = dataset_type\n",
    "        self.dataset = masks\n",
    "        self.transforms = transforms\n",
    "        print(\n",
    "            f\"Created {dataset_type} dataset with {len(self.dataset)} items. Base directory for data: {basedir}\"\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "\n",
    "        :return: (annotation, input_image, metadata) where annotation is (index, segmentation_mask)\n",
    "                 and metadata a dictionary with case and slice number\n",
    "        \"\"\"\n",
    "        mask_path = self.dataset[index]\n",
    "        # Open the image with OpenCV with `cv2.IMREAD_UNCHANGED` to prevent automatic\n",
    "        # conversion of 1-channel black and white images to 3-channel BGR images.\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        image_path = str(Path(mask_path.replace(\"segmentation\", \"imaging\")).with_suffix(\".jpg\"))\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        if img.shape[:2] != (512, 512):\n",
    "            img = cv2.resize(img, (512, 512))\n",
    "            mask = cv2.resize(mask, (512, 512))\n",
    "\n",
    "        annotation = (index, mask.astype(np.uint8))\n",
    "        input_image = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "        return (\n",
    "            annotation,\n",
    "            input_image,\n",
    "            {\"case\": Path(mask_path).parents[1].name, \"slice\": Path(mask_path).stem},\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "# The sigmoid function is used to transform the result of the network\n",
    "# to binary segmentation masks\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the KitsDataset class\n",
    "# If you set dataset_type to train, make sure that `basedir` contains training data\n",
    "dataset = KitsDataset(basedir=BASEDIR, dataset_type=\"val\", transforms=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "colormap = \"gray\"\n",
    "\n",
    "ie = IECore()\n",
    "net_ir = ie.read_network(IR_PATH)\n",
    "net_pot = ie.read_network(COMPRESSED_MODEL_PATH)\n",
    "\n",
    "exec_net_ir = ie.load_network(network=net_ir, device_name=\"CPU\")\n",
    "exec_net_pot = ie.load_network(network=net_pot, device_name=\"CPU\")\n",
    "input_layer = next(iter(net_ir.input_info))\n",
    "output_layer_ir = next(iter(net_ir.outputs))\n",
    "output_layer_pot = next(iter(net_pot.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Show Results\n",
    "\n",
    "Visualize the results of the model on four slices of the validation set. Compare the results of the FP16 IR model with the results of the quantized INT8 model and the reference segmentation annotation.\n",
    "\n",
    "Medical imaging datasets tend to be very imbalanced: most of the slices in a CT scan do not contain kidney data. The segmentation model should be good at finding kidneys where they exist (in medical terms: have good sensitivity) but also not find spurious kidneys that do not exist (have good specificity). In the next cell, we show four slices: two slices that have no kidney data, and two slices that contain kidney data. For this example, a slice has kidney data if at least 50 pixels in the slices are annotated as kidney.\n",
    "\n",
    "Run this cell again to show results on a different subset. The random seed is displayed to allow reproducing specific runs of this cell.\n",
    "\n",
    "> Note: the images are shown after optional augmenting and resizing. In the Kits19 dataset all but one of the cases has input shape `(512, 512)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset, and make a subset of the dataset for visualization\n",
    "# The dataset items are (annotation, image) where annotation is (index, mask)\n",
    "background_slices = (item for item in dataset if np.count_nonzero(item[0][1]) == 0)\n",
    "kidney_slices = (item for item in dataset if np.count_nonzero(item[0][1]) > 50)\n",
    "# Set seed to current time. To reproduce specific results, copy the printed seed\n",
    "# and manually set `seed` to that value.\n",
    "seed = int(time.time())\n",
    "random.seed(seed)\n",
    "print(f\"Visualizing results with seed {seed}\")\n",
    "data_subset = random.sample(list(background_slices), 2) + random.sample(list(kidney_slices), 2)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_images, ncols=4, figsize=(24, num_images * 4))\n",
    "for i, (annotation, image, meta) in enumerate(data_subset):\n",
    "    mask = annotation[1]\n",
    "    res_ir = exec_net_ir.infer(inputs={input_layer: image})\n",
    "    res_pot = exec_net_pot.infer(inputs={input_layer: image})\n",
    "    target_mask = mask.astype(np.uint8)\n",
    "\n",
    "    result_mask_ir = sigmoid(res_ir[output_layer_ir]).round().astype(np.uint8)[0, 0, ::]\n",
    "    result_mask_pot = sigmoid(res_pot[output_layer_pot]).round().astype(np.uint8)[0, 0, ::]\n",
    "\n",
    "    ax[i, 0].imshow(image[0, ::], cmap=colormap)\n",
    "    ax[i, 1].imshow(target_mask, cmap=colormap)\n",
    "    ax[i, 2].imshow(result_mask_ir, cmap=colormap)\n",
    "    ax[i, 3].imshow(result_mask_pot, cmap=colormap)\n",
    "    ax[i, 0].set_title(f\"{meta['slice']}\")\n",
    "    ax[i, 1].set_title(\"Annotation\")\n",
    "    ax[i, 2].set_title(\"Prediction on FP16 model\")\n",
    "    ax[i, 3].set_title(\"Prediction on INT8 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance of the Original and Quantized Models\n",
    "To measure the inference performance of the FP16 and INT8 models, we use [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html), OpenVINO's inference performance measurement tool. Benchmark tool is a command line application that can be run in the notebook with `! benchmark_app` or `%sx benchmark_app`. \n",
    "\n",
    "In this tutorial, we use a wrapper function from [Notebook Utils](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/utils/notebook_utils.ipynb). It prints the `benchmark_app` command with the chosen parameters.\n",
    "\n",
    "> NOTE: For the most accurate performance estimation, we recommended running `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app --help` to see all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, benchmark on MULTI:CPU,GPU if a GPU is available, otherwise on CPU.\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "# Uncomment one of the options below to benchmark on other devices\n",
    "# device = \"GPU\"\n",
    "# device = \"CPU\"\n",
    "# device = \"AUTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "seconds=15": "seconds=3"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark FP16 model\n",
    "benchmark_model(model_path=IR_PATH, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "seconds=15": "seconds=3"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark INT8 model\n",
    "benchmark_model(model_path=COMPRESSED_MODEL_PATH, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Show Live Inference\n",
    "\n",
    "To show live inference on the model in the notebook, we use the asynchronous processing feature of OpenVINO Inference Engine.\n",
    "\n",
    "If you use a GPU device, with `device=\"GPU\"` or `device=\"MULTI:CPU,GPU\"` to do inference on an integrated graphics card, model loading will be slow the first time you run this code. The model will be cached, so after the first time model loading will be fast. See the [OpenVINO API tutorial](../002-openvino-api/002-openvino-api.ipynb) for more information on Inference Engine, including Model Caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Functions\n",
    "\n",
    "We define a helper function `show_array` to efficiently show images in the notebook. The `do_inference` function uses [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)'s AsyncPipeline to perform asynchronous inference. After inference on the specified CT scan has completed, the total time and throughput (fps), including preprocessing and displaying, will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(frame: np.ndarray, display_handle=None):\n",
    "    \"\"\"\n",
    "    Display array `frame`. Replace information at `display_handle` with `frame`\n",
    "    encoded as jpeg image\n",
    "\n",
    "    Create a display_handle with: `display_handle = display(display_id=True)`\n",
    "    \"\"\"\n",
    "    _, frame = cv2.imencode(ext=\".jpeg\", img=frame)\n",
    "    if display_handle is None:\n",
    "        display_handle = display(Image(data=frame.tobytes()), display_id=True)\n",
    "    else:\n",
    "        display_handle.update(Image(data=frame.tobytes()))\n",
    "    return display_handle\n",
    "\n",
    "\n",
    "def do_inference(imagelist: List, model: omz_model.Model, device: str):\n",
    "    \"\"\"\n",
    "    Do inference of images in `imagelist` on `model` on the given `device` and show\n",
    "    the results in real time in a Jupyter Notebook\n",
    "\n",
    "    :param imagelist: list of images/frames to do inference on\n",
    "    :param model: Model instance for inference\n",
    "    :param device: Name of device to perform inference on. For example: \"CPU\"\n",
    "    \"\"\"\n",
    "    display_handle = None\n",
    "    next_frame_id = 0\n",
    "    next_frame_id_to_show = 0\n",
    "\n",
    "    input_layer = next(iter(model.net.input_info))\n",
    "\n",
    "    # Create asynchronous pipeline and print time it takes to load the model\n",
    "    load_start_time = time.perf_counter()\n",
    "    pipeline = CTAsyncPipeline(\n",
    "        ie=ie, model=model, plugin_config={}, device=device, max_num_requests=0\n",
    "    )\n",
    "    load_end_time = time.perf_counter()\n",
    "\n",
    "    # Perform asynchronous inference\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    while next_frame_id < len(imagelist) - 1:\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "\n",
    "        if results:\n",
    "            # Show next result from async pipeline\n",
    "            result, meta = results\n",
    "            display_handle = showarray(result, display_handle)\n",
    "\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "        if pipeline.is_ready():\n",
    "            # Submit new image to async pipeline\n",
    "            image = imagelist[next_frame_id]\n",
    "            pipeline.submit_data(\n",
    "                inputs={input_layer: image}, id=next_frame_id, meta={\"frame\": image}\n",
    "            )\n",
    "            next_frame_id += 1\n",
    "        else:\n",
    "            # If the pipeline is not ready yet and there are no results: wait\n",
    "            pipeline.await_any()\n",
    "\n",
    "    pipeline.await_all()\n",
    "\n",
    "    # Show all frames that are in the pipeline after all images have been submitted\n",
    "    while pipeline.has_completed_request():\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            result, meta = results\n",
    "            display_handle = showarray(result, display_handle)\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    fps = len(imagelist) / duration\n",
    "    print(f\"Loaded model to {device} in {load_end_time-load_start_time:.2f} seconds.\")\n",
    "    print(f\"Total time for {next_frame_id+1} frames: {duration:.2f} seconds, fps:{fps:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model and Images\n",
    "\n",
    "Load the segmentation model with `SegModel`, based on the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/) Model API. Load a  CT scan from the `BASEDIR` directory (by default: _kits19_frames_) to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "segmentation_model = SegModel(ie=ie, model_path=Path(COMPRESSED_MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [],
    "test_replace": {
     "demopattern))": "demopattern))[:5]"
    }
   },
   "outputs": [],
   "source": [
    "case = 16\n",
    "demopattern = f\"{BASEDIR}/case_{case:05d}/imaging_frames/*jpg\"\n",
    "imlist = sorted(glob.glob(demopattern))\n",
    "images = [cv2.imread(im, cv2.IMREAD_UNCHANGED) for im in imlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Show Inference\n",
    "\n",
    "In the next cell, we run the `do inference` function, which loads the model to the specified device (using caching for faster model loading on GPU devices), performs inference, and displays the results in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible options for device include \"CPU\", \"GPU\", \"AUTO\", \"MULTI\"\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "do_inference(imagelist=images, model=segmentation_model, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
