{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kidney Segmentation with PyTorch Lightning and OpenVINOâ„¢\n",
    "\n",
    "## **Part 3:** Show Live Inference\n",
    "\n",
    "This tutorial demonstrates training and inference with a kidney segmentation model. For training, [PyTorch Lightning](https://www.pytorchlightning.ai/) is used with a [UNet](https://arxiv.org/abs/1505.04597) segmentation model. The model is converted to OpenVINO IR with [Model Optimizer](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html), and quantized with OpenVINO's [Post-Training Optimization Tool](https://docs.openvinotoolkit.org/latest/pot_compression_api_README.html) API. \n",
    "\n",
    "The tutorial is split into three parts. This is the third part, which shows how to view inference results, and do performance benchmarks. The other parts will be added soon.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "This notebook needs a quantized OpenVINO IR model. We provide a pretrained model trained for 20 epochs with the full [Kits-19](https://github.com/neheller/kits19) frames dataset, which has an F1 score on the validation set of 0.9. To learn how this model was quantized, see the [Quantization Notebook](03-kidney-openvino.ipynb). For a full end-to-end demo that starts with training, see the [Training Notebook](02-kidney-train.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import time\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from operator import itemgetter\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import Image, Markdown, display\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "from async_inference import CTAsyncPipeline, SegModel\n",
    "from omz_python.models import model as omz_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "To use the pretrained model, set MODEL_DIR to `Path(\"pretrained_model\")` in the cell below. This is the default. To use the trained model, set MODEL_DIR to `Path(\"model\")` (assuming default settings in the training and quantization notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory that contains the CT scan data. This directory should contain subdirecties\n",
    "# case_00XXX where XXX is between 000 and 299, with data prepared according to #TODO\n",
    "basedir = \"kits19_frames\"\n",
    "# The CT scan case number. For example: 16 for data from the case_00016 directory\n",
    "# Currently only 16 is supported\n",
    "case = 16\n",
    "# The directory that contains the IR model files. Should contain unet44.xml and bin\n",
    "# and quantized_unet44.xml and bin.\n",
    "MODEL_DIR = Path(\"pretrained_model\")\n",
    "ir_path = MODEL_DIR / \"unet44.xml\"\n",
    "compressed_model_path = MODEL_DIR / \"quantized_unet44.xml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and Prepare Data\n",
    "\n",
    "Download one validation video for live inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(f\"data/case_00{case:03d}\").exists():\n",
    "    filename, _ = urllib.request.urlretrieve(\n",
    "        f\"https://s3.us-west-1.amazonaws.com/openvino.notebooks/case_00{case:03d}.zip\"\n",
    "    )\n",
    "    with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KitsDataset(object):\n",
    "    def __init__(self, basedir: str, dataset_type: str, transforms=None):\n",
    "        \"\"\"\n",
    "        Dataset class for prepared Kits19 data, for binary segmentation (background/kidney).\n",
    "\n",
    "        :param basedir: Directory that contains the prepared CT scans, in subdirectories\n",
    "                        case_00000 until case_00210\n",
    "        :param dataset_type: either \"train\" or \"val\"\n",
    "        :param transforms: Compose object with augmentations\n",
    "        \"\"\"\n",
    "        allmasks = sorted(glob.glob(f\"{basedir}/case_*/segmentation_frames/*png\"))\n",
    "\n",
    "        # Reserve 10% of the patients for the validation dataset\n",
    "        # Set a random seed to ensure that this list is reproducable\n",
    "        random.seed(2.71828)\n",
    "        self.valpatients = sorted(random.choices(range(210), k=21))\n",
    "\n",
    "        valcases = [f\"case_{i:05d}\" for i in self.valpatients]\n",
    "        if dataset_type == \"train\":\n",
    "            masks = [mask for mask in allmasks if Path(mask).parents[1].name not in valcases]\n",
    "        elif dataset_type == \"val\":\n",
    "            masks = [mask for mask in allmasks if Path(mask).parents[1].name in valcases]\n",
    "        else:\n",
    "            raise ValueError(\"Please choose train or val dataset split\")\n",
    "\n",
    "        if dataset_type == \"train\":\n",
    "            random.shuffle(masks)\n",
    "        self.basedir = basedir\n",
    "        self.dataset_type = dataset_type\n",
    "        self.dataset = masks\n",
    "        self.transforms = transforms\n",
    "        print(f\"Created {dataset_type} dataset with {len(self.dataset)} items.\")\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset at the specified index.\n",
    "        Labels are converted to binary labels (background/kidney).\n",
    "\n",
    "        :return: (annotation, input_image) where annotation is (index, segmentation_mask)\n",
    "        \"\"\"\n",
    "        mask_path = self.dataset[index]\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_UNCHANGED)\n",
    "        # The masks contain annotations for kidneys and tumors, in this tutorial we only segment\n",
    "        # kidneys so we can set all pixels that contain a non-background value to 1.\n",
    "        mask[mask > 0] = 1\n",
    "\n",
    "        image_path = str(Path(mask_path.replace(\"segmentation\", \"imaging\")).with_suffix(\".jpg\"))\n",
    "        img = cv2.imread(image_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "        if img.shape != (512, 512, 3):\n",
    "            img = cv2.resize(img, (512, 512))\n",
    "            mask = cv2.resize(mask, (512, 512))\n",
    "\n",
    "        # TODO: add transforms with torchvision.transforms instead of albumentations\n",
    "        # if self.transforms is not None:\n",
    "\n",
    "        annotation = (index, mask.astype(np.uint8))\n",
    "        input_image = np.expand_dims(img, axis=0).astype(np.float32)\n",
    "        return annotation, input_image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Show Results\n",
    "\n",
    "Visualize the results of the model on four slices of the validation set. Compare the results of the FP16 IR model with the results of the quantized INT8 model and the reference segmentation annotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The sigmoid function is used to transform the result of the network\n",
    "# to binary segmentation masks\n",
    "def sigmoid(x):\n",
    "    return np.exp(-np.logaddexp(0, -x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = KitsDataset(\"data\", \"val\", None)\n",
    "num_images = 4\n",
    "colormap = \"gray\"\n",
    "\n",
    "ie = IECore()\n",
    "net_ir = ie.read_network(ir_path)\n",
    "net_pot = ie.read_network(compressed_model_path)\n",
    "\n",
    "exec_net_ir = ie.load_network(network=net_ir, device_name=\"CPU\")\n",
    "exec_net_pot = ie.load_network(network=net_pot, device_name=\"CPU\")\n",
    "input_layer = next(iter(net_ir.input_info))\n",
    "output_layer_ir = next(iter(net_ir.outputs))\n",
    "output_layer_pot = next(iter(net_pot.outputs))\n",
    "\n",
    "# data_subset = random.choices(data_loader, k=num_images)\n",
    "data_subset = itemgetter(28, 30, 38, 60)(data_loader)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=num_images, ncols=4, figsize=(24, num_images * 4))\n",
    "\n",
    "for i, (ma, im) in enumerate(data_subset):\n",
    "    res_ir = exec_net_ir.infer(inputs={input_layer: im})\n",
    "    res_pot = exec_net_pot.infer(inputs={input_layer: im})\n",
    "    target_mask = ma[1].astype(np.uint8)\n",
    "\n",
    "    result_mask_ir = sigmoid(res_ir[output_layer_ir]).round().astype(np.uint8)[0, 0, ::]\n",
    "    result_mask_pot = sigmoid(res_pot[output_layer_pot]).round().astype(np.uint8)[0, 0, ::]\n",
    "\n",
    "    ax[i, 0].imshow(im[0, ::], cmap=colormap)\n",
    "    ax[i, 1].imshow(target_mask, cmap=colormap)\n",
    "    ax[i, 2].imshow(result_mask_ir, cmap=colormap)\n",
    "    ax[i, 3].imshow(result_mask_pot, cmap=colormap)\n",
    "\n",
    "    ax[i, 1].set_title(\"Annotation\")\n",
    "    ax[i, 2].set_title(\"Prediction on FP16 model\")\n",
    "    ax[i, 3].set_title(\"Prediction on INT8 model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Performance of the Original and Quantized Models\n",
    "To measure the inference performance of the FP16 and INT8 models, we use [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html), OpenVINO's inference performance measurement tool. Benchmark tool is a command line application that can be run in the notebook with `! benchmark_app` or `%sx benchmark_app`. We create a helper function that makes it easy to compare several configurations. It prints the `benchmark_app` command with the command line options for the chosen parameters. \n",
    "\n",
    "> NOTE: For the most accurate performance estimation, we recommended running `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app --help` to see all command line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_model(model_xml, device=\"CPU\", seconds=60, api=\"async\", batch=1):\n",
    "    ie = IECore()\n",
    "    model_path = Path(model_xml)\n",
    "    if (\"GPU\" in device) and (\"GPU\" not in ie.available_devices):\n",
    "        raise ValueError(f\"A GPU device is not available. Available devices are: {ie.available_devices}\")\n",
    "    else:\n",
    "        benchmark_command = f\"benchmark_app -m {model_path} -d {device} -t {seconds} -api {api} -b {batch} -cdir model_cache\"\n",
    "        display(Markdown(f\"**Benchmark {model_path.name} with {device} for {seconds} seconds with {api} inference**\"));\n",
    "        display(Markdown(f\"Benchmark command: `{benchmark_command}`\"));\n",
    "\n",
    "        benchmark_output = %sx $benchmark_command\n",
    "        benchmark_result = [line for line in benchmark_output\n",
    "                            if not (line.startswith(r\"[\") or line.startswith(\"  \") or line == \"\")]\n",
    "        print(\"\\n\".join(benchmark_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, benchmark on MULTI:CPU,GPU if a GPU is available, otherwise on CPU.\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "# Uncomment one of the options below to benchmark on other devices\n",
    "# device = \"GPU\"\n",
    "# device = \"CPU\"\n",
    "# device = \"AUTO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(model_xml=ir_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_model(model_xml=compressed_model_path, device=device, seconds=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Show Live Inference\n",
    "\n",
    "To show live inference on the model in the notebook, we use the asynchronous processing feature of OpenVINO Inference Engine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showarray(frame: np.ndarray, display_handle: str):\n",
    "    \"\"\"\n",
    "    Display array `frame`. Replace information at `display_handle` with `frame`\n",
    "    encoded as jpeg image.\n",
    "\n",
    "    Create a display_handle with: `display_handle = display(display_id=True)`\n",
    "    \"\"\"\n",
    "    _, frame = cv2.imencode(ext=\".jpeg\", img=frame)\n",
    "    display_handle.update(Image(data=frame.tobytes()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_inference(imagelist: List, model: omz_model.Model, device: str):\n",
    "    \"\"\"\n",
    "    Do inference of images in `imagelist` on `model` on the given `device`.\n",
    "\n",
    "    :param imagelist: list of images/frames to do inference on\n",
    "    :param model: Model instance for inference\n",
    "    :param device: Name of device to perform inference on. For example: \"CPU\"\n",
    "    \"\"\"\n",
    "    display_handle = display(\"\", display_id=True)\n",
    "    input_layer = next(iter(model.net.input_info))\n",
    "\n",
    "    # Create asynchronous pipeline and print time it takes to load the model\n",
    "    s = time.perf_counter()\n",
    "    pipeline = CTAsyncPipeline(\n",
    "        ie=ie, model=model, plugin_config={}, device=device, max_num_requests=0\n",
    "    )\n",
    "    e = time.perf_counter()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    # Perform asynchronous inference\n",
    "    next_frame_id = 0\n",
    "    next_frame_id_to_show = 0\n",
    "\n",
    "    while next_frame_id < len(imagelist) - 1:\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "\n",
    "        if results:\n",
    "            # Show next result from async pipeline\n",
    "            result, meta = results\n",
    "            showarray(result, display_handle)\n",
    "\n",
    "            if next_frame_id_to_show == 0:\n",
    "                print(f\"Loaded model to {device} in {e-s:.2f} seconds.\")\n",
    "\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "        if pipeline.is_ready():\n",
    "            # Submit new image to async pipeline\n",
    "            image = imagelist[next_frame_id]\n",
    "            pipeline.submit_data(\n",
    "                inputs={input_layer: image}, id=next_frame_id, meta={\"frame\": image}\n",
    "            )\n",
    "            next_frame_id += 1\n",
    "        else:\n",
    "            # If the pipeline is not ready yet and there are no results: wait\n",
    "            pipeline.await_any()\n",
    "\n",
    "    pipeline.await_all()\n",
    "\n",
    "    # Show all frames that are in the pipeline after all images have been submitted\n",
    "    while pipeline.has_completed_request():\n",
    "        results = pipeline.get_result(next_frame_id_to_show)\n",
    "        if results:\n",
    "            result, meta = results\n",
    "            showarray(result, display_handle)\n",
    "            next_frame_id_to_show += 1\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    duration = end_time - start_time\n",
    "    fps = len(imagelist) / duration\n",
    "    print(f\"Total time for {next_frame_id+1} frames: {duration:.2f} seconds, fps:{fps:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Model and Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "segmentation_model = SegModel(ie=ie, model_path=Path(compressed_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demopattern = f\"data/case_00{case:03d}/imaging_frames/*jpg\"\n",
    "imlist = sorted(glob.glob(demopattern))\n",
    "images = [cv2.imread(im, cv2.IMREAD_UNCHANGED) for im in imlist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Show Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Possible options for device include \"CPU\", \"GPU\", \"AUTO\", \"MULTI\"\n",
    "device = \"MULTI:CPU,GPU\" if \"GPU\" in ie.available_devices else \"CPU\"\n",
    "do_inference(imagelist=images, model=segmentation_model, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
