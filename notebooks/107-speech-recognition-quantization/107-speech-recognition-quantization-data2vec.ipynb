{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d91c190-956f-44ef-9e3a-1428aa020493",
   "metadata": {},
   "source": [
    "# Quantize Speech Recognition Models using NNCF PTQ API\n",
    "This tutorial demonstrates how to use the NNCF (Neural Network Compression Framework) 8-bit quantization in post-training mode (without the fine-tuning pipeline) to optimize the speech recognition model, known as [Data2Vec](https://arxiv.org/abs/2202.03555) for the high-speed inference via OpenVINOâ„¢ Toolkit. This notebook uses a fine-tuned [data2vec-audio-base-960h](https://huggingface.co/facebook/data2vec-audio-base-960h) [PyTorch](https://pytorch.org/) model trained on the [LibriSpeech ASR corpus](https://www.openslr.org/12). The tutorial is designed to be extendable to custom models and datasets. It consists of the following steps:\n",
    "\n",
    "- Download and prepare model.\n",
    "- Define data loading and accuracy validation functionality.\n",
    "- Prepare the model for quantization and quantize.\n",
    "- Compare performance of the original and quantized models.\n",
    "- Compare Accuracy of the Original and Quantized Models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "03f79f99-6baf-4f4b-8bc4-a2b0a4e40030",
   "metadata": {},
   "source": [
    "## Download and prepare model\n",
    "\n",
    "data2vec is a framework for self-supervised representation learning for images, speech, and text as described in [data2vec: A General Framework for Self-supervised Learning in Speech, Vision and Language (Baevski et al., 2022)](https://ai.facebook.com/research/data2vec-a-general-framework-for-self-supervised-learning-in-speech-vision-and-language). The algorithm uses the same learning mechanism for different modalities.\n",
    "\n",
    "![pre-trained pipeline](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/data2vec.png)\n",
    "\n",
    "In our case, we will use `data2vec-audio-base-960h` model, which was finetuned on 960 hours of audio from LibriSpeech Automatic Speech Recognition corpus and distributed as part of HuggingFace transformers.\n",
    "\n",
    "### Obtain Pytorch model representation\n",
    "\n",
    "For instantiating PyTorch model class, we should use `Data2VecAudioForCTC.from_pretrained` method with providing model ID for downloading from HuggingFace hub. Model weights and configuration files will be downloaded automatically in first time usage.\n",
    "Keep in mind that downloading the files can take several minutes and depends on your internet connection.\n",
    "\n",
    "Additionally, we can create processor class which is responsible for model specific pre- and post-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3936821",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18e8a87-1064-4100-af21-8d392f236d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Data2VecAudioForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/data2vec-audio-base-960h\")\n",
    "model = Data2VecAudioForCTC.from_pretrained(\"facebook/data2vec-audio-base-960h\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f16b2f-fae8-4a4b-a8dc-5b49aec358a8",
   "metadata": {},
   "source": [
    "### Convert model to OpenVINO Intermediate Representation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# Set model directory\n",
    "MODEL_DIR = Path(\"model\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize, Core\n",
    "import torch\n",
    "\n",
    "core = Core()\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 30480\n",
    "\n",
    "\n",
    "def export_model_to_onnx(model, path):\n",
    "    # switch model to evaluation mode\n",
    "    model.eval()\n",
    "    # disallow gradient propagation for reducing memory during export\n",
    "    with torch.no_grad():\n",
    "        # define dummy input with specific shape\n",
    "        default_input = torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.float)\n",
    "        inputs = {\n",
    "            \"inputs\": default_input\n",
    "        }\n",
    "\n",
    "        # define names for dynamic dimentions\n",
    "        symbolic_names = {0: \"batch_size\", 1: \"sequence_len\"}\n",
    "        # export model\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (inputs[\"inputs\"]),\n",
    "            path,\n",
    "            opset_version=11,\n",
    "            input_names=[\"inputs\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"inputs\": symbolic_names,\n",
    "                \"logits\": symbolic_names,\n",
    "            },\n",
    "        )\n",
    "        print(\"ONNX model saved to {}\".format(path))\n",
    "\n",
    "\n",
    "onnx_model_path = MODEL_DIR / \"data2vec-audo-base.onnx\"\n",
    "ir_model_path = onnx_model_path.with_suffix('.xml')\n",
    "\n",
    "if not ir_model_path.exists():\n",
    "    if not onnx_model_path.exists():\n",
    "        export_model_to_onnx(model, onnx_model_path)\n",
    "    ov_model = mo.convert_model(onnx_model_path, compress_to_fp16=True)\n",
    "    serialize(ov_model, str(ir_model_path))\n",
    "    print(\"IR model saved to {}\".format(ir_model_path))\n",
    "else:\n",
    "    print(\"Read IR model from {}\".format(ir_model_path))\n",
    "    ov_model = core.read_model(ir_model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea7106bc-7cfb-4158-a9c7-bbbb76787523",
   "metadata": {},
   "source": [
    "### Prepare inference data\n",
    "\n",
    "For demonstration purposes, we will use short dummy version of librispeach dataset - `patrickvonplaten/librispeech_asr_dummy` to speed up model evaluation. Model accuracy can be different from reported in the paper. For reproducing original accuracy, use `librispeech_asr` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c312b02e-2c99-4211-b7f1-4e491550b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets \"torchmetrics>=0.11.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0873e12-ae73-4ac3-847f-d431818b7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "\n",
    "# define preprocessing function for converting audio to input values for model\n",
    "def map_to_input(batch):\n",
    "    preprocessed_signal = processor(batch[\"audio\"][\"array\"], return_tensors=\"pt\", padding=\"longest\", sampling_rate=batch['audio']['sampling_rate'])\n",
    "    input_values = preprocessed_signal.input_values\n",
    "    batch['input_values'] = input_values\n",
    "    return batch\n",
    "\n",
    "\n",
    "# apply preprocessing function to dataset and remove audio column, to save memory as we do not need it anymore\n",
    "dataset = ds.map(map_to_input, batched=False, remove_columns=[\"audio\"])\n",
    "\n",
    "test_sample = ds[0][\"audio\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dc35d830-4b64-4083-bd0c-aaef68ba3d86",
   "metadata": {},
   "source": [
    "## Check model inference result\n",
    "\n",
    "The code below is used for running model inference on a single sample from the dataset. It contains the following steps:\n",
    "\n",
    "* Get the input_values tensor as model input.\n",
    "* Run model inference and obtain logits.\n",
    "* Find logits ids with highest probability, using argmax.\n",
    "* Decode predicted token ids, using processor.\n",
    "\n",
    "For reference, see the same function provided for OpenVINO model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a11f44-2fe1-4aa9-a360-ba4b23bba87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# inference function for pytorch\n",
    "def torch_infer(model, sample):\n",
    "    logits = model(torch.Tensor(sample['input_values'])).logits\n",
    "    # take argmax and decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "    return transcription\n",
    "\n",
    "\n",
    "# inference function for openvino\n",
    "def ov_infer(model, sample):\n",
    "    output = model.output(0)\n",
    "    logits = model(np.array(sample['input_values']))[output]\n",
    "    predicted_ids = np.argmax(logits, axis=-1)\n",
    "    transcription = processor.batch_decode(torch.from_numpy(predicted_ids))\n",
    "    return transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb36df3c-b461-4400-9ebc-19d9f307032c",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()\n",
    "\n",
    "pt_transcription = torch_infer(model, dataset[0])\n",
    "compiled_model = core.compile_model(ov_model)\n",
    "ov_transcription = ov_infer(compiled_model, dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d02ca6e-9939-4d10-a65f-f03b19c4153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "print(f\"[Reference]:     {dataset[0]['text']}\")\n",
    "print(f\"[PyTorch]:       {pt_transcription[0]}\")\n",
    "print(f\"[OpenVINO FP16]: {ov_transcription[0]}\")\n",
    "ipd.Audio(test_sample[\"array\"], rate=16000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7281085-47ef-4f6e-b778-8d5921a97644",
   "metadata": {},
   "source": [
    "## Validate model accuracy on dataset\n",
    "\n",
    "For model accuracy evaluation, [Word Error Rate](https://en.wikipedia.org/wiki/Word_error_rate) metric can be used. Word Error Rate or WER is the ratio of errors in a transcript to the total words spoken. A lower WER in speech-to-text means better accuracy in recognizing speech.\n",
    "\n",
    "For WER calculation, we will use [torchmetrics](https://torchmetrics.readthedocs.io/en/stable/text/word_error_rate.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf626f-a1c3-4bd7-b31d-4542fed04236",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import WordErrorRate\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "def compute_wer(dataset, model, infer_fn):\n",
    "    wer = WordErrorRate()\n",
    "    for sample in tqdm(dataset):\n",
    "        # run infer function on sample\n",
    "        transcription = infer_fn(model, sample)\n",
    "        # update metric on sample result\n",
    "        wer.update(transcription, [sample['text']])\n",
    "    # finalize metric calculation\n",
    "    result = wer.compute()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4ceda6-1535-4816-ab1b-bf3dc80a636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_result = compute_wer(dataset, model, torch_infer)\n",
    "ov_result = compute_wer(dataset, compiled_model, ov_infer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3879e414-42c2-4b12-a06f-bcc0538c8834",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'[PyTorch]   Word Error Rate: {pt_result:.4f}')\n",
    "print(f'[OpenVino]  Word Error Rate: {ov_result:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fa9af259-42c7-4a2d-8c46-b3a94002a4f8",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "\n",
    "Create a quantized model from the pre-trained `FP16` model and the calibration dataset. The optimization process contains the following steps:\n",
    "    1. Create a Dataset for quantization.\n",
    "    2. Run `nncf.quantize` for getting an optimized model. The `nncf.quantize` function provides an interface for model quantization. It requires an instance of the OpenVINO Model and quantization dataset.\n",
    "Optionally, some additional parameters for the configuration quantization process (number of samples for quantization, preset, ignored scope, etc.) can be provided. For more accurate results, we should keep the operation in the postprocessing subgraph in floating point precision, using the `ignored_scope` parameter. `advanced_parameters` can be used to specify advanced quantization parameters for fine-tuning the quantization algorithm. In this tutorial we pass range estimator parameters for activations. For more information see [Tune quantization parameters](https://docs.openvino.ai/2023.0/basic_quantization_flow.html#tune-quantization-parameters).\n",
    "    3. Serialize OpenVINO IR model using `openvino.runtime.serialize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nncf\n",
    "from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters\n",
    "from nncf.quantization.range_estimator import AggregatorType\n",
    "from nncf.quantization.range_estimator import RangeEstimatorParameters\n",
    "from nncf.quantization.range_estimator import StatisticsCollectorParameters\n",
    "from nncf.quantization.range_estimator import StatisticsType\n",
    "from nncf.parameters import ModelType\n",
    "\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    \"\"\"\n",
    "    Extract the model's input from the data item.\n",
    "    The data item here is the data item that is returned from the data source per iteration.\n",
    "    This function should be passed when the data item cannot be used as model's input.\n",
    "    \"\"\"\n",
    "    return np.array(data_item[\"input_values\"])\n",
    "\n",
    "\n",
    "calibration_dataset = nncf.Dataset(dataset, transform_fn)\n",
    "\n",
    "\n",
    "quantized_model = nncf.quantize(\n",
    "    ov_model,\n",
    "    calibration_dataset,\n",
    "    model_type=ModelType.TRANSFORMER,  # specify additional transformer patterns in the model\n",
    "    subset_size=len(dataset),\n",
    "    ignored_scope=nncf.IgnoredScope(\n",
    "        names=[\n",
    "            \"/data2vec_audio/encoder/layers.3/feed_forward/intermediate_dense/MatMul\",\n",
    "            \"/data2vec_audio/feature_extractor/conv_layers.2/conv/Conv\",\n",
    "            \"/data2vec_audio/encoder/layers.3/Add_1\",\n",
    "            \"/data2vec_audio/encoder/layers.2/feed_forward/intermediate_dense/MatMul\",\n",
    "            \"/data2vec_audio/feature_extractor/conv_layers.0/conv/Conv\",\n",
    "            \"/data2vec_audio/encoder/layers.4/Add_1\",\n",
    "            \"/data2vec_audio/encoder/layers.4/feed_forward/intermediate_dense/MatMul\",\n",
    "            \"/data2vec_audio/encoder/layers.4/final_layer_norm/Div\",\n",
    "            \"/data2vec_audio/encoder/layers.4/feed_forward/output_dense/MatMul\",\n",
    "            \"/data2vec_audio/encoder/layers.8/attention/MatMul_1\",\n",
    "            \"/data2vec_audio/feature_extractor/conv_layers.1/conv/Conv\",\n",
    "            \"/data2vec_audio/encoder/layers.2/Add_1\",\n",
    "            \"/data2vec_audio/feature_extractor/conv_layers.0/layer_norm/Div\",\n",
    "            \"/data2vec_audio/encoder/layers.1/feed_forward/intermediate_dense/MatMul\",\n",
    "            \"/data2vec_audio/encoder/layers.1/Add_1\",\n",
    "            \"/data2vec_audio/feature_extractor/conv_layers.3/layer_norm/Div\"\n",
    "        ],\n",
    "    ),\n",
    "    advanced_parameters=AdvancedQuantizationParameters(\n",
    "        activations_range_estimator_params=RangeEstimatorParameters(\n",
    "            min=StatisticsCollectorParameters(\n",
    "                statistics_type=StatisticsType.MIN,\n",
    "                aggregator_type=AggregatorType.MIN\n",
    "            ),\n",
    "            max=StatisticsCollectorParameters(\n",
    "                statistics_type=StatisticsType.QUANTILE,\n",
    "                aggregator_type=AggregatorType.MEAN,\n",
    "                quantile_outlier_prob=0.0001\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc9e942-58d4-49d3-b115-758583661093",
   "metadata": {},
   "source": [
    "After quantization is finished, compressed model representation can be saved using `serialize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898ecbdd-1f89-47c7-8877-b7acf57ec8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'quantized_data2vec_base'\n",
    "quantized_model_path = Path(f\"{MODEL_NAME}_openvino_model/{MODEL_NAME}_quantized.xml\")\n",
    "serialize(quantized_model, str(quantized_model_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fda5391-9a7c-4606-8d5e-fed5fd6cf0fc",
   "metadata": {},
   "source": [
    "## Check INT8 model inference result\n",
    "\n",
    "`INT8` model is the same in usage like the original one. We need to read it, using the `core.read_model` method and load on the device, using `core.compile_model`. After that, we can reuse the same `ov_infer` function for getting model inference result on test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ea3977-c5bd-42ec-b700-4bcc43a8cd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_compiled_model = core.compile_model(quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518b56cb-5617-4ad6-a7b5-747e9698829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = ov_infer(int8_compiled_model, dataset[0])\n",
    "print(f\"[Reference]:     {dataset[0]['text']}\")\n",
    "print(f\"[OpenVINO INT8]: {transcription[0]}\")\n",
    "ipd.Audio(test_sample[\"array\"], rate=16000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98777233-6cc5-4d97-bbcd-f8338c75b55e",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "[Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) is used to measure the inference performance of the `FP16` and `INT8` models.\n",
    "\n",
    "> **NOTE**: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65ef565-5b65-4efb-b663-526c8e9f0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP16 model (OpenVINO IR)\n",
    "! benchmark_app -m $ir_model_path -shape [1,30480] -d CPU -api async -t 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32960bce-373a-4421-ba7c-d9a7e3f9b678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "! benchmark_app -m $quantized_model_path -shape [1,30480] -d CPU -api async -t 15"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34347a28-73bb-4bfc-8048-25e03d70f80e",
   "metadata": {},
   "source": [
    "## Compare Accuracy of the Original and Quantized Models\n",
    "\n",
    "Finally, calculate WER metric for the `INT8` model representation and compare it with the `FP16` result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4937fb-ba00-490b-b116-d6c731d35450",
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_ov_result = compute_wer(dataset, int8_compiled_model, ov_infer)\n",
    "print(f'[OpenVino FP16] Word Error Rate: {ov_result:.4}')\n",
    "print(f'[OpenVino INT8] Word Error Rate: {int8_ov_result:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
