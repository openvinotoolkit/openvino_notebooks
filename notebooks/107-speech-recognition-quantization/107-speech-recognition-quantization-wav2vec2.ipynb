{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cacbe6b4",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Quantize Speech Recognition Models using NNCF PTQ API\n",
    "This tutorial demonstrates how to apply `INT8` quantization to the speech recognition model, known as [Wav2Vec2](https://huggingface.co/docs/transformers/model_doc/wav2vec2), using the NNCF (Neural Network Compression Framework) 8-bit quantization in post-training mode (without the fine-tuning pipeline). This notebook uses a fine-tuned [Wav2Vec2-Base-960h](https://huggingface.co/facebook/wav2vec2-base-960h) [PyTorch](https://pytorch.org/) model trained on the [LibriSpeech ASR corpus](https://www.openslr.org/12). The tutorial is designed to be extendable to custom models and datasets. It consists of the following steps:\n",
    "\n",
    "- Download and prepare the Wav2Vec2 model and LibriSpeech dataset.\n",
    "- Define data loading and accuracy validation functionality.\n",
    "- Model quantization.\n",
    "- Compare Accuracy of original PyTorch model, OpenVINO FP16 and INT8 models.\n",
    "- Compare performance of the original and quantized models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f1f601b-c012-4e48-ab50-262ec3c0af2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q 'openvino-dev>=2023.0.0' 'nncf>=2.5.0'\n",
    "!pip install -q soundfile librosa transformers onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b41e6-132b-40da-b3b9-91bacba29e31",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "629e2d2c",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import tarfile\n",
    "from itertools import groupby\n",
    "import soundfile as sf\n",
    "import IPython.display as ipd\n",
    "\n",
    "from transformers import Wav2Vec2ForCTC\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e66896-d439-4065-868a-65b44d31525a",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "284e9a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set the data and model directories, model source URL and model filename.\n",
    "MODEL_DIR = Path(\"model\")\n",
    "DATA_DIR = Path(\"../data/datasets/librispeech\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44dc335d",
   "metadata": {
    "id": "YytHDzLE0uOJ"
   },
   "source": [
    "## Prepare the Model\n",
    "Perform the following:\n",
    "- Download and unpack a pre-trained Wav2Vec2 model.\n",
    "- Convert the model to ONNX.\n",
    "- Run Model Optimizer to convert the model from the ONNX representation to the OpenVINO Intermediate Representation (OpenVINO IR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be9fc64c",
   "metadata": {
    "id": "f7i6dWUmhloy"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3deb4bd30a14435880cf96ae6ec2d76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model/pytorch/pytorch_model.bin:   0%|          | 0.00/360M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/ea/work/openvino_notebooks/notebooks/107-speech-recognition-quantization/model/pytorch/config.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "download_file(\"https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/pytorch_model.bin\", directory=Path(MODEL_DIR) / 'pytorch', show_progress=True)\n",
    "download_file(\"https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/config.json\", directory=Path(MODEL_DIR) / 'pytorch', show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca2fa0",
   "metadata": {
    "id": "ehX7F6KB0uPu"
   },
   "source": [
    "Import all dependencies to load the original PyTorch model and convert it to the ONNX representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d7401c0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at model/pytorch and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">30</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">27 </span>torch_model = Wav2Vec2ForCTC.from_pretrained(Path(MODEL_DIR) / <span style=\"color: #808000; text-decoration-color: #808000\">'pytorch'</span>)                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">28 </span>onnx_model_path = Path(MODEL_DIR) / <span style=\"color: #808000; text-decoration-color: #808000\">\"wav2vec2_base.onnx\"</span>                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">29 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> onnx_model_path.exists():                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>30 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>export_model_to_onnx(torch_model, onnx_model_path)                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">31 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">export_model_to_onnx</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">6</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 3 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 4 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 5 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">export_model_to_onnx</span>(model, path):                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 6 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> torch.no_grad():                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 7 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>default_input = torch.zeros([<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>, MAX_SEQ_LENGTH], dtype=torch.float)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 8 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inputs = {                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 9 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"inputs\"</span>: default_input                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">NameError: </span>name <span style=\"color: #008000; text-decoration-color: #008000\">'torch'</span> is not defined\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m30\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m27 \u001b[0mtorch_model = Wav2Vec2ForCTC.from_pretrained(Path(MODEL_DIR) / \u001b[33m'\u001b[0m\u001b[33mpytorch\u001b[0m\u001b[33m'\u001b[0m)                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m28 \u001b[0monnx_model_path = Path(MODEL_DIR) / \u001b[33m\"\u001b[0m\u001b[33mwav2vec2_base.onnx\u001b[0m\u001b[33m\"\u001b[0m                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m29 \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m onnx_model_path.exists():                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m30 \u001b[2m│   \u001b[0mexport_model_to_onnx(torch_model, onnx_model_path)                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m31 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mexport_model_to_onnx\u001b[0m:\u001b[94m6\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 3 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 4 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 5 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mexport_model_to_onnx\u001b[0m(model, path):                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 6 \u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 7 \u001b[0m\u001b[2m│   │   \u001b[0mdefault_input = torch.zeros([\u001b[94m1\u001b[0m, MAX_SEQ_LENGTH], dtype=torch.float)                 \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 8 \u001b[0m\u001b[2m│   │   \u001b[0minputs = {                                                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 9 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33minputs\u001b[0m\u001b[33m\"\u001b[0m: default_input                                                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'torch'\u001b[0m is not defined\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "MAX_SEQ_LENGTH = 30480\n",
    "\n",
    "\n",
    "def export_model_to_onnx(model, path):\n",
    "    with torch.no_grad():\n",
    "        default_input = torch.zeros([1, MAX_SEQ_LENGTH], dtype=torch.float)\n",
    "        inputs = {\n",
    "            \"inputs\": default_input\n",
    "        }\n",
    "        symbolic_names = {0: \"batch_size\", 1: \"sequence_len\"}\n",
    "        torch.onnx.export(\n",
    "            model,\n",
    "            (inputs[\"inputs\"]),\n",
    "            path,\n",
    "            opset_version=11,\n",
    "            input_names=[\"inputs\"],\n",
    "            output_names=[\"logits\"],\n",
    "            dynamic_axes={\n",
    "                \"inputs\": symbolic_names,\n",
    "                \"logits\": symbolic_names,\n",
    "            },\n",
    "        )\n",
    "        print(\"ONNX model saved to {}\".format(path))\n",
    "\n",
    "\n",
    "torch_model = Wav2Vec2ForCTC.from_pretrained(Path(MODEL_DIR) / 'pytorch')\n",
    "onnx_model_path = Path(MODEL_DIR) / \"wav2vec2_base.onnx\"\n",
    "if not onnx_model_path.exists():\n",
    "    export_model_to_onnx(torch_model, onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2684e8a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import Core, serialize\n",
    "import torch\n",
    "\n",
    "\n",
    "ov_model = mo.convert_model(onnx_model_path, compress_to_fp16=True)\n",
    "\n",
    "ir_model_path = MODEL_DIR / \"wav2vec2_base.xml\"\n",
    "serialize(ov_model, str(ir_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f4b0d",
   "metadata": {
    "id": "LBbY7c4NsHzT"
   },
   "source": [
    "## Prepare LibriSpeech Dataset\n",
    "\n",
    "Use the code below to download and unpack the archives with 'dev-clean' and 'test-clean' subsets of LibriSpeech Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43070514",
   "metadata": {
    "id": "NN-qRME1a-Sm"
   },
   "outputs": [],
   "source": [
    "download_file(\"http://openslr.elda.org/resources/12/dev-clean.tar.gz\", directory=DATA_DIR, show_progress=True)\n",
    "download_file(\"http://openslr.elda.org/resources/12/test-clean.tar.gz\", directory=DATA_DIR, show_progress=True)\n",
    "\n",
    "if not os.path.exists(f'{DATA_DIR}/LibriSpeech/dev-clean'):\n",
    "    with tarfile.open(f\"{DATA_DIR}/dev-clean.tar.gz\") as tar:\n",
    "        tar.extractall(path=DATA_DIR)\n",
    "if not os.path.exists(f'{DATA_DIR}/LibriSpeech/test-clean'):\n",
    "    with tarfile.open(f\"{DATA_DIR}/test-clean.tar.gz\") as tar:\n",
    "        tar.extractall(path=DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da802acd",
   "metadata": {
    "id": "E5hsOsj-0uSc"
   },
   "source": [
    "## Define DataLoader\n",
    "Wav2Vec2 model accepts a raw waveform of the speech signal as input and produces vocabulary class estimations as output. Since the dataset contains\n",
    "audio files in FLAC format, use the 'soundfile' package to convert them to waveform.\n",
    "\n",
    "> **NOTE**: Consider increasing `samples_limit` to get more precise results. A suggested value is `300` or more, as it will take longer time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a378cb",
   "metadata": {
    "id": "6xnl2PhM0uSn",
    "tags": [],
    "test_replace": {
     "return len(self._ds)": "return 2",
     "self.samples_limit = samples_limit": "self.amples_limit = 2"
    }
   },
   "outputs": [],
   "source": [
    "class LibriSpeechDataLoader:\n",
    "\n",
    "    @staticmethod\n",
    "    def read_flac(file_name):\n",
    "        speech, samplerate = sf.read(file_name)\n",
    "        assert samplerate == 16000, \"read_flac: only 16kHz supported!\"\n",
    "        return speech\n",
    "\n",
    "    # Required methods\n",
    "    def __init__(self, config, samples_limit=300):\n",
    "        \"\"\"Constructor\n",
    "        :param config: data loader specific config\n",
    "        \"\"\"\n",
    "        self.samples_limit = samples_limit\n",
    "        self._data_dir = config[\"data_source\"]\n",
    "        self._ds = []\n",
    "        self._prepare_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns size of the dataset\"\"\"\n",
    "        return len(self._ds)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns annotation, data and metadata at the specified index.\n",
    "        Possible formats:\n",
    "        (index, annotation), data\n",
    "        (index, annotation), data, metadata\n",
    "        \"\"\"\n",
    "        label = self._ds[index][0]\n",
    "        inputs = {'inputs': np.expand_dims(self._ds[index][1], axis=0)}\n",
    "        return label, inputs\n",
    "\n",
    "    # Methods specific to the current implementation\n",
    "    def _prepare_dataset(self):\n",
    "        pattern = re.compile(r'([0-9\\-]+)\\s+(.+)')\n",
    "        data_folder = Path(self._data_dir)\n",
    "        txts = list(data_folder.glob('**/*.txt'))\n",
    "        counter = 0\n",
    "        for txt in txts:\n",
    "            content = txt.open().readlines()\n",
    "            for line in content:\n",
    "                res = pattern.search(line)\n",
    "                if not res:\n",
    "                    continue\n",
    "                name = res.group(1)\n",
    "                transcript = res.group(2)\n",
    "                fname = txt.parent / name\n",
    "                fname = fname.with_suffix('.flac')\n",
    "                identifier = str(fname.relative_to(data_folder))\n",
    "                self._ds.append(((counter, transcript.upper()), LibriSpeechDataLoader.read_flac(os.path.join(self._data_dir, identifier))))\n",
    "                counter += 1\n",
    "                if counter >= self.samples_limit:\n",
    "                    # Limit exceeded\n",
    "                    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbbca4a",
   "metadata": {
    "id": "CclWk-fVd9Wi"
   },
   "source": [
    "## Run Quantization\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf) provides a suite of advanced algorithms for Neural Networks inference optimization in OpenVINO with minimal accuracy drop.\n",
    "\n",
    "Create a quantized model from the pre-trained `FP16` model and the calibration dataset. The optimization process contains the following steps:\n",
    "    1. Create a Dataset for quantization.\n",
    "    2. Run `nncf.quantize` for getting an optimized model. The `nncf.quantize` function provides an interface for model quantization. It requires an instance of the OpenVINO Model and quantization dataset.\n",
    "Optionally, some additional parameters for the configuration quantization process (number of samples for quantization, preset, ignored scope, etc.) can be provided. For more accurate results, we should keep the operation in the postprocessing subgraph in floating point precision, using the `ignored_scope` parameter. `advanced_parameters` can be used to specify advanced quantization parameters for fine-tuning the quantization algorithm. In this tutorial we pass range estimator parameters for activations. For more information see [Tune quantization parameters](https://docs.openvino.ai/2023.0/basic_quantization_flow.html#tune-quantization-parameters).\n",
    "    3. Serialize OpenVINO IR model using `openvino.runtime.serialize` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16457bd4",
   "metadata": {
    "id": "PiAvrwo0tr6Z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nncf\n",
    "from nncf.quantization.advanced_parameters import AdvancedQuantizationParameters, RangeEstimatorParameters\n",
    "from nncf.quantization.range_estimator import StatisticsCollectorParameters, StatisticsType, AggregatorType\n",
    "from nncf.parameters import ModelType\n",
    "\n",
    "\n",
    "def transform_fn(data_item):\n",
    "    \"\"\"\n",
    "    Extract the model's input from the data item.\n",
    "    The data item here is the data item that is returned from the data source per iteration.\n",
    "    This function should be passed when the data item cannot be used as model's input.\n",
    "    \"\"\"\n",
    "    _, inputs = data_item\n",
    "\n",
    "    return inputs[\"inputs\"]\n",
    "\n",
    "\n",
    "dataset_config = {\"data_source\": os.path.join(DATA_DIR, \"LibriSpeech/dev-clean\")}\n",
    "data_loader = LibriSpeechDataLoader(dataset_config, samples_limit=300)\n",
    "calibration_dataset = nncf.Dataset(data_loader, transform_fn)\n",
    "\n",
    "\n",
    "quantized_model = nncf.quantize(\n",
    "    ov_model,\n",
    "    calibration_dataset,\n",
    "    model_type=ModelType.TRANSFORMER,  # specify additional transformer patterns in the model\n",
    "    ignored_scope=nncf.IgnoredScope(\n",
    "        names=[\n",
    "            '/wav2vec2/feature_extractor/conv_layers.1/conv/Conv',\n",
    "            '/wav2vec2/feature_extractor/conv_layers.2/conv/Conv',\n",
    "            '/wav2vec2/encoder/layers.7/feed_forward/output_dense/MatMul'\n",
    "        ],\n",
    "    ),\n",
    "    advanced_parameters=AdvancedQuantizationParameters(\n",
    "        activations_range_estimator_params=RangeEstimatorParameters(\n",
    "            min=StatisticsCollectorParameters(\n",
    "                statistics_type=StatisticsType.MIN,\n",
    "                aggregator_type=AggregatorType.MIN\n",
    "            ),\n",
    "            max=StatisticsCollectorParameters(\n",
    "                statistics_type=StatisticsType.QUANTILE,\n",
    "                aggregator_type=AggregatorType.MEAN,\n",
    "                quantile_outlier_prob=0.0001\n",
    "            ),\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05b3999",
   "metadata": {
    "id": "hPj_fcDAG8xG"
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = 'quantized_wav2vec2_base'\n",
    "quantized_model_path = Path(f\"{MODEL_NAME}_openvino_model/{MODEL_NAME}_quantized.xml\")\n",
    "serialize(quantized_model, str(quantized_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754b4d84",
   "metadata": {},
   "source": [
    "## Model Usage Example with Inference Pipeline\n",
    "Both initial (`FP16`) and quantized (`INT8`) models are exactly the same in use.\n",
    "\n",
    "Start with taking one example from the dataset to show inference steps for it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7538271f",
   "metadata": {},
   "source": [
    "Next, load the quantized model to the inference pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0431ac4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = LibriSpeechDataLoader.read_flac(f'{DATA_DIR}/LibriSpeech/test-clean/121/127105/121-127105-0017.flac')\n",
    "\n",
    "ipd.Audio(audio, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f89c0f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "core = Core()\n",
    "\n",
    "compiled_model = core.compile_model(model=quantized_model, device_name='CPU')\n",
    "\n",
    "input_data = np.expand_dims(audio, axis=0)\n",
    "output_layer = compiled_model.outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8274a6",
   "metadata": {},
   "source": [
    "Next, make a prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc43396",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = compiled_model([input_data])[output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4095f9",
   "metadata": {},
   "source": [
    "Now, you just need to decode predicted probabilities to text, using tokenizer `decode_logits`.\n",
    "\n",
    "Alternatively, use a built-in `Wav2Vec2Processor` tokenizer from the `transformers` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516f6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_logits(logits):\n",
    "    decoding_vocab = dict(enumerate(MetricWER.alphabet))\n",
    "    token_ids = np.squeeze(np.argmax(logits, -1))\n",
    "    tokens = [decoding_vocab[idx] for idx in token_ids]\n",
    "    tokens = [token_group[0] for token_group in groupby(tokens)]\n",
    "    tokens = [t for t in tokens if t != MetricWER.pad_token]\n",
    "    res_string = ''.join([t if t != MetricWER.words_delimiter else ' ' for t in tokens]).strip()\n",
    "    res_string = ' '.join(res_string.split(' '))\n",
    "    res_string = res_string.lower()\n",
    "    return res_string\n",
    "\n",
    "\n",
    "predicted_text = decode_logits(predictions)\n",
    "predicted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67f6a2",
   "metadata": {
    "id": "vQACMfAUo52V",
    "tags": []
   },
   "source": [
    "## Validate model accuracy on dataset\n",
    "The code below is used for running model inference on a single sample from the dataset. It contains the following steps:\n",
    "\n",
    "* Define `MetricWER` class to calculate Word Error Rate.\n",
    "* Define dataloader for test dataset.\n",
    "* Define functions to get inference for PyTorch and OpenVINO models.\n",
    "* Define functions to compute Word Error Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6047d49",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MetricWER:\n",
    "    alphabet = [\n",
    "        \"<pad>\", \"<s>\", \"</s>\", \"<unk>\", \"|\",\n",
    "        \"e\", \"t\", \"a\", \"o\", \"n\", \"i\", \"h\", \"s\", \"r\", \"d\", \"l\", \"u\",\n",
    "        \"m\", \"w\", \"c\", \"f\", \"g\", \"y\", \"p\", \"b\", \"v\", \"k\", \"'\", \"x\", \"j\", \"q\", \"z\"]\n",
    "    words_delimiter = '|'\n",
    "    pad_token = '<pad>'\n",
    "\n",
    "    # Required methods\n",
    "    def __init__(self):\n",
    "        self._name = \"WER\"\n",
    "        self._sum_score = 0\n",
    "        self._sum_words = 0\n",
    "        self._cur_score = 0\n",
    "        self._decoding_vocab = dict(enumerate(self.alphabet))\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        \"\"\"Returns accuracy metric value for the last model output.\"\"\"\n",
    "        return {self._name: self._cur_score}\n",
    "\n",
    "    @property\n",
    "    def avg_value(self):\n",
    "        \"\"\"Returns accuracy metric value for all model outputs.\"\"\"\n",
    "        return {self._name: self._sum_score / self._sum_words if self._sum_words != 0 else 0}\n",
    "\n",
    "    def update(self, output, target):\n",
    "        \"\"\"\n",
    "        Updates prediction matches.\n",
    "\n",
    "        :param output: model output\n",
    "        :param target: annotations\n",
    "        \"\"\"\n",
    "        decoded = [decode_logits(i) for i in output]\n",
    "        target = [i.lower() for i in target]\n",
    "        assert len(output) == len(target), \"sizes of output and target mismatch!\"\n",
    "        for i in range(len(output)):\n",
    "            self._get_metric_per_sample(decoded[i], target[i])\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets collected matches\n",
    "        \"\"\"\n",
    "        self._sum_score = 0\n",
    "        self._sum_words = 0\n",
    "\n",
    "    def get_attributes(self):\n",
    "        \"\"\"\n",
    "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
    "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
    "                             'type': metric type\n",
    "        \"\"\"\n",
    "        return {self._name: {\"direction\": \"higher-worse\", \"type\": \"WER\"}}\n",
    "\n",
    "    # Methods specific to the current implementation\n",
    "    def _get_metric_per_sample(self, annotation, prediction):\n",
    "        cur_score = self._editdistance_eval(annotation.split(), prediction.split())\n",
    "        cur_words = len(annotation.split())\n",
    "\n",
    "        self._sum_score += cur_score\n",
    "        self._sum_words += cur_words\n",
    "        self._cur_score = cur_score / cur_words\n",
    "\n",
    "        result = cur_score / cur_words if cur_words != 0 else 0\n",
    "        return result\n",
    "\n",
    "    def _editdistance_eval(self, source, target):\n",
    "        n, m = len(source), len(target)\n",
    "\n",
    "        distance = np.zeros((n + 1, m + 1), dtype=int)\n",
    "        distance[:, 0] = np.arange(0, n + 1)\n",
    "        distance[0, :] = np.arange(0, m + 1)\n",
    "\n",
    "        for i in range(1, n + 1):\n",
    "            for j in range(1, m + 1):\n",
    "                cost = 0 if source[i - 1] == target[j - 1] else 1\n",
    "\n",
    "                distance[i][j] = min(distance[i - 1][j] + 1,\n",
    "                                     distance[i][j - 1] + 1,\n",
    "                                     distance[i - 1][j - 1] + cost)\n",
    "        return distance[n][m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7a74d0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset_config = {\"data_source\": os.path.join(DATA_DIR, \"LibriSpeech/test-clean\")}\n",
    "test_data_loader = LibriSpeechDataLoader(dataset_config, samples_limit=300)\n",
    "\n",
    "\n",
    "# inference function for pytorch\n",
    "def torch_infer(model, sample):\n",
    "    output = model(torch.Tensor(sample[1]['inputs'])).logits\n",
    "    output = output.detach().cpu().numpy()\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# inference function for openvino\n",
    "def ov_infer(model, sample):\n",
    "    output = model.output(0)\n",
    "    output = model(np.array(sample[1]['inputs']))[output]\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def compute_wer(dataset, model, infer_fn):\n",
    "    wer = MetricWER()\n",
    "    for sample in tqdm(dataset):\n",
    "        # run infer function on sample\n",
    "        output = infer_fn(model, sample)\n",
    "        # update metric on sample result\n",
    "        wer.update(output, [sample[0][1]])\n",
    "\n",
    "    return wer.avg_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3430624f",
   "metadata": {},
   "source": [
    "Now, compute WER for the original PyTorch model, OpenVINO IR model and quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0ced60",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "compiled_fp32_ov_model = core.compile_model(ov_model)\n",
    "\n",
    "pt_result = compute_wer(test_data_loader, torch_model, torch_infer)\n",
    "ov_fp32_result = compute_wer(test_data_loader, compiled_fp32_ov_model, ov_infer)\n",
    "quantized_result = compute_wer(test_data_loader, compiled_model, ov_infer)\n",
    "\n",
    "print(f'[PyTorch]   Word Error Rate: {pt_result[\"WER\"]:.4f}')\n",
    "print(f'[OpenVino]  Word Error Rate: {ov_fp32_result[\"WER\"]:.4f}')\n",
    "print(f'[Quantized OpenVino]  Word Error Rate: {quantized_result[\"WER\"]:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b80b03f",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "Finally, use [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance of the `FP16` and `INT8` models.\n",
    "\n",
    "> **NOTE**: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46b4a90",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Inference FP16 model (OpenVINO IR)\n",
    "! benchmark_app -m $ir_model_path -shape [1,30480] -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bf78ec",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference INT8 model (OpenVINO IR)\n",
    "! benchmark_app -m $quantized_model_path -shape [1,30480] -d CPU -api async"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
