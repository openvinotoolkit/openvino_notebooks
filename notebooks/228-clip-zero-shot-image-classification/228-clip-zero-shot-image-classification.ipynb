{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot Image Classification with OpenAI CLIP and OpenVINOâ„¢\n",
    "\n",
    "Zero-shot image classification is a computer vision task to classify images into one of several classes without any prior training or knowledge of the classes.\n",
    "\n",
    "![zero-shot-pipeline](https://user-images.githubusercontent.com/29454499/207773481-d77cacf8-6cdc-4765-a31b-a1669476d620.png)\n",
    "\n",
    "[**image source*](https://huggingface.co/tasks/zero-shot-image-classification)\n",
    "\n",
    "\n",
    "Zero-shot learning resolves several challenges in image retrieval systems. For example, with the rapid growth of categories on the web, it is challenging to index images based on unseen categories. We can associate unseen categories to images with zero-shot learning by exploiting attributes to model's relationship between visual features and labels.\n",
    "In this tutorial, we will use the [OpenAI CLIP](https://github.com/openai/CLIP) model to perform zero-shot image classification. The notebook contains the following steps:\n",
    "1. Download the model.\n",
    "2. Instantiate the PyTorch model.\n",
    "3. Export the ONNX model and convert it to OpenVINO IR, using the Model Optimizer tool.\n",
    "4. Run CLIP with OpenVINO.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on various (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task.\n",
    "CLIP uses a [ViT](https://arxiv.org/abs/2010.11929) like transformer to get visual features and a causal language model to get the text features. The text and visual features are then projected into a latent space with identical dimensions. The dot product between the projected image and text features is then used as a similarity score.\n",
    "\n",
    "![clip](https://raw.githubusercontent.com/openai/CLIP/main/CLIP.png)\n",
    "\n",
    "[**image_source*](https://github.com/openai/CLIP/blob/main/README.md)\n",
    "\n",
    "You can find more information about this model in the [research paper](https://arxiv.org/abs/2103.00020), [OpenAI blog](https://openai.com/blog/clip/), [model card](https://github.com/openai/CLIP/blob/main/model-card.md) and GitHub [repository](https://github.com/openai/CLIP).\n",
    "\n",
    "In this notebook, we will use [openai/clip-vit-base-patch16](https://huggingface.co/openai/clip-vit-base-patch16), available via Hugging Face Transformers, but the same steps are applicable for other CLIP family models.\n",
    "\n",
    "First, we need to create `CLIPModel` class object and initialize it with model configuration and weights, using `from_pretrained` method. The model will be automatically downloaded from Hugging Face Hub and cached for the next usage.\n",
    "`CLIPProcessor` class is a wrapper for input data preprocessing. It includes both encoding the text using tokenizer and preparing the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "# load pre-trained model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\")\n",
    "# load preprocessor for model input\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def visualize_result(image:Image, labels:List[str], probs:np.ndarray, top:int = 5):\n",
    "    \"\"\" \n",
    "    Utility function for visualization classification results\n",
    "    params:\n",
    "      image: input image\n",
    "      labels: list of classification labels\n",
    "      probs: model predicted softmaxed probabilities for each label\n",
    "      top: number of the highest probability results for visualization\n",
    "    returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(64, 64))\n",
    "    top_labels = np.argsort(-probs)[:min(top, probs.shape[0])]\n",
    "    top_probs = probs[top_labels]\n",
    "    plt.subplot(8, 8, 1)\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(8, 8, 2)\n",
    "    y = np.arange(top_probs.shape[-1])\n",
    "    plt.grid()\n",
    "    plt.barh(y, top_probs)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.gca().set_axisbelow(True)\n",
    "    plt.yticks(y, [labels[index] for index in top_labels])\n",
    "    plt.xlabel(\"probability\")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run PyTorch model inference\n",
    "\n",
    "To perform classification, define labels and load an image in RGB format. To give the model wider text context and improve guidance, we extend the labels description using the template \"This is a photo of a\".\n",
    "Both the list of label descriptions and image should be passed through the processor to obtain a dictionary with input data in the model-specific format. The model predicts an image-text similarity score in raw logits format, which can be normalized to the `[0, 1]` range using the `softmax` function. Then, we select labels with the highest similarity score for the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open('../data/image/coco.jpg')\n",
    "input_labels = ['cat', 'dog', 'wolf', 'tiger', 'man', 'horse', 'frog', 'tree', 'house', 'computer']\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in input_labels]\n",
    "\n",
    "inputs = processor(text=text_descriptions, images=[image], return_tensors=\"pt\", padding=True)\n",
    "\n",
    "results = model(**inputs)\n",
    "logits_per_image = results['logits_per_image']  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1).detach().numpy()  # we can take the softmax to get the label probabilities\n",
    "visualize_result(image, input_labels, probs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to OpenVINO Intermediate Representation (IR) format.\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/29454499/208048580-8264e54c-151c-43ef-9e25-1302cd0dd7a2.png\" alt=\"conversion path\" style=\"width:50%;height:50%\"/>\n",
    "\n",
    "\n",
    "For best results with OpenVINO, it is recommended to convert the model to OpenVINO IR format. OpenVINO supports PyTorch via ONNX conversion. The `torch.onnx.export` function enables conversion of PyTorch models to ONNX format. It requires to provide initialized model object, example of inputs for tracing and path for saving result. The model contains operations which supported for ONNX tracing starting with opset 14, it is recommended to use it as `opset_version` parameter. Besides that, we need to have opportunity to provide descriptions various of length and images with different sizes, for preserving this capability after ONNX conversion, `dynamic_axes` parameter can be used. More information about PyTorch to ONNX exporting can be found in this [tutorial](https://pytorch.org/tutorials/advanced/super_resolution_with_onnxruntime.html) and [PyTorch documentation](https://pytorch.org/docs/stable/onnx.html). We will use `mo.convert_model` functionality to convert the ONNX model. The `mo.convert_model` Python function returns an OpenVINO model ready to load on the device and start making predictions. We can save it on disk for the next usage with `openvino.runtime.serialize`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,  # model being run\n",
    "    # model input in one of acceptable format: torch.Tensor (for single input), tuple or list of tensors for multiple inputs or dictionary with string keys and tensors as values.\n",
    "    dict(inputs),\n",
    "    \"clip-vit-base-patch16.onnx\",  # where to save the model\n",
    "    opset_version=14,  # the ONNX version to export the model to\n",
    "    input_names=[\"input_ids\", \"pixel_values\", \"attention_mask\"],  # the model's input names\n",
    "    output_names=[\"logits_per_image\", \"logits_per_text\", \"text_embeds\", \"image_embeds\"],  # the model's output names\n",
    "    dynamic_axes={  # variable length axes\n",
    "        \"input_ids\": {0: \"batch\", 1: \"sequence\"},\n",
    "        \"pixel_values\": {0: \"batch\", 1: \"num_channels\", 2: \"height\", 3: \"width\"},\n",
    "        \"attention_mask\": {0: \"batch\", 1: \"sequence\"},\n",
    "        \"logits_per_image\": {0: \"batch\"},\n",
    "        \"logits_per_text\": {0: \"batch\"},\n",
    "        \"text_embeds\": {0: \"batch\"},\n",
    "        \"image_embeds\": {0: \"batch\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import serialize\n",
    "from openvino.tools import mo\n",
    "\n",
    "ov_model = mo.convert_model('clip-vit-base-patch16.onnx', compress_to_fp16=True)\n",
    "serialize(ov_model, 'clip-vit-base-patch16.xml')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run OpenVINO model\n",
    "\n",
    "The steps for making predictions with the OpenVINO CLIP model are similar to the PyTorch model. Let us check the model result using the same input data from the example above with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "from openvino.runtime import Core\n",
    "\n",
    "# create OpenVINO core object instance\n",
    "core = Core()\n",
    "# compile model for loading on device\n",
    "compiled_model = core.compile_model(ov_model)\n",
    "# obtain output tensor for getting predictions\n",
    "logits_per_image_out = compiled_model.output(0)\n",
    "# run inference on preprocessed data and get image-text similarity score\n",
    "ov_logits_per_image = compiled_model(dict(inputs))[logits_per_image_out]\n",
    "# perform softmax on score\n",
    "probs = softmax(ov_logits_per_image, axis=1)\n",
    "# visualize prediction\n",
    "visualize_result(image, input_labels, probs[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Looks like we got the same result."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is your turn! You can provide your own image and comma-separated list of labels for zero-shot classification.\n",
    "\n",
    "Feel free to upload an image, using the file upload window and type label names into the text field, using comma as the separator (for example, `cat,dog,bird`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "style = {'description_width': 'initial'}\n",
    "\n",
    "image_widget = widgets.FileUpload(\n",
    "    accept='',\n",
    "    multiple=False,\n",
    "    description='Upload image',\n",
    "    style=style\n",
    ")\n",
    "\n",
    "labels_widget = widgets.Textarea(\n",
    "    value='cat,dog,bird',\n",
    "    placeholder='Type something',\n",
    "    description='Enter your classes separated by ,:',\n",
    "    disabled=False,\n",
    "    style=style\n",
    ")\n",
    "widgets.VBox(children=[image_widget, labels_widget])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to get the result for your submitted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "# read uploaded image\n",
    "image = Image.open(io.BytesIO(image_widget.value[-1]['content'])) if image_widget.value else image\n",
    "# obtain list of labels\n",
    "labels = labels_widget.value.split(',')\n",
    "# convert labels to text description\n",
    "text_descriptions = [f\"This is a photo of a {label}\" for label in labels]\n",
    "\n",
    "# preprocess input\n",
    "inputs = processor(text=text_descriptions, images=[image], return_tensors=\"np\", padding=True)\n",
    "# run inference\n",
    "ov_logits_per_image = compiled_model(dict(inputs))[logits_per_image_out]\n",
    "# perform softmax on score\n",
    "probs = softmax(ov_logits_per_image, axis=1)\n",
    "# visualize prediction\n",
    "visualize_result(image, labels, probs[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
