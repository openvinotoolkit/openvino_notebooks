{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d34647",
   "metadata": {},
   "source": [
    "# Chat with document using OpenVINO and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39b66a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f077b32-5d36-44b0-9041-407e996283a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"gradio\"\\\n",
    "\"onnx\" \"chromadb\" \"sentence_transformers\" \"langchain\" \"langchainhub\" \"transformers>=4.34.0\" \"unstructured\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd7fbe",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b57cfb-e727-43a5-b2c9-8f1b1ba72061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 07:34:04.793766: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 07:34:04.795769: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-05 07:34:04.822059: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 07:34:04.822082: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 07:34:04.822105: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 07:34:04.827929: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-05 07:34:04.828700: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 07:34:05.433114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from embedding import OpenVINO_Embeddings\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig, TextIteratorStreamer, pipeline\n",
    "from optimum.intel import OVQuantizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import torch\n",
    "import nncf\n",
    "import logging\n",
    "import numpy as np\n",
    "import shutil\n",
    "import gc\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ff5a9",
   "metadata": {},
   "source": [
    "## Convert embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff80e6eb-7923-40ef-93d8-5e6c56e50667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef1f5b4afa2f4505967a9c1253327aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Embedding Model:', options=('all-mpnet-base-v2',), value='all-mpnet-base-v2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUPPORTED_EMBEDDING_MODELS = {\"all-mpnet-base-v2\": {\"model_id\": \"sentence-transformers/all-mpnet-base-v2\"}}\n",
    "embedding_model_id = list(SUPPORTED_EMBEDDING_MODELS)\n",
    "\n",
    "embedding_model_id = widgets.Dropdown(\n",
    "    options=embedding_model_id,\n",
    "    value=embedding_model_id[-1],\n",
    "    description=\"Embedding Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "embedding_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "790afcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected embedding model all-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "embedding_model_configuration = SUPPORTED_EMBEDDING_MODELS[embedding_model_id.value]\n",
    "print(f\"Selected embedding model {embedding_model_id.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d75dad-2eeb-4edd-8d12-d77a365f8eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('embedding_model/tokenizer_config.json',\n",
       " 'embedding_model/special_tokens_map.json',\n",
       " 'embedding_model/vocab.txt',\n",
       " 'embedding_model/added_tokens.json',\n",
       " 'embedding_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(embedding_model_configuration[\"model_id\"])\n",
    "\n",
    "input_shape = ov.PartialShape([-1, -1])\n",
    "dummy_inputs = {\"input_ids\": torch.ones((1, 10), dtype=torch.long), \"attention_mask\": torch.ones(\n",
    "    (1, 10), dtype=torch.long)}\n",
    "input_info = [(\"input_ids\", input_shape, np.int64),\n",
    "              (\"attention_mask\", input_shape, np.int64)]\n",
    "\n",
    "ov_model = ov.convert_model(model, example_input=dummy_inputs)\n",
    "ov.save_model(ov_model, Path(embedding_model_id.value)  / \"openvino_model.xml\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model_configuration[\"model_id\"])\n",
    "tokenizer.save_pretrained(Path(embedding_model_id.value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa52ea0e",
   "metadata": {},
   "source": [
    "## Convert LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bf49d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c86d2194eb7b4ddebc8644813aa29350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='LLM Model:', options=('llama-2-chat-7b',), value='llama-2-chat-7b')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SUPPORTED_LLM_MODELS = {\"llama-2-chat-7b\": {\"model_id\": \"meta-llama/Llama-2-7b-chat-hf\"}}\n",
    "llm_model_id = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "llm_model_id = widgets.Dropdown(\n",
    "    options=llm_model_id,\n",
    "    value=llm_model_id[-1],\n",
    "    description=\"LLM Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "llm_model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ea95f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected LLM model llama-2-chat-7b\n"
     ]
    }
   ],
   "source": [
    "llm_model_configuration = SUPPORTED_LLM_MODELS[llm_model_id.value]\n",
    "print(f\"Selected LLM model {llm_model_id.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6a38153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f35fa2f5f74ab9bc94714c42f5fb2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2283ffe41c664dee8a9df0a50aba723a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8755c61ead548ee94c26b064d5c53ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "prepare_int4_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_fp16_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_fp16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2020d522",
   "metadata": {},
   "outputs": [],
   "source": [
    "nncf.set_log_level(logging.ERROR)\n",
    "\n",
    "pt_model_id = llm_model_configuration[\"model_id\"]\n",
    "model_type = AutoConfig.from_pretrained(pt_model_id, trust_remote_code=True).model_type\n",
    "fp16_model_dir = Path(llm_model_id.value) / \"FP16\"\n",
    "int8_model_dir = Path(llm_model_id.value) / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(llm_model_id.value) / \"INT4_compressed_weights\"\n",
    "\n",
    "\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    ov_model = OVModelForCausalLM.from_pretrained(\n",
    "        pt_model_id, export=True, compile=False\n",
    "    )\n",
    "    ov_model.half()\n",
    "    ov_model.save_pretrained(fp16_model_dir)\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if fp16_model_dir.exists():\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(fp16_model_dir, compile=False)\n",
    "    else:\n",
    "        ov_model = OVModelForCausalLM.from_pretrained(\n",
    "            pt_model_id, export=True, compile=False\n",
    "        )\n",
    "        ov_model.half()\n",
    "    quantizer = OVQuantizer.from_pretrained(ov_model)\n",
    "    quantizer.quantize(save_directory=int8_model_dir, weights_only=True)\n",
    "    del quantizer\n",
    "    del ov_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"llama-2-chat-7b\": {\n",
    "            \"mode\": nncf.CompressWeightsMode.INT4_SYM,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"default\": {\n",
    "            \"mode\": nncf.CompressWeightsMode.INT4_ASYM,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    model_compression_params = compression_configs.get(\n",
    "        llm_model_id.value, compression_configs[\"default\"]\n",
    "    )\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int4_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    if not fp16_model_dir.exists():\n",
    "        model = OVModelForCausalLM.from_pretrained(\n",
    "            pt_model_id, export=True, compile=False\n",
    "        ).half()\n",
    "        model.config.save_pretrained(int4_model_dir)\n",
    "        ov_model = model.model\n",
    "        del model\n",
    "        gc.collect()\n",
    "    else:\n",
    "        ov_model = ov.Core().read_model(fp16_model_dir / \"openvino_model.xml\")\n",
    "        shutil.copy(fp16_model_dir / \"config.json\", int4_model_dir / \"config.json\")\n",
    "    compressed_model = nncf.compress_weights(ov_model, **model_compression_params)\n",
    "    ov.save_model(compressed_model, int4_model_dir / \"openvino_model.xml\")\n",
    "    del ov_model\n",
    "    del compressed_model\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "if prepare_fp16_model.value:\n",
    "    convert_to_fp16()\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749b5bbd",
   "metadata": {},
   "source": [
    "## Select device for inference and model variant\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    ">**Note**: There may be no speedup for INT4/INT8 compressed models on dGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e11e73cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6bb91e414ff4c559805c66cef1f091f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc225391",
   "metadata": {},
   "source": [
    "## Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df3e8fd1-d4c1-4e33-b46e-7840e392f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = OpenVINO_Embeddings.from_model_id(Path(embedding_model_id.value), model_kwargs={\n",
    "                                              \"device_name\": device.value,  \"config\": {\"PERFORMANCE_HINT\": \"THROUGHPUT\"}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fe990a",
   "metadata": {},
   "source": [
    "## Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b014f24-aa5b-4d40-924d-d579ad7fcec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18964384edbd41b99f5c68c8a4fc793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4', 'FP16'), value='INT4')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7f708db-8de1-4efd-94b2-fcabc48d52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from llama-2-chat-7b/FP16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Setting OpenVINO CACHE_DIR to llama-2-chat-7b/FP16/model_cache\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "if model_to_run.value == \"INT4\":\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "model_name = llm_model_configuration[\"model_id\"]\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(model_dir, device=device.value)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "pipe = pipeline(\"text-generation\", model=ov_model, tokenizer=tokenizer, max_new_tokens=256, streamer=streamer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361aebb",
   "metadata": {},
   "source": [
    "## Chat with Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0908e5e9-4dcb-4fc8-8480-3cf70fd5e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://10.3.233.70:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://10.3.233.70:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 661, which is longer than the specified 500\n",
      "Created a chunk of size 1558, which is longer than the specified 500\n",
      "Created a chunk of size 812, which is longer than the specified 500\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py:388: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py:388: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from threading import Event, Thread\n",
    "import gradio as gr\n",
    "import time\n",
    "from uuid import uuid4\n",
    "\n",
    "\n",
    "def build_chain(pdf_doc, chunk_size, chunk_overlap):\n",
    "    loader = UnstructuredMarkdownLoader(pdf_doc.name)\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    db = Chroma.from_documents(texts, embedding)\n",
    "    retriever = db.as_retriever()\n",
    "\n",
    "    global rag_chain\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "    )\n",
    "\n",
    "    return \"Ready\"\n",
    "\n",
    "\n",
    "def user(message, history):\n",
    "    \"\"\"\n",
    "    callback function for updating user messages in interface on submit button click\n",
    "\n",
    "    Params:\n",
    "      message: current message\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    # Append the user's message to the conversation history\n",
    "    return \"\", history + [(message, None)]\n",
    "\n",
    "\n",
    "def bot(history):\n",
    "    stream_complete = Event()\n",
    "\n",
    "    def infer(question):\n",
    "        rag_chain.run(question)\n",
    "        stream_complete.set()\n",
    "\n",
    "    t1 = Thread(target=infer, args=(history[-1][0],))\n",
    "    t1.start()\n",
    "    history[-1][1] = \"\"\n",
    "    for chunk in streamer:\n",
    "        history[-1][1] += chunk\n",
    "        time.sleep(0.05)\n",
    "        yield history\n",
    "\n",
    "\n",
    "def get_uuid():\n",
    "    \"\"\"\n",
    "    universal unique identifier for thread\n",
    "    \"\"\"\n",
    "    return str(uuid4())\n",
    "\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
    ") as demo:\n",
    "    conversation_id = gr.State(get_uuid)\n",
    "    gr.Markdown(f\"\"\"<h1><center>Chat with Documents</center></h1>\"\"\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            md_doc = gr.File(label=\"Load a markdown file\", file_types=[\".md\"])\n",
    "            load_md = gr.Button(\"Build Retriever\")\n",
    "            retriever_argument = gr.Accordion(\"Retriever Configuration\")\n",
    "            with retriever_argument:\n",
    "                chunk_size = gr.Slider(\n",
    "                        label=\"Chunk size\",\n",
    "                        value=500,\n",
    "                        minimum=100,\n",
    "                        maximum=2000,\n",
    "                        step=50,\n",
    "                        interactive=True,\n",
    "                        info=\"Size of chunk\",)\n",
    "\n",
    "                chunk_overlap = gr.Slider(\n",
    "                        label=\"Chunk overlap\",\n",
    "                        value=100,\n",
    "                        minimum=0,\n",
    "                        maximum=200,\n",
    "                        step=10,\n",
    "                        interactive=True,\n",
    "                        info=(\n",
    "                            \"Overlap between 2 chunks\"\n",
    "                        ),)\n",
    "            langchain_status = gr.Textbox(\n",
    "                label=\"Status\", placeholder=\"\", interactive=False\n",
    "            )\n",
    "            # model_argument = gr.Accordion(\"Model Configuration\")\n",
    "            # with model_argument:\n",
    "            #     top_k = gr.Slider(\n",
    "            #             label=\"Top-k\",\n",
    "            #             value=50,\n",
    "            #             minimum=0.0,\n",
    "            #             maximum=200,\n",
    "            #             step=1,\n",
    "            #             interactive=True,\n",
    "            #             info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",)\n",
    "\n",
    "            #     top_p = gr.Slider(\n",
    "            #             label=\"Top-p (nucleus sampling)\",\n",
    "            #             value=1.0,\n",
    "            #             minimum=0.0,\n",
    "            #             maximum=1,\n",
    "            #             step=0.01,\n",
    "            #             interactive=True,\n",
    "            #             info=(\n",
    "            #                 \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
    "            #                 \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
    "            #             ),)\n",
    "\n",
    "            #     repetition_penalty = gr.Slider(\n",
    "            #             label=\"Repetition Penalty\",\n",
    "            #             value=1.1,\n",
    "            #             minimum=1.0,\n",
    "            #             maximum=2.0,\n",
    "            #             step=0.1,\n",
    "            #             interactive=True,\n",
    "            #             info=\"Penalize repetition — 1.0 to disable.\",)\n",
    "\n",
    "            #     temperature = gr.Slider(\n",
    "            #             label=\"Temperature\",\n",
    "            #             value=0.1,\n",
    "            #             minimum=0.0,\n",
    "            #             maximum=1.0,\n",
    "            #             step=0.1,\n",
    "            #             interactive=True,\n",
    "            #             info=\"Higher values produce more diverse outputs\",)\n",
    "\n",
    "        with gr.Column(scale=4):\n",
    "            chatbot = gr.Chatbot(height=500)\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    msg = gr.Textbox(\n",
    "                        label=\"Chat Message Box\",\n",
    "                        placeholder=\"Chat Message Box\",\n",
    "                        show_label=False,\n",
    "                        container=False,\n",
    "                    )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        submit = gr.Button(\"Submit\")\n",
    "                        stop = gr.Button(\"Stop\")\n",
    "                        clear = gr.Button(\"Clear\")\n",
    "    load_md.click(build_chain, inputs=[md_doc, chunk_size, chunk_overlap], outputs=[langchain_status], queue=False)\n",
    "    submit_event = msg.submit(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot, queue=True\n",
    "    )\n",
    "    submit_click_event = submit.click(user, [msg, chatbot], [msg, chatbot], queue=False).then(\n",
    "        bot, chatbot, chatbot, queue=True\n",
    "    )\n",
    "    stop.click(\n",
    "        fn=None,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event, submit_click_event],\n",
    "        queue=False,\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.queue(max_size=2)\n",
    "demo.launch(server_name=\"10.3.233.70\", server_port=7862, share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6f4b5a84-bebf-49b9-b2fa-5e788ed2cbac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7862\n"
     ]
    }
   ],
   "source": [
    "demo.close()\n",
    "del rag_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3fef2-4203-4e50-aa5b-e0fad20e9479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
