{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d34647",
   "metadata": {},
   "source": [
    "# Chat with document using OpenVINO and LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b39b66a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f077b32-5d36-44b0-9041-407e996283a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"gradio\"\\\n",
    "\"onnx\" \"chromadb\" \"sentence_transformers\" \"langchain\" \"langchainhub\" \"transformers>=4.34.0\" \"unstructured\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbd7fbe",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b57cfb-e727-43a5-b2c9-8f1b1ba72061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-05 00:08:03.956856: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-05 00:08:03.959012: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-05 00:08:03.984891: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-05 00:08:03.984911: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-05 00:08:03.984933: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-05 00:08:03.990497: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-12-05 00:08:03.990974: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-05 00:08:04.668540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from pathlib import Path\n",
    "from embedding import OpenVINO_Embeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import UnstructuredMarkdownLoader\n",
    "from transformers import AutoModel, AutoTokenizer, TextIteratorStreamer\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "import openvino as ov\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ff5a9",
   "metadata": {},
   "source": [
    "## Convert embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff80e6eb-7923-40ef-93d8-5e6c56e50667",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model_id = 'sentence-transformers/all-mpnet-base-v2'\n",
    "embedding_model_path = Path(\"./embedding_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58d75dad-2eeb-4edd-8d12-d77a365f8eda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('embedding_model/tokenizer_config.json',\n",
       " 'embedding_model/special_tokens_map.json',\n",
       " 'embedding_model/vocab.txt',\n",
       " 'embedding_model/added_tokens.json',\n",
       " 'embedding_model/tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(embedding_model_id)\n",
    "\n",
    "input_shape = ov.PartialShape([-1, -1])\n",
    "dummy_inputs = {\"input_ids\": torch.ones((1, 10), dtype=torch.long), \"attention_mask\": torch.ones(\n",
    "    (1, 10), dtype=torch.long)}\n",
    "input_info = [(\"input_ids\", input_shape, np.int64),\n",
    "              (\"attention_mask\", input_shape, np.int64)]\n",
    "\n",
    "ov_model = ov.convert_model(model, example_input=dummy_inputs)\n",
    "ov.save_model(ov_model, embedding_model_path / \"openvino_model.xml\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(embedding_model_id)\n",
    "tokenizer.save_pretrained(embedding_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc225391",
   "metadata": {},
   "source": [
    "## Load embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3e8fd1-d4c1-4e33-b46e-7840e392f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'CPU'\n",
    "embedding = OpenVINO_Embeddings.from_model_id(embedding_model_path, model_kwargs={\n",
    "                                              \"device_name\": device,  \"config\": {\"PERFORMANCE_HINT\": \"THROUGHPUT\"}})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9029e6aa",
   "metadata": {},
   "source": [
    "## Load LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7f708db-8de1-4efd-94b2-fcabc48d52f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Framework not specified. Using pt to export to ONNX.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf013f4aa7044c4a143fca53865531c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the export variant default. Available variants are:\n",
      "    - default: The default ONNX variant.\n",
      "Using framework PyTorch: 2.1.0+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:94: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/optimum/exporters/onnx/model_patcher.py:392: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:140: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:392: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:399: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:409: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error TLS/SSL connection has been closed (EOF) (_ssl.c:997)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error TLS/SSL connection has been closed (EOF) (_ssl.c:997)>\n",
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "llm_model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = OVModelForCausalLM.from_pretrained(model_id=llm_model_id, device=device, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(llm_model_id)\n",
    "streamer = TextIteratorStreamer(tokenizer, timeout=10.0, skip_prompt=True, skip_special_tokens=True)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256, streamer=streamer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361aebb",
   "metadata": {},
   "source": [
    "## Gradio Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0908e5e9-4dcb-4fc8-8480-3cf70fd5e934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:    [Errno 98] error while attempting to bind on address ('10.3.233.70', 7860): address already in use\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://10.3.233.70:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://10.3.233.70:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from threading import Event, Thread\n",
    "import gradio as gr\n",
    "import time\n",
    "from threading import Thread\n",
    "\n",
    "\n",
    "def loading_md():\n",
    "    return \"Loading...\"\n",
    "\n",
    "\n",
    "def build_chain(pdf_doc):\n",
    "    loader = UnstructuredMarkdownLoader(pdf_doc.name)\n",
    "    documents = loader.load()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    db = Chroma.from_documents(texts, embedding)\n",
    "    retriever = db.as_retriever()\n",
    "    global rag_chain\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    chain_type_kwargs = {\"prompt\": prompt}\n",
    "    rag_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs=chain_type_kwargs,\n",
    "    )\n",
    "\n",
    "    return \"Ready\"\n",
    "\n",
    "\n",
    "def user(message, history):\n",
    "    \"\"\"\n",
    "    callback function for updating user messages in interface on submit button click\n",
    "\n",
    "    Params:\n",
    "      message: current message\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    # Append the user's message to the conversation history\n",
    "    return \"\", history + [(message, None)]\n",
    "\n",
    "\n",
    "def bot(history):\n",
    "    stream_complete = Event()\n",
    "\n",
    "    def infer(question):\n",
    "        rag_chain.run(question)\n",
    "        stream_complete.set()\n",
    "\n",
    "    t1 = Thread(target=infer, args=(history[-1][0],))\n",
    "    t1.start()\n",
    "    history[-1][1] = \"\"\n",
    "    for chunk in streamer:\n",
    "        history[-1][1] += chunk\n",
    "        time.sleep(0.05)\n",
    "        yield history\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    block = gr.Blocks()\n",
    "    with block as demo:\n",
    "        gr.Markdown(\"\"\"<h1><center>Chat with Documents</center></h1>\"\"\")\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=1):\n",
    "                md_doc = gr.File(label=\"Load a Markdown\", file_types=[\".md\"])\n",
    "                load_md = gr.Button(\"Build Retriever\")\n",
    "                langchain_status = gr.Textbox(\n",
    "                    label=\"Status\", placeholder=\"\", interactive=False\n",
    "                )\n",
    "                # model_argument = gr.Accordion(\"Model Configuration\")\n",
    "                # with model_argument:\n",
    "                #     top_k = gr.Slider(\n",
    "                #             label=\"Top-k\",\n",
    "                #             value=50,\n",
    "                #             minimum=0.0,\n",
    "                #             maximum=200,\n",
    "                #             step=1,\n",
    "                #             interactive=True,\n",
    "                #             info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",)\n",
    "                    \n",
    "                #     top_p = gr.Slider(\n",
    "                #             label=\"Top-p (nucleus sampling)\",\n",
    "                #             value=1.0,\n",
    "                #             minimum=0.0,\n",
    "                #             maximum=1,\n",
    "                #             step=0.01,\n",
    "                #             interactive=True,\n",
    "                #             info=(\n",
    "                #                 \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
    "                #                 \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
    "                #             ),)\n",
    "\n",
    "                #     repetition_penalty = gr.Slider(\n",
    "                #             label=\"Repetition Penalty\",\n",
    "                #             value=1.1,\n",
    "                #             minimum=1.0,\n",
    "                #             maximum=2.0,\n",
    "                #             step=0.1,\n",
    "                #             interactive=True,\n",
    "                #             info=\"Penalize repetition — 1.0 to disable.\",)\n",
    "\n",
    "                #     temperature = gr.Slider(\n",
    "                #             label=\"Temperature\",\n",
    "                #             value=0.1,\n",
    "                #             minimum=0.0,\n",
    "                #             maximum=1.0,\n",
    "                #             step=0.1,\n",
    "                #             interactive=True,\n",
    "                #             info=\"Higher values produce more diverse outputs\",)\n",
    "\n",
    "            with gr.Column(scale=4):\n",
    "                chatbot = gr.Chatbot(height=500)\n",
    "                question = gr.Textbox(\n",
    "                    label=\"Question\", placeholder=\"Type your question and hit Enter \"\n",
    "                )\n",
    "                with gr.Row():\n",
    "                    submit = gr.Button(\"Submit\")\n",
    "                    clear = gr.Button(\"Clear\")\n",
    "        load_md.click(\n",
    "            build_chain, inputs=[md_doc], outputs=[langchain_status], queue=False\n",
    "        )\n",
    "        question.submit(\n",
    "            user, [question, chatbot], [question, chatbot], queue=False\n",
    "        ).then(bot, chatbot, chatbot, queue=True)\n",
    "        submit.click(\n",
    "            user, [question, chatbot], [question, chatbot], queue=False\n",
    "        ).then(bot, chatbot, chatbot, queue=True)\n",
    "        clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "    demo.queue().launch(server_name='10.3.233.70', share=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f4b5a84-bebf-49b9-b2fa-5e788ed2cbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1edb18-d675-47b7-89b6-7da24bc31a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc535d2-d82b-4112-9805-1f4a42663379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
