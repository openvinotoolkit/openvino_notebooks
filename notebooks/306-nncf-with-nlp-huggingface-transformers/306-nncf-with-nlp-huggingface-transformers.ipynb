{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af7b6f13",
   "metadata": {},
   "source": [
    "# INT8 acceleration of NLP models from HuggingFace transformers with NNCF for OpenVINO\n",
    "\n",
    "This notebook runs through the process of enabling [NNCF](https://github.com/openvinotoolkit/nncf) in an NLP pipeline for training BERT-base on MRPC for the task of text classification.",
    "\n",
    "> NOTE: _For this notebook to function, please make sure that your Python environment has `openvino`, `openvino-dev` and `nncf[torch]` packages installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb5c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "python = sys.executable\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c29922",
   "metadata": {},
   "source": [
    "Clone the original transformers repository at the 4.12.3 release tag and apply a patch to allow exporting the target model to ONNX (so that it can further be ingested by OpenVINO and baseline accuracy/performance numbers could be obtained):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33c1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/huggingface/transformers --branch v4.12.3 --single-branch\n",
    "%cd transformers\n",
    "!pip install -e . torch==1.9.1\n",
    "!pip install -r examples/pytorch/text-classification/requirements.txt\n",
    "!patch -p1 < ../0001-Allow-ONNX-export-for-GLUE.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d32df7",
   "metadata": {},
   "source": [
    "### Obtaining the uncompressed (FP32) performance and accuracy baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a2f0f0",
   "metadata": {},
   "source": [
    "Run evaluation of the baseline FP32 pre-trained BERT-base-cased for MRPC in PyTorch and produce an ONNX for future OpenVINO ingestion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64ffea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0; $python examples/pytorch/text-classification/run_glue.py --model_name_or_path bert-base-cased-finetuned-mrpc --task_name mrpc --do_eval --max_seq_length 128 --per_device_eval_batch_size 1 --output_dir bert_mrpc_fp32 --to_onnx bert_mrpc_fp32.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02839c0",
   "metadata": {},
   "source": [
    "Evaluate the FP32 model on OpenVINO (accuracy and performance); first, convert the ONNX file to the intermediate representation (IR) using the [Model Optimizer](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b03409d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo --input_model bert_mrpc_fp32.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f004b53",
   "metadata": {},
   "source": [
    "Download the MRPC dev split in explicit `dev.tsv` form so that it could be supplied to Accuracy Checker (see below) and the OpenVINO accuracy measurement is done on the same subset of data that was used for validation in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!$python ../download_mrpc_dev_tsv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c358c68",
   "metadata": {},
   "source": [
    "Measure the accuracy (mIoU metric) on the target dataset using the Accuracy Checker tool (part of the `openvino-dev` package with the prepared .yml specification of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b96b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c ../bert_mrpc_fp32.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b197df4b",
   "metadata": {},
   "source": [
    "For measuring performance, we use the [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - OpenVINO's inference performance measurement tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6c9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m bert_mrpc_fp32.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103e4f3",
   "metadata": {},
   "source": [
    "### Integrating NNCF for INT8 quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5375835c",
   "metadata": {},
   "source": [
    "The line below apples the patch to allow for producing NNCF-compressed INT8 models. Several modifications (excluding import statements) and a simple .json config is enough for this integration - note, however, that the integration presented here is limited and covers only the INT8 quantization with NNCF for MRPC specifically. For a more complete patch offering broader scope of algorithms, models and quality-of-life improvements, refer to the complete integration patch at https://github.com/openvinotoolkit/nncf/tree/develop/third_party_integration/huggingface_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd3a06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!patch -p1 < ../0002-Use-NNCF.patch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6eb082",
   "metadata": {},
   "source": [
    "Perform compression-aware fine-tuning using NNCF, starting from the pre-trained bert-base-cased-finetuned-mrpc which was evaluated above, for 5 epochs, exporting the resulting model into an INT8 ONNX file (bert_mrpc_int8.onnx). The training takes about 10 minutes on a single NVIDIA RTX 2080 Ti GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5407b25b",
   "metadata": {
    "test_replace": {
     "epochs 5": "epochs 1"
    }
   },
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=0; $python examples/pytorch/text-classification/run_glue.py --model_name_or_path bert-base-cased-finetuned-mrpc --task_name mrpc --do_train --do_eval --num_train_epochs 5.0 --per_device_eval_batch_size 1 --output_dir bert_mrpc_int8 --overwrite_output_dir --evaluation_strategy epoch --save_strategy epoch --nncf_config nncf_bert_config_mrpc.json --to_onnx bert_mrpc_int8.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7757acfe",
   "metadata": {},
   "source": [
    "Convert the NNCF-INT8 ONNX file into the NNCF-INT8 IR for OpenVINO ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8600f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mo --input_model bert_mrpc_int8.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa1555",
   "metadata": {},
   "source": [
    "Evaluate the NNCF-INT8 model in OpenVINO, accuracy and performance-wise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3631088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!accuracy_check -c ../bert_mrpc_int8.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beadd11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m bert_mrpc_int8.xml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
