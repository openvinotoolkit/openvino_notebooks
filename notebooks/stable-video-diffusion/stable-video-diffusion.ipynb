{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21709ff7-a138-4256-9d2c-ba789a897162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: typer 0.12.3 does not provide the extra 'all'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"torch>=2.1\" \"diffusers>=0.25\" \"peft==0.6.2\" \"transformers\" \"openvino-nightly\" Pillow opencv-python tqdm  \"gradio>=4.19\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e29229d-821d-4367-8f91-ad8375a38895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-17 18:28:48.284765: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-17 18:28:48.287271: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-17 18:28:48.317811: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-17 18:28:48.317846: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-17 18:28:48.317872: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-17 18:28:48.324606: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-17 18:28:48.325422: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-17 18:28:49.211284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from diffusers import StableVideoDiffusionPipeline\n",
    "from diffusers.utils import load_image, export_to_video\n",
    "from diffusers.models.attention_processor import AttnProcessor\n",
    "import gc\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "\n",
    "IMAGE_ENCODER_PATH = MODEL_DIR / \"image_encoder.xml\"\n",
    "VAE_ENCODER_PATH = MODEL_DIR / \"vae_encoder.xml\"\n",
    "VAE_DECODER_PATH = MODEL_DIR / \"vae_decoder.xml\"\n",
    "UNET_PATH = MODEL_DIR / \"unet.xml\"\n",
    "\n",
    "\n",
    "load_pt_pipeline = not (VAE_ENCODER_PATH.exists() and VAE_DECODER_PATH.exists() and UNET_PATH.exists() and IMAGE_ENCODER_PATH.exists())\n",
    "\n",
    "unet, vae, image_encoder = None, None, None\n",
    "if load_pt_pipeline:\n",
    "    pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
    "        \"stabilityai/stable-video-diffusion-img2vid-xt\", variant=\"fp16\"\n",
    "    )\n",
    "    pipe.unet.set_attn_processor(AttnProcessor())\n",
    "    pipe.scheduler.save_pretrained(MODEL_DIR / \"scheduler\")\n",
    "    pipe.feature_extractor.save_pretrained(MODEL_DIR / \"feature_extractor\")\n",
    "    unet = pipe.unet\n",
    "    unet.eval()\n",
    "    vae = pipe.vae\n",
    "    vae.eval()\n",
    "    image_encoder = pipe.image_encoder\n",
    "    image_encoder.eval()\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "\n",
    "# Load the conditioning image\n",
    "image = load_image(\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/svd/rocket.png?download=true\")\n",
    "image = image.resize((512, 256))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60e89c71-4cf6-4e87-b788-8fa5265bca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()\n",
    "\n",
    "if not IMAGE_ENCODER_PATH. exists():\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(image_encoder, example_input=torch.zeros((1, 3, 224, 224)), input=[-1, 3, 224, 224])\n",
    "    ov.save_model(ov_model, IMAGE_ENCODER_PATH)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    print(f\"Image Encoder successfully converted to IR and saved to {IMAGE_ENCODER_PATH}\")\n",
    "\n",
    "del image_encoder\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffe83a70-125b-4ceb-9c14-184ff882b28e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "if not UNET_PATH.exists():\n",
    "    unet_inputs = {\"sample\": torch.ones([2, 2, 8, 32, 32]), \"timestep\": torch.tensor(1.256), \"encoder_hidden_states\": torch.zeros([2, 1, 1024]), \"added_time_ids\": torch.ones([2, 3])}\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(unet, example_input=unet_inputs)\n",
    "    ov.save_model(ov_model, UNET_PATH)\n",
    "    del ov_model\n",
    "    cleenup_torchscript_cache()\n",
    "    print(f\"UNet successfully converted to IR and saved to {UNET_PATH}\")\n",
    "\n",
    "del unet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "996339ea-c674-4581-baf3-cb6a0443c74c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VAEEncoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.vae.encode(x=image)[\"latent_dist\"].sample()\n",
    "\n",
    "\n",
    "class VAEDecoderWrapper(torch.nn.Module):\n",
    "    def __init__(self, vae):\n",
    "        super().__init__()\n",
    "        self.vae = vae\n",
    "\n",
    "    def forward(self, latents, num_frames:int):\n",
    "        return self.vae.decode(latents, num_frames=num_frames)\n",
    "\n",
    "\n",
    "if not VAE_ENCODER_PATH.exists():\n",
    "    vae_encoder = VAEEncoderWrapper(vae)\n",
    "    with torch.no_grad():\n",
    "        ov_model =  ov.convert_model(vae_encoder, example_input=torch.zeros((1, 3, 576, 1024)))\n",
    "    ov.save_model(ov_model, \"vae_encoder.xml\")\n",
    "    cleenup_torchscript_cache()\n",
    "    print(f\"VAE Encoder successfully converted to IR and saved to {VAE_ENCODER_PATH}\")\n",
    "    del vae_encoder\n",
    "    gc.collect()\n",
    "\n",
    "if not VAE_DECODER_PATH.exists():\n",
    "    vae_decoder = VAEDecoderWrapper(vae)\n",
    "    with torch.no_grad():\n",
    "        ov_model =  ov.convert_model(vae_decoder, example_input=(torch.zeros((8, 4, 72, 128)), torch.tensor(8)))\n",
    "    ov.save_model(ov_model, VAE_DECODER_PATH)\n",
    "    cleenup_torchscript_cache()\n",
    "    print(f\"VAE Decoder successfully converted to IR and saved to {VAE_ENCODER_PATH}\")\n",
    "    del vae_decoder\n",
    "    gc.collect()\n",
    "\n",
    "del vae\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b073909-aa5c-4252-bff4-0a7e34a6c983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from typing import Callable, Dict, List, Optional, Union\n",
    "from diffusers.pipelines.stable_video_diffusion import StableVideoDiffusionPipelineOutput\n",
    "\n",
    "\n",
    "def _append_dims(x, target_dims):\n",
    "    \"\"\"Appends dimensions to the end of a tensor until it has target_dims dimensions.\"\"\"\n",
    "    dims_to_append = target_dims - x.ndim\n",
    "    if dims_to_append < 0:\n",
    "        raise ValueError(f\"input has {x.ndim} dims but target_dims is {target_dims}, which is less\")\n",
    "    return x[(...,) + (None,) * dims_to_append]\n",
    "\n",
    "\n",
    "def tensor2vid(video: torch.Tensor, processor, output_type=\"np\"):\n",
    "    # Based on:\n",
    "    # https://github.com/modelscope/modelscope/blob/1509fdb973e5871f37148a4b5e5964cafd43e64d/modelscope/pipelines/multi_modal/text_to_video_synthesis_pipeline.py#L78\n",
    "\n",
    "    batch_size, channels, num_frames, height, width = video.shape\n",
    "    outputs = []\n",
    "    for batch_idx in range(batch_size):\n",
    "        batch_vid = video[batch_idx].permute(1, 0, 2, 3)\n",
    "        batch_output = processor.postprocess(batch_vid, output_type)\n",
    "\n",
    "        outputs.append(batch_output)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "class OVStableVideoDiffusionPipeline(DiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    Pipeline to generate video from an input image using Stable Video Diffusion.\n",
    "\n",
    "    This model inherits from [`DiffusionPipeline`]. Check the superclass documentation for the generic methods\n",
    "    implemented for all pipelines (downloading, saving, running on a particular device, etc.).\n",
    "\n",
    "    Args:\n",
    "        vae ([`AutoencoderKL`]):\n",
    "            Variational Auto-Encoder (VAE) model to encode and decode images to and from latent representations.\n",
    "        image_encoder ([`~transformers.CLIPVisionModelWithProjection`]):\n",
    "            Frozen CLIP image-encoder ([laion/CLIP-ViT-H-14-laion2B-s32B-b79K](https://huggingface.co/laion/CLIP-ViT-H-14-laion2B-s32B-b79K)).\n",
    "        unet ([`UNetSpatioTemporalConditionModel`]):\n",
    "            A `UNetSpatioTemporalConditionModel` to denoise the encoded image latents.\n",
    "        scheduler ([`EulerDiscreteScheduler`]):\n",
    "            A scheduler to be used in combination with `unet` to denoise the encoded image latents.\n",
    "        feature_extractor ([`~transformers.CLIPImageProcessor`]):\n",
    "            A `CLIPImageProcessor` to extract features from generated images.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae_encoder,\n",
    "        image_encoder,\n",
    "        unet,\n",
    "        vae_decoder,\n",
    "        scheduler,\n",
    "        feature_extractor,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vae_encoder=vae_encoder\n",
    "        self.vae_decoder=vae_decoder\n",
    "        self.image_encoder=image_encoder\n",
    "        self.unet=unet\n",
    "        self.scheduler=scheduler\n",
    "        self.feature_extractor=feature_extractor\n",
    "        self.vae_scale_factor = 2 ** (4 - 1)\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
    "\n",
    "    def _encode_image(self, image, device, num_videos_per_prompt, do_classifier_free_guidance):\n",
    "        dtype = torch.float32\n",
    "\n",
    "        if not isinstance(image, torch.Tensor):\n",
    "            image = self.image_processor.pil_to_numpy(image)\n",
    "            image = self.image_processor.numpy_to_pt(image)\n",
    "\n",
    "            # We normalize the image before resizing to match with the original implementation.\n",
    "            # Then we unnormalize it after resizing.\n",
    "            image = image * 2.0 - 1.0\n",
    "            image = _resize_with_antialiasing(image, (224, 224))\n",
    "            image = (image + 1.0) / 2.0\n",
    "\n",
    "            # Normalize the image with for CLIP input\n",
    "            image = self.feature_extractor(\n",
    "                images=image,\n",
    "                do_normalize=True,\n",
    "                do_center_crop=False,\n",
    "                do_resize=False,\n",
    "                do_rescale=False,\n",
    "                return_tensors=\"pt\",\n",
    "            ).pixel_values\n",
    "\n",
    "        image = image.to(device=device, dtype=dtype)\n",
    "        image_embeddings = torch.from_numpy(self.image_encoder(image)[0])\n",
    "        image_embeddings = image_embeddings.unsqueeze(1)\n",
    "\n",
    "        # duplicate image embeddings for each generation per prompt, using mps friendly method\n",
    "        bs_embed, seq_len, _ = image_embeddings.shape\n",
    "        image_embeddings = image_embeddings.repeat(1, num_videos_per_prompt, 1)\n",
    "        image_embeddings = image_embeddings.view(bs_embed * num_videos_per_prompt, seq_len, -1)\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            negative_image_embeddings = torch.zeros_like(image_embeddings)\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            image_embeddings = torch.cat([negative_image_embeddings, image_embeddings])\n",
    "        return image_embeddings\n",
    "\n",
    "    def _encode_vae_image(\n",
    "        self,\n",
    "        image: torch.Tensor,\n",
    "        device,\n",
    "        num_videos_per_prompt,\n",
    "        do_classifier_free_guidance,\n",
    "    ):\n",
    "        image_latents = torch.from_numpy(self.vae_encoder(image)[0])\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            negative_image_latents = torch.zeros_like(image_latents)\n",
    "\n",
    "            # For classifier free guidance, we need to do two forward passes.\n",
    "            # Here we concatenate the unconditional and text embeddings into a single batch\n",
    "            # to avoid doing two forward passes\n",
    "            image_latents = torch.cat([negative_image_latents, image_latents])\n",
    "\n",
    "        # duplicate image_latents for each generation per prompt, using mps friendly method\n",
    "        image_latents = image_latents.repeat(num_videos_per_prompt, 1, 1, 1)\n",
    "\n",
    "        return image_latents\n",
    "\n",
    "    def _get_add_time_ids(\n",
    "        self,\n",
    "        fps,\n",
    "        motion_bucket_id,\n",
    "        noise_aug_strength,\n",
    "        dtype,\n",
    "        batch_size,\n",
    "        num_videos_per_prompt,\n",
    "        do_classifier_free_guidance,\n",
    "    ):\n",
    "        add_time_ids = [fps, motion_bucket_id, noise_aug_strength]\n",
    "\n",
    "        passed_add_embed_dim = 256 * len(add_time_ids)\n",
    "        expected_add_embed_dim = 3 * 256\n",
    "\n",
    "        if expected_add_embed_dim != passed_add_embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"Model expects an added time embedding vector of length {expected_add_embed_dim}, but a vector of {passed_add_embed_dim} was created. The model has an incorrect config. Please check `unet.config.time_embedding_type` and `text_encoder_2.config.projection_dim`.\"\n",
    "            )\n",
    "\n",
    "        add_time_ids = torch.tensor([add_time_ids], dtype=dtype)\n",
    "        add_time_ids = add_time_ids.repeat(batch_size * num_videos_per_prompt, 1)\n",
    "\n",
    "        if do_classifier_free_guidance:\n",
    "            add_time_ids = torch.cat([add_time_ids, add_time_ids])\n",
    "\n",
    "        return add_time_ids\n",
    "\n",
    "    def decode_latents(self, latents, num_frames, decode_chunk_size=14):\n",
    "        # [batch, frames, channels, height, width] -> [batch*frames, channels, height, width]\n",
    "        latents = latents.flatten(0, 1)\n",
    "\n",
    "        latents = 1 / 0.18215 * latents\n",
    "\n",
    "\n",
    "        # decode decode_chunk_size frames at a time to avoid OOM\n",
    "        frames = []\n",
    "        for i in range(0, latents.shape[0], decode_chunk_size):\n",
    "            num_frames_in = latents[i : i + decode_chunk_size].shape[0]\n",
    "            frame = torch.from_numpy(self.vae_decoder([latents[i : i + decode_chunk_size], num_frames])[0])\n",
    "            frames.append(frame)\n",
    "        frames = torch.cat(frames, dim=0)\n",
    "\n",
    "        # [batch*frames, channels, height, width] -> [batch, channels, frames, height, width]\n",
    "        frames = frames.reshape(-1, num_frames, *frames.shape[1:]).permute(0, 2, 1, 3, 4)\n",
    "\n",
    "        # we always cast to float32 as this does not cause significant overhead and is compatible with bfloat16\n",
    "        frames = frames.float()\n",
    "        return frames\n",
    "\n",
    "    def check_inputs(self, image, height, width):\n",
    "        if (\n",
    "            not isinstance(image, torch.Tensor)\n",
    "            and not isinstance(image, PIL.Image.Image)\n",
    "            and not isinstance(image, list)\n",
    "        ):\n",
    "            raise ValueError(\n",
    "                \"`image` has to be of type `torch.FloatTensor` or `PIL.Image.Image` or `List[PIL.Image.Image]` but is\"\n",
    "                f\" {type(image)}\"\n",
    "            )\n",
    "\n",
    "        if height % 8 != 0 or width % 8 != 0:\n",
    "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
    "\n",
    "    def prepare_latents(\n",
    "        self,\n",
    "        batch_size,\n",
    "        num_frames,\n",
    "        num_channels_latents,\n",
    "        height,\n",
    "        width,\n",
    "        dtype,\n",
    "        device,\n",
    "        generator,\n",
    "        latents=None,\n",
    "    ):\n",
    "        shape = (\n",
    "            batch_size,\n",
    "            num_frames,\n",
    "            num_channels_latents // 2,\n",
    "            height // self.vae_scale_factor,\n",
    "            width // self.vae_scale_factor,\n",
    "        )\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\"\n",
    "            )\n",
    "\n",
    "        if latents is None:\n",
    "            latents = randn_tensor(shape, generator=generator, device=device, dtype=dtype)\n",
    "        else:\n",
    "            latents = latents.to(device)\n",
    "\n",
    "        # scale the initial noise by the standard deviation required by the scheduler\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        return latents\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        image: Union[PIL.Image.Image, List[PIL.Image.Image], torch.FloatTensor],\n",
    "        height: int = 320,\n",
    "        width: int = 512,\n",
    "        num_frames: Optional[int] = 8,\n",
    "        num_inference_steps: int = 10,\n",
    "        min_guidance_scale: float = 1.0,\n",
    "        max_guidance_scale: float = 3.0,\n",
    "        fps: int = 7,\n",
    "        motion_bucket_id: int = 127,\n",
    "        noise_aug_strength: int = 0.02,\n",
    "        decode_chunk_size: Optional[int] = None,\n",
    "        num_videos_per_prompt: Optional[int] = 1,\n",
    "        generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None,\n",
    "        callback_on_step_end_tensor_inputs: List[str] = [\"latents\"],\n",
    "        return_dict: bool = True,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        The call function to the pipeline for generation.\n",
    "\n",
    "        Args:\n",
    "            image (`PIL.Image.Image` or `List[PIL.Image.Image]` or `torch.FloatTensor`):\n",
    "                Image or images to guide image generation. If you provide a tensor, it needs to be compatible with\n",
    "                [`CLIPImageProcessor`](https://huggingface.co/lambdalabs/sd-image-variations-diffusers/blob/main/feature_extractor/preprocessor_config.json).\n",
    "            height (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n",
    "                The height in pixels of the generated image.\n",
    "            width (`int`, *optional*, defaults to `self.unet.config.sample_size * self.vae_scale_factor`):\n",
    "                The width in pixels of the generated image.\n",
    "            num_frames (`int`, *optional*):\n",
    "                The number of video frames to generate. Defaults to 14 for `stable-video-diffusion-img2vid` and to 25 for `stable-video-diffusion-img2vid-xt`\n",
    "            num_inference_steps (`int`, *optional*, defaults to 25):\n",
    "\n",
    "\n",
    "                The number of denoising steps. More denoising steps usually lead to a higher quality image at the\n",
    "                expense of slower inference. This parameter is modulated by `strength`.\n",
    "            min_guidance_scale (`float`, *optional*, defaults to 1.0):\n",
    "                The minimum guidance scale. Used for the classifier free guidance with first frame.\n",
    "            max_guidance_scale (`float`, *optional*, defaults to 3.0):\n",
    "                The maximum guidance scale. Used for the classifier free guidance with last frame.\n",
    "            fps (`int`, *optional*, defaults to 7):\n",
    "                Frames per second. The rate at which the generated images shall be exported to a video after generation.\n",
    "                Note that Stable Diffusion Video's UNet was micro-conditioned on fps-1 during training.\n",
    "            motion_bucket_id (`int`, *optional*, defaults to 127):\n",
    "                The motion bucket ID. Used as conditioning for the generation. The higher the number the more motion will be in the video.\n",
    "            noise_aug_strength (`int`, *optional*, defaults to 0.02):\n",
    "                The amount of noise added to the init image, the higher it is the less the video will look like the init image. Increase it for more motion.\n",
    "            decode_chunk_size (`int`, *optional*):\n",
    "                The number of frames to decode at a time. The higher the chunk size, the higher the temporal consistency\n",
    "                between frames, but also the higher the memory consumption. By default, the decoder will decode all frames at once\n",
    "                for maximal quality. Reduce `decode_chunk_size` to reduce memory usage.\n",
    "            num_videos_per_prompt (`int`, *optional*, defaults to 1):\n",
    "                The number of images to generate per prompt.\n",
    "            generator (`torch.Generator` or `List[torch.Generator]`, *optional*):\n",
    "                A [`torch.Generator`](https://pytorch.org/docs/stable/generated/torch.Generator.html) to make\n",
    "                generation deterministic.\n",
    "            latents (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated noisy latents sampled from a Gaussian distribution, to be used as inputs for image\n",
    "                generation. Can be used to tweak the same generation with different prompts. If not provided, a latents\n",
    "                tensor is generated by sampling using the supplied random `generator`.\n",
    "            output_type (`str`, *optional*, defaults to `\"pil\"`):\n",
    "                The output format of the generated image. Choose between `PIL.Image` or `np.array`.\n",
    "            callback_on_step_end (`Callable`, *optional*):\n",
    "                A function that calls at the end of each denoising steps during the inference. The function is called\n",
    "                with the following arguments: `callback_on_step_end(self: DiffusionPipeline, step: int, timestep: int,\n",
    "                callback_kwargs: Dict)`. `callback_kwargs` will include a list of all tensors as specified by\n",
    "                `callback_on_step_end_tensor_inputs`.\n",
    "            callback_on_step_end_tensor_inputs (`List`, *optional*):\n",
    "                The list of tensor inputs for the `callback_on_step_end` function. The tensors specified in the list\n",
    "                will be passed as `callback_kwargs` argument. You will only be able to include variables listed in the\n",
    "                `._callback_tensor_inputs` attribute of your pipeline class.\n",
    "            return_dict (`bool`, *optional*, defaults to `True`):\n",
    "                Whether or not to return a [`~pipelines.stable_diffusion.StableDiffusionPipelineOutput`] instead of a\n",
    "                plain tuple.\n",
    "\n",
    "        Returns:\n",
    "            [`~pipelines.stable_diffusion.StableVideoDiffusionPipelineOutput`] or `tuple`:\n",
    "                If `return_dict` is `True`, [`~pipelines.stable_diffusion.StableVideoDiffusionPipelineOutput`] is returned,\n",
    "                otherwise a `tuple` is returned where the first element is a list of list with the generated frames.\n",
    "\n",
    "        Examples:\n",
    "\n",
    "        ```py\n",
    "        from diffusers import StableVideoDiffusionPipeline\n",
    "        from diffusers.utils import load_image, export_to_video\n",
    "\n",
    "        pipe = StableVideoDiffusionPipeline.from_pretrained(\"stabilityai/stable-video-diffusion-img2vid-xt\", torch_dtype=torch.float16, variant=\"fp16\")\n",
    "        pipe.to(\"cuda\")\n",
    "\n",
    "        image = load_image(\"https://lh3.googleusercontent.com/y-iFOHfLTwkuQSUegpwDdgKmOjRSTvPxat63dQLB25xkTs4lhIbRUFeNBWZzYf370g=s1200\")\n",
    "        image = image.resize((1024, 576))\n",
    "\n",
    "        frames = pipe(image, num_frames=25, decode_chunk_size=8).frames[0]\n",
    "        export_to_video(frames, \"generated.mp4\", fps=7)\n",
    "        ```\n",
    "        \"\"\"\n",
    "        # 0. Default height and width to unet\n",
    "        height = height or 96 * self.vae_scale_factor\n",
    "        width = width or 96 * self.vae_scale_factor\n",
    "\n",
    "        num_frames = num_frames if num_frames is not None else 25\n",
    "        decode_chunk_size = decode_chunk_size if decode_chunk_size is not None else num_frames\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(image, height, width)\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if isinstance(image, PIL.Image.Image):\n",
    "            batch_size = 1\n",
    "        elif isinstance(image, list):\n",
    "            batch_size = len(image)\n",
    "        else:\n",
    "            batch_size = image.shape[0]\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "        # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "        # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "        # corresponds to doing no classifier free guidance.\n",
    "        do_classifier_free_guidance = max_guidance_scale > 1.0\n",
    "\n",
    "        # 3. Encode input image\n",
    "        image_embeddings = self._encode_image(image, device, num_videos_per_prompt, do_classifier_free_guidance)\n",
    "\n",
    "        # NOTE: Stable Diffusion Video was conditioned on fps - 1, which\n",
    "        # is why it is reduced here.\n",
    "        # See: https://github.com/Stability-AI/generative-models/blob/ed0997173f98eaf8f4edf7ba5fe8f15c6b877fd3/scripts/sampling/simple_video_sample.py#L188\n",
    "        fps = fps - 1\n",
    "\n",
    "        # 4. Encode input image using VAE\n",
    "        image = self.image_processor.preprocess(image, height=height, width=width)\n",
    "        noise = randn_tensor(image.shape, generator=generator, device=image.device, dtype=image.dtype)\n",
    "        image = image + noise_aug_strength * noise\n",
    "\n",
    "\n",
    "\n",
    "        image_latents = self._encode_vae_image(image, device, num_videos_per_prompt, do_classifier_free_guidance)\n",
    "        image_latents = image_latents.to(image_embeddings.dtype)\n",
    "\n",
    "        # Repeat the image latents for each frame so we can concatenate them with the noise\n",
    "        # image_latents [batch, channels, height, width] ->[batch, num_frames, channels, height, width]\n",
    "        image_latents = image_latents.unsqueeze(1).repeat(1, num_frames, 1, 1, 1)\n",
    "\n",
    "        # 5. Get Added Time IDs\n",
    "        added_time_ids = self._get_add_time_ids(\n",
    "            fps,\n",
    "            motion_bucket_id,\n",
    "            noise_aug_strength,\n",
    "            image_embeddings.dtype,\n",
    "            batch_size,\n",
    "            num_videos_per_prompt,\n",
    "            do_classifier_free_guidance,\n",
    "        )\n",
    "        added_time_ids = added_time_ids\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, device=device)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = 8\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_videos_per_prompt,\n",
    "            num_frames,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            image_embeddings.dtype,\n",
    "            device,\n",
    "            generator,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        # 7. Prepare guidance scale\n",
    "        guidance_scale = torch.linspace(min_guidance_scale, max_guidance_scale, num_frames).unsqueeze(0)\n",
    "        guidance_scale = guidance_scale.to(device, latents.dtype)\n",
    "        guidance_scale = guidance_scale.repeat(batch_size * num_videos_per_prompt, 1)\n",
    "        guidance_scale = _append_dims(guidance_scale, latents.ndim)\n",
    "\n",
    "\n",
    "        # 8. Denoising loop\n",
    "        num_warmup_steps = len(timesteps) - num_inference_steps * self.scheduler.order\n",
    "        num_timesteps = len(timesteps)\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if do_classifier_free_guidance else latents\n",
    "                latent_model_input = self.scheduler.scale_model_input(latent_model_input, t)\n",
    "\n",
    "                # Concatenate image_latents over channels dimention\n",
    "                latent_model_input = torch.cat([latent_model_input, image_latents], dim=2)\n",
    "                # predict the noise residual\n",
    "                noise_pred = torch.from_numpy(self.unet([\n",
    "                    latent_model_input,\n",
    "                    t,\n",
    "                    image_embeddings,\n",
    "                    added_time_ids,\n",
    "                ]\n",
    "                )[0])\n",
    "                # perform guidance\n",
    "                if do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_cond = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            # cast back to fp16 if needed\n",
    "            #if needs_upcasting:\n",
    "            #    self.vae.to(dtype=torch.float16)\n",
    "            frames = self.decode_latents(latents, num_frames, decode_chunk_size)\n",
    "            frames = tensor2vid(frames, self.image_processor, output_type=output_type)\n",
    "        else:\n",
    "            frames = latents\n",
    "\n",
    "        if not return_dict:\n",
    "            return frames\n",
    "\n",
    "        return StableVideoDiffusionPipelineOutput(frames=frames)\n",
    "\n",
    "\n",
    "# resizing utils\n",
    "def _resize_with_antialiasing(input, size, interpolation=\"bicubic\", align_corners=True):\n",
    "    h, w = input.shape[-2:]\n",
    "    factors = (h / size[0], w / size[1])\n",
    "\n",
    "    # First, we have to determine sigma\n",
    "    # Taken from skimage: https://github.com/scikit-image/scikit-image/blob/v0.19.2/skimage/transform/_warps.py#L171\n",
    "    sigmas = (\n",
    "        max((factors[0] - 1.0) / 2.0, 0.001),\n",
    "        max((factors[1] - 1.0) / 2.0, 0.001),\n",
    "    )\n",
    "    # Now kernel size. Good results are for 3 sigma, but that is kind of slow. Pillow uses 1 sigma\n",
    "    # https://github.com/python-pillow/Pillow/blob/master/src/libImaging/Resample.c#L206\n",
    "    # But they do it in the 2 passes, which gives better results. Let's try 2 sigmas for now\n",
    "    ks = int(max(2.0 * 2 * sigmas[0], 3)), int(max(2.0 * 2 * sigmas[1], 3))\n",
    "\n",
    "    # Make sure it is odd\n",
    "    if (ks[0] % 2) == 0:\n",
    "        ks = ks[0] + 1, ks[1]\n",
    "\n",
    "    if (ks[1] % 2) == 0:\n",
    "\n",
    "        ks = ks[0], ks[1] + 1\n",
    "\n",
    "    input = _gaussian_blur2d(input, ks, sigmas)\n",
    "\n",
    "    output = torch.nn.functional.interpolate(input, size=size, mode=interpolation, align_corners=align_corners)\n",
    "    return output\n",
    "\n",
    "\n",
    "def _compute_padding(kernel_size):\n",
    "    \"\"\"Compute padding tuple.\"\"\"\n",
    "    # 4 or 6 ints:  (padding_left, padding_right,padding_top,padding_bottom)\n",
    "    # https://pytorch.org/docs/stable/nn.html#torch.nn.functional.pad\n",
    "    if len(kernel_size) < 2:\n",
    "        raise AssertionError(kernel_size)\n",
    "    computed = [k - 1 for k in kernel_size]\n",
    "\n",
    "    # for even kernels we need to do asymmetric padding :(\n",
    "    out_padding = 2 * len(kernel_size) * [0]\n",
    "\n",
    "    for i in range(len(kernel_size)):\n",
    "        computed_tmp = computed[-(i + 1)]\n",
    "\n",
    "        pad_front = computed_tmp // 2\n",
    "        pad_rear = computed_tmp - pad_front\n",
    "\n",
    "        out_padding[2 * i + 0] = pad_front\n",
    "        out_padding[2 * i + 1] = pad_rear\n",
    "\n",
    "    return out_padding\n",
    "\n",
    "\n",
    "def _filter2d(input, kernel):\n",
    "    # prepare kernel\n",
    "    b, c, h, w = input.shape\n",
    "    tmp_kernel = kernel[:, None, ...].to(device=input.device, dtype=input.dtype)\n",
    "\n",
    "    tmp_kernel = tmp_kernel.expand(-1, c, -1, -1)\n",
    "\n",
    "    height, width = tmp_kernel.shape[-2:]\n",
    "\n",
    "    padding_shape: list[int] = _compute_padding([height, width])\n",
    "    input = torch.nn.functional.pad(input, padding_shape, mode=\"reflect\")\n",
    "\n",
    "    # kernel and input tensor reshape to align element-wise or batch-wise params\n",
    "    tmp_kernel = tmp_kernel.reshape(-1, 1, height, width)\n",
    "    input = input.view(-1, tmp_kernel.size(0), input.size(-2), input.size(-1))\n",
    "\n",
    "    # convolve the tensor with the kernel.\n",
    "    output = torch.nn.functional.conv2d(input, tmp_kernel, groups=tmp_kernel.size(0), padding=0, stride=1)\n",
    "\n",
    "    out = output.view(b, c, h, w)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _gaussian(window_size: int, sigma):\n",
    "    if isinstance(sigma, float):\n",
    "        sigma = torch.tensor([[sigma]])\n",
    "\n",
    "    batch_size = sigma.shape[0]\n",
    "\n",
    "    x = (torch.arange(window_size, device=sigma.device, dtype=sigma.dtype) - window_size // 2).expand(batch_size, -1)\n",
    "\n",
    "    if window_size % 2 == 0:\n",
    "        x = x + 0.5\n",
    "\n",
    "    gauss = torch.exp(-x.pow(2.0) / (2 * sigma.pow(2.0)))\n",
    "\n",
    "    return gauss / gauss.sum(-1, keepdim=True)\n",
    "\n",
    "\n",
    "def _gaussian_blur2d(input, kernel_size, sigma):\n",
    "    if isinstance(sigma, tuple):\n",
    "        sigma = torch.tensor([sigma], dtype=input.dtype)\n",
    "    else:\n",
    "        sigma = sigma.to(dtype=input.dtype)\n",
    "\n",
    "    ky, kx = int(kernel_size[0]), int(kernel_size[1])\n",
    "    bs = sigma.shape[0]\n",
    "    kernel_x = _gaussian(kx, sigma[:, 1].view(bs, 1))\n",
    "    kernel_y = _gaussian(ky, sigma[:, 0].view(bs, 1))\n",
    "    out_x = _filter2d(input, kernel_x[..., None, :])\n",
    "    out = _filter2d(out_x, kernel_y[..., None])\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a9c9ba7-8234-44ac-bbb3-708e3bea5640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c2a460af1d84a359ad29185a9bf6f71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=3, options=('CPU', 'GPU.0', 'GPU.1', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbaf9418-bbff-4275-b43b-b1c14cc92aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.schedulers import EulerDiscreteScheduler\n",
    "from transformers import CLIPImageProcessor\n",
    "\n",
    "\n",
    "vae_encoder = core.compile_model(VAE_ENCODER_PATH, device.value)\n",
    "image_encoder = core.compile_model(IMAGE_ENCODER_PATH, device.value)\n",
    "unet = core.compile_model(UNET_PATH, device.value)\n",
    "vae_decoder = core.compile_model(VAE_DECODER_PATH, device.value)\n",
    "scheduler = EulerDiscreteScheduler.from_pretrained(MODEL_DIR / \"scheduler\")\n",
    "feature_extractor = CLIPImageProcessor.from_pretrained(MODEL_DIR / \"feature_extractor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c722800-7800-4a81-8a39-369dd182237e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_pipe = OVStableVideoDiffusionPipeline(vae_encoder, image_encoder, unet, vae_decoder, scheduler, feature_extractor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02b62761-35d4-46be-a7eb-bdc8774de7cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d3f06df8bd14d68888bb3f451800040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "frames = ov_pipe(image, num_inference_steps=15, num_frames=8, height=256, width=512).frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e55dee5-fbb9-4616-a4d1-14f411093bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = Path(\"generated.mp4\")\n",
    "\n",
    "export_to_video(frames, str(out_path), fps=7)\n",
    "frames[0].save(\"generated.gif\", save_all=True, append_images=frames[1:], optimize=False, duration=120, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abf5294e-d76a-496d-a5d1-0b3f7e5eafc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"generated.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"generated.gif\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe35f3-4f07-4ebd-9a1e-ae0431450c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import random\n",
    "\n",
    "max_64_bit_int = 2**63 - 1\n",
    "\n",
    "def sample(\n",
    "    image: PIL.Image,\n",
    "    seed: Optional[int] = 42,\n",
    "    randomize_seed: bool = True,\n",
    "    motion_bucket_id: int = 127,\n",
    "    fps_id: int = 6,\n",
    "    num_inference_steps:int = 15,\n",
    "    num_frames: int = 4,\n",
    "    decoding_t: int = 8,  # Number of frames decoded at a time! This eats most VRAM. Reduce if necessary.\n",
    "    output_folder: str = \"outputs\",https://github.com/openvinotoolkit/openvino_notebooks/pull/1934\n",
    "):\n",
    "    if image.mode == \"RGBA\":\n",
    "        image = image.convert(\"RGB\")\n",
    "        \n",
    "    if(randomize_seed):\n",
    "        seed = random.randint(0, max_64_bit_int)\n",
    "    generator = torch.manual_seed(seed)\n",
    "    \n",
    "    output_folder = Path(output_folder)\n",
    "    output_folder.mkdir(exist_ok=True)\n",
    "    base_count = len(list(output_folder.glob(\"*.mp4\")))\n",
    "    video_path = output_folder / f\"{base_count:06d}.mp4\"\n",
    "\n",
    "    frames = ov_pipe(image, decode_chunk_size=decoding_t, generator=generator, motion_bucket_id=motion_bucket_id, noise_aug_strength=0.1, num_frames=num_frames, num_inference_steps=num_inference_steps).frames[0]\n",
    "    export_to_video(frames, str(video_path), fps=fps_id)\n",
    "    \n",
    "    return video_path, seed\n",
    "\n",
    "\n",
    "def resize_image(image, output_size=(512, 320)):\n",
    "    # Calculate aspect ratios\n",
    "    target_aspect = output_size[0] / output_size[1]  # Aspect ratio of the desired size\n",
    "    image_aspect = image.width / image.height  # Aspect ratio of the original image\n",
    "\n",
    "    # Resize then crop if the original image is larger\n",
    "    if image_aspect > target_aspect:\n",
    "        # Resize the image to match the target height, maintaining aspect ratio\n",
    "        new_height = output_size[1]\n",
    "        new_width = int(new_height * image_aspect)\n",
    "        resized_image = image.resize((new_width, new_height), PIL.Image.LANCZOS)\n",
    "        # Calculate coordinates for cropping\n",
    "        left = (new_width - output_size[0]) / 2\n",
    "        top = 0\n",
    "        right = (new_width + output_size[0]) / 2\n",
    "        bottom = output_size[1]\n",
    "    else:\n",
    "        # Resize the image to match the target width, maintaining aspect ratio\n",
    "        new_width = output_size[0]\n",
    "        new_height = int(new_width / image_aspect)\n",
    "        resized_image = image.resize((new_width, new_height), PIL.Image.LANCZOS)\n",
    "        # Calculate coordinates for cropping\n",
    "        left = 0\n",
    "        top = (new_height - output_size[1]) / 2\n",
    "        right = output_size[0]\n",
    "        bottom = (new_height + output_size[1]) / 2\n",
    "\n",
    "    # Crop the image\n",
    "    cropped_image = resized_image.crop((left, top, right, bottom))\n",
    "    return cropped_image\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "  gr.Markdown('''# Stable Video Diffusion: Image to Video Generation with OpenVINO.\n",
    "  ''')\n",
    "  with gr.Row():\n",
    "    with gr.Column():\n",
    "        image = gr.Image(label=\"Upload your image\", type=\"pil\")\n",
    "        generate_btn = gr.Button(\"Generate\")\n",
    "    video = gr.Video()\n",
    "  with gr.Accordion(\"Advanced options\", open=False):\n",
    "      seed = gr.Slider(label=\"Seed\", value=42, randomize=True, minimum=0, maximum=max_64_bit_int, step=1)\n",
    "      randomize_seed = gr.Checkbox(label=\"Randomize seed\", value=True)\n",
    "      motion_bucket_id = gr.Slider(label=\"Motion bucket id\", info=\"Controls how much motion to add/remove from the image\", value=127, minimum=1, maximum=255)\n",
    "      fps_id = gr.Slider(label=\"Frames per second\", info=\"The length of your video in seconds will be num_frames / fps\", value=6, minimum=5, maximum=30, step=1)\n",
    "      num_frames = gr.Slider(label=\"Number of Frames\", value=8, minimum=2, maximum=25, step=1)\n",
    "      num_steps = gr.Slider(label=\"Number of generation steps\", value=25, minimum=1, maximum=50, step=1) \n",
    "      \n",
    "  image.upload(fn=resize_image, inputs=image, outputs=image)\n",
    "  generate_btn.click(fn=sample, inputs=[image, seed, randomize_seed, motion_bucket_id, fps_id, num_steps, num_frames], outputs=[video, seed], api_name=\"video\")\n",
    "\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "005d0812cce84eb7a8d78dae04f0778a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "048744ac591e4f2a82885fd9faa0088d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_4da5756338b848778bdb611a2ae8ee6d",
        "IPY_MODEL_d4383efa9fbe46ca89378cdee35ff7d5",
        "IPY_MODEL_0d026277434145ff9256615707cb5653"
       ],
       "layout": "IPY_MODEL_aa685699a9b34821b99302b18505d211"
      }
     },
     "0d026277434145ff9256615707cb5653": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_efc6a5d15c584550b0fa7ad05c04e643",
       "style": "IPY_MODEL_eaea402d584f4cf6baff128a9eda49c6",
       "value": " 5/5 [00:00&lt;00:00,  6.47it/s]"
      }
     },
     "4528ff0d3a754d5a93ac136ea11faabb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4da5756338b848778bdb611a2ae8ee6d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4528ff0d3a754d5a93ac136ea11faabb",
       "style": "IPY_MODEL_6db8eb98cdce42bf8ecca903d6380b47",
       "value": "Loading pipeline components...: 100%"
      }
     },
     "6db8eb98cdce42bf8ecca903d6380b47": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7fec3e3f559140a89944f9dd656590fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "aa685699a9b34821b99302b18505d211": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d4383efa9fbe46ca89378cdee35ff7d5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_005d0812cce84eb7a8d78dae04f0778a",
       "max": 5,
       "style": "IPY_MODEL_7fec3e3f559140a89944f9dd656590fb",
       "value": 5
      }
     },
     "eaea402d584f4cf6baff128a9eda49c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "efc6a5d15c584550b0fa7ad05c04e643": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
