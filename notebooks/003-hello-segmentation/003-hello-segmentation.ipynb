{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fab52a",
   "metadata": {},
   "source": [
    "# Hello Segmentation\n",
    "\n",
    "A very basic introduction to using segmentation models with OpenVINO.\n",
    "\n",
    "We use the pre-trained [road-segmentation-adas-0001](https://docs.openvinotoolkit.org/latest/omz_models_model_road_segmentation_adas_0001.html) model from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). ADAS stands for Advanced Driver Assistance Services. The model recognizes four classes: background, road, curb and mark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f2f808",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485ef549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import segmentation_map_to_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a340210",
   "metadata": {},
   "source": [
    "## Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a398a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "\n",
    "net = ie.read_network(\n",
    "    model=\"model/road-segmentation-adas-0001.xml\")\n",
    "exec_net = ie.load_network(net, \"CPU\")\n",
    "\n",
    "output_layer_ir = next(iter(exec_net.outputs))\n",
    "input_layer_ir = next(iter(exec_net.input_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18397f8",
   "metadata": {},
   "source": [
    "## Load an Image\n",
    "A sample image from the [Mapillary Vistas](https://www.mapillary.com/dataset/vistas) dataset is provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23134719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The segmentation network expects images in BGR format\n",
    "image = cv2.imread(\"data/empty_road_mapillary.jpg\")\n",
    "\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image_h, image_w, _ = image.shape\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width\n",
    "N, C, H, W = net.input_info[input_layer_ir].tensor_desc.dims\n",
    "\n",
    "# OpenCV resize expects the destination size as (width, height)\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# reshape to network input shape\n",
    "input_image = np.expand_dims(\n",
    "    resized_image.transpose(2, 0, 1), 0\n",
    ")  \n",
    "plt.imshow(rgb_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e7703",
   "metadata": {},
   "source": [
    "## Do Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d7c5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the infernece\n",
    "result = exec_net.infer(inputs={input_layer_ir: input_image})\n",
    "result_ir = result[output_layer_ir]\n",
    "\n",
    "# Prepare data for visualization\n",
    "segmentation_mask = np.argmax(result_ir, axis=1)\n",
    "plt.imshow(segmentation_mask[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9503a83b",
   "metadata": {},
   "source": [
    "## Prepare Data for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colormap, each color represents a class\n",
    "colormap = np.array([[68, 1, 84], [48, 103, 141], [53, 183, 120], [199, 216, 52]])\n",
    "\n",
    "# Define the transparency of the segmentation mask on the photo\n",
    "alpha = 0.3\n",
    "\n",
    "# Use function from notebook_utils.py to transform mask to an RGB image\n",
    "mask = segmentation_map_to_image(segmentation_mask, colormap)\n",
    "resized_mask = cv2.resize(mask, (image_w, image_h))\n",
    "\n",
    "# Create image with mask put on\n",
    "image_with_mask = cv2.addWeighted(resized_mask, alpha, rgb_image, 1 - alpha, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d16e71",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce182b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define titles with images\n",
    "data = {\"Base Photo\": rgb_image, \"Segmentation\": mask, \"Masked Photo\": image_with_mask}\n",
    "\n",
    "# Create subplot to visualize images\n",
    "f, axs = plt.subplots(1, len(data.items()), figsize=(15, 10))\n",
    "\n",
    "# Fill subplot\n",
    "for ax, (name, image) in zip(axs, data.items()):\n",
    "    ax.axis('off')\n",
    "    ax.set_title(name)\n",
    "    ax.imshow(image)\n",
    "\n",
    "# Display image\n",
    "plt.show(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
