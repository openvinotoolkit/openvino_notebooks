{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance tricks in OpenVINO for latency mode\n",
    "\n",
    "The goal of this notebook is to be a step-by-step tutorial for improving performance for inferencing in a latency mode. Low latency is especially desired in real-time applications when the results are needed as soon as possible after the data appears. This notebook assumes computer vision workflow and uses [YOLOv5n](https://github.com/ultralytics/yolov5) model. We will simulate a camera application that provides frames one by one.\n",
    "\n",
    "The performance tips applied in this notebook could be summarized in the following figure. The quantization and pre-post-processing API are not included here as they change the precision (quantization) or processing graph (prepostprocessor). You can find examples of how to apply them to optimize performance on OpenVINO IR files in [111-detection-quantization](../111-detection-quantization) and [118-optimize-preprocessing](../118-optimize-preprocessing).\n",
    "\n",
    "![](https://user-images.githubusercontent.com/4547501/227522233-29f7bbc8-b83e-4b35-9cc7-8c5e041adc9e.png)\n",
    "\n",
    "> **NOTE**: Many of the steps presented below will give you better performance. However, some of them may not change anything if they are strongly dependent on either the hardware or the model. Please run this notebook on your computer with your model to learn which of them makes sense in your case._\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use the same image of the dog sitting on a bicycle for all experiments below. The image is resized and preprocessed to fulfill the requirements of this particular object detection model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "\n",
    "# load image\n",
    "image = utils.load_image(\"../data/image/coco_bike.jpg\")\n",
    "image = cv2.resize(image, dsize=(IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# preprocess it for YOLOv5\n",
    "input_image = image / 255.0\n",
    "input_image = np.transpose(input_image, axes=(2, 0, 1))\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# show the image\n",
    "utils.show_array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "We decided to go with [YOLOv5n](https://github.com/ultralytics/yolov5), one of the state-of-the-art object detection models, easily available through the PyTorch Hub and small enough to see the difference in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# directory for all models\n",
    "base_model_dir = Path(\"model\")\n",
    "\n",
    "model_name = \"yolov5n\"\n",
    "model_path = base_model_dir / model_name\n",
    "\n",
    "# load YOLOv5n from PyTorch Hub\n",
    "pytorch_model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=model_path, device='cpu')\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware\n",
    "\n",
    "The code below lists the available hardware we will use in the benchmarking process.\n",
    "\n",
    "> **NOTE**: The hardware you have is probably completely different from ours. It means you can see completely different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "\n",
    "# initialize OpenVINO\n",
    "core = ov.Core()\n",
    "\n",
    "# print available devices\n",
    "for device in core.available_devices:\n",
    "    device_name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "We're defining a benchmark model function to use for all optimized models below. It runs inference 1000 times, averages the latency time, and prints two measures: seconds per image and frames per second (FPS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INFER_NUMBER = 1000\n",
    "\n",
    "def benchmark_model(model: Any, input_data: np.ndarray, benchmark_name: str, device_name: str=\"CPU\") -> float:\n",
    "    \"\"\"\n",
    "    Helper function for benchmarking the model. It measures the time and prints results.\n",
    "    \"\"\"\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(INFER_NUMBER):\n",
    "        model(input_data)\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    # elapsed time\n",
    "    infer_time = end - start\n",
    "\n",
    "    # print second per image and FPS\n",
    "    mean_infer_time = infer_time / INFER_NUMBER\n",
    "    mean_fps = INFER_NUMBER / infer_time\n",
    "    print(f\"{benchmark_name} on {device_name}: {mean_infer_time :.4f} seconds per image ({mean_fps :.2f} FPS)\")\n",
    "\n",
    "    return mean_infer_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The following functions aim to post-process results and draw boxes on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda\n",
    "classes = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\",\n",
    "    \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
    "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\",\n",
    "    \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\",\n",
    "    \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\",\n",
    "    \"cell phone\", \"microwave\", \"oven\", \"oaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
    "    \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# Colors for the classes above (Rainbow Color Map).\n",
    "colors = cv2.applyColorMap(\n",
    "    src=np.arange(0, 255, 255 / len(classes), dtype=np.float32).astype(np.uint8),\n",
    "    colormap=cv2.COLORMAP_RAINBOW,\n",
    ").squeeze()\n",
    "\n",
    "\n",
    "def postprocess(detections: np.ndarray) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Postprocess the raw results from the model.\n",
    "    \"\"\"\n",
    "    # candidates - probability > 0.25\n",
    "    detections = detections[detections[..., 4] > 0.25]\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for obj in detections:\n",
    "        xmin, ymin, ww, hh = obj[:4]\n",
    "        score = obj[4]\n",
    "        label = np.argmax(obj[5:])\n",
    "        # Create a box with pixels coordinates from the box with normalized coordinates [0,1].\n",
    "        boxes.append(\n",
    "            tuple(map(int, (xmin - ww // 2, ymin - hh // 2, ww, hh)))\n",
    "        )\n",
    "        labels.append(int(label))\n",
    "        scores.append(float(score))\n",
    "\n",
    "    # Apply non-maximum suppression to get rid of many overlapping entities.\n",
    "    # See https://paperswithcode.com/method/non-maximum-suppression\n",
    "    # This algorithm returns indices of objects to keep.\n",
    "    indices = cv2.dnn.NMSBoxes(\n",
    "        bboxes=boxes, scores=scores, score_threshold=0.25, nms_threshold=0.5\n",
    "    )\n",
    "\n",
    "    # If there are no boxes.\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "\n",
    "    # Filter detected objects.\n",
    "    return [(labels[idx], scores[idx], boxes[idx]) for idx in indices.flatten()]\n",
    "\n",
    "def draw_boxes(img: np.ndarray, boxes):\n",
    "    \"\"\"\n",
    "    Draw detected boxes on the image.\n",
    "    \"\"\"\n",
    "    for label, score, box in boxes:\n",
    "        # Choose color for the label.\n",
    "        color = tuple(map(int, colors[label]))\n",
    "        # Draw a box.\n",
    "        x2 = box[0] + box[2]\n",
    "        y2 = box[1] + box[3]\n",
    "        cv2.rectangle(img=img, pt1=box[:2], pt2=(x2, y2), color=color, thickness=2)\n",
    "\n",
    "        # Draw a label name inside the box.\n",
    "        cv2.putText(\n",
    "            img=img,\n",
    "            text=f\"{classes[label]} {score:.2f}\",\n",
    "            org=(box[0] + 10, box[1] + 20),\n",
    "            fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            fontScale=img.shape[1] / 1200,\n",
    "            color=color,\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "def show_result(results: np.ndarray):\n",
    "    \"\"\"\n",
    "    Postprocess the raw results, draw boxes and show the image.\n",
    "    \"\"\"\n",
    "    output_img = image.copy()\n",
    "\n",
    "    detections = postprocess(results)\n",
    "    draw_boxes(output_img, detections)\n",
    "\n",
    "    utils.show_array(output_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizations\n",
    "\n",
    "Below, we present the performance tricks for faster inference in the latency mode. We release resources after every benchmarking to be sure the same amount is available for any experiment.\n",
    "\n",
    "### PyTorch model\n",
    "\n",
    "First, we're benchmarking the original PyTorch model without any optimizations applied. We will treat it as our baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = pytorch_model(torch.as_tensor(input_image)).detach().numpy()[0]\n",
    "    show_result(result)\n",
    "    pytorch_infer_time = benchmark_model(pytorch_model, input_data=torch.as_tensor(input_image).float(), benchmark_name=\"PyTorch model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX model\n",
    "\n",
    "The first optimization is exporting the PyTorch model to ONNX and running it in OpenVINO. It's possible, thanks to the ONNX frontend. It means we don't necessarily have to convert the model to Intermediate Representation (IR) to leverage the OpenVINO Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = base_model_dir / Path(f\"{model_name}_{IMAGE_WIDTH}_{IMAGE_HEIGHT}\").with_suffix(\".onnx\")\n",
    "\n",
    "# export PyTorch model to ONNX if it doesn't already exist\n",
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    torch.onnx.export(pytorch_model, dummy_input, onnx_path)\n",
    "\n",
    "# load and compile in OpenVINO\n",
    "onnx_model = core.read_model(onnx_path)\n",
    "onnx_model = core.compile_model(onnx_model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = onnx_model(input_image)[onnx_model.output(0)][0]\n",
    "show_result(result)\n",
    "onnx_infer_time = benchmark_model(model=onnx_model, input_data=input_image, benchmark_name=\"ONNX model\")\n",
    "\n",
    "del onnx_model  # release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO IR model\n",
    "\n",
    "Let's convert the ONNX model to OpenVINO Intermediate Representation (IR) and run it. We shouldn't expect much better performance, but a slight improvement is nice too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "\n",
    "ov_model = mo.convert_model(onnx_path)\n",
    "ov_cpu_model = core.compile_model(ov_model, device_name=\"CPU\")\n",
    "\n",
    "result = ov_cpu_model(input_image)[ov_cpu_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_cpu_infer_time = benchmark_model(model=ov_cpu_model, input_data=input_image, benchmark_name=\"OpenVINO model\")\n",
    "\n",
    "del ov_cpu_model  # release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO IR FP16 model\n",
    "\n",
    "Reducing the precision is one of the well-know methods for faster inference. We could also use quantization, but we should experience a little accuracy drop. That's why we skip that step in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model_fp16 = mo.convert_model(onnx_path, compress_to_fp16=True)\n",
    "ov_cpu_model_fp16 = core.compile_model(ov_model_fp16, device_name=\"CPU\")\n",
    "\n",
    "result = ov_cpu_model_fp16(input_image)[ov_cpu_model_fp16.output(0)][0]\n",
    "show_result(result)\n",
    "ov_cpu_fp16_infer_time = benchmark_model(model=ov_cpu_model_fp16, input_data=input_image, benchmark_name=\"OpenVINO FP16 model\")\n",
    "\n",
    "del ov_cpu_model_fp16  # release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO IR FP16 model on GPU\n",
    "\n",
    "Usually, a GPU device is faster than a CPU, so let's run the above model on the GPU. Please note you need to have an Intel GPU and [install drivers](https://github.com/openvinotoolkit/openvino_notebooks/wiki/Ubuntu#1-install-python-git-and-gpu-drivers-optional) to be able to run this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_gpu_infer_time = 0.0\n",
    "if \"GPU\" in core.available_devices:\n",
    "    ov_gpu_model = core.compile_model(ov_model_fp16, device_name=\"GPU\")\n",
    "\n",
    "    result = ov_gpu_model(input_image)[ov_gpu_model.output(0)][0]\n",
    "    show_result(result)\n",
    "    ov_gpu_infer_time = benchmark_model(model=ov_gpu_model, input_data=input_image, benchmark_name=\"OpenVINO model\", device_name=\"GPU\")\n",
    "\n",
    "    del ov_gpu_model  # release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### OpenVINO IR model + additional config\n",
    "\n",
    "There is a possibility to add a config for any device (CPU in this case). We will increase the number of threads to an equal number of our cores. It should help us a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cores = os.cpu_count()\n",
    "\n",
    "ov_cpu_config_model = core.compile_model(ov_model, device_name=\"CPU\", config={\"INFERENCE_NUM_THREADS\": num_cores})\n",
    "\n",
    "result = ov_cpu_config_model(input_image)[ov_cpu_config_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_cpu_config_infer_time = benchmark_model(model=ov_cpu_config_model, input_data=input_image, benchmark_name=\"OpenVINO model + config\")\n",
    "\n",
    "del ov_cpu_config_model  # release resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO IR model in latency mode\n",
    "\n",
    "OpenVINO offers a virtual device called [AUTO](https://docs.openvino.ai/latest/openvino_docs_OV_UG_supported_plugins_AUTO.html), which can select the best device for us based on a performance hint. There are three different hints: `LATENCY`, `THROUGHPUT`, and `CUMULATIVE_THROUGHPUT`. As this notebook is focused on the latency mode, we will use `LATENCY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_auto_model = core.compile_model(ov_model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\": \"LATENCY\"})\n",
    "\n",
    "result = ov_auto_model(input_image)[ov_auto_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_auto_infer_time = benchmark_model(model=ov_auto_model, input_data=input_image, benchmark_name=\"OpenVINO model\", device_name=\"AUTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenVINO IR model in latency mode + shared memory\n",
    "\n",
    "OpenVINO is a C++ toolkit with Python wrappers (API). The default behavior in the Python API is copying the input to the additional buffer and then running processing in C++. It prevents to have many multiprocessing-related issues. However, it also takes some time. We can create a tensor with enabled shared memory (keeping in mind we cannot overwrite our input), save time for copying and improve the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it must be assigned to a variable, not to be garbage collected\n",
    "c_input_image = np.ascontiguousarray(input_image, dtype=np.float32)\n",
    "input_tensor = ov.Tensor(c_input_image, shared_memory=True)\n",
    "\n",
    "result = ov_auto_model(input_image)[ov_auto_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_auto_shared_infer_time = benchmark_model(model=ov_auto_model, input_data=input_tensor, benchmark_name=\"OpenVINO model\", device_name=\"AUTO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Other tricks\n",
    "\n",
    "There are other tricks for performance improvement, especially quantization and prepostprocessing. To get even more from your model, please visit [111-detection-quantization](../111-detection-quantization) and [118-optimize-preprocessing](../118-optimize-preprocessing).\n",
    "\n",
    "## Performance comparison\n",
    "\n",
    "The following graphical comparison is valid for the selected model and hardware simultaneously. If you cannot see any improvement between some steps, just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "labels = [\"PyTorch model\", \"ONNX model\", \"OpenVINO IR model\", \"OpenVINO IR FP16 model\", \"OpenVINO IR FP16 model on GPU\",\n",
    "          \"OpenVINO IR model in latency mode\", \"OpenVINO IR model in latency mode + shared memory\"]\n",
    "times = [pytorch_infer_time, onnx_infer_time, ov_cpu_infer_time, ov_cpu_fp16_infer_time,\n",
    "          ov_gpu_infer_time, ov_auto_infer_time, ov_auto_shared_infer_time]\n",
    "bar_colors = colors[::10] / 255.0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.bar(labels, times, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel(\"Inference time [ms]\")\n",
    "ax.set_title(\"Performance difference\")\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "We already showed the steps needed to improve the performance of an object detection model. Even if you experience much better performance after running this notebook, please note this may not be valid for every hardware or every model. For the most accurate results, please use `benchmark_app` [command-line tool](https://docs.openvino.ai/latest/openvino_inference_engine_samples_benchmark_app_README.html). Note that `benchmark_app` cannot measure the impact of some tricks above, e.g., shared memory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
