{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# latency\n",
    "- original framework\n",
    "- onnx\n",
    "- ir\n",
    "- gpu\n",
    "- auto\n",
    "- shared memory\n",
    "- additional configs\n",
    "- prepostprocessor\n",
    "- async mode?\n",
    "- async infer queue\n",
    "- callback?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The goal of this notebook is to be a step-by-step tutorial for improving performance for inferencing in a latency mode. Low latency is especially desired in real-time applications, when the results are needed as soon as possible after the data appeared. This notebook assumes computer vision workflow and uses A model. We will simulate a camera application which provides frames one by one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Data\n",
    "\n",
    "For all experiments below we're using the same image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "IMAGE_WIDTH = 512\n",
    "IMAGE_HEIGHT = 512\n",
    "\n",
    "# or maybe better a video?\n",
    "image = utils.load_image(\"../data/image/intel_rnb.jpg\")\n",
    "input_image = cv2.resize(image, dsize=(IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "input_image = np.expand_dims(np.transpose(input_image, axes=(2, 0, 1)), axis=0)\n",
    "utils.show_array(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model\n",
    "\n",
    "The model we selected is for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import detection\n",
    "\n",
    "base_model_dir = Path(\"model\")\n",
    "model_name = \"maskrcnn_resnet50_fpn_v2\"\n",
    "\n",
    "pytorch_model = detection.maskrcnn_resnet50_fpn_v2(weigts=detection.MaskRCNN_ResNet50_FPN_V2_Weights)\n",
    "pytorch_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Hardware\n",
    "\n",
    "The following hardware is used in the benchmarking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "for device in core.available_devices:\n",
    "    device_name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Optimizations\n",
    "\n",
    "We're defining a benchmark model function to use it for all optimized models below. It runs inference 100 times and average the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it 100\n",
    "INFER_NUMBER = 10\n",
    "\n",
    "def benchmark_model(model, input, model_name, device=\"CPU\"):\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(INFER_NUMBER):\n",
    "        model(input)\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    infer_time = end - start\n",
    "\n",
    "    print(f\"{model_name} on {device}: {infer_time/INFER_NUMBER:.3f} seconds per image ({INFER_NUMBER/infer_time:.2f} FPS)\")\n",
    "\n",
    "def show_result(model, result):\n",
    "    # draw results\n",
    "    # utils.viz_result_image(image, result, resize=True)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### PyTorch model\n",
    "\n",
    "First, we're benchmarking the original PyTorch model without any optimizations applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = pytorch_model(torch.as_tensor(input_image).float())[0][\"boxes\"].detach().numpy()\n",
    "    show_result(pytorch_model, result=result)\n",
    "    benchmark_model(pytorch_model, input=torch.as_tensor(input_image).float(), model_name=\"PyTorch model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### ONNX model\n",
    "\n",
    "The first optimization is exporting the PyTorch model to ONNX and run it in OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = base_model_dir / Path(f\"{model_name}_{IMAGE_WIDTH}_{IMAGE_HEIGHT}\").with_suffix(\".onnx\")\n",
    "\n",
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "    torch.onnx.export(pytorch_model, dummy_input, onnx_path)\n",
    "\n",
    "onnx_model = core.read_model(onnx_path)\n",
    "onnx_model = core.compile_model(onnx_model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_result(model=onnx_model, result=result)\n",
    "benchmark_model(model=onnx_model, input=input_image, model_name=\"ONNX model\")\n",
    "\n",
    "del onnx_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## OpenVINO IR model\n",
    "\n",
    "Let's convert the ONNX model to OpenVINO Intermediate Representation (IR) and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "\n",
    "ov_model = mo.convert_model(onnx_path)\n",
    "ov_cpu_model = core.compile_model(ov_model, device_name=\"CPU\")\n",
    "\n",
    "show_result(model=ov_cpu_model, result=result)\n",
    "benchmark_model(model=ov_cpu_model, input=input_image, model_name=\"OpenVINO model\")\n",
    "\n",
    "del ov_cpu_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "We already showed the steps needed to improve the performance for an object detection model. Even if you experience much better performance after running this notebook, please note this may not be a true for every hardware or every model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
