{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Performance tricks in OpenVINO for throughput mode\n",
    "\n",
    "The goal of this notebook is to provide a step-by-step tutorial for improving performance for inferencing in a throughput mode. High throughput is especially desired in applications when the results are not expected to appear as soon as possible but to lower the whole processing time. This notebook assumes computer vision workflow and uses [YOLOv5n](https://github.com/ultralytics/yolov5) model. We will simulate a video processing application that provides all frames at once (e.g. video editing).\n",
    "\n",
    "The quantization and pre-post-processing API are not included here as they change the precision (quantization) or processing graph (prepostprocessor). You can find examples of how to apply them to optimize performance on OpenVINO IR files in [111-detection-quantization](../111-detection-quantization) and [118-optimize-preprocessing](../118-optimize-preprocessing).\n",
    "\n",
    "![](https://user-images.githubusercontent.com/4547501/229120774-01f4f972-424d-4280-8395-220dd432985a.png)\n",
    "\n",
    "> **NOTE**: Many of the steps presented below will give you better performance. However, some of them may not change anything if they are strongly dependent on either the hardware or the model. Please run this notebook on your computer with your model to learn which of them makes sense in your case.\n",
    "\n",
    "A similar notebook focused on the latency mode is available [here](109-latency-tricks.ipynb).\n",
    "\n",
    "## Prerequisites"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q seaborn ultralytics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "We will use the same image of the dog sitting on a bicycle copied 1000 times to simulate the video with 1000 frames (about 33s). The image is resized and preprocessed to fulfill the requirements of this particular object detection model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "FRAMES_NUMBER = 1000\n",
    "\n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "\n",
    "# load image\n",
    "image = utils.load_image(\"../data/image/coco_bike.jpg\")\n",
    "image = cv2.resize(image, dsize=(IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# preprocess it for YOLOv5\n",
    "input_image = image / 255.0\n",
    "input_image = np.transpose(input_image, axes=(2, 0, 1))\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# simulate video with 1000 frames\n",
    "video_frames = np.tile(input_image, (FRAMES_NUMBER, 1, 1, 1))\n",
    "\n",
    "# show the image\n",
    "utils.show_array(image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "We decided to go with [YOLOv5n](https://github.com/ultralytics/yolov5), one of the state-of-the-art object detection models, easily available through the PyTorch Hub and small enough to see the difference in performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.utils import io\n",
    "\n",
    "# directory for all models\n",
    "base_model_dir = Path(\"model\")\n",
    "\n",
    "model_name = \"yolov5n\"\n",
    "model_path = base_model_dir / model_name\n",
    "\n",
    "# load YOLOv5n from PyTorch Hub\n",
    "pytorch_model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=model_path, device=\"cpu\", skip_validation=True)\n",
    "# don't print full model architecture\n",
    "with io.capture_output():\n",
    "    pytorch_model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hardware\n",
    "\n",
    "The code below lists the available hardware we will use in the benchmarking process.\n",
    "\n",
    "> **NOTE**: The hardware you have is probably completely different from ours. It means you can see completely different results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "\n",
    "# initialize OpenVINO\n",
    "core = ov.Core()\n",
    "\n",
    "# print available devices\n",
    "for device in core.available_devices:\n",
    "    device_name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper functions\n",
    "\n",
    "We're defining a benchmark model function to use for all optimizations below. It runs inference for 1000 frames and prints average frames per second (FPS)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def benchmark_model(model: Any, input_data: np.ndarray, benchmark_name: str, device_name: str = \"CPU\") -> float:\n",
    "    \"\"\"\n",
    "    Helper function for benchmarking the model. It measures the time and prints results.\n",
    "    \"\"\"\n",
    "    # measure the first inference separately -  it may be slower as it contains also initialization\n",
    "    start = time.perf_counter()\n",
    "    model(input_data[0])\n",
    "    end = time.perf_counter()\n",
    "    first_infer_time = end - start\n",
    "    print(f\"{benchmark_name} on {device_name}. First inference time: {first_infer_time :.4f} seconds\")\n",
    "\n",
    "    # benchmarking\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(INFER_NUMBER):\n",
    "        model(input_data)\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    # elapsed time\n",
    "    infer_time = end - start\n",
    "\n",
    "    # print second per image and FPS\n",
    "    mean_infer_time = infer_time / INFER_NUMBER\n",
    "    mean_fps = INFER_NUMBER / infer_time\n",
    "    print(f\"{benchmark_name} on {device_name}: {mean_infer_time :.4f} seconds per image ({mean_fps :.2f} FPS)\")\n",
    "\n",
    "    return mean_infer_time"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
