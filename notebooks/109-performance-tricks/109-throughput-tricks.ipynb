{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Performance tricks in OpenVINO for throughput mode\n",
    "\n",
    "The goal of this notebook is to provide a step-by-step tutorial for improving performance for inferencing in a throughput mode. High throughput is especially desired in applications when the results are not expected to appear as soon as possible but to lower the whole processing time. This notebook assumes computer vision workflow and uses [YOLOv5n](https://github.com/ultralytics/yolov5) model. We will simulate a video processing application that has access to all frames at once (e.g. video editing).\n",
    "\n",
    "The performance tips applied in this notebook could be summarized in the following figure. Some of the steps below can be applied to any device at any stage, e.g., batch size; some can be used only to specific devices, e.g., \"inference num threads\" to CPU. As the number of potential configurations is vast, we recommend looking at the steps below and then apply a trial-and-error approach. You can incorporate many hints simultaneously, like more inference threads + async processing. It should give even better performance, but we recommend testing it anyway.\n",
    "\n",
    "The quantization and pre-post-processing API are not included here as they change the precision (quantization) or processing graph (prepostprocessor). You can find examples of how to apply them to optimize performance on OpenVINO IR files in [111-detection-quantization](../111-detection-quantization) and [118-optimize-preprocessing](../118-optimize-preprocessing).\n",
    "\n",
    "![](https://github.com/openvinotoolkit/openvino_notebooks/assets/4547501/2009a531-5aa3-49cf-8729-3080320d6b75)\n",
    "\n",
    "> **NOTE**: Many of the steps presented below will give you better performance. However, some of them may not change anything if they are strongly dependent on either the hardware or the model. Please run this notebook on your computer with your model to learn which of them makes sense in your case.\n",
    "\n",
    "A similar notebook focused on the latency mode is available [here](109-latency-tricks.ipynb).\n",
    "\n",
    "## Prerequisites"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -q seaborn ultralytics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import Any, List, Tuple\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data\n",
    "\n",
    "We will use the same image of the dog sitting on a bicycle copied 1000 times to simulate the video with 1000 frames (about 33s). The image is resized and preprocessed to fulfill the requirements of this particular object detection model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "FRAMES_NUMBER = 1024\n",
    "\n",
    "IMAGE_WIDTH = 640\n",
    "IMAGE_HEIGHT = 480\n",
    "\n",
    "# load image\n",
    "image = utils.load_image(\"../data/image/coco_bike.jpg\")\n",
    "image = cv2.resize(image, dsize=(IMAGE_WIDTH, IMAGE_HEIGHT), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# preprocess it for YOLOv5\n",
    "input_image = image / 255.0\n",
    "input_image = np.transpose(input_image, axes=(2, 0, 1))\n",
    "input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "# simulate video with many frames\n",
    "video_frames = np.tile(input_image, (FRAMES_NUMBER, 1, 1, 1, 1))\n",
    "\n",
    "# show the image\n",
    "utils.show_array(image)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model\n",
    "\n",
    "We decided to go with [YOLOv5n](https://github.com/ultralytics/yolov5), one of the state-of-the-art object detection models, easily available through the PyTorch Hub and small enough to see the difference in performance."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.utils import io\n",
    "\n",
    "# directory for all models\n",
    "base_model_dir = Path(\"model\")\n",
    "\n",
    "model_name = \"yolov5n\"\n",
    "model_path = base_model_dir / model_name\n",
    "\n",
    "# load YOLOv5n from PyTorch Hub\n",
    "pytorch_model = torch.hub.load(\"ultralytics/yolov5\", \"custom\", path=model_path, device=\"cpu\", skip_validation=True)\n",
    "# don't print full model architecture\n",
    "with io.capture_output():\n",
    "    pytorch_model.eval()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hardware\n",
    "\n",
    "The code below lists the available hardware we will use in the benchmarking process.\n",
    "\n",
    "> **NOTE**: The hardware you have is probably completely different from ours. It means you can see completely different results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openvino.runtime as ov\n",
    "\n",
    "# initialize OpenVINO\n",
    "core = ov.Core()\n",
    "\n",
    "# print available devices\n",
    "for device in core.available_devices:\n",
    "    device_name = core.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helper functions\n",
    "\n",
    "We're defining a benchmark model function to use for all optimizations below. It runs inference for 1000 frames and prints average frames per second (FPS)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openvino.runtime import AsyncInferQueue\n",
    "\n",
    "\n",
    "def benchmark_model(model: Any, frames: np.ndarray, async_queue: AsyncInferQueue = None, benchmark_name: str = \"OpenVINO model\", device_name: str = \"CPU\") -> float:\n",
    "    \"\"\"\n",
    "    Helper function for benchmarking the model. It measures the time and prints results.\n",
    "    \"\"\"\n",
    "    # measure the first inference separately - it may be slower as it contains also initialization\n",
    "    start = time.perf_counter()\n",
    "    model(frames[0])\n",
    "    if async_queue:\n",
    "        async_queue.wait_all()\n",
    "    end = time.perf_counter()\n",
    "    first_infer_time = end - start\n",
    "    print(f\"{benchmark_name} on {device_name}. First inference time: {first_infer_time :.4f} seconds\")\n",
    "\n",
    "    # benchmarking\n",
    "    start = time.perf_counter()\n",
    "    for batch in frames:\n",
    "        model(batch)\n",
    "    # wait for all threads if async processing\n",
    "    if async_queue:\n",
    "        async_queue.wait_all()\n",
    "    end = time.perf_counter()\n",
    "\n",
    "    # elapsed time\n",
    "    infer_time = end - start\n",
    "\n",
    "    # print second per image and FPS\n",
    "    mean_infer_time = infer_time / FRAMES_NUMBER\n",
    "    mean_fps = FRAMES_NUMBER / infer_time\n",
    "    print(f\"{benchmark_name} on {device_name}: {mean_infer_time :.4f} seconds per image ({mean_fps :.2f} FPS)\")\n",
    "\n",
    "    return mean_fps"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following functions aim to post-process results and draw boxes on the image."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# https://gist.github.com/AruniRC/7b3dadd004da04c80198557db5da4bda\n",
    "classes = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\",\n",
    "    \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\",\n",
    "    \"baseball bat\", \"baseball glove\", \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\",\n",
    "    \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\",\n",
    "    \"cake\", \"chair\", \"couch\", \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\",\n",
    "    \"cell phone\", \"microwave\", \"oven\", \"oaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\",\n",
    "    \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "# Colors for the classes above (Rainbow Color Map).\n",
    "colors = cv2.applyColorMap(\n",
    "    src=np.arange(0, 255, 255 / len(classes), dtype=np.float32).astype(np.uint8),\n",
    "    colormap=cv2.COLORMAP_RAINBOW,\n",
    ").squeeze()\n",
    "\n",
    "\n",
    "def postprocess(detections: np.ndarray) -> List[Tuple]:\n",
    "    \"\"\"\n",
    "    Postprocess the raw results from the model.\n",
    "    \"\"\"\n",
    "    # candidates - probability > 0.25\n",
    "    detections = detections[detections[..., 4] > 0.25]\n",
    "\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    scores = []\n",
    "    for obj in detections:\n",
    "        xmin, ymin, ww, hh = obj[:4]\n",
    "        score = obj[4]\n",
    "        label = np.argmax(obj[5:])\n",
    "        # Create a box with pixels coordinates from the box with normalized coordinates [0,1].\n",
    "        boxes.append(\n",
    "            tuple(map(int, (xmin - ww // 2, ymin - hh // 2, ww, hh)))\n",
    "        )\n",
    "        labels.append(int(label))\n",
    "        scores.append(float(score))\n",
    "\n",
    "    # Apply non-maximum suppression to get rid of many overlapping entities.\n",
    "    # See https://paperswithcode.com/method/non-maximum-suppression\n",
    "    # This algorithm returns indices of objects to keep.\n",
    "    indices = cv2.dnn.NMSBoxes(\n",
    "        bboxes=boxes, scores=scores, score_threshold=0.25, nms_threshold=0.5\n",
    "    )\n",
    "\n",
    "    # If there are no boxes.\n",
    "    if len(indices) == 0:\n",
    "        return []\n",
    "\n",
    "    # Filter detected objects.\n",
    "    return [(labels[idx], scores[idx], boxes[idx]) for idx in indices.flatten()]\n",
    "\n",
    "\n",
    "def draw_boxes(img: np.ndarray, boxes):\n",
    "    \"\"\"\n",
    "    Draw detected boxes on the image.\n",
    "    \"\"\"\n",
    "    for label, score, box in boxes:\n",
    "        # Choose color for the label.\n",
    "        color = tuple(map(int, colors[label]))\n",
    "        # Draw a box.\n",
    "        x2 = box[0] + box[2]\n",
    "        y2 = box[1] + box[3]\n",
    "        cv2.rectangle(img=img, pt1=box[:2], pt2=(x2, y2), color=color, thickness=2)\n",
    "\n",
    "        # Draw a label name inside the box.\n",
    "        cv2.putText(\n",
    "            img=img,\n",
    "            text=f\"{classes[label]} {score:.2f}\",\n",
    "            org=(box[0] + 10, box[1] + 20),\n",
    "            fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "            fontScale=img.shape[1] / 1200,\n",
    "            color=color,\n",
    "            thickness=1,\n",
    "            lineType=cv2.LINE_AA,\n",
    "        )\n",
    "\n",
    "\n",
    "def show_result(results: np.ndarray):\n",
    "    \"\"\"\n",
    "    Postprocess the raw results, draw boxes and show the image.\n",
    "    \"\"\"\n",
    "    output_img = image.copy()\n",
    "\n",
    "    detections = postprocess(results)\n",
    "    draw_boxes(output_img, detections)\n",
    "\n",
    "    utils.show_array(output_img)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Optimizations\n",
    "\n",
    "Below, we present the performance tricks for faster inference in the throughput mode. We release resources after every benchmarking to be sure the same amount of resource is available for every experiment.\n",
    "\n",
    "### PyTorch model\n",
    "\n",
    "First, we're benchmarking the original PyTorch model without any optimizations applied. We will treat it as our baseline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    result = pytorch_model(torch.as_tensor(video_frames[0])).detach().numpy()[0]\n",
    "    show_result(result)\n",
    "    pytorch_fps = benchmark_model(pytorch_model, frames=torch.as_tensor(video_frames).float(), benchmark_name=\"PyTorch model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenVINO IR model\n",
    "\n",
    "The first optimization is exporting the PyTorch model to OpenVINO Intermediate Representation (IR) FP16 and running it. Reducing the precision is one of the well-known methods for faster inference provided the hardware that supports lower precision, such as FP16 or even INT8. If the hardware doesn't support lower precisions, the model will be inferred in FP32 automatically. We could also use quantization (INT8), but we should experience a little accuracy drop. That's why we skip that step in this notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "\n",
    "# convert VOLOv5 model to IR, use FP16\n",
    "ov_model = mo.convert_model(pytorch_model, input_shape=[1, 3, IMAGE_HEIGHT, IMAGE_WIDTH], compress_to_fp16=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ov_cpu_model = core.compile_model(ov_model, device_name=\"CPU\")\n",
    "\n",
    "result = ov_cpu_model(video_frames[0])[ov_cpu_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_cpu_fps = benchmark_model(model=ov_cpu_model, frames=video_frames, benchmark_name=\"OpenVINO model\")\n",
    "\n",
    "del ov_cpu_model  # release resources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenVINO IR model on GPU\n",
    "\n",
    "Usually, a GPU device provides more frames per second than a CPU, so let's run the above model on the GPU. Please note you need to have an Intel GPU and [install drivers](https://github.com/openvinotoolkit/openvino_notebooks/wiki/Ubuntu#1-install-python-git-and-gpu-drivers-optional) to be able to run this step. In addition, offloading to the GPU helps reduce CPU load and memory consumption, allowing it to be left for routine processes. If you cannot observe a higher throughput on GPU, it may be because the model is too light to benefit from massive parallel execution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ov_gpu_fps = 0.0\n",
    "if \"GPU\" in core.available_devices:\n",
    "    # compile for GPU\n",
    "    ov_gpu_model = core.compile_model(ov_model, device_name=\"GPU\")\n",
    "\n",
    "    result = ov_gpu_model(video_frames[0])[ov_gpu_model.output(0)][0]\n",
    "    show_result(result)\n",
    "    ov_gpu_fps = benchmark_model(model=ov_gpu_model, frames=video_frames, benchmark_name=\"OpenVINO model\", device_name=\"GPU\")\n",
    "\n",
    "    del ov_gpu_model  # release resources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenVINO IR model + more inference threads\n",
    "\n",
    "There is a possibility to add a config for any device (CPU in this case). We will increase the number of threads to an equal number of our cores. It should help us a lot. There are [more options](https://docs.openvino.ai/2023.0/groupov_runtime_cpp_prop_api.html) to be changed, so it's worth playing with them to see what works best in our case."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_cores = os.cpu_count()\n",
    "\n",
    "# use more inference threads\n",
    "ov_cpu_config_model = core.compile_model(ov_model, device_name=\"CPU\", config={\"INFERENCE_NUM_THREADS\": num_cores})\n",
    "\n",
    "result = ov_cpu_config_model(video_frames[0])[ov_cpu_config_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_cpu_config_fps = benchmark_model(model=ov_cpu_config_model, frames=video_frames, benchmark_name=\"OpenVINO model + more threads\")\n",
    "\n",
    "del ov_cpu_config_model  # release resources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenVINO IR model + more inference threads + bigger batch\n",
    "\n",
    "Batch processing often gives higher throughput as more inputs are processed at once. To use bigger batches (than 1), we must convert the model again, specifying a new input shape, and reshape input frames. In our case, a batch size equal to 4 is the best choice, but we recommend trying different sizes for other hardware and model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "\n",
    "# export the model with the bigger batch size\n",
    "ov_batch_model = mo.convert_model(pytorch_model, input_shape=[batch_size, 3, IMAGE_HEIGHT, IMAGE_WIDTH], compress_to_fp16=True)\n",
    "ov_cpu_batch_model = core.compile_model(ov_batch_model, device_name=\"CPU\", config={\"INFERENCE_NUM_THREADS\": num_cores})\n",
    "\n",
    "batched_video_frames = video_frames.reshape([-1, batch_size, 3, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "\n",
    "result = ov_cpu_batch_model(batched_video_frames[0])[ov_cpu_batch_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_cpu_batch_fps = benchmark_model(model=ov_cpu_batch_model, frames=batched_video_frames, benchmark_name=\"OpenVINO model + more threads + bigger batch\")\n",
    "\n",
    "del ov_cpu_batch_model  # release resources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenVINO IR model in throughput mode\n",
    "\n",
    "OpenVINO offers a virtual device called [AUTO](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_supported_plugins_AUTO.html), which can select the best device for us based on a performance hint. There are three different hints: `LATENCY`, `THROUGHPUT`, and `CUMULATIVE_THROUGHPUT`. As this notebook is focused on the throughput mode, we will use the latter two. The above hints can be used with other devices as well. Throughput mode implicitly triggers using the [Automatic Batching](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Automatic_Batching.html) feature, which sets the batch size to the optimal level."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ov_auto_model = core.compile_model(ov_model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\": \"THROUGHPUT\"})\n",
    "\n",
    "result = ov_auto_model(video_frames[0])[ov_auto_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_auto_fps = benchmark_model(model=ov_auto_model, frames=video_frames, benchmark_name=\"OpenVINO model\", device_name=\"AUTO (THROUGHPUT)\")\n",
    "\n",
    "del ov_auto_model  # release resources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenVINO IR model in cumulative throughput mode\n",
    "\n",
    "The AUTO device in throughput mode will select the best, but one physical device to bring the highest throughput. However, if we have more Intel devices like CPU, iGPUs, and dGPUs in one machine, we may benefit from them all. To do so, we must use cumulative throughput to activate all devices."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ov_auto_cumulative_model = core.compile_model(ov_model, device_name=\"AUTO\", config={\"PERFORMANCE_HINT\": \"CUMULATIVE_THROUGHPUT\"})\n",
    "\n",
    "result = ov_auto_cumulative_model(video_frames[0])[ov_auto_cumulative_model.output(0)][0]\n",
    "show_result(result)\n",
    "ov_auto_cumulative_fps = benchmark_model(model=ov_auto_cumulative_model, frames=video_frames, benchmark_name=\"OpenVINO model\", device_name=\"AUTO (CUMULATIVE THROUGHPUT)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### OpenVINO IR model in cumulative throughput mode + asynchronous processing\n",
    "\n",
    "Asynchronous mode means that OpenVINO immediately returns from an inference call and doesn't wait for the result. It requires more concurrent code to be written, but should offer better processing time utilization e.g. we can run some pre- or post-processing code while waiting for the result. Although we could use async processing directly (start_async() function), it's recommended to use AsyncInferQueue, which is an easier approach to achieve the same outcome. This class automatically spawns the pool of InferRequest objects (also called “jobs”) and provides synchronization mechanisms to control the flow of the pipeline."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from openvino.runtime import AsyncInferQueue\n",
    "\n",
    "\n",
    "def callback(infer_request, info):\n",
    "    result = infer_request.get_output_tensor(0).data[0]\n",
    "    show_result(result)\n",
    "    pass\n",
    "\n",
    "infer_queue = AsyncInferQueue(ov_auto_cumulative_model)\n",
    "infer_queue.set_callback(callback)  # set callback to post-process (show) results\n",
    "\n",
    "infer_queue.start_async(video_frames[0])\n",
    "infer_queue.wait_all()\n",
    "\n",
    "# don't show output for the remaining frames\n",
    "infer_queue.set_callback(lambda x, y: {})\n",
    "ov_async_model = benchmark_model(model=infer_queue.start_async, frames=video_frames, async_queue=infer_queue, benchmark_name=\"OpenVINO model in asynchronous processing\", device_name=\"AUTO (CUMULATIVE THROUGHPUT)\")\n",
    "\n",
    "del infer_queue  # release resources"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Other tricks\n",
    "\n",
    "There are other tricks for performance improvement, such as quantization and pre-post-processing or dedicated to latency mode. To get even more from your model, please visit [109-latency-tricks](109-latency-tricks.ipynb), [111-detection-quantization](../111-detection-quantization), and [118-optimize-preprocessing](../118-optimize-preprocessing).\n",
    "\n",
    "## Performance comparison\n",
    "\n",
    "The following graphical comparison is valid for the selected model and hardware simultaneously. If you cannot see any improvement between some steps, just skip them."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "labels = [\"PyTorch model\", \"OpenVINO IR model\", \"OpenVINO IR model on GPU\", \"OpenVINO IR model + more inference threads\", \"OpenVINO IR model + more inference threads + bigger batch\",\n",
    "          \"OpenVINO IR model in throughput mode\", \"OpenVINO IR model in cumulative throughput mode\", \"OpenVINO IR model in cumulative throughput mode + asynchronous processing\"]\n",
    "\n",
    "fps = [pytorch_fps, ov_cpu_fps, ov_gpu_fps, ov_cpu_config_fps, ov_cpu_batch_fps, ov_auto_fps, ov_auto_cumulative_fps, ov_async_model]\n",
    "\n",
    "bar_colors = colors[::10] / 255.0\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "ax.bar(labels, fps, color=bar_colors)\n",
    "\n",
    "ax.set_ylabel(\"Throughput [FPS]\")\n",
    "ax.set_title(\"Performance difference\")\n",
    "\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusions\n",
    "\n",
    "We already showed the steps needed to improve the throughput of an object detection model. Even if you experience much better performance after running this notebook, please note this may not be valid for every hardware or every model. For the most accurate results, please use `benchmark_app` [command-line tool](https://docs.openvino.ai/2023.0/openvino_inference_engine_samples_benchmark_app_README.html). Note that `benchmark_app` cannot measure the impact of some tricks above."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
