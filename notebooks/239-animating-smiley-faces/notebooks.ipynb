{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting openvino==2022.03\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/d0/ed/86496d4cb9235415c94e4d3b7ab06166e111dc867f4d039738f3b83fe16b/openvino-2022.3.0-9052-cp37-cp37m-manylinux_2_17_x86_64.whl (36.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.5/36.5 MB\u001b[0m \u001b[31m786.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy<=1.23.4,>=1.16.6 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from openvino==2022.03) (1.19.5)\n",
      "Installing collected packages: openvino\n",
      "Successfully installed openvino-2022.3.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install openvino==2022.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !pip install ppgan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!python -c \"from openvino.runtime import Core\"\n",
    "from openvino.runtime import Core\n",
    "ie_core=Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CPU']\n"
     ]
    }
   ],
   "source": [
    "devices = ie_core.available_devices\r\n",
    "print(devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ConstOutput: names[eager_tmp_0] shape[1,3,256,256] type: f32>\n",
      "输出<ConstOutput: names[reshape2_6.tmp_0] shape[1,10,2,2] type: f32>\n",
      "输出<ConstOutput: names[sum_0.tmp_0] shape[1,10,2] type: f32>\n"
     ]
    }
   ],
   "source": [
    "onnx_kp = \"/home/aistudio/onnx_model/paddlegan_fder.onnx\"\n",
    "model_kp = ie_core.read_model(model=onnx_kp)\n",
    "kp_net=ie_core.compile_model(model=model_kp,device_name= \"AUTO\")\n",
    "output_kpjacobian = kp_net.output(0) #jacobian\n",
    "output_kpvalue = kp_net.output(1)#value\n",
    "x = iter(kp_net.inputs)\n",
    "for i in x:\n",
    "    print(i)\n",
    "y = iter(kp_net.outputs)\n",
    "for i in y:\n",
    "    print('输出'+str(i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "AUTOload all devices failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/3431055128.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mgenerator_kp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/home/aistudio/onnx_model/generator_fder.onnx\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_generator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mie_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerator_kp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerator_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mie_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"AUTO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36mcompile_model\u001b[0;34m(self, model, device_name, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         return CompiledModel(\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         )\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: AUTOload all devices failed"
     ]
    }
   ],
   "source": [
    "#RuntimeError: AUTOload all devices failed\r\n",
    "generator_kp = \"/home/aistudio/onnx_model/generator_fder.onnx\"\r\n",
    "model_generator = ie_core.read_model(model=generator_kp)\r\n",
    "generator_net=ie_core.compile_model(model=model_generator,device_name= \"AUTO\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "There should be only one instance of RegistersPool per thread",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/1732744184.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mie_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"CPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36mcompile_model\u001b[0;34m(self, model, device_name, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         return CompiledModel(\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         )\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: There should be only one instance of RegistersPool per thread"
     ]
    }
   ],
   "source": [
    "#RuntimeError: There should be only one instance of RegistersPool per thread\r\n",
    "generator_net=ie_core.compile_model(model=model_generator,device_name= \"CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to create plugin /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/openvino/libs/libopenvino_intel_gpu_plugin.so for device GPU\nPlease, check your environment\nCheck 'error_code == 0' failed at src/plugins/intel_gpu/src/runtime/ocl/ocl_device_detector.cpp:194:\n[GPU] No supported OCL devices found or unexpected error happened during devices query.\n[GPU] Please check OpenVINO documentation for GPU drivers setup guide.\n[GPU] clGetPlatformIDs error code: -1001\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_129/3716502272.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator_net\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mie_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"GPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/openvino/runtime/ie_api.py\u001b[0m in \u001b[0;36mcompile_model\u001b[0;34m(self, model, device_name, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m         return CompiledModel(\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         )\n\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to create plugin /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/openvino/libs/libopenvino_intel_gpu_plugin.so for device GPU\nPlease, check your environment\nCheck 'error_code == 0' failed at src/plugins/intel_gpu/src/runtime/ocl/ocl_device_detector.cpp:194:\n[GPU] No supported OCL devices found or unexpected error happened during devices query.\n[GPU] Please check OpenVINO documentation for GPU drivers setup guide.\n[GPU] clGetPlatformIDs error code: -1001\n\n"
     ]
    }
   ],
   "source": [
    "#Please, check your environment\r\n",
    "# Check 'error_code == 0' failed at src/plugins/intel_gpu/src/runtime/ocl/ocl_device_detector.cpp:194:\r\n",
    "# [GPU] No supported OCL devices found or unexpected error happened during devices query.\r\n",
    "# [GPU] Please check OpenVINO documentation for GPU drivers setup guide.\r\n",
    "# [GPU] clGetPlatformIDs error code: -1001\r\n",
    "generator_net=ie_core.compile_model(model=model_generator,device_name= \"GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#onnx_model/generator_fder.onnx,This model can only be deployed on Intel GPUs\r\n",
    "output_generator =generator_net.output(0)\r\n",
    "x = iter(generator_net.inputs)\r\n",
    "for i in x:\r\n",
    "    print(i)\r\n",
    "y = iter(generator_net.outputs)\r\n",
    "for i in y:\r\n",
    "    print('输出'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#数据处理类函数\r\n",
    "from enum import Enum\r\n",
    "# from .utils import *\r\n",
    "import sys\r\n",
    "class LandmarksType(Enum):\r\n",
    "    _2D = 1\r\n",
    "    _2halfD = 2\r\n",
    "    _3D = 3\r\n",
    "class NetworkSize(Enum):\r\n",
    "    # TINY = 1\r\n",
    "    # SMALL = 2\r\n",
    "    # MEDIUM = 3\r\n",
    "    LARGE = 4\r\n",
    "\r\n",
    "    def __new__(cls, value):\r\n",
    "        member = object.__new__(cls)\r\n",
    "        member._value_ = value\r\n",
    "        return member\r\n",
    "\r\n",
    "    def __int__(self):\r\n",
    "        return self.value\r\n",
    "\r\n",
    "\r\n",
    "class FaceAlignment:\r\n",
    "    def __init__(self,\r\n",
    "                 landmarks_type,\r\n",
    "                 network_size=NetworkSize.LARGE,\r\n",
    "                 flip_input=False,\r\n",
    "                 face_detector='sfd',\r\n",
    "                 verbose=False):\r\n",
    "        self.flip_input = flip_input\r\n",
    "        self.landmarks_type = landmarks_type\r\n",
    "        self.verbose = verbose\r\n",
    "\r\n",
    "        network_size = int(network_size)\r\n",
    "\r\n",
    "        # Get the face detector\r\n",
    "        face_detector_module = __import__(\r\n",
    "            'ppgan.faceutils.face_detection.detection.' + face_detector,\r\n",
    "            globals(), locals(), [face_detector], 0)\r\n",
    "        self.face_detector = face_detector_module.FaceDetector(verbose=verbose)\r\n",
    "\r\n",
    "    def get_detections_for_batch(self, images):\r\n",
    "        images = images[..., ::-1]\r\n",
    "        detected_faces = self.face_detector.detect_from_batch(images.copy())\r\n",
    "        results = []\r\n",
    "\r\n",
    "        for i, d in enumerate(detected_faces):\r\n",
    "            if len(d) == 0:\r\n",
    "                results.append(None)\r\n",
    "                continue\r\n",
    "            d = d[0]\r\n",
    "            d = np.clip(d, 0, None)\r\n",
    "\r\n",
    "            x1, y1, x2, y2 = map(int, d[:4])\r\n",
    "            results.append((x1, y1, x2, y2))\r\n",
    "\r\n",
    "        return results\r\n",
    "\r\n",
    "    def get_detections_for_image(self, images):\r\n",
    "        images = images[..., ::-1]\r\n",
    "        detected_faces = self.face_detector.detect_from_batch(images.copy())\r\n",
    "        results = []\r\n",
    "\r\n",
    "        for i, d in enumerate(detected_faces[0]):\r\n",
    "            if len(d) == 0:\r\n",
    "                results.append(None)\r\n",
    "                continue\r\n",
    "            d = np.clip(d, 0, None)\r\n",
    "\r\n",
    "            x1, y1, x2, y2 = map(int, d[:-1])\r\n",
    "            results.append((x1, y1, x2, y2))\r\n",
    "\r\n",
    "        return results\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "from scipy.spatial import ConvexHull\r\n",
    "\r\n",
    "import paddle\r\n",
    "\r\n",
    "\r\n",
    "def animate_normalize_kp(kp_source,\r\n",
    "                 kp_driving,\r\n",
    "                 kp_driving_initial,\r\n",
    "                 adapt_movement_scale=False,\r\n",
    "                 use_relative_movement=False,\r\n",
    "                 use_relative_jacobian=False):\r\n",
    "    if adapt_movement_scale:\r\n",
    "        source_area = ConvexHull(kp_source[output_kpvalue][0].numpy()).volume\r\n",
    "        driving_area = ConvexHull(kp_driving_initial[output_kpvalue][0].numpy()).volume\r\n",
    "        adapt_movement_scale = np.sqrt(source_area) / np.sqrt(driving_area)\r\n",
    "    else:\r\n",
    "        adapt_movement_scale = 1\r\n",
    "\r\n",
    "    kp_new = {k: v for k, v in kp_driving.items()}\r\n",
    "\r\n",
    "    if use_relative_movement:\r\n",
    "        kp_value_diff = (kp_driving[output_kpvalue] - kp_driving_initial[output_kpvalue])\r\n",
    "        kp_value_diff *= adapt_movement_scale\r\n",
    "        kp_new['value'] = kp_value_diff + kp_source[output_kpvalue]\r\n",
    "\r\n",
    "        if use_relative_jacobian:\r\n",
    "            jacobian_diff = paddle.matmul(\r\n",
    "                kp_driving[output_kpjacobian],\r\n",
    "                np.matrix(kp_driving_initial[output_kpjacobian]))#paddle.inverse\r\n",
    "            kp_new['jacobian'] = paddle.matmul(jacobian_diff,\r\n",
    "                                               kp_source[output_kpjacobian])\r\n",
    "\r\n",
    "    return kp_new\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import yaml\n",
    "import pickle\n",
    "import imageio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import paddle\n",
    "# from ppgan.utils.download import get_path_from_url\n",
    "# from ppgan.utils.animate import normalize_kp\n",
    "# from ppgan.faceutils import face_detection\n",
    "\n",
    "adapt_scale=False,\n",
    "face_detector='sfd'\n",
    "# /home/aistudio/work/配乐10s.mov -i /home/aistudio/work/result.mp4\n",
    "face_path='/home/aistudio/work/src.jpg'\n",
    "driving_path='/home/aistudio/work/2.MP4'\n",
    "\n",
    "def smake_animation(source_image,driving_video,relative=True,adapt_movement_scale=True):\n",
    "\n",
    "    batch_size=1\n",
    "    predictions = []\n",
    "    source = paddle.to_tensor(source_image[np.newaxis].astype(\n",
    "        np.float32)).transpose([0, 3, 1, 2])\n",
    "\n",
    "    driving_video_np = np.array(driving_video).astype(np.float32)\n",
    "    driving_n, driving_h, driving_w, driving_c = driving_video_np.shape\n",
    "\n",
    "    driving_slices = []\n",
    "    # whole driving as a single slice\n",
    "    driving = paddle.to_tensor(\n",
    "        np.array(driving_video).astype(np.float32)).transpose(\n",
    "            [0, 3, 1, 2])\n",
    "    frame_count_in_slice = driving_n\n",
    "    driving_slices.append(driving)\n",
    "    #加模型\n",
    "\n",
    "    # kp_source = kp_detector(source)\n",
    "    # kp_driving_initial = kp_detector(driving_slices[0][0:1])\n",
    "    kp_source = kp_net(source)\n",
    "\n",
    "    kp_driving_initial = kp_net(driving_slices[0][0:1])\n",
    "    #加模型\n",
    "\n",
    "    kp_source_batch = {}\n",
    "    # from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "    kp_source_batch[\"value\"] = paddle.tile(\n",
    "        paddle.to_tensor(kp_source[output_kpvalue],dtype='float32'), repeat_times=[batch_size, 1, 1])\n",
    "    kp_source_batch[\"jacobian\"] = paddle.tile(\n",
    "         paddle.to_tensor(kp_source[output_kpjacobian],dtype='float32'), repeat_times=[batch_size, 1, 1, 1])\n",
    "    source = paddle.tile(source,\n",
    "                            repeat_times=[batch_size, 1, 1, 1])\n",
    "    begin_idx = 0\n",
    "    for frame_idx in tqdm(\n",
    "            range(int(np.ceil(float(driving_n) / batch_size)))):\n",
    "        frame_num = min(batch_size, driving_n - begin_idx)\n",
    "        slice_id = int(frame_idx * batch_size /\n",
    "                        frame_count_in_slice)\n",
    "\n",
    "        internal_start = frame_idx - slice_id * frame_count_in_slice\n",
    "        internal_end = frame_idx - slice_id * frame_count_in_slice + frame_num\n",
    "\n",
    "        driving_frame = driving_slices[slice_id][\n",
    "            internal_start:internal_end]\n",
    "        #  #加模型 kp_source[output_kp]\n",
    "        # kp_driving = kp_detector(driving_frame)\n",
    "        kp_driving = kp_net(driving_frame)\n",
    "            #加模型\n",
    "\n",
    "        kp_source_img = {}\n",
    "        kp_source_img[\"value\"] = kp_source_batch[\"value\"][0:frame_num]\n",
    "        kp_source_img[\"jacobian\"] = kp_source_batch[\"jacobian\"][\n",
    "            0:frame_num]\n",
    "\n",
    "        kp_norm = animate_normalize_kp(\n",
    "            kp_source=kp_source,\n",
    "            kp_driving= kp_driving,\n",
    "            kp_driving_initial= paddle.to_tensor(kp_driving_initial[output_kpjacobian],dtype='float32'),\n",
    "            use_relative_movement=relative,\n",
    "            use_relative_jacobian=relative,\n",
    "            adapt_movement_scale=adapt_movement_scale)\n",
    "        inputsorce=(source[0:frame_num],kp_source_img,kp_norm)\n",
    "        # out = generator(source[0:frame_num],\n",
    "        #                 kp_source=kp_source_img,\n",
    "        #                 kp_driving=kp_norm)\n",
    "        out = generator_net(inputsorce)[output_generator]\n",
    "        img = np.transpose(out['prediction'].numpy(),\n",
    "                            [0, 2, 3, 1]) * 255.0\n",
    "\n",
    "        predictions.append(img)\n",
    "        begin_idx += frame_num\n",
    "    return np.concatenate(predictions)\n",
    "\n",
    "def get_prediction(face_image,driving_video):\n",
    "        # driving_forward = driving_video[i:]\n",
    "        # driving_backward = driving_video[:(i + 1)][::-1]\n",
    "        predictions =smake_animation(\n",
    "            face_image,\n",
    "            driving_video,\n",
    "            relative=True,\n",
    "            adapt_movement_scale=False)\n",
    "        return predictions\n",
    "def read_img(path):\n",
    "    img = imageio.imread(path)\n",
    "    if img.ndim == 2:\n",
    "        img = np.expand_dims(img, axis=2)\n",
    "    # som images have 4 channels\n",
    "    if img.shape[2] > 3:\n",
    "        img = img[:, :, :3]\n",
    "    return img  \n",
    "def extract_bbox(image):\n",
    "    detector = FaceAlignment(\n",
    "        LandmarksType._2D,\n",
    "        flip_input=False,\n",
    "        face_detector=face_detector)\n",
    "\n",
    "    frame = [image]\n",
    "    predictions = detector.get_detections_for_image(np.array(frame))\n",
    "    person_num = len(predictions)\n",
    "    if person_num == 0:\n",
    "        return np.array([])\n",
    "    results = []\n",
    "    face_boxs = []\n",
    "    h, w, _ = image.shape\n",
    "    for rect in predictions:\n",
    "        bh = rect[3] - rect[1]\n",
    "        bw = rect[2] - rect[0]\n",
    "        cy = rect[1] + int(bh / 2)\n",
    "        cx = rect[0] + int(bw / 2)\n",
    "        margin = max(bh, bw)\n",
    "        y1 = max(0, cy - margin)\n",
    "        x1 = max(0, cx - int(0.8 * margin))\n",
    "        y2 = min(h, cy + margin)\n",
    "        x2 = min(w, cx + int(0.8 * margin))\n",
    "        area = (y2 - y1) * (x2 - x1)\n",
    "        results.append([x1, y1, x2, y2, area])\n",
    "    # if a person has more than one bbox, keep the largest one\n",
    "    # maybe greedy will be better?\n",
    "    sorted(results, key=lambda area: area[4], reverse=True)\n",
    "    results_box = [results[0]]\n",
    "    for i in range(1, person_num):\n",
    "        num = len(results_box)\n",
    "        add_person = True\n",
    "        for j in range(num):\n",
    "            pre_person = results_box[j]\n",
    "            iou =IOU(pre_person[0], pre_person[1], pre_person[2],\n",
    "                            pre_person[3], pre_person[4], results[i][0],\n",
    "                            results[i][1], results[i][2], results[i][3],\n",
    "                            results[i][4])\n",
    "            if iou > 0.5:\n",
    "                add_person = False\n",
    "                break\n",
    "        if add_person:\n",
    "            results_box.append(results[i])\n",
    "    boxes = np.array(results_box)\n",
    "    return boxes\n",
    "\n",
    "def IOU(ax1, ay1, ax2, ay2, sa, bx1, by1, bx2, by2, sb):\n",
    "    #sa = abs((ax2 - ax1) * (ay2 - ay1))\n",
    "    #sb = abs((bx2 - bx1) * (by2 - by1))\n",
    "    x1, y1 = max(ax1, bx1), max(ay1, by1)\n",
    "    x2, y2 = min(ax2, bx2), min(ay2, by2)\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    if w < 0 or h < 0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 1.0 * w * h / (sa + sb - w * h)\n",
    "\n",
    "\n",
    "image_size=256\n",
    "source_image =read_img(face_path)\n",
    "reader = imageio.get_reader(driving_path)\n",
    "fps = reader.get_meta_data()['fps']\n",
    "driving_video = []\n",
    "try:\n",
    "    for im in reader:\n",
    "        driving_video.append(im)\n",
    "except RuntimeError:\n",
    "    print(\"Read driving video error!\")\n",
    "    pass\n",
    "reader.close()\n",
    "#from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "driving_video = [\n",
    "    cv2.resize(frame, (image_size, image_size)) / 255.0\n",
    "    for frame in driving_video\n",
    "]    \n",
    "#切片\n",
    "results = []\n",
    "multi_person=False\n",
    "bboxes =extract_bbox(source_image.copy())\n",
    "print(str(len(bboxes)) + \" persons have been detected\")\n",
    "\n",
    "# for multi person\n",
    "for rec in bboxes:\n",
    "    face_image = source_image.copy()[rec[1]:rec[3], rec[0]:rec[2]]\n",
    "    face_image = cv2.resize(face_image,\n",
    "                            (image_size, image_size)) / 255.0\n",
    "    predictions = get_prediction(face_image,driving_video)\n",
    "    results.append({\n",
    "        'rec':\n",
    "        rec,\n",
    "        'predict':\n",
    "        [predictions[i] for i in range(predictions.shape[0])]\n",
    "    })\n",
    "    if len(bboxes) == 1 or not multi_person:\n",
    "        break\n",
    "\n",
    "#---\n",
    "out_frame = []\n",
    "ratio=1.0\n",
    "output='output'\n",
    "filename='result.mp4'\n",
    "for i in range(len(driving_video)):\n",
    "    frame = source_image.copy()\n",
    "    for result in results:\n",
    "        x1, y1, x2, y2, _ = result['rec']\n",
    "        h = y2 - y1\n",
    "        w = x2 - x1\n",
    "        out = result['predict'][i]\n",
    "        out = cv2.resize(out.astype(np.uint8), (x2 - x1, y2 - y1))\n",
    "        if len(results) == 1:\n",
    "            frame[y1:y2, x1:x2] = out\n",
    "            break\n",
    "        else:\n",
    "            patch = np.zeros(frame.shape).astype('uint8')\n",
    "            patch[y1:y2, x1:x2] = out\n",
    "            mask = np.zeros(frame.shape[:2]).astype('uint8')\n",
    "            cx = int((x1 + x2) / 2)\n",
    "            cy = int((y1 + y2) / 2)\n",
    "            cv2.circle(mask, (cx, cy), math.ceil(h * ratio),\n",
    "                        (255, 255, 255), -1, 8, 0)\n",
    "            frame = cv2.copyTo(patch, mask, frame)\n",
    "\n",
    "    out_frame.append(frame)\n",
    "imageio.mimsave(os.path.join(output, filename),\n",
    "                [frame for frame in out_frame],\n",
    "                fps=fps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "请点击[此处](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576)查看本环境基本用法.  <br>\n",
    "Please click [here ](https://ai.baidu.com/docs#/AIStudio_Project_Notebook/a38e5576) for more detailed instructions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
