{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Language Classification with OpenVINO\n",
    "## Overview\n",
    "This tutorial will be divided in 2 parts:\n",
    "1. Create a simple inference pipeline with a pre-trained model using the OpenVINO™ IR format.\n",
    "2. Conduct [post-training quantization](https://docs.openvino.ai/latest/ptq_introduction.html) on a pre-trained model using Hugging Face Optimum and benchmark performance.\n",
    "\n",
    "Feel free to use the notebook outline in Jupyter or your IDE for easy navigation. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "### Task\n",
    "**Programming language classification** is the task of identifying which programming language is used in an arbitrary code snippet. This can be useful to label new data to include in a dataset, and potentially serve as an intermediary step when input snippets need to be process based on their programming language.\n",
    "\n",
    "It is a relatively easy machine learning task given that each programming language has its own formal symbols, syntax, and grammar. However, there are some potential edge cases:\n",
    "- **Ambiguous short snippets**: For example, TypeScript is a superset of JavaScript, meaning it does everything Javascript can and more. For a short input snippet, it might be impossible to distinguish between the two. Given we know TypeScript is a superset, and the model doesn't, we should default to classifying the input as JavaScript in a post-processing step.\n",
    "- **Nested programming languages**: Some languages are typically used in tandem. For example, most HTML contains CSS and JavaScript, and it is not uncommon to see SQL nested in other scripting languages. For such input, it is unclear what the expected output class should be. \n",
    "- **Evolving programming language**: Even though programming languages are formal, their symbols, syntax, and grammar can be revised and updated. For example, the walrus operator (`:=`) was a symbol distinctively used in Golang, but was later introduced in Python 3.8."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "The classification model that will be used in this notebook is [`CodeBERTa-language-id`](https://huggingface.co/huggingface/CodeBERTa-language-id) by HuggingFace. This model was fine-tuned from the masked language modeling model [`CodeBERTa-small-v1`](https://huggingface.co/huggingface/CodeBERTa-small-v1) trained on the [`CodeSearchNet`](https://huggingface.co/huggingface/CodeBERTa-small-v1) dataset (Husain, 2019).\n",
    "\n",
    "It supports 6 programming languages:\n",
    "- Go\n",
    "- Java\n",
    "- JavaScript\n",
    "- PHP\n",
    "- Python\n",
    "- Ruby"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Inference pipeline with OpenVINO\n",
    "For this section, we will use the [**HuggingFace Optimum**](https://huggingface.co/docs/optimum/index) library, which aims to optimize inference on specific hardware and integrates with the OpenVINO toolkit. The code will be very similar to the [**HuggingFace Transformers**](https://huggingface.co/docs/transformers/index), but will allow to automatically convert models to the OpenVINO™ IR format."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install prerequesites\n",
    "First, complete the [repository installation steps](../../README.md).\n",
    "\n",
    "Then, the following cell will install:\n",
    "- HuggingFace Optimum with OpenVINO support\n",
    "- HuggingFace Evaluate to benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: optimum[openvino] in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (1.7.3)\n",
      "Requirement already satisfied: evaluate in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (0.4.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (15.0.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (0.14.1+cpu)\n",
      "Requirement already satisfied: sympy in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (1.11.1)\n",
      "Requirement already satisfied: torch>=1.9 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (1.13.1+cpu)\n",
      "Requirement already satisfied: datasets in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (2.10.1)\n",
      "Requirement already satisfied: transformers[sentencepiece]>=4.26.0 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (4.27.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.8.0 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (0.13.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (1.23.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from optimum[openvino]) (21.3)\n",
      "Requirement already satisfied: optimum-intel[openvino] in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum[openvino]) (1.7.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from evaluate) (3.2.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from evaluate) (1.3.5)\n",
      "Requirement already satisfied: dill in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from evaluate) (0.3.4)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from evaluate) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from evaluate) (2022.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from evaluate) (2.28.2)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from evaluate) (0.70.12.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from evaluate) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from datasets->optimum[openvino]) (11.0.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from datasets->optimum[openvino]) (3.8.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from datasets->optimum[openvino]) (6.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (3.10.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from huggingface-hub>=0.8.0->optimum[openvino]) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from packaging->optimum[openvino]) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.26.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->evaluate) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from requests>=2.19.0->evaluate) (2.0.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (0.13.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (2023.3.23)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (0.1.97)\n",
      "Requirement already satisfied: protobuf<=3.20.2 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from transformers[sentencepiece]>=4.26.0->optimum[openvino]) (3.19.6)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from coloredlogs->optimum[openvino]) (10.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum-intel[openvino]->optimum[openvino]) (1.9.1)\n",
      "Requirement already satisfied: onnxruntime in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum-intel[openvino]->optimum[openvino]) (1.14.1)\n",
      "Requirement already satisfied: onnx in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum-intel[openvino]->optimum[openvino]) (1.12.0)\n",
      "Requirement already satisfied: openvino>=2023.0.0.dev20230217 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from optimum-intel[openvino]->optimum[openvino]) (2023.0.0.dev20230217)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from pandas->evaluate) (2023.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from sympy->optimum[openvino]) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from torchvision->optimum[openvino]) (9.4.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->optimum[openvino]) (3.4.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from aiohttp->datasets->optimum[openvino]) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from aiohttp->datasets->optimum[openvino]) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from aiohttp->datasets->optimum[openvino]) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from aiohttp->datasets->optimum[openvino]) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from aiohttp->datasets->optimum[openvino]) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\tj\\miniconda3\\envs\\openvino_env\\lib\\site-packages (from aiohttp->datasets->optimum[openvino]) (1.8.2)\n",
      "Requirement already satisfied: flatbuffers in c:\\users\\tj\\appdata\\roaming\\python\\python38\\site-packages (from onnxruntime->optimum-intel[openvino]->optimum[openvino]) (1.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.3.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\TJ\\miniconda3\\envs\\openvino_env\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install optimum[openvino] evaluate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "The import `OVModelForSequenceClassification` from Optimum is equivalent to `AutoModelForSequenceClassification` from Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TJ\\miniconda3\\envs\\openvino_env\\lib\\site-packages\\openvino\\offline_transformations\\__init__.py:10: FutureWarning: The module is private and following namespace `offline_transformations` will be removed in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA\\v11.6'\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "import evaluate\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from optimum.intel import OVModelForSequenceClassification  \n",
    "from optimum.intel.openvino import OVConfig, OVQuantizer\n",
    "from huggingface_hub.utils import RepositoryNotFoundError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up HuggingFace cache\n",
    "Resources from HuggingFace will be downloaded in the local folder `./model` (next to this notebook) instead of the device global cache for easy cleanup. Learn more [here](https://huggingface.co/docs/transformers/installation?highlight=transformers_cache#cache-setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"CodeBERTa-language-id\"\n",
    "MODEL_ID = f\"huggingface/{MODEL_NAME}\"\n",
    "MODEL_LOCAL_PATH = Path(\"./model\").joinpath(MODEL_NAME)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download resources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded resources from local path: c:\\projects\\openvino_notebooks\\notebooks\\204-GSoC\\model\\CodeBERTa-language-id\n"
     ]
    }
   ],
   "source": [
    "# try to load resources locally\n",
    "try:\n",
    "    model = OVModelForSequenceClassification.from_pretrained(MODEL_LOCAL_PATH)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_PATH)\n",
    "    print(f\"Loaded resources from local path: {MODEL_LOCAL_PATH.absolute()}\")\n",
    "\n",
    "# if not found, download from HuggingFace Hub then save locally\n",
    "except (RepositoryNotFoundError, OSError) as e:\n",
    "    print(\"Downloading resources from HuggingFace Hub\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    tokenizer.save_pretrained(MODEL_LOCAL_PATH)\n",
    "\n",
    "    # export=True is needed to convert the PyTorch model to OpenVINO\n",
    "    model = OVModelForSequenceClassification.from_pretrained(MODEL_ID, export=True)\n",
    "    model.save_pretrained(MODEL_LOCAL_PATH)\n",
    "    print(f\"Ressources cached locally at: {MODEL_LOCAL_PATH.absolute()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_classification_pipe = pipeline(\"text-classification\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference on new input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input snippet:\n",
      "  df['speed'] = df.distance / df.time\n",
      "\n",
      "Predicted label: python\n",
      "Predicted score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# change input snippet to test model\n",
    "input_snippet = \"df['speed'] = df.distance / df.time\"\n",
    "output = code_classification_pipe(input_snippet)\n",
    "\n",
    "print(f\"Input snippet:\\n  {input_snippet}\\n\")\n",
    "print(f\"Predicted label: {output[0]['label']}\")\n",
    "print(f\"Predicted score: {output[0]['score']:.2}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: OpenVINO post-training quantization with HuggingFace Optimum\n",
    "In this section, we will quantize a trained model. At a high-level, this process consists of using lower precision numbers in the model, which results in a smaller model size and faster inference at the cost of a potential marginal performance degradation. [Learn more](https://docs.openvino.ai/latest/ptq_introduction.html).\n",
    "\n",
    "The HuggingFace Optimum library supports post-training quantization for OpenVINO. [Learn more](https://huggingface.co/docs/optimum/main/en/intel/index).\n",
    "\n",
    "![illustration of quantization](https://docs.openvino.ai/latest/_images/quantization_picture.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define constants and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUANTIZED_MODEL_LOCAL_PATH = MODEL_LOCAL_PATH.with_name(f\"{MODEL_NAME}-quantized\")\n",
    "DATASET_NAME = \"code_search_net\"\n",
    "LABEL_MAPPING = {\"go\": 0, \"java\": 1, \"javascript\": 2, \"php\": 3, \"python\": 4, \"ruby\": 5}\n",
    "\n",
    "def preprocess_function(examples: dict, tokenizer):\n",
    "    \"\"\"Preprocess inputs by tokenizing the `func_code_string` column\"\"\"\n",
    "    return tokenizer(\n",
    "        examples[\"func_code_string\"],\n",
    "        padding=\"max_length\",\n",
    "        max_length=tokenizer.model_max_length,\n",
    "        truncation=True,\n",
    "    )\n",
    "    \n",
    "def map_labels(example: dict) -> dict:\n",
    "    \"\"\"Convert string labels to integers\"\"\"\n",
    "    label_mapping = {\"go\": 0, \"java\": 1, \"javascript\": 2, \"php\": 3, \"python\": 4, \"ruby\": 5}\n",
    "    example[\"language\"] = label_mapping[example[\"language\"]]\n",
    "    return example \n",
    "\n",
    "def get_dataset_sample(dataset_split: str, num_samples: int) -> Dataset:\n",
    "    \"\"\"Create a sample with equal representation of each class without downloading the entire data\"\"\"\n",
    "    labels = [\"go\", \"java\", \"javascript\", \"php\", \"python\", \"ruby\"]\n",
    "    example_per_label = num_samples // len(labels)\n",
    "\n",
    "    examples = []\n",
    "    for label in labels:\n",
    "        subset = load_dataset(\"code_search_net\", split=dataset_split, name=label, streaming=True)\n",
    "        subset = subset.map(map_labels)\n",
    "        examples.extend([example for example in subset.shuffle().take(example_per_label)])\n",
    "    \n",
    "    return Dataset.from_list(examples)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load resources\n",
    "NOTE: the base model is loaded using `AutoModelForSequenceClassification` from `Transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at huggingface/CodeBERTa-language-id were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_LOCAL_PATH)\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID)\n",
    "\n",
    "quantizer = OVQuantizer.from_pretrained(base_model)\n",
    "quantization_config = OVConfig()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load calibration dataset\n",
    "The `get_dataset_sample()` function will sample up to `num_samples`, with an equal number of examples across the 6 programming languages.\n",
    "\n",
    "NOTE: Uncomment the method below to download and use the full dataset (5+ Gb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eec4a6472886474bafb426f9a6399427",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "calibration_sample = get_dataset_sample(dataset_split=\"train\", num_samples=300)\n",
    "calibration_sample = calibration_sample.map(partial(preprocess_function, tokenizer=tokenizer))\n",
    "\n",
    "# calibration_sample = quantizer.get_calibration_dataset(\n",
    "#     DATASET_NAME,\n",
    "#     preprocess_function=partial(preprocess_function, tokenizer=tokenizer),\n",
    "#     num_samples=300,\n",
    "#     dataset_split=\"train\",\n",
    "#     preprocess_batch=True,\n",
    "# )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantize model\n",
    "Calling `quantizer.quantize(...)` will iterate through the calibration dataset to quantize and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Ignored adding weight quantizer for: RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/NNCFEmbedding[word_embeddings]/embedding_0\n",
      "INFO:nncf:Ignored adding weight quantizer for: RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/NNCFEmbedding[token_type_embeddings]/embedding_0\n",
      "INFO:nncf:Ignored adding weight quantizer for: RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/NNCFEmbedding[position_embeddings]/embedding_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 11 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/NNCFEmbedding[word_embeddings]/embedding_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 13 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/__add___2\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 3 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/ne_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 4 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/int_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 5 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/cumsum_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 6 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/type_as_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 7 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 8 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/__mul___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 9 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/long_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 10 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/__add___1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 14 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/NNCFEmbedding[position_embeddings]/embedding_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 15 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/__iadd___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 16 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 17 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEmbeddings[embeddings]/Dropout[dropout]/dropout_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 30 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[0]/RobertaAttention[attention]/RobertaSelfAttention[self]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 33 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[0]/RobertaAttention[attention]/RobertaSelfAttention[self]/matmul_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 39 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[0]/RobertaAttention[attention]/RobertaSelfOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 40 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[0]/RobertaAttention[attention]/RobertaSelfOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 45 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[0]/RobertaOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 46 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[0]/RobertaOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 59 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[1]/RobertaAttention[attention]/RobertaSelfAttention[self]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 62 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[1]/RobertaAttention[attention]/RobertaSelfAttention[self]/matmul_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 68 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[1]/RobertaAttention[attention]/RobertaSelfOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 69 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[1]/RobertaAttention[attention]/RobertaSelfOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 74 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[1]/RobertaOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 75 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[1]/RobertaOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 88 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[2]/RobertaAttention[attention]/RobertaSelfAttention[self]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 91 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[2]/RobertaAttention[attention]/RobertaSelfAttention[self]/matmul_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 97 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[2]/RobertaAttention[attention]/RobertaSelfOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 98 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[2]/RobertaAttention[attention]/RobertaSelfOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 103 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[2]/RobertaOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 104 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[2]/RobertaOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 117 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[3]/RobertaAttention[attention]/RobertaSelfAttention[self]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 120 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[3]/RobertaAttention[attention]/RobertaSelfAttention[self]/matmul_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 126 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[3]/RobertaAttention[attention]/RobertaSelfOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 127 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[3]/RobertaAttention[attention]/RobertaSelfOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 132 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[3]/RobertaOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 133 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[3]/RobertaOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 146 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[4]/RobertaAttention[attention]/RobertaSelfAttention[self]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 149 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[4]/RobertaAttention[attention]/RobertaSelfAttention[self]/matmul_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 155 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[4]/RobertaAttention[attention]/RobertaSelfOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 156 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[4]/RobertaAttention[attention]/RobertaSelfOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 161 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[4]/RobertaOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 162 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[4]/RobertaOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 175 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[5]/RobertaAttention[attention]/RobertaSelfAttention[self]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 178 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[5]/RobertaAttention[attention]/RobertaSelfAttention[self]/matmul_1\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 184 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[5]/RobertaAttention[attention]/RobertaSelfOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 185 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[5]/RobertaAttention[attention]/RobertaSelfOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 190 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[5]/RobertaOutput[output]/__add___0\n",
      "INFO:nncf:Not adding activation input quantizer for operation: 191 RobertaForSequenceClassification/RobertaModel[roberta]/RobertaEncoder[encoder]/ModuleList[layer]/RobertaLayer[5]/RobertaOutput[output]/NNCFLayerNorm[LayerNorm]/layer_norm_0\n",
      "INFO:nncf:Collecting tensor statistics |█               | 4 / 38\n",
      "INFO:nncf:Collecting tensor statistics |███             | 8 / 38\n",
      "INFO:nncf:Collecting tensor statistics |█████           | 12 / 38\n",
      "INFO:nncf:Collecting tensor statistics |██████          | 16 / 38\n",
      "INFO:nncf:Collecting tensor statistics |████████        | 20 / 38\n",
      "INFO:nncf:Collecting tensor statistics |██████████      | 24 / 38\n",
      "INFO:nncf:Collecting tensor statistics |███████████     | 28 / 38\n",
      "INFO:nncf:Collecting tensor statistics |█████████████   | 32 / 38\n",
      "INFO:nncf:Collecting tensor statistics |███████████████ | 36 / 38\n",
      "INFO:nncf:Collecting tensor statistics |████████████████| 38 / 38\n",
      "INFO:nncf:Compiling and loading torch extension: quantized_functions_cpu...\n",
      "WARNING:nncf:Could not compile CPU quantization extensions. Falling back on torch native operations - CPU quantization fine-tuning may be slower than expected.\n",
      "Reason: Command '['where', 'cl']' returned non-zero exit status 1.\n",
      "INFO:nncf:Finished loading torch extension: quantized_functions_cpu\n"
     ]
    }
   ],
   "source": [
    "quantizer.quantize(\n",
    "    quantization_config=quantization_config,\n",
    "    calibration_dataset=calibration_sample,\n",
    "    save_directory=QUANTIZED_MODEL_LOCAL_PATH,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load quantized model\n",
    "NOTE: the argument `export=True` is not required since the quantized model is already in the OpenVINO format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = OVModelForSequenceClassification.from_pretrained(QUANTIZED_MODEL_LOCAL_PATH)\n",
    "quantized_code_classification_pipe = pipeline(\"text-classification\", model=quantized_model, tokenizer=tokenizer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load evaluation set\n",
    "NOTE: Uncomment the method below to download and use the full dataset (5+ Gb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_sample = get_dataset_sample(dataset_split=\"validation\", num_samples=300)\n",
    "\n",
    "# validation_sample = load_dataset(DATASET_NAME, split=\"validation\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class is needed due to a current limitation of the Evaluate library with multiclass metrics\n",
    "# ref: https://discuss.huggingface.co/t/combining-metrics-for-multiclass-predictions-evaluations/21792/16\n",
    "class ConfiguredMetric:\n",
    "    def __init__(self, metric, *metric_args, **metric_kwargs):\n",
    "        self.metric = metric\n",
    "        self.metric_args = metric_args\n",
    "        self.metric_kwargs = metric_kwargs\n",
    "    \n",
    "    def add(self, *args, **kwargs):\n",
    "        return self.metric.add(*args, **kwargs)\n",
    "    \n",
    "    def add_batch(self, *args, **kwargs):\n",
    "        return self.metric.add_batch(*args, **kwargs)\n",
    "\n",
    "    def compute(self, *args, **kwargs):\n",
    "        return self.metric.compute(*args, *self.metric_args, **kwargs, **self.metric_kwargs)\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.metric.name\n",
    "\n",
    "    def _feature_names(self):\n",
    "        return self.metric._feature_names()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, an `Evaluator` object for `text-classification` and a set of `EvaluationModule` are instantiated. Then, the evaluator `.compute()` method is called on both the base `code_classification_pipe` and the quantized `quantized_code_classification_pipeline`. Finally, results are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_classification_evaluator = evaluate.evaluator(\"text-classification\")\n",
    "# instantiate an object that can contain multiple `evaluate` metrics\n",
    "metrics = evaluate.combine([\n",
    "    ConfiguredMetric(evaluate.load('f1'), average='macro'),\n",
    "])\n",
    "\n",
    "base_results = code_classification_evaluator.compute(\n",
    "    model_or_pipeline=code_classification_pipe,\n",
    "    data=validation_sample,\n",
    "    input_column=\"func_code_string\",\n",
    "    label_column=\"language\",\n",
    "    label_mapping=LABEL_MAPPING,\n",
    "    metric=metrics,\n",
    ")\n",
    "\n",
    "quantized_results = code_classification_evaluator.compute(\n",
    "    model_or_pipeline=quantized_code_classification_pipe,\n",
    "    data=validation_sample,\n",
    "    input_column=\"func_code_string\",\n",
    "    label_column=\"language\",\n",
    "    label_mapping=LABEL_MAPPING,\n",
    "    metric=metrics,\n",
    ")\n",
    "\n",
    "results_df = pd.DataFrame.from_records([base_results, quantized_results], index=[\"base\", \"quantized\"])\n",
    "results_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "- [Grammatical Error Correction with OpenVINO](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/214-grammar-correction/214-grammar-correction.ipynb)\n",
    "- [Quantize a Hugging Face Question-Answering Model with OpenVINO](https://github.com/huggingface/optimum-intel/blob/main/notebooks/openvino/question_answering_quantization.ipynb)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Uncomment and run cell below to delete all resources cached locally in ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# try:\n",
    "#     shutil.rmtree(path=QUANTIZED_MODEL_LOCAL_PATH)\n",
    "#     shutil.rmtree(path=MODEL_LOCAL_PATH)\n",
    "#     os.remove(path=\"./compressed_graph.dot\")\n",
    "#     os.remove(path=\"./original_graph.dot\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Directory was already deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
