{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "216b3122-6b4d-4d35-bee2-53571fb99e63",
   "metadata": {},
   "source": [
    "# Visual-language assistant with LLaVA Next and OpenVINO\n",
    "\n",
    "[LLaVA-NeXT](https://llava-vl.github.io/blog/2024-01-30-llava-next/) is new generation of LLaVA model family that marks breakthrough in advanced language reasoning over images, introducing improved OCR and expanded world knowledge. [LLaVA](https://llava-vl.github.io) (Large Language and Vision Assistant) is large multimodal model that aims to develop a general-purpose visual assistant that can follow both language and image instructions to complete various real-world tasks. The idea is to combine the power of large language models (LLMs) with vision encoders like CLIP to create an end-to-end trained neural assistant that understands and acts upon multimodal instructions.\n",
    "\n",
    "In this tutorial we consider how to convert and optimize LLaVa-NeXT model from Transformers library for creating multimodal chatbot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17da1d-5c17-4cdf-83f3-a8e2b047d93a",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cca742-3d79-495a-8c8b-508b19c9ab8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -q \"openvino>=2024.0.0\" \"nncf>=2.9.0\" \"torch>=2.1\" \"transformers>=4.39.1\" \"pillow\" \"gradio\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65e9e75f-da6e-4a6f-bb0a-df74ad4322dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "MODEL_DIR = Path(\"model\")\n",
    "IMAGE_ENCODER_PATH = MODEL_DIR / \"image_encoder.xml\"\n",
    "INPUT_EMBEDDING_PATH = MODEL_DIR / \"input_embeddings.xml\"\n",
    "LANGUAGE_MODEL_PATH = MODEL_DIR / \"language_model.xml\"\n",
    "\n",
    "requires_pt_model_loading = not all([p.exists() for p in [IMAGE_ENCODER_PATH, INPUT_EMBEDDING_PATH, LANGUAGE_MODEL_PATH]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d0be85-fa45-4070-aece-b18bdac57075",
   "metadata": {},
   "source": [
    "## Download PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d3a6d7-6f89-4812-9442-d272cfa04f02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-02 18:44:43.544801: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-02 18:44:43.546798: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-02 18:44:43.583801: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-02 18:44:43.584756: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-02 18:44:44.208548: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79746b9d65e1484fbb559f6733c11799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "processor = LlavaNextProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n",
    "image_encoder_model, input_embedding_model, language_model = None, None, None\n",
    "\n",
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self, config, vision_tower, multi_modal_projector):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.vision_tower = vision_tower\n",
    "        self.multi_modal_projector = multi_modal_projector\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        batch_size, num_patches, num_channels, height, width = pixel_values.shape\n",
    "        reshaped_pixel_values = pixel_values.view(batch_size * num_patches, num_channels, height, width)\n",
    "        image_features = self.vision_tower(reshaped_pixel_values, output_hidden_states=True)\n",
    "        selected_image_feature = image_features.hidden_states[self.config.vision_feature_layer]\n",
    "        if self.config.vision_feature_select_strategy == \"default\":\n",
    "            selected_image_feature = selected_image_feature[:, 1:]\n",
    "        elif self.config.vision_feature_select_strategy == \"full\":\n",
    "            selected_image_feature = selected_image_feature\n",
    "        image_features = self.multi_modal_projector(selected_image_feature)\n",
    "        return image_features\n",
    "\n",
    "if requires_pt_model_loading:\n",
    "    model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", low_cpu_mem_usage=True)\n",
    "    model.config.save_pretrained(MODEL_DIR)\n",
    "    image_encoder_model = ImageEncoder(model.config, model.vision_tower, model.multi_modal_projector)\n",
    "    input_embedding_model = input_embedding_model = model.get_input_embeddings()\n",
    "    language_model = model.language_model\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea880b-e80c-46c9-a06f-53d9125bd5a3",
   "metadata": {},
   "source": [
    "## Convert model to OpenVINO Intermediate Representation\n",
    "\n",
    "### Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "113c303e-c7a3-4131-9e58-b80a2b3aa964",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "import gc\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()\n",
    "\n",
    "\n",
    "if not IMAGE_ENCODER_PATH.exists():\n",
    "    ov_image_encoder = ov.convert_model(image_encoder_model, example_input=torch.zeros((1, 5, 3, 336, 336)))\n",
    "    ov.save_model(ov_image_encoder, IMAGE_ENCODER_PATH)\n",
    "    del ov_image_encoder\n",
    "    cleanup_torchscript_cache()\n",
    "\n",
    "del image_encoder_model\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4174f9-30d1-45e5-932d-b6fed0cbc224",
   "metadata": {},
   "source": [
    "### Text Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53f2a7a7-f985-47a4-9961-d3557c62f53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_input = None\n",
    "\n",
    "if not LANGUAGE_MODEL_PATH.exists():\n",
    "    llm_input = input_embedding_model(torch.ones((2, 2), dtype=torch.int64))\n",
    "\n",
    "if not INPUT_EMBEDDING_PATH.exists():\n",
    "    ov_input_embeddings_model = ov.convert_model(input_embedding_model, example_input=torch.ones((2, 2), dtype=torch.int64))\n",
    "    ov.save_model(ov_input_embeddings_model, INPUT_EMBEDDING_PATH)\n",
    "    del ov_input_embeddings_model\n",
    "    cleanup_torchscript_cache()\n",
    "\n",
    "del input_embedding_model\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463a4b42-0fe1-484c-81d7-269c683c7439",
   "metadata": {},
   "source": [
    "### Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70012cbc-4eb5-4894-a266-a5b9bfc3b97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "import nncf\n",
    "from openvino.runtime import opset13\n",
    "import numpy as np\n",
    "\n",
    "def model_has_state(ov_model: ov.Model):\n",
    "    # TODO: Provide a better way based on the variables availability, but OV Python API doesn't expose required methods\n",
    "    return len(ov_model.get_sinks()) > 0\n",
    "\n",
    "\n",
    "def model_has_input_output_name(ov_model: ov.Model, name: str):\n",
    "    \"\"\"\n",
    "    Helper function for checking that model has specified input or output name\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):   # TODO: Can we derive the dimensions from the model topology?\n",
    "      name (str):\n",
    "          name of input or output\n",
    "\n",
    "    Returns:\n",
    "      True if input or output with requested name exists else False\n",
    "    \"\"\"\n",
    "    return name in sum([list(t.get_names()) for t in ov_model.inputs + ov_model.outputs], [])\n",
    "\n",
    "def fuse_cache_reorder(\n",
    "    ov_model: ov.Model, not_kv_inputs: List[str], key_value_input_names: List[str], gather_dim: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Fuses reored_cache during generate cycle into ov.Model. Used with stateful models, because we can not modify model state directly.\n",
    "\n",
    "    Adds a new beam_idx parameter and Gather op per each kv-cache input in a given model.\n",
    "    Should be run before make_stateful. Implements optimumum's _reorder_cache\n",
    "    inside the model in the beginning of each iteration.\n",
    "    Gather works along given gather_dim dimension that may vary from model to model.\n",
    "    KV-cache inputs are identified based on names in key_value_input_names.\n",
    "    Append the new beam_idx parameter to not_kv_inputs.\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (`ov.Model`):\n",
    "          openvino model for processing\n",
    "      not_kv_inputs (`List[str]`):\n",
    "          list of input nodes in model that not related to past key values\n",
    "      key_value_input_names (`List[str]`):\n",
    "          list of names for key value input layers\n",
    "      gather_dim (int):\n",
    "          dimension for gathering cache during reorder pass\n",
    "    \"\"\"\n",
    "\n",
    "    if model_has_input_output_name(ov_model, \"beam_idx\"):\n",
    "        raise ValueError(\"Model already has fused cache\")\n",
    "    input_batch = ov_model.input(\"inputs_embeds\").get_partial_shape()[0]\n",
    "    beam_idx = opset13.parameter(name=\"beam_idx\", dtype=ov.Type.i32, shape=ov.PartialShape([input_batch]))\n",
    "    beam_idx.output(0).get_tensor().add_names({\"beam_idx\"})  # why list is not accepted?\n",
    "    ov_model.add_parameters([beam_idx])\n",
    "    not_kv_inputs.append(ov_model.inputs[-1])\n",
    "    # Go over all cache parameters and fuse _reorder_cache with indices provided by the new parameter beam_idx\n",
    "    for input_name in key_value_input_names:\n",
    "        parameter_output_port = ov_model.input(input_name)\n",
    "        consumers = parameter_output_port.get_target_inputs()\n",
    "        gather = opset13.gather(parameter_output_port, beam_idx, opset13.constant(gather_dim))\n",
    "        for consumer in consumers:\n",
    "            consumer.replace_source_output(gather.output(0))\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def build_state_initializer(ov_model: ov.Model, batch_dim: int):\n",
    "    \"\"\"\n",
    "    Build initialization ShapeOf Expression for all ReadValue ops\n",
    "\n",
    "    Parameters:\n",
    "      ov_model (ov.Model):\n",
    "          openvino model\n",
    "      batch_dim (int):\n",
    "          index of dimension corresponding to batch size\n",
    "    \"\"\"\n",
    "    input_ids = ov_model.input(\"inputs_embeds\")\n",
    "    batch = opset13.gather(opset13.shape_of(input_ids, output_type=\"i64\"), opset13.constant([0]), opset13.constant(0))\n",
    "    for op in ov_model.get_ops():\n",
    "        if op.get_type_name() == \"ReadValue\":\n",
    "            dims = [dim.min_length for dim in list(op.get_output_partial_shape(0))]\n",
    "            dims[batch_dim] = batch\n",
    "            dims = [opset13.constant(np.array([dim], dtype=np.int64)) if isinstance(dim, int) else dim for dim in dims]\n",
    "            shape = opset13.concat(dims, axis=0)\n",
    "            broadcast = opset13.broadcast(opset13.constant(0.0, dtype=op.get_output_element_type(0)), shape)\n",
    "            op.set_arguments([broadcast])\n",
    "    ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "\n",
    "def make_stateful(\n",
    "    ov_model: ov.Model,\n",
    "    not_kv_inputs: List[str],\n",
    "    key_value_input_names: List[str],\n",
    "    key_value_output_names: List[str],\n",
    "    batch_dim: int,\n",
    "    num_attention_heads: int,\n",
    "    num_beams_and_batch: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Hides kv-cache inputs and outputs inside the model as variables.\n",
    "\n",
    "    Parameters:\n",
    "        ov_model (ov.Model):\n",
    "            openvino model\n",
    "        not_kv_inputs (`List[str]`):\n",
    "            list of input nodes in model that not related to past key values\n",
    "        key_value_input_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        key_value_output_names (`List[str]`):\n",
    "            list of names for key value input layers\n",
    "        batch_dim (int):\n",
    "            index of batch dimension in key value layers\n",
    "        num_attention_heads (int):\n",
    "            number of attention heads for batch dimension initialization\n",
    "        num_beams_an_batch (int):\n",
    "            precalculated number of beams and batch for shapes initialization\n",
    "    \"\"\"\n",
    "    from openvino._offline_transformations import apply_make_stateful_transformation\n",
    "\n",
    "    input_output_map = {}\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Set batch size for input_ids and attention mask to avoid dynamic dimension got propagated from the end of the model back to ReadValue\n",
    "        for input in not_kv_inputs:\n",
    "            shape = input.get_partial_shape()\n",
    "            if shape.rank.get_length() <= 2:  # == 1 for beam_index\n",
    "                shape[0] = num_beams_and_batch\n",
    "                input.get_node().set_partial_shape(shape)\n",
    "            else:\n",
    "                log.warn(f\"Rank of {input.get_any_name()} input of the model is not 2, batch size is not set\")\n",
    "\n",
    "    for kv_name_pair in zip(key_value_input_names, key_value_output_names):\n",
    "        input_output_map[kv_name_pair[0]] = kv_name_pair[1]\n",
    "        if num_beams_and_batch is not None:\n",
    "            input = ov_model.input(kv_name_pair[0])\n",
    "            shape = input.get_partial_shape()\n",
    "            shape[batch_dim] = num_beams_and_batch * num_attention_heads\n",
    "            input.get_node().set_partial_shape(shape)\n",
    "\n",
    "    if num_beams_and_batch is not None:\n",
    "        # Re-validation model if shapes are altered above\n",
    "        ov_model.validate_nodes_and_infer_types()\n",
    "\n",
    "    apply_make_stateful_transformation(ov_model, input_output_map)\n",
    "    if num_beams_and_batch is None:\n",
    "        build_state_initializer(ov_model, batch_dim)\n",
    "\n",
    "\n",
    "def patch_stateful(ov_model):\n",
    "    key_value_input_names = [\n",
    "        key.get_any_name() for key in ov_model.inputs[2:-1]\n",
    "    ]\n",
    "    key_value_output_names = [\n",
    "        key.get_any_name() for key in ov_model.outputs[1:]\n",
    "    ]\n",
    "    not_kv_inputs = [\n",
    "        input for input in ov_model.inputs if not any(name in key_value_input_names for name in input.get_names())\n",
    "    ]\n",
    "    if not key_value_input_names or not key_value_output_names:\n",
    "        return\n",
    "    batch_dim =  0\n",
    "    num_attention_heads = 1\n",
    "\n",
    "    fuse_cache_reorder(ov_model, not_kv_inputs, key_value_input_names, batch_dim)\n",
    "    make_stateful(\n",
    "        ov_model, not_kv_inputs, key_value_input_names, key_value_output_names, batch_dim, num_attention_heads, None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7e0b233-0df9-41ca-96dd-ce7b77c4099b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.base has been moved to tensorflow.python.trackable.base. The old module will be deleted in version 2.11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARNING ]  Please fix your imports. Module %s has been moved to %s. The old module will be deleted in version %s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:nncf:NNCF provides best results with torch==2.2.*, while current torch version is 2.1.2+cpu. If you encounter issues, consider switching to torch==2.2.*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/modeling_utils.py:4225: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:120: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/mistral/modeling_mistral.py:676: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/jit/_trace.py:160: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at aten/src/ATen/core/TensorBody.h:489.)\n",
      "  if a.grad is not None:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fdd11a4ee2a4d60af764f4831fc309b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "+--------------+---------------------------+-----------------------------------+\n",
      "| Num bits (N) | % all parameters (layers) |    % ratio-defining parameters    |\n",
      "|              |                           |             (layers)              |\n",
      "+==============+===========================+===================================+\n",
      "| 8            | 41% (135 / 225)           | 40% (134 / 224)                   |\n",
      "+--------------+---------------------------+-----------------------------------+\n",
      "| 4            | 59% (90 / 225)            | 60% (90 / 224)                    |\n",
      "+--------------+---------------------------+-----------------------------------+\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24735b15c93446d6bd4628b63dca98d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nncf\n",
    "\n",
    "make_stateful_model = True\n",
    "compress_weights = True\n",
    "\n",
    "compression_configuration = {\"mode\": nncf.CompressWeightsMode.INT4_SYM, \"group_size\": 64, \"ratio\": 0.6}\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "if not LANGUAGE_MODEL_PATH.exists():\n",
    "    pkv = language_model(inputs_embeds=llm_input, attention_mask=torch.ones((2, 2), dtype=torch.int64))[1]\n",
    "    model_inputs = [\"attention_mask\", \"position_ids\"]\n",
    "    model_outputs = [\"logits\"]\n",
    "    for idx in range(len(pkv)):\n",
    "        model_inputs.extend([f\"past_key_values.{idx}.key\", f\"past_key_values.{idx}.value\"])\n",
    "        model_outputs.extend([f\"present.{idx}.key\", f\"present.{idx}.value\"])\n",
    "    model_inputs.append(\"inputs_embeds\")\n",
    "    language_model.config.torchscript = True\n",
    "    position_ids = torch.tensor([[2, 3], [2, 3]])\n",
    "    ov_model = ov.convert_model(language_model, example_input={\"inputs_embeds\": llm_input, \"attention_mask\": torch.ones((2, 4)), \"past_key_values\": pkv, \"position_ids\": position_ids})\n",
    "\n",
    "    for input, input_name in zip(ov_model.inputs, model_inputs):\n",
    "        input.get_tensor().set_names({input_name})\n",
    "\n",
    "    for output, output_name in zip(ov_model.outputs, model_outputs):\n",
    "        output.get_tensor().set_names({output_name})\n",
    "    if make_stateful_model:\n",
    "        patch_stateful(ov_model)\n",
    "    ov.save_model(ov_model, LANGUAGE_MODEL_PATH)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    del language_model\n",
    "    gc.collect()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbde5d6-dba5-4a26-8fc1-1c65d4f44992",
   "metadata": {},
   "source": [
    "### Compress Language Model werights to INT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4464bd-b728-45c6-8c14-525bce7b98fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "LANGUAGE_MODEL_PATH_INT4 = LANGUAGE_MODEL_PATH.parent / LANGUAGE_MODEL_PATH.name.replace(\".xml\", \"-int4.xml\")\n",
    "if compress_weights and not LANGUAGE_MODEL_PATH_INT4.exists():\n",
    "    ov_model = core.read_model(LANGUAGE_MODEL_PATH)\n",
    "    ov_model = nncf.compress_weights(ov_model, **compression_configuration)\n",
    "    ov.save_model(ov_model, LANGUAGE_MODEL_PATH_INT4)\n",
    "    del ov_model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f49ab0-6ac4-4a96-9506-f1c6a3a0ed86",
   "metadata": {},
   "source": [
    "### Quantize Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a04a5d9-2b47-4067-a1f3-59e0f57a9131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a4161d-00d7-4ab7-be5c-5dfaed0db592",
   "metadata": {},
   "source": [
    "### Prepare model inference pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f3ff928-ec76-4806-b34d-b9213f523eec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.generation import GenerationConfig, GenerationMixin\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import AutoConfig\n",
    "from transformers.models.llava_next.modeling_llava_next import get_anyres_image_grid_shape, unpad_image\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Optional, Tuple\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "class OVLlavaForCausalLM(GenerationMixin):\n",
    "    def __init__(self, core, model_dir, device):\n",
    "        self.image_encoder = core.compile_model(model_dir / \"image_encoder.xml\", device)\n",
    "        self.input_embeddings = core.compile_model(model_dir / \"input_embeddings.xml\", device)\n",
    "        self.model = core.read_model(model_dir / \"language_model-int4.xml\")\n",
    "        self.input_names = {\n",
    "            key.get_any_name(): idx for idx, key in enumerate(self.model.inputs)\n",
    "        }\n",
    "        self.output_names = {\n",
    "            idx: key for idx, key in enumerate(self.model.outputs)\n",
    "        }\n",
    "        self.key_value_input_names = [\n",
    "            key for key in list(self.input_names) if key not in  [\"beam_idx\", \"inputs_embeds\", \"attention_mask\", \"position_ids\"]\n",
    "        ]\n",
    "        self.key_value_output_names = [\n",
    "            key for key in list(self.output_names)[1:]\n",
    "        ]\n",
    "        self.stateful = len(self.key_value_input_names) == 0\n",
    "        compiled_model = core.compile_model(self.model, device)\n",
    "        self.request = compiled_model.create_infer_request()\n",
    "        self.config = AutoConfig.from_pretrained(model_dir)\n",
    "        self.generation_config = GenerationConfig.from_model_config(self.config)\n",
    "        self.main_input_name = \"input_ids\"\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.num_pkv = 2\n",
    "        self.next_beam_idx = None\n",
    "        self.image_newline = torch.nn.Parameter(torch.empty(self.config.text_config.hidden_size, dtype=torch.float32))\n",
    "        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n",
    "        self.past_len = 0\n",
    "\n",
    "    def can_generate(self):\n",
    "        \"\"\"Returns True to validate the check that the model using `GenerationMixin.generate()` can indeed generate.\"\"\"\n",
    "        return True\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.Tensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        image_sizes=None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        return self.forward(\n",
    "            input_ids, pixel_values, attention_mask, past_key_values, position_ids, image_sizes, **kwargs\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        pixel_values: torch.Tensor,skip_prompt=True\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        image_sizes=None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        \"\"\"General inference method\"\"\"\n",
    "        inputs = {}\n",
    "        if past_key_values is not None:\n",
    "            inputs = {}\n",
    "            if not self.stateful:\n",
    "                past_key_values = tuple(\n",
    "                    past_key_value\n",
    "                    for pkv_per_layer in past_key_values\n",
    "                    for past_key_value in pkv_per_layer\n",
    "                )\n",
    "                # Add the past_key_values to the decoder inputs\n",
    "                inputs = dict(zip(self.key_value_input_names, past_key_values))\n",
    "            input_ids = np.array(input_ids)[:, -1:]\n",
    "            inputs_embeds = self.input_embeddings(input_ids)[0]\n",
    "            inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "            # inputs[\"attention_mask\"] = attention_mask\n",
    "            if \"beam_idx\" in self.input_names:\n",
    "                inputs[\"beam_idx\"] = (\n",
    "                    self.next_beam_idx if self.next_beam_idx is not None else np.arange(batch_size, dtype=int)\n",
    "                )\n",
    "\n",
    "            if not self.stateful:\n",
    "                first_layer_past_key_value = torch.from_numpy(past_key_values[0][0][:, :, :, 0])\n",
    "            else:\n",
    "                first_layer_past_key_value = torch.from_numpy(ov_llava_model.request.query_state()[0].state.data[:, :, :, 0])\n",
    "\n",
    "            # Sum all dimensions of head_dim (-2) to avoid random errors such as: https://github.com/huggingface/transformers/pull/28032#issuecomment-1863691941\n",
    "            batch_index, non_attended_tokens = torch.where(first_layer_past_key_value.float().sum(-2) == 0)\n",
    "\n",
    "            # Get the target length\n",
    "            target_length = input_ids.shape[1]\n",
    "            past_length = first_layer_past_key_value.shape[-1]\n",
    "\n",
    "            extended_attention_mask = torch.ones(\n",
    "                    (attention_mask.shape[0], past_length),\n",
    "                    dtype=attention_mask.dtype,\n",
    "                    device=attention_mask.device,\n",
    "            )\n",
    "\n",
    "            # Filter out only the tokens that can be un-attended, this can happen\n",
    "            # if one uses Llava + Fused modules where the cache on the\n",
    "            # first iteration is already big enough, or if one passes custom cache\n",
    "            valid_indices = non_attended_tokens < extended_attention_mask.size(-1)\n",
    "            new_batch_index = batch_index[valid_indices]\n",
    "            new_non_attended_tokens = non_attended_tokens[valid_indices]\n",
    "\n",
    "            # Zero-out the places where we don't need to attend\n",
    "            extended_attention_mask[new_batch_index, new_non_attended_tokens] = 0\n",
    "\n",
    "            attention_mask = torch.cat((extended_attention_mask, attention_mask[:, -target_length:]), dim=1)\n",
    "            position_ids = torch.sum(attention_mask, dim=1).unsqueeze(-1) - 1\n",
    "            inputs[\"attention_mask\"] = attention_mask\n",
    "            inputs[\"position_ids\"] = position_ids\n",
    "                \n",
    "        else:\n",
    "            inputs = self.prepare_multimodal_input(\n",
    "            input_ids, pixel_values, attention_mask, position_ids, image_sizes\n",
    "            )\n",
    "\n",
    "        # Run inference\n",
    "        self.request.start_async(inputs, share_inputs=True)\n",
    "        self.request.wait()\n",
    "\n",
    "        logits = torch.from_numpy(self.request.get_tensor(self.output_names[0]).data)\n",
    "\n",
    "        if not self.stateful:\n",
    "\n",
    "            # Tuple of length equal to : number of layer * number of past_key_value per decoder layer (2 corresponds to the self-attention layer)\n",
    "            past_key_values = tuple(\n",
    "                self.request.get_tensor(key).data for key in self.key_value_output_names\n",
    "            )\n",
    "            # Tuple of tuple of length `n_layers`, with each tuple of length equal to 2 (k/v of self-attention)\n",
    "            past_key_values = tuple(\n",
    "                past_key_values[i : i + self.num_pkv]\n",
    "                for i in range(0, len(past_key_values), self.num_pkv)\n",
    "            )\n",
    "        else:\n",
    "            past_key_values = ((),)\n",
    "        self.past_len += inputs[\"inputs_embeds\"].shape[1]\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=past_key_values)\n",
    "\n",
    "\n",
    "    def prepare_multimodal_input(self, input_ids, pixel_values, attention_mask, position_ids, image_sizes=None):\n",
    "        \"\"\"Preprocessing function for embedding multimodal data\"\"\"\n",
    "        inputs = {}\n",
    "        inputs_embeds = torch.from_numpy(self.input_embeddings(input_ids)[0])\n",
    "        batch_size = input_ids.shape[0]\n",
    "        if not self.stateful:\n",
    "            for input_name in self.key_value_input_names:\n",
    "                model_inputs = self.modeget_anyres_image_grid_shapel.input(input_name)\n",
    "                shape = model_inputs.get_partial_shape()\n",
    "                shape[0] = batch_size\n",
    "                if shape[2].is_dynamic:\n",
    "                    shape[2] = 0\n",
    "                else:\n",
    "                    shape[1] = 0\n",
    "                inputs[input_name] = ov.Tensor(model_inputs.get_element_type(), shape.get_shape())\n",
    "        else:\n",
    "            self.request.reset_state()\n",
    "            # Set initial value for the next beam_idx input that will be used at the current iteration\n",
    "            # and will be optionally updated by _reorder_cache at the next iterations if beam_search is used\n",
    "            self.next_beam_idx = np.arange(batch_size, dtype=int)\n",
    "\n",
    "        if \"beam_idx\" in self.input_names:\n",
    "            inputs[\"beam_idx\"] = (\n",
    "                    self.next_beam_idx if self.next_beam_idx is not None else np.arange(batch_size, dtype=int)\n",
    "            )\n",
    "        if pixel_values is None:\n",
    "            inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "            inputs[\"attention_mask\"] = attention_mask\n",
    "            if position_ids is None:\n",
    "                position_ids = torch.cumsum(attention_mask, axis=1) - 1\n",
    "                position_ids[attention_mask == 0] = 1\n",
    "            inputs[\"position_ids\"] = position_ids\n",
    "        res = self.image_encoder(pixel_values)\n",
    "        image_features = torch.from_numpy(res[0])\n",
    "        split_sizes = [image.shape[0] for image in pixel_values]\n",
    "        image_features = torch.split(image_features, split_sizes, dim=0)\n",
    "\n",
    "        # NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\n",
    "        height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n",
    "\n",
    "        new_image_features = []\n",
    "        for image_idx, image_feature in enumerate(image_features):\n",
    "            if image_feature.shape[0] > 1:\n",
    "                base_image_feature = image_feature[0]\n",
    "                image_feature = image_feature[1:]\n",
    "\n",
    "                if height * width != base_image_feature.shape[0]:\n",
    "                    raise ValueError(\"The number of patches is not consistent with the image size.\")\n",
    "                num_patch_height, num_patch_width = get_anyres_image_grid_shape(image_sizes[image_idx], self.config.image_grid_pinpoints, self.config.vision_config.image_size)\n",
    "                image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n",
    "                image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n",
    "                image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n",
    "                image_feature = unpad_image(image_feature, image_sizes[image_idx])\n",
    "                image_feature = torch.cat((image_feature, self.image_newline[:, None, None].expand(*image_feature.shape[:-1], 1)), dim=-1)\n",
    "                image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n",
    "                image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n",
    "            else:\n",
    "                image_feature = image_feature[0]\n",
    "                image_feature = torch.cat((image_feature, self.image_newline[None]), dim=0)\n",
    "            new_image_features.append(image_feature)\n",
    "        image_features = torch.stack(new_image_features, dim=0)\n",
    "\n",
    "        inputs_embeds, attention_mask, position_ids = self._merge_input_ids_with_image_features(image_features, inputs_embeds, input_ids, attention_mask, None)\n",
    "        inputs[\"inputs_embeds\"] = inputs_embeds\n",
    "        inputs[\"attention_mask\"] = attention_mask\n",
    "        inputs[\"position_ids\"] = position_ids\n",
    "        \n",
    "        return inputs\n",
    "\n",
    "\n",
    "    def _reorder_cache(\n",
    "        self, past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n",
    "    ) -> Tuple[Tuple[torch.Tensor]]:\n",
    "        \"\"\"\n",
    "        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n",
    "        [`~PreTrainedModel.beam_sample`] is called.\n",
    "        This is required to match `past_key_values` with the correct beam_idx at every generation step.\n",
    "        \"\"\"\n",
    "\n",
    "        # from transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel._reorder_cache\n",
    "        return tuple(\n",
    "            tuple(np.take(past_state, beam_idx, 0) for past_state in layer_past)\n",
    "            for layer_past in past_key_values\n",
    "        )\n",
    "\n",
    "    \n",
    "    def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, input_ids, attention_mask, labels):\n",
    "        num_images, num_image_patches, embed_dim = image_features.shape\n",
    "        batch_size, sequence_length = input_ids.shape\n",
    "        left_padding = not torch.sum(input_ids[:, -1] == torch.tensor(self.pad_token_id))\n",
    "        # 1. Create a mask to know where special image tokens are\n",
    "        special_image_token_mask = input_ids == self.config.image_token_index\n",
    "        num_special_image_tokens = torch.sum(special_image_token_mask, dim=-1)\n",
    "        # Compute the maximum embed dimension\n",
    "        max_embed_dim = (num_special_image_tokens.max() * (num_image_patches - 1)) + sequence_length\n",
    "        batch_indices, non_image_indices = torch.where(input_ids != self.config.image_token_index)\n",
    "\n",
    "        # 2. Compute the positions where text should be written\n",
    "        # Calculate new positions for text tokens in merged image-text sequence.\n",
    "        # `special_image_token_mask` identifies image tokens. Each image token will be replaced by `nb_text_tokens_per_images - 1` text tokens.\n",
    "        # `torch.cumsum` computes how each image token shifts subsequent text token positions.\n",
    "        # - 1 to adjust for zero-based indexing, as `cumsum` inherently increases indices by one.\n",
    "        new_token_positions = torch.cumsum((special_image_token_mask * (num_image_patches - 1) + 1), -1) - 1\n",
    "        nb_image_pad = max_embed_dim - 1 - new_token_positions[:, -1]\n",
    "        if left_padding:\n",
    "            new_token_positions += nb_image_pad[:, None]  # offset for left padding\n",
    "        text_to_overwrite = new_token_positions[batch_indices, non_image_indices]\n",
    "\n",
    "        # 3. Create the full embedding, already padded to the maximum position\n",
    "        final_embedding = torch.zeros(\n",
    "            batch_size, max_embed_dim, embed_dim, dtype=inputs_embeds.dtype, device=inputs_embeds.device\n",
    "        )\n",
    "        final_attention_mask = torch.zeros(\n",
    "            batch_size, max_embed_dim, dtype=attention_mask.dtype, device=inputs_embeds.device\n",
    "        )\n",
    "        # In case the Vision model or the Language model has been offloaded to CPU, we need to manually\n",
    "        # set the corresponding tensors into their correct target device.\n",
    "        target_device = inputs_embeds.device\n",
    "        batch_indices, non_image_indices, text_to_overwrite = (\n",
    "            batch_indices.to(target_device),\n",
    "            non_image_indices.to(target_device),\n",
    "            text_to_overwrite.to(target_device),\n",
    "        )\n",
    "        attention_mask = attention_mask.to(target_device)\n",
    "\n",
    "        # 4. Fill the embeddings based on the mask. If we have [\"hey\" \"<image>\", \"how\", \"are\"]\n",
    "        # we need to index copy on [0, 577, 578, 579] for the text and [1:576] for the image features\n",
    "        final_embedding[batch_indices, text_to_overwrite] = inputs_embeds[batch_indices, non_image_indices]\n",
    "        final_attention_mask[batch_indices, text_to_overwrite] = attention_mask[batch_indices, non_image_indices]\n",
    "        if labels is not None:\n",
    "            final_labels[batch_indices, text_to_overwrite] = labels[batch_indices, non_image_indices]\n",
    "\n",
    "        # 5. Fill the embeddings corresponding to the images. Anything that is still zeros needs filling\n",
    "        image_to_overwrite = torch.all(final_embedding == 0, dim=-1)\n",
    "        image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n",
    "\n",
    "        if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n",
    "            raise ValueError(\n",
    "                f\"The input provided to the model are wrong. The number of image tokens is {torch.sum(special_image_token_mask)} while\"\n",
    "                f\" the number of image given to the model is {num_images}. This prevents correct indexing and breaks batch generation.\"\n",
    "            )\n",
    "\n",
    "        final_embedding[image_to_overwrite] = image_features.contiguous().reshape(-1, embed_dim).to(target_device)\n",
    "        final_attention_mask |= image_to_overwrite\n",
    "        position_ids = (final_attention_mask.cumsum(-1) - 1).masked_fill_((final_attention_mask == 0), 1)\n",
    "\n",
    "        # 6. Mask out the embedding at padding positions, as we later use the past_key_value value to determine the non-attended tokens.\n",
    "        batch_indices, pad_indices = torch.where(input_ids == self.pad_token_id)\n",
    "        indices_to_mask = new_token_positions[batch_indices, pad_indices]\n",
    "\n",
    "        final_embedding[batch_indices, indices_to_mask] = 0\n",
    "\n",
    "        return final_embedding, final_attention_mask, position_ids\n",
    "\n",
    "    def prepare_inputs_for_generation(\n",
    "        self,\n",
    "        input_ids,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        pixel_values=None,\n",
    "        image_sizes=None,\n",
    "        attention_mask=None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        if past_key_values is not None:\n",
    "            if not self.stateful:\n",
    "                cache_length = past_length = past_key_values[0][0].shape[2]\n",
    "            else:\n",
    "                cache_length = past_length = self.past_len\n",
    "\n",
    "            # Keep only the unprocessed tokens:\n",
    "            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n",
    "            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n",
    "            # input)\n",
    "            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n",
    "                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n",
    "            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n",
    "            # input_ids based on the past_length.llava\n",
    "            elif past_length < input_ids.shape[1]:\n",
    "                input_ids = input_ids[:, past_length:]\n",
    "            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n",
    "            elif self.config.image_token_index in input_ids:\n",
    "                input_ids = input_ids[:, input_ids.shape[1] - 1 :]\n",
    "            # If the cache has seen more tokens than it can hold, then the cache has a size limit. Let's discard the\n",
    "            # older attention values, as their corresponding values are not part of the input.\n",
    "            if cache_length < past_length and attention_mask is not None:\n",
    "                attention_mask = attention_mask[:, -(cache_length + input_ids.shape[1]) :]\n",
    "\n",
    "        position_ids = kwargs.get(\"position_ids\", None)\n",
    "        if attention_mask is not None and position_ids is None:\n",
    "            # create position_ids on the fly for batch gllavaeneration\n",
    "            position_ids = attention_mask.long().cumsum(-1) - 1\n",
    "            position_ids.masked_fill_(attention_mask == 0, 1)\n",
    "            if past_key_values:\n",
    "                position_ids = position_ids[:, -input_ids.shape[1] :]\n",
    "\n",
    "        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n",
    "        if inputs_embeds is not None and past_key_values is None:\n",
    "            model_inputs = {\"inputs_embeds\": inputs_embeds}\n",
    "        else:\n",
    "            model_inputs = {\"input_ids\": input_ids}\n",
    "\n",
    "        model_inputs.update(\n",
    "            {\n",
    "                \"position_ids\": position_ids,\n",
    "                \"past_key_values\": past_key_values,\n",
    "                \"use_cache\": kwargs.get(\"use_cache\"),\n",
    "                \"attention_mask\": attention_mask,\n",
    "                \"pixel_values\": pixel_values,\n",
    "                \"image_sizes\": image_sizes,\n",
    "            }\n",
    "        )\n",
    "        return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01acdd5-2836-41e9-93b4-e82cd9c5d46d",
   "metadata": {},
   "source": [
    "## Run OpenVINO model inference\n",
    "\n",
    "### Select device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "443b5c9c-ca8e-4c98-b55a-af38894f0875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9f21847080c4a91957fa7de8d497f0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU.0', 'GPU.1', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe6f88c3-9d69-4ca3-ac0f-931e290110ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ov_llava_model = OVLlavaForCausalLM(core, MODEL_DIR, device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "601747d4-e89e-42a2-94da-502dbc351180",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "\n",
    "from transformers import TextStreamer\n",
    "\n",
    "url = \"https://github.com/haotian-liu/LLaVA/blob/1a91fc274d7c35a9b50b3cb29c4247ae5837ce39/images/llava_v1_5_radar.jpg?raw=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "prompt = \"[INST] <image>\\nWhat is shown in this image? [/INST]\"\n",
    "streamer = TextStreamer(processor, skip_special_tokens=True, skip_prompt=True)\n",
    "\n",
    "inputs = processor(prompt, image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c77f713-42c8-40b6-afba-3546dae104b1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image appears to be a radar chart, which\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   733, 16289, 28793, 28705, 32000, 28705,    13,  3195,   349,\n",
       "          4894,   297,   456,  3469, 28804,   733, 28748, 16289, 28793,   415,\n",
       "          3469,  8045,   298,   347,   264, 24951, 10968, 28725,   690]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = ov_llava_model.generate(**inputs, max_new_tokens=20, streamer=streamer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c4c41-8d68-4015-a274-eb5be620a530",
   "metadata": {},
   "source": [
    "## Interactive demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e452f5c-a6fc-4c04-8917-f151cc20c603",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "import re\n",
    "import time \n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "def bot_streaming(message, history):\n",
    "    print(message)\n",
    "    if message[\"files\"]:\n",
    "        image = message[\"files\"][-1][\"path\"]\n",
    "    else:\n",
    "    # if there's no image uploaded for this turn, look for images in the past turns\n",
    "    # kept inside tuples, take the last one\n",
    "        for hist in history:\n",
    "            if isinstance(hist[0], tuple):\n",
    "                image = hist[0][0]\n",
    "\n",
    "    if image is None:\n",
    "        gr.Error(\"You need to upload an image for LLaVA to work.\")\n",
    "    prompt=f\"[INST] <image>\\n{message['text']} [/INST]\"\n",
    "    image = Image.open(image).convert(\"RGB\")\n",
    "    inputs = processor(prompt, image, return_tensors=\"pt\")\n",
    "\n",
    "    streamer = TextIteratorStreamer(processor, **{\"skip_special_tokens\": True})\n",
    "    generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=100)\n",
    "    generated_text = \"\"\n",
    "\n",
    "    thread = Thread(target=ov_llava_model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    text_prompt =f\"[INST]  \\n{message['text']} [/INST]\"\n",
    "  \n",
    "\n",
    "    buffer = \"\"\n",
    "    for new_text in streamer:\n",
    "        buffer += new_text\n",
    "        generated_text_without_prompt = buffer[len(text_prompt):]\n",
    "        yield generated_text_without_prompt\n",
    "\n",
    "\n",
    "demo = gr.ChatInterface(fn=bot_streaming, title=\"LLaVA NeXT\", #examples=[{\"text\": \"What is on the flower?\", \"files\":[\"./bee.jpg\"]},\n",
    "                                                              #        {\"text\": \"How to make this pastry?\", \"files\":[\"./baklava.png\"]}], \n",
    "                        description=\"Try [LLaVA NeXT](https://huggingface.co/docs/transformers/main/en/model_doc/llava_next) in this demo. Upload an image and start chatting about it, or simply try one of the examples below. If you don't upload an image, you will receive an error.\",\n",
    "                        stop_btn=\"Stop Generation\", multimodal=True)\n",
    "\n",
    "try:\n",
    "    demo.launch(debug=True)\n",
    "except Exception:\n",
    "    demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
