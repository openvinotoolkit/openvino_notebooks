{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MONODEPTH on OpenVINO IR Model\n",
    "\n",
    "This notebook demonstrates Monocular Depth Estimation with MidasNet in OpenVINO. Model information: https://docs.openvinotoolkit.org/latest/omz_models_model_midasnet.html"
   ],
   "metadata": {
    "id": "moved-collapse"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=\"data/monodepth.gif\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is Monodepth?\n",
    "Monocular Depth Estimation is the task of estimating scene depth using a single image. It has many potential applications in robotics, 3D reconstruction, medical imaging and autonomous systems. For this demo, we use a neural network model called [MiDaS](https://github.com/intel-isl/MiDaS) which was developed by the Intelligent Systems Lab at Intel. Check out their research paper to learn more. \n",
    "\n",
    "R. Ranftl, K. Lasinger, D. Hafner, K. Schindler and V. Koltun, [\"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer,\"](https://ieeexplore.ieee.org/document/9178977) in IEEE Transactions on Pattern Analysis and Machine Intelligence, doi: 10.1109/TPAMI.2020.3019967."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation "
   ],
   "metadata": {
    "id": "creative-cisco"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {
    "id": "faced-honolulu"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import time\r\n",
    "import urllib\r\n",
    "from pathlib import Path\r\n",
    "\r\n",
    "import cv2\r\n",
    "import matplotlib.cm\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "from IPython.display import (\r\n",
    "    HTML,\r\n",
    "    FileLink,\r\n",
    "    Pretty,\r\n",
    "    ProgressBar,\r\n",
    "    Video,\r\n",
    "    clear_output,\r\n",
    "    display,\r\n",
    ")\r\n",
    "from openvino.inference_engine import IECore"
   ],
   "outputs": [],
   "metadata": {
    "id": "ahead-spider"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Settings"
   ],
   "metadata": {
    "id": "contained-office"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DEVICE = \"CPU\"\r\n",
    "MODEL_FILE = \"model/MiDaS_small.xml\"\r\n",
    "\r\n",
    "model_name = os.path.basename(MODEL_FILE)\r\n",
    "model_xml_path = Path(MODEL_FILE).with_suffix(\".xml\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "amber-lithuania"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def normalize_minmax(data):\r\n",
    "    \"\"\"Normalizes the values in `data` between 0 and 1\"\"\"\r\n",
    "    return (data - data.min()) / (data.max() - data.min())\r\n",
    "\r\n",
    "\r\n",
    "def load_image(path: str):\r\n",
    "    \"\"\"\r\n",
    "    Loads an image from `path` and returns it as BGR numpy array. `path`\r\n",
    "    should point to an image file, either a local filename or an url.\r\n",
    "    \"\"\"\r\n",
    "    if path.startswith(\"http\"):\r\n",
    "        # Set User-Agent to Mozilla because some websites block\r\n",
    "        # requests with User-Agent Python\r\n",
    "        request = urllib.request.Request(\r\n",
    "            path, headers={\"User-Agent\": \"Mozilla/5.0\"}\r\n",
    "        )\r\n",
    "        response = urllib.request.urlopen(request)\r\n",
    "        array = np.asarray(bytearray(response.read()), dtype=\"uint8\")\r\n",
    "        image = cv2.imdecode(array, -1)  # Loads the image as BGR\r\n",
    "    else:\r\n",
    "        image = cv2.imread(path)\r\n",
    "    return image\r\n",
    "\r\n",
    "\r\n",
    "def convert_result_to_image(result, colormap=\"viridis\"):\r\n",
    "    \"\"\"\r\n",
    "    Convert network result of floating point numbers to an RGB image with\r\n",
    "    integer values from 0-255 by applying a colormap.\r\n",
    "\r\n",
    "    `result` is expected to be a single network result in 1,H,W shape\r\n",
    "    `colormap` is a matplotlib colormap.\r\n",
    "    See https://matplotlib.org/stable/tutorials/colors/colormaps.html\r\n",
    "    \"\"\"\r\n",
    "    cmap = matplotlib.cm.get_cmap(colormap)\r\n",
    "    result = result.squeeze(0)\r\n",
    "    result = normalize_minmax(result)\r\n",
    "    result = cmap(result)[:, :, :3] * 255\r\n",
    "    result = result.astype(np.uint8)\r\n",
    "    return result\r\n",
    "\r\n",
    "\r\n",
    "def to_rgb(image_data) -> np.ndarray:\r\n",
    "    \"\"\"\r\n",
    "    Convert image_data from BGR to RGB\r\n",
    "    \"\"\"\r\n",
    "    return cv2.cvtColor(image_data, cv2.COLOR_BGR2RGB)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load model and get model information\n",
    "\n",
    "Load the model in Inference Engine with `ie.read_network` and load it to the specified device with `ie.load_network`"
   ],
   "metadata": {
    "id": "sensitive-wagner"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ie = IECore()\r\n",
    "net = ie.read_network(\r\n",
    "    str(model_xml_path), str(model_xml_path.with_suffix(\".bin\"))\r\n",
    ")\r\n",
    "exec_net = ie.load_network(network=net, device_name=DEVICE)\r\n",
    "\r\n",
    "input_key = list(exec_net.input_info)[0]\r\n",
    "output_key = list(exec_net.outputs.keys())[0]\r\n",
    "\r\n",
    "network_input_shape = exec_net.input_info[input_key].tensor_desc.dims\r\n",
    "network_image_height, network_image_width = network_input_shape[2:]"
   ],
   "outputs": [],
   "metadata": {
    "id": "complete-brother"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monodepth on Image\n",
    "\n",
    "### Load, resize and reshape input image\n",
    "\n",
    "The input image is read with OpenCV, resized to network input size, and reshaped to (N,C,H,W) (H=height, W=width, C=number of channels, N=number of images). "
   ],
   "metadata": {
    "id": "compact-bargain"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "IMAGE_FILE = \"data/coco_bike.jpg\"\r\n",
    "image = load_image(IMAGE_FILE)\r\n",
    "# resize to input shape for network\r\n",
    "resized_image = cv2.resize(image, (network_image_height, network_image_width))\r\n",
    "# reshape image to network input shape NCHW\r\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "central-psychology",
    "outputId": "d864ee96-3fbd-488d-da1a-88e730f34aad",
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do inference on image\n",
    "\n",
    "Do the inference, convert the result to an image, and resize it to the original image shape"
   ],
   "metadata": {
    "id": "taken-spanking"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = exec_net.infer(inputs={input_key: input_image})[output_key]\r\n",
    "# convert network result of disparity map to an image that shows\r\n",
    "# distance as colors\r\n",
    "result_image = convert_result_to_image(result)\r\n",
    "# resize back to original image shape. cv2.resize expects shape\r\n",
    "# in (width, height), [::-1] reverses the (height, width) shape to match this.\r\n",
    "result_image = cv2.resize(result_image, image.shape[:2][::-1])"
   ],
   "outputs": [],
   "metadata": {
    "id": "banner-kruger"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display monodepth image"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 15))\r\n",
    "ax[0].imshow(to_rgb(image))\r\n",
    "ax[1].imshow(result_image);"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "ranging-executive",
    "outputId": "30373e8e-34e9-4820-e32d-764aa99d4b25"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Monodepth on Video\n",
    "\n",
    "By default, only the first 100 frames are processed, in order to quickly check that everything works. Change NUM_FRAMES in the cell below to modify this. Set NUM_FRAMES to 0 to process the whole video."
   ],
   "metadata": {
    "id": "descending-cache"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Video Settings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Video source: https://www.youtube.com/watch?v=fu1xcQdJRws (Public Domain)\r\n",
    "VIDEO_FILE = \"data/Coco Walking in Berkeley.mp4\"\r\n",
    "# Number of seconds of input video to process. Set to 0 to process\r\n",
    "# the full video.\r\n",
    "NUM_SECONDS = 4\r\n",
    "# Set ADVANCE_FRAMES to 1 to process every frame from the input video\r\n",
    "# Set ADVANCE_FRAMES to 2 to process every second frame. This reduces\r\n",
    "# the time it takes to process the video\r\n",
    "ADVANCE_FRAMES = 2\r\n",
    "# Set SCALE_OUTPUT to reduce the size of the result video\r\n",
    "# If SCALE_OUTPUT is 0.5, the width and height of the result video\r\n",
    "# will be half the width and height of the input video\r\n",
    "SCALE_OUTPUT = 0.5\r\n",
    "# The format to use for video encoding. vp09 is slow,\r\n",
    "# but it works on most systems.\r\n",
    "# Try the THEO encoding if you have FFMPEG installed.\r\n",
    "# FOURCC = cv2.VideoWriter_fourcc(*\"THEO\")\r\n",
    "FOURCC = cv2.VideoWriter_fourcc(*\"vp09\")\r\n",
    "\r\n",
    "# Create Path objects for the input video and the resulting video\r\n",
    "video_path = Path(VIDEO_FILE)\r\n",
    "result_video_path = video_path.with_name(f\"{video_path.stem}_monodepth.mp4\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "terminal-dividend",
    "outputId": "87f5ada0-8caf-49c3-fe54-626e2b1967f3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Load video\n",
    "\n",
    "Load video from `VIDEO_FILE`, set in the *Video Settings* cell above. Open the video to read the frame width and height and fps, and compute values for these properties for the monodepth video."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cap = cv2.VideoCapture(str(video_path))\r\n",
    "ret, image = cap.read()\r\n",
    "if not ret:\r\n",
    "    raise ValueError(f\"The video at {video_path} cannot be read.\")\r\n",
    "input_fps = cap.get(cv2.CAP_PROP_FPS)\r\n",
    "input_video_frame_height, input_video_frame_width = image.shape[:2]\r\n",
    "\r\n",
    "target_fps = input_fps / ADVANCE_FRAMES\r\n",
    "target_frame_height = int(input_video_frame_height * SCALE_OUTPUT)\r\n",
    "target_frame_width = int(input_video_frame_width * SCALE_OUTPUT)\r\n",
    "\r\n",
    "cap.release()\r\n",
    "print(\r\n",
    "    f\"The input video has a frame width of {input_video_frame_width}, \"\r\n",
    "    f\"frame height of {input_video_frame_height} and runs at {input_fps:.2f} fps\"\r\n",
    ")\r\n",
    "print(\r\n",
    "    \"The monodepth video will be scaled with a factor \"\r\n",
    "    f\"{SCALE_OUTPUT}, have width {target_frame_width}, \"\r\n",
    "    f\" height {target_frame_height}, and run at {target_fps:.2f} fps\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do Inference on video and create monodepth video"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize variables\r\n",
    "input_video_frame_nr = 0  \r\n",
    "start_time = time.perf_counter()\r\n",
    "total_inference_duration = 0\r\n",
    "\r\n",
    "# Open input video\r\n",
    "cap = cv2.VideoCapture(str(video_path))\r\n",
    "\r\n",
    "# Create result video\r\n",
    "out_video = cv2.VideoWriter(\r\n",
    "    str(result_video_path),\r\n",
    "    FOURCC,\r\n",
    "    target_fps,\r\n",
    "    (target_frame_width * 2, target_frame_height),\r\n",
    ")\r\n",
    "\r\n",
    "num_frames = int(NUM_SECONDS * input_fps)\r\n",
    "total_frames = (\r\n",
    "    cap.get(cv2.CAP_PROP_FRAME_COUNT) if num_frames == 0 else num_frames\r\n",
    ")\r\n",
    "progress_bar = ProgressBar(total=total_frames)\r\n",
    "progress_bar.display()\r\n",
    "\r\n",
    "try:\r\n",
    "    while cap.isOpened():\r\n",
    "        ret, image = cap.read()\r\n",
    "        if not ret:\r\n",
    "            cap.release()\r\n",
    "            break\r\n",
    "\r\n",
    "        if input_video_frame_nr >= total_frames:\r\n",
    "            break\r\n",
    "\r\n",
    "        # Only process every second frame\r\n",
    "        # Prepare frame for inference\r\n",
    "        # resize to input shape for network\r\n",
    "        resized_image = cv2.resize(\r\n",
    "            image, (network_image_height, network_image_width)\r\n",
    "        )\r\n",
    "        # reshape image to network input shape NCHW\r\n",
    "        input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)\r\n",
    "\r\n",
    "        # Do inference\r\n",
    "        inference_start_time = time.perf_counter()\r\n",
    "        result = exec_net.infer(inputs={input_key: input_image})[output_key]\r\n",
    "        inference_stop_time = time.perf_counter()\r\n",
    "        inference_duration = inference_stop_time - inference_start_time\r\n",
    "        total_inference_duration += inference_duration\r\n",
    "\r\n",
    "        if input_video_frame_nr % (10 * ADVANCE_FRAMES) == 0:\r\n",
    "            clear_output(wait=True)\r\n",
    "            progress_bar.display()\r\n",
    "            # input_video_frame_nr // ADVANCE_FRAMES gives the number of \r\n",
    "            # frames that have been processed by the network\r\n",
    "            display(\r\n",
    "                Pretty(\r\n",
    "                    f\"Processed frame {input_video_frame_nr // ADVANCE_FRAMES}\"\r\n",
    "                    f\"/{total_frames // ADVANCE_FRAMES}. \"\r\n",
    "                    f\"Inference time: {inference_duration:.2f} seconds \"\r\n",
    "                    f\"({1/inference_duration:.2f} FPS)\"\r\n",
    "                )\r\n",
    "            )\r\n",
    "\r\n",
    "        # Transform network result to RGB image\r\n",
    "        result_frame = to_rgb(convert_result_to_image(result))\r\n",
    "        # Resize image and result to target frame shape\r\n",
    "        result_frame = cv2.resize(\r\n",
    "            result_frame, (target_frame_width, target_frame_height)\r\n",
    "        )\r\n",
    "        image = cv2.resize(image, (target_frame_width, target_frame_height))\r\n",
    "        # Put image and result side by side\r\n",
    "        stacked_frame = np.hstack((image, result_frame))\r\n",
    "        # Save frame to video\r\n",
    "        out_video.write(stacked_frame)\r\n",
    "\r\n",
    "        input_video_frame_nr = input_video_frame_nr + ADVANCE_FRAMES\r\n",
    "        cap.set(1, input_video_frame_nr)\r\n",
    "\r\n",
    "        progress_bar.progress = input_video_frame_nr\r\n",
    "        progress_bar.update()\r\n",
    "\r\n",
    "except KeyboardInterrupt:\r\n",
    "    print(\"Processing interrupted.\")\r\n",
    "finally:\r\n",
    "    clear_output()\r\n",
    "    processed_frames = num_frames // ADVANCE_FRAMES\r\n",
    "    out_video.release()\r\n",
    "    cap.release()\r\n",
    "    end_time = time.perf_counter()\r\n",
    "    duration = end_time - start_time\r\n",
    "\r\n",
    "    print(\r\n",
    "        f\"Processed {processed_frames} frames in {duration:.2f} seconds. \"\r\n",
    "        f\"Total FPS (including video processing): {processed_frames/duration:.2f}.\"\r\n",
    "        f\"Inference FPS: {processed_frames/total_inference_duration:.2f} \"\r\n",
    "    )\r\n",
    "    print(f\"Monodepth Video saved to '{str(result_video_path)}'.\")"
   ],
   "outputs": [],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "present-albany",
    "outputId": "600edb69-af12-44dc-ec8e-95005b74179c",
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Display monodepth video"
   ],
   "metadata": {
    "id": "bZ89ZI369KjA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "video = Video(result_video_path, width=800, embed=True)\r\n",
    "if not result_video_path.exists():\r\n",
    "    plt.imshow(stacked_frame)\r\n",
    "    raise ValueError(\r\n",
    "        \"OpenCV was unable to write the video file. Showing one video frame.\"\r\n",
    "    )\r\n",
    "else:\r\n",
    "    print(f\"Showing monodepth video saved at\\n{result_video_path.resolve()}\")\r\n",
    "    print(\r\n",
    "        \"If you cannot see the video in your browser, please click on the \"\r\n",
    "        \"following link to download the video \"\r\n",
    "    )\r\n",
    "    video_link = FileLink(result_video_path)\r\n",
    "    video_link.html_link_str = \"<a href='%s' download>%s</a>\"\r\n",
    "    display(HTML(video_link._repr_html_()))\r\n",
    "    display(video)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "monodepth.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}