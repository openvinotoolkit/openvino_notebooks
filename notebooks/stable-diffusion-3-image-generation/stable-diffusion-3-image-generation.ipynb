{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b394231-61b2-40a9-938f-c65dd81aa651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q \"diffusers>=0.29.0\" \"gradio\" \"torch>=2.1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f81dd37b-9992-46d2-accc-b4d53a449c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cd17162ce7408eb939e6d0a7011448",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3304356bba3343948cb928cd34de8c00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\")\n",
    "\n",
    "image = pipe(\n",
    "    \"A cat holding a sign that says hello world\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.0,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "321b0d62-53e3-4914-97f9-d91189ba3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "from functools import partial\n",
    "\n",
    "pipe.transformer.forward = partial(pipe.transformer.forward, return_dict=False)\n",
    "\n",
    "ov_model = ov.convert_model(pipe.transformer, example_input={\"hidden_states\": torch.zeros((2, 16, 64, 64)), \"timestep\": torch.tensor([1, 1]), \"encoder_hidden_states\": torch.ones([2, 154, 4096]), \"pooled_projections\": torch.ones([2, 2048])})\n",
    "\n",
    "ov.save_model(ov_model, \"transformer.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96ce5aaa-56a9-42b9-883f-a019cc54da3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = ov.convert_model(pipe.text_encoder_3, example_input=torch.ones([1, 77], dtype=torch.long))\n",
    "\n",
    "ov.save_model(ov_model, \"text_encoder_3.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68aeba61-1e80-4509-b297-bb04a8b77fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.text_encoder.forward = partial(pipe.text_encoder.forward, output_hidden_states=True, return_dict=False)\n",
    "\n",
    "ov_model = ov.convert_model(pipe.text_encoder, example_input=torch.ones([1, 77], dtype=torch.long))\n",
    "\n",
    "ov.save_model(ov_model, \"text_encoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "238f9deb-b3d2-4040-9c18-4cc50d68aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.text_encoder_2.forward = partial(pipe.text_encoder_2.forward, output_hidden_states=True, return_dict=False)\n",
    "\n",
    "ov_model = ov.convert_model(pipe.text_encoder_2, example_input=torch.ones([1, 77], dtype=torch.long))\n",
    "\n",
    "ov.save_model(ov_model, \"text_encoder_2.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5ed8e2f-1cbd-4fa3-a019-418961f74787",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/diffusers/models/upsampling.py:146: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert hidden_states.shape[1] == self.channels\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/diffusers/models/upsampling.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if hidden_states.shape[0] >= 64:\n"
     ]
    }
   ],
   "source": [
    "pipe.vae.forward = pipe.vae.decode\n",
    "\n",
    "ov_model = ov.convert_model(pipe.vae, example_input=torch.ones([1, 16, 64, 64]))\n",
    "\n",
    "ov.save_model(ov_model, \"vae_decoder.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fac6f80-7f18-4391-82d4-c51e75f10e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from typing import Any, Callable, Dict, List, Optional, Union\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    CLIPTextModelWithProjection,\n",
    "    CLIPTokenizer,\n",
    "    T5EncoderModel,\n",
    "    T5TokenizerFast,\n",
    ")\n",
    "\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "from diffusers.models.autoencoders import AutoencoderKL\n",
    "from diffusers.models.transformers import SD3Transformer2DModel\n",
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from diffusers.utils import (\n",
    "    logging,\n",
    ")\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from diffusers.pipelines.pipeline_utils import DiffusionPipeline\n",
    "from diffusers.pipelines.stable_diffusion_3.pipeline_output import StableDiffusion3PipelineOutput\n",
    "\n",
    "\n",
    "logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "\n",
    "# Copied from diffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.retrieve_timesteps\n",
    "def retrieve_timesteps(\n",
    "    scheduler,\n",
    "    num_inference_steps: Optional[int] = None,\n",
    "    device: Optional[Union[str, torch.device]] = None,\n",
    "    timesteps: Optional[List[int]] = None,\n",
    "    sigmas: Optional[List[float]] = None,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Calls the scheduler's `set_timesteps` method and retrieves timesteps from the scheduler after the call. Handles\n",
    "    custom timesteps. Any kwargs will be supplied to `scheduler.set_timesteps`.\n",
    "\n",
    "    Args:\n",
    "        scheduler (`SchedulerMixin`):\n",
    "            The scheduler to get timesteps from.\n",
    "        num_inference_steps (`int`):\n",
    "            The number of diffusion steps used when generating samples with a pre-trained model. If used, `timesteps`\n",
    "            must be `None`.\n",
    "        device (`str` or `torch.device`, *optional*):\n",
    "            The device to which the timesteps should be moved to. If `None`, the timesteps are not moved.\n",
    "        timesteps (`List[int]`, *optional*):\n",
    "            Custom timesteps used to override the timestep spacing strategy of the scheduler. If `timesteps` is passed,\n",
    "            `num_inference_steps` and `sigmas` must be `None`.\n",
    "        sigmas (`List[float]`, *optional*):\n",
    "            Custom sigmas used to override the timestep spacing strategy of the scheduler. If `sigmas` is passed,\n",
    "            `num_inference_steps` and `timesteps` must be `None`.\n",
    "\n",
    "    Returns:\n",
    "        `Tuple[torch.Tensor, int]`: A tuple where the first element is the timestep schedule from the scheduler and the\n",
    "        second element is the number of inference steps.\n",
    "    \"\"\"\n",
    "    if timesteps is not None and sigmas is not None:\n",
    "        raise ValueError(\"Only one of `timesteps` or `sigmas` can be passed. Please choose one to set custom values\")\n",
    "    if timesteps is not None:\n",
    "        accepts_timesteps = \"timesteps\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accepts_timesteps:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" timestep schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(timesteps=timesteps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    elif sigmas is not None:\n",
    "        accept_sigmas = \"sigmas\" in set(inspect.signature(scheduler.set_timesteps).parameters.keys())\n",
    "        if not accept_sigmas:\n",
    "            raise ValueError(\n",
    "                f\"The current scheduler class {scheduler.__class__}'s `set_timesteps` does not support custom\"\n",
    "                f\" sigmas schedules. Please check whether you are using the correct scheduler.\"\n",
    "            )\n",
    "        scheduler.set_timesteps(sigmas=sigmas, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "        num_inference_steps = len(timesteps)\n",
    "    else:\n",
    "        scheduler.set_timesteps(num_inference_steps, device=device, **kwargs)\n",
    "        timesteps = scheduler.timesteps\n",
    "    return timesteps, num_inference_steps\n",
    "\n",
    "\n",
    "class OVStableDiffusion3Pipeline(DiffusionPipeline):\n",
    "    r\"\"\"\n",
    "    Args:\n",
    "        transformer ([`SD3Transformer2DModel`]):\n",
    "            Conditional Transformer (MMDiT) architecture to denoise the encoded image latents.\n",
    "        scheduler ([`FlowMatchEulerDiscreteScheduler`]):\n",
    "            A scheduler to be used in combination with `transformer` to denoise the encoded image latents.\n",
    "        vae ([`AutoencoderKL`]):\n",
    "            Variational Auto-Encoder (VAE) Model to encode and decode images to and from latent representations.\n",
    "        text_encoder ([`CLIPTextModelWithProjection`]):\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),\n",
    "            specifically the [clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) variant,\n",
    "            with an additional added projection layer that is initialized with a diagonal matrix with the `hidden_size`\n",
    "            as its dimension.\n",
    "        text_encoder_2 ([`CLIPTextModelWithProjection`]):\n",
    "            [CLIP](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModelWithProjection),\n",
    "            specifically the\n",
    "            [laion/CLIP-ViT-bigG-14-laion2B-39B-b160k](https://huggingface.co/laion/CLIP-ViT-bigG-14-laion2B-39B-b160k)\n",
    "            variant.\n",
    "        text_encoder_3 ([`T5EncoderModel`]):\n",
    "            Frozen text-encoder. Stable Diffusion 3 uses\n",
    "            [T5](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5EncoderModel), specifically the\n",
    "            [t5-v1_1-xxl](https://huggingface.co/google/t5-v1_1-xxl) variant.\n",
    "        tokenizer (`CLIPTokenizer`):\n",
    "            Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        tokenizer_2 (`CLIPTokenizer`):\n",
    "            Second Tokenizer of class\n",
    "            [CLIPTokenizer](https://huggingface.co/docs/transformers/v4.21.0/en/model_doc/clip#transformers.CLIPTokenizer).\n",
    "        tokenizer_3 (`T5TokenizerFast`):\n",
    "            Tokenizer of class\n",
    "            [T5Tokenizer](https://huggingface.co/docs/transformers/model_doc/t5#transformers.T5Tokenizer).\n",
    "    \"\"\"\n",
    "    _optional_components = []\n",
    "    _callback_tensor_inputs = [\"latents\", \"prompt_embeds\", \"negative_prompt_embeds\", \"negative_pooled_prompt_embeds\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transformer: SD3Transformer2DModel,\n",
    "        scheduler: FlowMatchEulerDiscreteScheduler,\n",
    "        vae: AutoencoderKL,\n",
    "        text_encoder: CLIPTextModelWithProjection,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        text_encoder_2: CLIPTextModelWithProjection,\n",
    "        tokenizer_2: CLIPTokenizer,\n",
    "        text_encoder_3: T5EncoderModel,\n",
    "        tokenizer_3: T5TokenizerFast,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_modules(\n",
    "            vae=vae,\n",
    "            text_encoder=text_encoder,\n",
    "            text_encoder_2=text_encoder_2,\n",
    "            text_encoder_3=text_encoder_3,\n",
    "            tokenizer=tokenizer,\n",
    "            tokenizer_2=tokenizer_2,\n",
    "            tokenizer_3=tokenizer_3,\n",
    "            transformer=transformer,\n",
    "            scheduler=scheduler,\n",
    "        )\n",
    "        self.vae_scale_factor = (\n",
    "            2 ** 3\n",
    "        )\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
    "        self.tokenizer_max_length = (\n",
    "            self.tokenizer.model_max_length if hasattr(self, \"tokenizer\") and self.tokenizer is not None else 77\n",
    "        )\n",
    "        self.vae_scaling_factor = 1.5305\n",
    "        self.vae_shift_factor = 0.0609\n",
    "        self.default_sample_size = 64\n",
    "\n",
    "    def _get_t5_prompt_embeds(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        num_images_per_prompt: int = 1,\n",
    "    ):\n",
    "\n",
    "        prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "        batch_size = len(prompt)\n",
    "\n",
    "        if self.text_encoder_3 is None:\n",
    "            return torch.zeros(\n",
    "                (batch_size, self.tokenizer_max_length, 4096),\n",
    "            )\n",
    "\n",
    "        text_inputs = self.tokenizer_3(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer_max_length,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        prompt_embeds = torch.from_numpy(self.text_encoder_3(text_input_ids)[0])\n",
    "        _, seq_len, _ = prompt_embeds.shape\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        return prompt_embeds\n",
    "\n",
    "    def _get_clip_prompt_embeds(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]],\n",
    "        num_images_per_prompt: int = 1,\n",
    "        clip_skip: Optional[int] = None,\n",
    "        clip_model_index: int = 0,\n",
    "    ):\n",
    "\n",
    "        clip_tokenizers = [self.tokenizer, self.tokenizer_2]\n",
    "        clip_text_encoders = [self.text_encoder, self.text_encoder_2]\n",
    "\n",
    "        tokenizer = clip_tokenizers[clip_model_index]\n",
    "        text_encoder = clip_text_encoders[clip_model_index]\n",
    "\n",
    "        prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "        batch_size = len(prompt)\n",
    "\n",
    "        text_inputs = tokenizer(\n",
    "            prompt,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer_max_length,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\")\n",
    "\n",
    "        text_input_ids = text_inputs.input_ids\n",
    "        prompt_embeds = text_encoder(text_input_ids)\n",
    "        pooled_prompt_embeds = torch.from_numpy(prompt_embeds[0])\n",
    "        hidden_states = list(prompt_embeds.values())[1:]\n",
    "\n",
    "        if clip_skip is None:\n",
    "            prompt_embeds = torch.from_numpy(hidden_states[-2])\n",
    "        else:\n",
    "            prompt_embeds = torch.from_numpy(hidden_states[-(clip_skip + 2)])\n",
    "\n",
    "        _, seq_len, _ = prompt_embeds.shape\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(batch_size * num_images_per_prompt, seq_len, -1)\n",
    "\n",
    "        pooled_prompt_embeds = pooled_prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        pooled_prompt_embeds = pooled_prompt_embeds.view(batch_size * num_images_per_prompt, -1)\n",
    "\n",
    "        return prompt_embeds, pooled_prompt_embeds\n",
    "\n",
    "    def encode_prompt(\n",
    "            self,\n",
    "            prompt: Union[str, List[str]],\n",
    "            prompt_2: Union[str, List[str]],\n",
    "            prompt_3: Union[str, List[str]],\n",
    "            num_images_per_prompt: int = 1,\n",
    "            do_classifier_free_guidance: bool = True,\n",
    "            negative_prompt: Optional[Union[str, List[str]]] = None,\n",
    "            negative_prompt_2: Optional[Union[str, List[str]]] = None,\n",
    "            negative_prompt_3: Optional[Union[str, List[str]]] = None,\n",
    "            prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "            negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "            pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "            negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "            clip_skip: Optional[int] = None,\n",
    "    ):\n",
    "\n",
    "        prompt = [prompt] if isinstance(prompt, str) else prompt\n",
    "        if prompt is not None:\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        if prompt_embeds is None:\n",
    "            prompt_2 = prompt_2 or prompt\n",
    "            prompt_2 = [prompt_2] if isinstance(prompt_2, str) else prompt_2\n",
    "\n",
    "            prompt_3 = prompt_3 or prompt\n",
    "            prompt_3 = [prompt_3] if isinstance(prompt_3, str) else prompt_3\n",
    "\n",
    "            prompt_embed, pooled_prompt_embed = self._get_clip_prompt_embeds(\n",
    "                prompt=prompt,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "                clip_skip=clip_skip,\n",
    "                clip_model_index=0,)\n",
    "            prompt_2_embed, pooled_prompt_2_embed = self._get_clip_prompt_embeds(\n",
    "                prompt=prompt_2,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "                clip_skip=clip_skip,\n",
    "                clip_model_index=1,)\n",
    "            clip_prompt_embeds = torch.cat([prompt_embed, prompt_2_embed], dim=-1)\n",
    "\n",
    "            t5_prompt_embed = self._get_t5_prompt_embeds(\n",
    "                prompt=prompt_3,\n",
    "                num_images_per_prompt=num_images_per_prompt,)\n",
    "\n",
    "            clip_prompt_embeds = torch.nn.functional.pad(\n",
    "                clip_prompt_embeds, (0, t5_prompt_embed.shape[-1] - clip_prompt_embeds.shape[-1]))\n",
    "\n",
    "            prompt_embeds = torch.cat([clip_prompt_embeds, t5_prompt_embed], dim=-2)\n",
    "            pooled_prompt_embeds = torch.cat([pooled_prompt_embed, pooled_prompt_2_embed], dim=-1)\n",
    "\n",
    "        if do_classifier_free_guidance and negative_prompt_embeds is None:\n",
    "            negative_prompt = negative_prompt or \"\"\n",
    "            negative_prompt_2 = negative_prompt_2 or negative_prompt\n",
    "            negative_prompt_3 = negative_prompt_3 or negative_prompt\n",
    "\n",
    "            # normalize str to list\n",
    "            negative_prompt = batch_size * [negative_prompt] if isinstance(negative_prompt, str) else negative_prompt\n",
    "            negative_prompt_2 = (\n",
    "                batch_size * [negative_prompt_2] if isinstance(negative_prompt_2, str) else negative_prompt_2)\n",
    "            negative_prompt_3 = (\n",
    "                batch_size * [negative_prompt_3] if isinstance(negative_prompt_3, str) else negative_prompt_3)\n",
    "\n",
    "            if prompt is not None and type(prompt) is not type(negative_prompt):\n",
    "                raise TypeError(\n",
    "                    f\"`negative_prompt` should be the same type to `prompt`, but got {type(negative_prompt)} !=\"\n",
    "                    f\" {type(prompt)}.\")\n",
    "            elif batch_size != len(negative_prompt):\n",
    "                raise ValueError(\n",
    "                    f\"`negative_prompt`: {negative_prompt} has batch size {len(negative_prompt)}, but `prompt`:\"\n",
    "                    f\" {prompt} has batch size {batch_size}. Please make sure that passed `negative_prompt` matches\"\n",
    "                    \" the batch size of `prompt`.\")\n",
    "\n",
    "            negative_prompt_embed, negative_pooled_prompt_embed = self._get_clip_prompt_embeds(\n",
    "                negative_prompt,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "                clip_skip=None,\n",
    "                clip_model_index=0,)\n",
    "            negative_prompt_2_embed, negative_pooled_prompt_2_embed = self._get_clip_prompt_embeds(\n",
    "                negative_prompt_2,\n",
    "                num_images_per_prompt=num_images_per_prompt,\n",
    "                clip_skip=None,\n",
    "                clip_model_index=1,)\n",
    "            negative_clip_prompt_embeds = torch.cat([negative_prompt_embed, negative_prompt_2_embed], dim=-1)\n",
    "\n",
    "            t5_negative_prompt_embed = self._get_t5_prompt_embeds(prompt=negative_prompt_3, num_images_per_prompt=num_images_per_prompt)\n",
    "\n",
    "            negative_clip_prompt_embeds = torch.nn.functional.pad(\n",
    "                negative_clip_prompt_embeds,\n",
    "                (0, t5_negative_prompt_embed.shape[-1] - negative_clip_prompt_embeds.shape[-1]),\n",
    "                )\n",
    "\n",
    "            negative_prompt_embeds = torch.cat([negative_clip_prompt_embeds, t5_negative_prompt_embed], dim=-2)\n",
    "            negative_pooled_prompt_embeds = torch.cat([negative_pooled_prompt_embed, negative_pooled_prompt_2_embed], dim=-1)\n",
    "\n",
    "        return prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds\n",
    "    \n",
    "    def check_inputs(self, prompt, prompt_2, prompt_3, height, width, negative_prompt=None, negative_prompt_2=None, negative_prompt_3=None, prompt_embeds=None, negative_prompt_embeds=None, pooled_prompt_embeds=None, negative_pooled_prompt_embeds=None, callback_on_step_end_tensor_inputs=None):\n",
    "        if height % 8 != 0 or width % 8 != 0:\n",
    "            raise ValueError(f\"`height` and `width` have to be divisible by 8 but are {height} and {width}.\")\n",
    "\n",
    "        if callback_on_step_end_tensor_inputs is not None and not all(k in self._callback_tensor_inputs for k in callback_on_step_end_tensor_inputs):\n",
    "            raise ValueError(\n",
    "                f\"`callback_on_step_end_tensor_inputs` has to be in {self._callback_tensor_inputs}, but found {[k for k in callback_on_step_end_tensor_inputs if k not in self._callback_tensor_inputs]}\")\n",
    "\n",
    "        if prompt is not None and prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `prompt`: {prompt} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n",
    "                \" only forward one of the two.\")\n",
    "        elif prompt_2 is not None and prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `prompt_2`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n",
    "                \" only forward one of the two.\")\n",
    "        elif prompt_3 is not None and prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `prompt_3`: {prompt_2} and `prompt_embeds`: {prompt_embeds}. Please make sure to\"\n",
    "                \" only forward one of the two.\")\n",
    "        elif prompt is None and prompt_embeds is None:\n",
    "            raise ValueError(\n",
    "                \"Provide either `prompt` or `prompt_embeds`. Cannot leave both `prompt` and `prompt_embeds` undefined.\")\n",
    "        elif prompt is not None and (not isinstance(prompt, str) and not isinstance(prompt, list)):\n",
    "            raise ValueError(f\"`prompt` has to be of type `str` or `list` but is {type(prompt)}\")\n",
    "        elif prompt_2 is not None and (not isinstance(prompt_2, str) and not isinstance(prompt_2, list)):\n",
    "            raise ValueError(f\"`prompt_2` has to be of type `str` or `list` but is {type(prompt_2)}\")\n",
    "        elif prompt_3 is not None and (not isinstance(prompt_3, str) and not isinstance(prompt_3, list)):\n",
    "            raise ValueError(f\"`prompt_3` has to be of type `str` or `list` but is {type(prompt_3)}\")\n",
    "\n",
    "        if negative_prompt is not None and negative_prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `negative_prompt`: {negative_prompt} and `negative_prompt_embeds`:\"\n",
    "                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\")\n",
    "        elif negative_prompt_2 is not None and negative_prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `negative_prompt_2`: {negative_prompt_2} and `negative_prompt_embeds`:\"\n",
    "                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\")\n",
    "        elif negative_prompt_3 is not None and negative_prompt_embeds is not None:\n",
    "            raise ValueError(\n",
    "                f\"Cannot forward both `negative_prompt_3`: {negative_prompt_3} and `negative_prompt_embeds`:\"\n",
    "                f\" {negative_prompt_embeds}. Please make sure to only forward one of the two.\")\n",
    "\n",
    "        if prompt_embeds is not None and negative_prompt_embeds is not None:\n",
    "            if prompt_embeds.shape != negative_prompt_embeds.shape:\n",
    "                raise ValueError(\n",
    "                    \"`prompt_embeds` and `negative_prompt_embeds` must have the same shape when passed directly, but\"\n",
    "                    f\" got: `prompt_embeds` {prompt_embeds.shape} != `negative_prompt_embeds`\"\n",
    "                    f\" {negative_prompt_embeds.shape}.\")\n",
    "\n",
    "        if prompt_embeds is not None and pooled_prompt_embeds is None:\n",
    "            raise ValueError(\n",
    "                \"If `prompt_embeds` are provided, `pooled_prompt_embeds` also have to be passed. Make sure to generate `pooled_prompt_embeds` from the same text encoder that was used to generate `prompt_embeds`.\")\n",
    "\n",
    "        if negative_prompt_embeds is not None and negative_pooled_prompt_embeds is None:\n",
    "            raise ValueError(\n",
    "                \"If `negative_prompt_embeds` are provided, `negative_pooled_prompt_embeds` also have to be passed. Make sure to generate `negative_pooled_prompt_embeds` from the same text encoder that was used to generate `negative_prompt_embeds`.\")\n",
    "\n",
    "    def prepare_latents(self, batch_size, num_channels_latents, height, width, generator, latents=None):\n",
    "        if latents is not None:\n",
    "            return latents\n",
    "\n",
    "        shape = (batch_size, num_channels_latents, int(height) // self.vae_scale_factor, int(width) // self.vae_scale_factor)\n",
    "\n",
    "        if isinstance(generator, list) and len(generator) != batch_size:\n",
    "            raise ValueError(\n",
    "                f\"You have passed a list of generators of length {len(generator)}, but requested an effective batch\"\n",
    "                f\" size of {batch_size}. Make sure the batch size matches the length of the generators.\")\n",
    "\n",
    "        latents = randn_tensor(shape, generator=generator, device=torch.device(\"cpu\"), dtype=torch.float32)\n",
    "\n",
    "        return latents\n",
    "\n",
    "    @property\n",
    "    def guidance_scale(self):\n",
    "        return self._guidance_scale\n",
    "\n",
    "    @property\n",
    "    def clip_skip(self):\n",
    "        return self._clip_skip\n",
    "\n",
    "    # here `guidance_scale` is defined analog to the guidance weight `w` of equation (2)\n",
    "    # of the Imagen paper: https://arxiv.org/pdf/2205.11487.pdf . `guidance_scale = 1`\n",
    "    # corresponds to doing no classifier free guidance.\n",
    "    @property\n",
    "    def do_classifier_free_guidance(self):\n",
    "        return self._guidance_scale > 1\n",
    "\n",
    "    @property\n",
    "    def joint_attention_kwargs(self):\n",
    "        return self._joint_attention_kwargs\n",
    "\n",
    "    @property\n",
    "    def num_timesteps(self):\n",
    "        return self._num_timesteps\n",
    "\n",
    "    @property\n",
    "    def interrupt(self):\n",
    "        return self._interrupt\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, prompt: Union[str, List[str]] = None, prompt_2: Optional[Union[str, List[str]]] = None, prompt_3: Optional[Union[str, List[str]]] = None, height: Optional[int] = None, width: Optional[int] = None, num_inference_steps: int = 28, timesteps: List[int] = None, guidance_scale: float = 7.0, negative_prompt: Optional[Union[str, List[str]]] = None, negative_prompt_2: Optional[Union[str, List[str]]] = None, negative_prompt_3: Optional[Union[str, List[str]]] = None, num_images_per_prompt: Optional[int] = 1,\n",
    "                 generator: Optional[Union[torch.Generator, List[torch.Generator]]] = None,\n",
    "                 latents: Optional[torch.FloatTensor] = None, prompt_embeds: Optional[torch.FloatTensor] = None, negative_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "                 pooled_prompt_embeds: Optional[torch.FloatTensor] = None, negative_pooled_prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "                 output_type: Optional[str] = \"pil\", return_dict: bool = True, clip_skip: Optional[int] = None, callback_on_step_end: Optional[Callable[[int, int, Dict], None]] = None, callback_on_step_end_tensor_inputs: List[str] = [\"latents\"]):\n",
    "\n",
    "        height = height or self.default_sample_size * self.vae_scale_factor\n",
    "        width = width or self.default_sample_size * self.vae_scale_factor\n",
    "\n",
    "        # 1. Check inputs. Raise error if not correct\n",
    "        self.check_inputs(prompt, prompt_2, prompt_3, height, width,\n",
    "                          negative_prompt=negative_prompt, negative_prompt_2=negative_prompt_2,\n",
    "                          negative_prompt_3=negative_prompt_3, prompt_embeds=prompt_embeds,\n",
    "                          negative_prompt_embeds=negative_prompt_embeds, pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                          negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                          callback_on_step_end_tensor_inputs=callback_on_step_end_tensor_inputs)\n",
    "\n",
    "        self._guidance_scale = guidance_scale\n",
    "        self._clip_skip = clip_skip\n",
    "        self._interrupt = False\n",
    "\n",
    "        # 2. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "        results = self.encode_prompt(prompt=prompt, prompt_2=prompt_2, prompt_3=prompt_3,\n",
    "                                     negative_prompt=negative_prompt, negative_prompt_2=negative_prompt_2,\n",
    "                                     negative_prompt_3=negative_prompt_3,\n",
    "                                     do_classifier_free_guidance=self.do_classifier_free_guidance,\n",
    "                                     prompt_embeds=prompt_embeds,\n",
    "                                     negative_prompt_embeds=negative_prompt_embeds,\n",
    "                                     pooled_prompt_embeds=pooled_prompt_embeds,\n",
    "                                     negative_pooled_prompt_embeds=negative_pooled_prompt_embeds,\n",
    "                                     clip_skip=self.clip_skip, num_images_per_prompt=num_images_per_prompt)\n",
    "        \n",
    "        (prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds) = results\n",
    "\n",
    "        if self.do_classifier_free_guidance:\n",
    "            prompt_embeds = torch.cat([negative_prompt_embeds, prompt_embeds], dim=0)\n",
    "            pooled_prompt_embeds = torch.cat([negative_pooled_prompt_embeds, pooled_prompt_embeds], dim=0)\n",
    "\n",
    "        # 4. Prepare timesteps\n",
    "        timesteps, num_inference_steps = retrieve_timesteps(self.scheduler, num_inference_steps, timesteps)\n",
    "        num_warmup_steps = max(len(timesteps) - num_inference_steps * self.scheduler.order, 0)\n",
    "        self._num_timesteps = len(timesteps)\n",
    "\n",
    "        # 5. Prepare latent variables\n",
    "        num_channels_latents = 16\n",
    "        latents = self.prepare_latents(batch_size * num_images_per_prompt, num_channels_latents, height, width, generator, latents)\n",
    "\n",
    "        # 6. Denoising loop\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "                if self.interrupt:\n",
    "                    continue\n",
    "\n",
    "                # expand the latents if we are doing classifier free guidance\n",
    "                latent_model_input = torch.cat([latents] * 2) if self.do_classifier_free_guidance else latents\n",
    "                # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "                timestep = t.expand(latent_model_input.shape[0])\n",
    "\n",
    "                noise_pred = self.transformer([latent_model_input, prompt_embeds, pooled_prompt_embeds, timestep])[0]\n",
    "\n",
    "                noise_pred = torch.from_numpy(noise_pred)\n",
    "\n",
    "                # perform guidance\n",
    "                if self.do_classifier_free_guidance:\n",
    "                    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "                    noise_pred = noise_pred_uncond + self.guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents = self.scheduler.step(noise_pred, t, latents, return_dict=False)[0]\n",
    "\n",
    "                if callback_on_step_end is not None:\n",
    "                    callback_kwargs = {}\n",
    "                    for k in callback_on_step_end_tensor_inputs:\n",
    "                        callback_kwargs[k] = locals()[k]\n",
    "                    callback_outputs = callback_on_step_end(self, i, t, callback_kwargs)\n",
    "\n",
    "                    latents = callback_outputs.pop(\"latents\", latents)\n",
    "                    prompt_embeds = callback_outputs.pop(\"prompt_embeds\", prompt_embeds)\n",
    "                    negative_prompt_embeds = callback_outputs.pop(\"negative_prompt_embeds\", negative_prompt_embeds)\n",
    "                    negative_pooled_prompt_embeds = callback_outputs.pop(\"negative_pooled_prompt_embeds\", negative_pooled_prompt_embeds)\n",
    "\n",
    "                # call the callback, if provided\n",
    "                if i == len(timesteps) - 1 or ((i + 1) > num_warmup_steps and (i + 1) % self.scheduler.order == 0):\n",
    "                    progress_bar.update()\n",
    "\n",
    "        if output_type == \"latent\":\n",
    "            image = latents\n",
    "\n",
    "        else:\n",
    "            latents = (latents / self.vae_scaling_factor) + self.vae_shift_factor\n",
    "\n",
    "            image = torch.from_numpy(self.vae(latents)[0])\n",
    "            image = self.image_processor.postprocess(image, output_type=output_type)\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image,)\n",
    "\n",
    "        return StableDiffusion3PipelineOutput(images=image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f274e72b-5bf9-449a-a152-d6c6c970fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "849b4849-eb36-46c8-bec9-e854ab7170fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = core.compile_model(\"transformer.xml\")\n",
    "vae = core.compile_model(\"vae_decoder.xml\")\n",
    "text_encoder = core.compile_model(\"text_encoder.xml\")\n",
    "text_encoder_2 = core.compile_model(\"text_encoder_2.xml\")\n",
    "text_encoder_3 = core.compile_model(\"text_encoder_3.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2d383fb-89af-4f7d-b7a3-c51c178206bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer_3/tokenizer_config.json',\n",
       " 'tokenizer_3/special_tokens_map.json',\n",
       " 'tokenizer_3/spiece.model',\n",
       " 'tokenizer_3/added_tokens.json',\n",
       " 'tokenizer_3/tokenizer.json')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipe.scheduler.save_pretrained(\"scheduler\")\n",
    "# pipe.tokenizer.save_pretrained(\"tokenizer\")\n",
    "# pipe.tokenizer_2.save_pretrained(\"tokenizer_2\")\n",
    "# pipe.tokenizer_3.save_pretrained(\"tokenizer_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "54fdd66f-e0f6-4745-bdaa-45c1d43ad704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25239"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import gc\n",
    "\n",
    "# del pipe\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9886f548-d81e-4de3-b54b-9de10e9b19c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "from diffusers.schedulers import FlowMatchEulerDiscreteScheduler\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\"scheduler\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer\")\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(\"tokenizer_2\")\n",
    "tokenizer_3 = AutoTokenizer.from_pretrained(\"tokenizer_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4541bcc4-f207-4c49-988d-e99396fabd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_pipe = OVStableDiffusion3Pipeline(transformer, scheduler, vae, text_encoder, tokenizer, text_encoder_2, tokenizer_2, text_encoder_3, tokenizer_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdb8b0f-b2fb-4f70-b0e1-3ecb06e30c7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f754579f2df04361a9cee1c45f031658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = ov_pipe(\n",
    "    \"A cat holding a sign that says hello world\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.0,\n",
    "    height=800,\n",
    "    width=800\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b013c-f054-4dbf-8221-7ef66b2de3fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0adaf90f6f0a45a3a803db4250acbeb9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "24e2414bad504bd9ba893fcb6ef808f7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "290cb24f07814effb3236202ef17c22f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "31d632734483458993a4fbe37357889b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "layout": "IPY_MODEL_95c3cdef713940679936d3f6601eb980",
       "max": 28,
       "style": "IPY_MODEL_c7c88dc481364cb7aa3f3652d724b808",
       "value": 13
      }
     },
     "41861743e9dc4614aab24cd08d5feb8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_290cb24f07814effb3236202ef17c22f",
       "style": "IPY_MODEL_24e2414bad504bd9ba893fcb6ef808f7",
       "value": " 13/28 [01:49&lt;02:02,  8.14s/it]"
      }
     },
     "5feb7939f58141d3bc1dc5239f509fa0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_0adaf90f6f0a45a3a803db4250acbeb9",
       "style": "IPY_MODEL_de1ab8f893ff4e6394acb93e11cb47c0",
       "value": " 46%"
      }
     },
     "7e44ffc3518f49a3a71828e47f659691": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "95c3cdef713940679936d3f6601eb980": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c7c88dc481364cb7aa3f3652d724b808": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "de1ab8f893ff4e6394acb93e11cb47c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f754579f2df04361a9cee1c45f031658": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_5feb7939f58141d3bc1dc5239f509fa0",
        "IPY_MODEL_31d632734483458993a4fbe37357889b",
        "IPY_MODEL_41861743e9dc4614aab24cd08d5feb8c"
       ],
       "layout": "IPY_MODEL_7e44ffc3518f49a3a71828e47f659691"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
