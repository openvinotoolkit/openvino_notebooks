{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Optical Character Recognition\n",
    "\n",
    "In this tutorial optical character recognition is presented. This notebook is continuation of [004-hello-detection](../004-hello-detection) notebook.\n",
    "\n",
    "Now in addition of previously used [horizontal-text-detection-0001](https://docs.openvinotoolkit.org/latest/omz_models_model_horizontal_text_detection_0001.html), [text-recognition-resnet](https://docs.openvinotoolkit.org/latest/omz_models_model_text_recognition_resnet_fc.html) is used."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports modules required to run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "from shutil import copy\n",
    "from os import path, makedirs, listdir"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ie = IECore()\n",
    "\n",
    "model_folder = \"model\"\n",
    "download_folder = \"output\"\n",
    "\n",
    "precision = \"FP16\"\n",
    "detection_model_name = \"horizontal-text-detection-0001\"\n",
    "recognition_model_name = \"text-recognition-resnet-fc\"\n",
    "model_extensions = (\"bin\", \"xml\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download models and convert public model\n",
    "\n",
    "If it is your first run models will download and convert here. It might take up to ten minutes. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "makedirs(model_folder, exist_ok=True)\n",
    "makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Check if models are already downloaded in download directory\n",
    "try:\n",
    "    for model_name, folder_name in ((detection_model_name, f'intel/{detection_model_name}'), (recognition_model_name, f'public/{recognition_model_name}')):\n",
    "        for extension in model_extensions:\n",
    "            if not path.isfile(f'{download_folder}/{folder_name}/{precision}/{model_name}.{extension}'):\n",
    "                raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    download_command = f\"omz_downloader --name {detection_model_name},{recognition_model_name} --output_dir {download_folder} --precision {precision}\"\n",
    "    convert_command = f\"omz_converter --name {recognition_model_name} --precisions {precision} --download_dir {download_folder} --output_dir {download_folder}\"\n",
    "    # Run commands, first download model than convert it to inferable \n",
    "    ! $download_command\n",
    "    # Models are downloaded straight to output folder, we will keep all not used files outside of models directory\n",
    "    ! $convert_command"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Copy models to model folder\n",
    "\n",
    "At this point both models are kept in download_folder (by default named 'output'). We need only .bin and .xml files from there that we will copy to model directory."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for text_detection_model in listdir(f\"{download_folder}/intel/{detection_model_name}/{precision}\"):\n",
    "    copy(src=f\"{download_folder}/intel/{detection_model_name}/{precision}/{text_detection_model}\", dst=model_folder)\n",
    "\n",
    "for text_recognition_model in listdir(f\"{download_folder}/public/{recognition_model_name}/{precision}\"):\n",
    "    copy(src=f\"{download_folder}/public/{recognition_model_name}/{precision}/{text_recognition_model}\", dst=model_folder)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "net = ie.read_network(\n",
    "    model=f\"{model_folder}/{detection_model_name}.xml\"\n",
    ")\n",
    "exec_net = ie.load_network(net, \"CPU\")\n",
    "\n",
    "input_layer_ir = next(iter(exec_net.input_info))"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load an Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Text detection models expects image in BGR format\n",
    "image = cv2.imread(\"data/intel_rnb.jpg\")\n",
    "\n",
    "# N,C,H,W = batch size, number of channels, height, width\n",
    "N, C, H, W = net.input_info[input_layer_ir].tensor_desc.dims\n",
    "\n",
    "# Resize image to meet network expected input sizes\n",
    "resized_image = cv2.resize(image, (W, H))\n",
    "\n",
    "# Reshape to network input shape\n",
    "input_image = np.expand_dims(\n",
    "    resized_image.transpose(2, 0, 1), 0\n",
    ")\n",
    "\n",
    "plt.imshow(image)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get boxes\n",
    "\n",
    "It detects texts in images and returns blob of data in shape of [100, 5]. For each detection description has format [x_min, y_min, x_max, y_max, conf]."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = exec_net.infer(inputs={input_layer_ir: input_image})\n",
    "\n",
    "# Extract list of boxes from results\n",
    "boxes = result['boxes']\n",
    "\n",
    "# Remove zero only boxes\n",
    "boxes = boxes[~np.all(boxes==0, axis=1)]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def multiply_by_ratio(ratio_x, ratio_y, box):\n",
    "    return [max(shape * ratio_y, 10) if idx % 2 else shape * ratio_x for idx, shape in enumerate(box[:-1])]\n",
    "\n",
    "def run_preprocesing_on_crop(crop, net_shape):\n",
    "    temp_img = cv2.resize(crop, net_shape)\n",
    "    temp_img = temp_img.reshape((1,) * 2 + temp_img.shape)\n",
    "    return temp_img\n",
    "\n",
    "\n",
    "def convert_result_to_image(bgr_image, resized_image, boxes, threshold=0.3, conf_labels=True): \n",
    "    # Define colors for boxes and descriptions\n",
    "    colors = {'red': (255, 0, 0), 'green': (0, 255, 0), 'white': (255, 255, 255)} \n",
    "\n",
    "    # Fetch image shapes to calculate ratio\n",
    "    (real_y, real_x), (resized_y, resized_x) = image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x/resized_x, real_y/resized_y\n",
    "\n",
    "    # Convert base image from bgr to rgb format\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB) \n",
    "\n",
    "    # Iterate through non-zero boxes\n",
    "    for box, annotation in boxes: \n",
    "        # Pick confidence factor from last place in array\n",
    "        conf = box[-1]\n",
    "        if conf > threshold: \n",
    "            # Convert float to int and multiply position of each box by x and y ratio\n",
    "            (x_min, y_min, x_max, y_max) = map(int, multiply_by_ratio(ratio_x, ratio_y, box)) \n",
    "\n",
    "            # Draw box based on position, parameters in rectangle function are: image, start_point, end_point, color, thickness \n",
    "            cv2.rectangle( \n",
    "                rgb_image, \n",
    "                (x_min, y_min), \n",
    "                (x_max, y_max), \n",
    "                colors['green'], \n",
    "                3\n",
    "            ) \n",
    "\n",
    "            # Add text to image based on position and confidence, parameters in putText function are: image, text, bottomleft_corner_textfield, font, font_scale, color, thickness, line_type \n",
    "            if conf_labels:\n",
    "                # Create background box based on annotation length\n",
    "                (text_w, text_h), _ = cv2.getTextSize(f\"{annotation}\", cv2.FONT_HERSHEY_TRIPLEX, 0.8, 1)\n",
    "                image_copy = rgb_image.copy()\n",
    "                cv2.rectangle( \n",
    "                    image_copy, \n",
    "                    (x_min, y_min - text_h - 10), \n",
    "                    (x_min + text_w, y_min - 10), \n",
    "                    colors['white'], \n",
    "                    -1\n",
    "                )\n",
    "                # Add weighted image copy with white boxes under text\n",
    "                cv2.addWeighted(image_copy, 0.4, rgb_image, 0.6, 0, rgb_image)\n",
    "                cv2.putText( \n",
    "                    rgb_image, \n",
    "                    f\"{annotation}\", \n",
    "                    (x_min, y_min - 10), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                    0.8, \n",
    "                    colors['red'], \n",
    "                    1, \n",
    "                    cv2.LINE_AA\n",
    "                ) \n",
    "            \n",
    "    return rgb_image"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "recognition_net = ie.read_network(\n",
    "    model=f\"{model_folder}/{recognition_model_name}.xml\"\n",
    ")\n",
    "\n",
    "exec_recognition_net = ie.load_network(recognition_net, \"CPU\")\n",
    "\n",
    "recognition_output_layer = next(iter(exec_recognition_net.outputs))\n",
    "recognition_input_layer = next(iter(exec_recognition_net.input_info))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get height and width of input layer\n",
    "_, _, H, W = recognition_net.input_info[recognition_input_layer].tensor_desc.dims\n",
    "\n",
    "# Calculate scale for image resizing\n",
    "(real_y, real_x), (resized_y, resized_x) = image.shape[:2], resized_image.shape[:2]\n",
    "ratio_x, ratio_y = real_x/resized_x, real_y/resized_y\n",
    "\n",
    "# Convert image to grayscale for text recognition model\n",
    "grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Get dictionary to encode output, based on model documentation\n",
    "letters = \"~0123456789abcdefghijklmnopqrstuvwxyz\"\n",
    "\n",
    "# Prepare empty list for annotations\n",
    "annotations = list()\n",
    "\n",
    "# For each crop, based on boxes given by detection model we want to get annotations\n",
    "for crop in boxes:\n",
    "    # Get coordinates on corners of crop\n",
    "    (x_min, y_min, x_max, y_max) = map(int, multiply_by_ratio(ratio_x, ratio_y, crop))\n",
    "    image_crop = run_preprocesing_on_crop(grayscale_image[y_min:y_max, x_min:x_max], (W, H))\n",
    "    \n",
    "    # Run inference with recognition model\n",
    "    recognition_result = exec_recognition_net.infer(inputs={recognition_input_layer: image_crop})\n",
    "    \n",
    "    # Squeeze output to remove unnececery dimension\n",
    "    recognition_results_test = np.squeeze(recognition_result[recognition_output_layer])\n",
    "    \n",
    "    # Read annotation based on probabilities from output layer\n",
    "    annotation = list()\n",
    "    for letter in recognition_results_test:\n",
    "        parsed_letter = letters[letter.argmax()]\n",
    "\n",
    "        # Returning 0 index from argmax signalises end of string\n",
    "        if parsed_letter == letters[0]:\n",
    "            break\n",
    "        annotation.append(parsed_letter)\n",
    "    annotations.append(''.join(annotation))\n",
    "\n",
    "boxes_with_annotations = zip(boxes, annotations)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.imshow(convert_result_to_image(image, resized_image, boxes_with_annotations, conf_labels=True))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
  "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}