{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50e1d1d5-3bdd-4224-9f93-bf5d9a83f424",
   "metadata": {},
   "source": [
    "# Create an LLM-powered Chatbot using OpenVINO\n",
    "\n",
    "In the rapidly evolving world of artificial intelligence (AI), chatbots have emerged as powerful tools for businesses to enhance customer interactions and streamline operations. \n",
    "Large Language Models (LLMs) are artificial intelligence systems that can understand and generate human language. They use deep learning algorithms and massive amounts of data to learn the nuances of language and produce coherent and relevant responses.\n",
    "While a decent intent-based chatbot can answer basic, one-touch inquiries like order management, FAQs, and policy questions, LLM chatbots can tackle more complex, multi-touch questions. LLM enables chatbots to provide support in a conversational manner, similar to how humans do, through contextual memory. Leveraging the capabilities of Language Models, chatbots are becoming increasingly intelligent, capable of understanding and responding to human language with remarkable accuracy.\n",
    "\n",
    "Previously, we already discussed how to build an instruction-following pipeline using OpenVINO and Optimum Intel, please check out [Dolly example](../dolly-2-instruction-following) for reference.\n",
    "In this tutorial, we consider how to use the power of OpenVINO for running Large Language Models for chat. We will use a pre-trained model from the [Hugging Face Transformers](https://huggingface.co/docs/transformers/index) library. To simplify the user experience, the [Hugging Face Optimum Intel](https://huggingface.co/docs/optimum/intel/index) library is used to convert the models to OpenVINO™ IR format and to create inference pipeline. The inference pipeline can also be created using [OpenVINO Generate API](https://github.com/openvinotoolkit/openvino.genai/tree/master/src), the example of that, please, see in the notebook [LLM chatbot with OpenVINO Generate API](./llm-chatbot-generate-api.ipynb)\n",
    "\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "\n",
    "- Install prerequisites\n",
    "- Download and convert the model from a public source using the [OpenVINO integration with Hugging Face Optimum](https://huggingface.co/blog/openvino).\n",
    "- Compress model weights to 4-bit or 8-bit data types using [NNCF](https://github.com/openvinotoolkit/nncf)\n",
    "- Create a chat inference pipeline\n",
    "- Run chat pipeline\n",
    "\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Select model for inference](#Select-model-for-inference)\n",
    "- [Convert model using Optimum-CLI tool](#Convert-model-using-Optimum-CLI-tool)\n",
    "- [Compress model weights](#Compress-model-weights)\n",
    "    - [Weights Compression using Optimum-CLI](#Weights-Compression-using-Optimum-CLI)\n",
    "    - [Weight compression with AWQ](#Weight-compression-with-AWQ)\n",
    "- [Select device for inference and model variant](#Select-device-for-inference-and-model-variant)\n",
    "- [Instantiate Model using Optimum Intel](#Instantiate-Model-using-Optimum-Intel)\n",
    "- [Run Chatbot](#Run-Chatbot)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/llm-chatbot/llm-chatbot.ipynb\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df233b0-0369-4fff-9952-7957a90394a5",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "563ecf9f-346b-4f14-85ef-c66ff0c95f65",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Error parsing dependencies of torchsde: .* suffix can only be used with `==` or `!=` operators\n",
      "    numpy (>=1.19.*) ; python_version >= \"3.7\"\n",
      "           ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Error parsing dependencies of torchsde: .* suffix can only be used with `==` or `!=` operators\n",
      "    numpy (>=1.19.*) ; python_version >= \"3.7\"\n",
      "           ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Error parsing dependencies of torchsde: .* suffix can only be used with `==` or `!=` operators\n",
      "    numpy (>=1.19.*) ; python_version >= \"3.7\"\n",
      "           ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "whowhatbench 1.0.0 requires pandas>=2.0.3, but you have pandas 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Error parsing dependencies of torchsde: .* suffix can only be used with `==` or `!=` operators\n",
      "    numpy (>=1.19.*) ; python_version >= \"3.7\"\n",
      "           ~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "laion-clap 1.1.4 requires wandb, which is not installed.\n",
      "llava 0.1.0 requires wandb, which is not installed.\n",
      "stable-audio-tools 0.0.16 requires wandb==0.15.4, which is not installed.\n",
      "ai2-olmo 0.3.0 requires torch<2.3,>=2.0, but you have torch 2.3.1+cpu which is incompatible.\n",
      "audiocraft 1.3.0a1 requires torch==2.1.0, but you have torch 2.3.1+cpu which is incompatible.\n",
      "audiocraft 1.3.0a1 requires torchaudio<2.1.2,>=2.0.0, but you have torchaudio 2.3.0 which is incompatible.\n",
      "llava 0.1.0 requires gradio==3.23, but you have gradio 4.38.1 which is incompatible.\n",
      "llava 0.1.0 requires tokenizers==0.12.1, but you have tokenizers 0.19.1 which is incompatible.\n",
      "mobileclip 0.1.0 requires torch==1.13.1, but you have torch 2.3.1+cpu which is incompatible.\n",
      "mobileclip 0.1.0 requires torchvision==0.14.1, but you have torchvision 0.16.2+cpu which is incompatible.\n",
      "optimum 1.22.0.dev0 requires transformers[sentencepiece]<4.43.0,>=4.26.0, but you have transformers 4.43.1 which is incompatible.\n",
      "parler-tts 0.1 requires transformers<4.41.0,>=4.39.0, but you have transformers 4.43.1 which is incompatible.\n",
      "stable-audio-tools 0.0.16 requires einops==0.7.0, but you have einops 0.8.0 which is incompatible.\n",
      "whowhatbench 1.0.0 requires pandas>=2.0.3, but you have pandas 2.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/eaidova/optimum-intel.git@transformers_4.43\"\\\n",
    "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "\"torch>=2.1\"\\\n",
    "\"datasets\" \\\n",
    "\"accelerate\"\\\n",
    "\"gradio>=4.19\"\\\n",
    "\"onnx\" \"einops\" \"transformers_stream_generator\" \"tiktoken\" \"transformers>=4.40\" \"bitsandbytes\"\n",
    "%pip install -q \"transformers>=4.43\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39ca954-61d2-45c5-a7f9-7fce1acc277f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# fetch model configuration\n",
    "\n",
    "config_shared_path = Path(\"../../utils/llm_config.py\")\n",
    "config_dst_path = Path(\"llm_config.py\")\n",
    "\n",
    "if not config_dst_path.exists():\n",
    "    if config_shared_path.exists():\n",
    "        try:\n",
    "            os.symlink(config_shared_path, config_dst_path)\n",
    "        except Exception:\n",
    "            shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)\n",
    "elif not os.path.islink(config_dst_path):\n",
    "    print(\"LLM config will be updated\")\n",
    "    if config_shared_path.exists():\n",
    "        shutil.copy(config_shared_path, config_dst_path)\n",
    "    else:\n",
    "        r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/llm_config.py\")\n",
    "        with open(\"llm_config.py\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81983176-e571-4652-ba21-4bd608c35146",
   "metadata": {},
   "source": [
    "## Select model for inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The tutorial supports different models, you can select one from the provided options to compare the quality of open source LLM solutions.\n",
    ">**Note**: conversion of some models can require additional actions from user side and at least 64GB RAM for conversion.\n",
    "\n",
    "<details>\n",
    "  <summary><b>Click here to see available models options</b></summary>\n",
    "\n",
    "* **tiny-llama-1b-chat** - This is the chat model finetuned on top of [TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T](https://huggingface.co/TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T). The TinyLlama project aims to pretrain a 1.1B Llama model on 3 trillion tokens with the adoption of the same architecture and tokenizer as Llama 2. This means TinyLlama can be plugged and played in many open-source projects built upon Llama. Besides, TinyLlama is compact with only 1.1B parameters. This compactness allows it to cater to a multitude of applications demanding a restricted computation and memory footprint. More details about model can be found in [model card](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0)\n",
    "*  **mini-cpm-2b-dpo** - MiniCPM is an End-Size LLM developed by ModelBest Inc. and TsinghuaNLP, with only 2.4B parameters excluding embeddings. After Direct Preference Optimization (DPO) fine-tuning, MiniCPM outperforms many popular 7b, 13b and 70b models. More details can be found in [model_card](https://huggingface.co/openbmb/MiniCPM-2B-dpo-fp16).\n",
    "*  **gemma-2b-it** - Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. This model is instruction-tuned version of 2B parameters model. More details about model can be found in [model card](https://huggingface.co/google/gemma-2b-it).\n",
    ">**Note**: run model with demo, you will need to accept license agreement. \n",
    ">You must be a registered user in 🤗 Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/google/gemma-2b-it), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:\n",
    " \n",
    "```python\n",
    "    ## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "```\n",
    "* **phi3-mini-instruct** - The Phi-3-Mini is a 3.8B parameters, lightweight, state-of-the-art open model trained with the Phi-3 datasets that includes both synthetic data and the filtered publicly available websites data with a focus on high-quality and reasoning dense properties. More details about model can be found in [model card](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct), [Microsoft blog](https://aka.ms/phi3blog-april) and [technical report](https://aka.ms/phi3-tech-report).\n",
    "* **red-pajama-3b-chat** - A 2.8B parameter pre-trained language model based on GPT-NEOX architecture. It was developed by Together Computer and leaders from the open-source AI community. The model is fine-tuned on OASST1 and Dolly2 datasets to enhance chatting ability. More details about model can be found in [HuggingFace model card](https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1).\n",
    "*  **gemma-7b-it** - Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. They are text-to-text, decoder-only large language models, available in English, with open weights, pre-trained variants, and instruction-tuned variants. Gemma models are well-suited for a variety of text generation tasks, including question answering, summarization, and reasoning. This model is instruction-tuned version of 7B parameters model. More details about model can be found in [model card](https://huggingface.co/google/gemma-7b-it).\n",
    ">**Note**: run model with demo, you will need to accept license agreement. \n",
    ">You must be a registered user in 🤗 Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/google/gemma-7b-it), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:\n",
    " \n",
    "```python\n",
    "    ## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "```\n",
    "\n",
    "* **llama-2-7b-chat** - LLama 2 is the second generation of LLama models developed by Meta. Llama 2 is a collection of pre-trained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. llama-2-7b-chat is 7 billions parameters version of LLama 2 finetuned and optimized for dialogue use case. More details about model can be found in the [paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/), [repository](https://github.com/facebookresearch/llama) and [HuggingFace model card](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf).\n",
    ">**Note**: run model with demo, you will need to accept license agreement. \n",
    ">You must be a registered user in 🤗 Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:\n",
    " \n",
    "```python\n",
    "    ## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "```\n",
    "* **llama-3-8b-instruct** - Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. More details about model can be found in [Meta blog post](https://ai.meta.com/blog/meta-llama-3/), [model website](https://llama.meta.com/llama3) and [model card](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n",
    ">**Note**: run model with demo, you will need to accept license agreement. \n",
    ">You must be a registered user in 🤗 Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:\n",
    " \n",
    "```python\n",
    "    ## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "```\n",
    "* **llama-3.1-8b-instruct** - The Llama 3.1 instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks. More details about model can be found in [Meta blog post](https://ai.meta.com/blog/meta-llama-3-1/), [model website](https://llama.meta.com) and [model card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct).\n",
    ">**Note**: run model with demo, you will need to accept license agreement. \n",
    ">You must be a registered user in 🤗 Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:\n",
    " \n",
    "```python\n",
    "    ## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "```\n",
    "\n",
    "* **qwen2-1.5b-instruct/qwen2-7b-instruct** - Qwen2 is the new series of Qwen large language models.Compared with the state-of-the-art open source language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most open source models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc.\n",
    "For more details, please refer to [model_card](https://huggingface.co/Qwen/Qwen2-7B-Instruct), [blog](https://qwenlm.github.io/blog/qwen2/), [GitHub](https://github.com/QwenLM/Qwen2), and [Documentation](https://qwen.readthedocs.io/en/latest/). \n",
    "* **qwen1.5-0.5b-chat/qwen1.5-1.8b-chat/qwen1.5-7b-chat** - Qwen1.5 is the beta version of Qwen2, a transformer-based decoder-only language model pretrained on a large amount of data. Qwen1.5 is a language model series including decoder language models of different model sizes. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention. You can find more details about model in the [model repository](https://huggingface.co/Qwen).\n",
    "* **qwen-7b-chat** - Qwen-7B is the 7B-parameter version of the large language model series, Qwen (abbr. Tongyi Qianwen), proposed by Alibaba Cloud. Qwen-7B is a Transformer-based large language model, which is pretrained on a large volume of data, including web texts, books, codes, etc. For more details about Qwen, please refer to the [GitHub](https://github.com/QwenLM/Qwen) code repository.\n",
    "* **mpt-7b-chat** - MPT-7B is part of the family of MosaicPretrainedTransformer (MPT) models, which use a modified transformer architecture optimized for efficient training and inference. These architectural changes include performance-optimized layer implementations and the elimination of context length limits by replacing positional embeddings with Attention with Linear Biases ([ALiBi](https://arxiv.org/abs/2108.12409)). Thanks to these modifications, MPT models can be trained with high throughput efficiency and stable convergence. MPT-7B-chat is a chatbot-like model for dialogue generation. It was built by finetuning MPT-7B on the [ShareGPT-Vicuna](https://huggingface.co/datasets/jeffwan/sharegpt_vicuna), [HC3](https://huggingface.co/datasets/Hello-SimpleAI/HC3), [Alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca), [HH-RLHF](https://huggingface.co/datasets/Anthropic/hh-rlhf), and [Evol-Instruct](https://huggingface.co/datasets/victor123/evol_instruct_70k) datasets. More details about the model can be found in [blog post](https://www.mosaicml.com/blog/mpt-7b), [repository](https://github.com/mosaicml/llm-foundry/) and [HuggingFace model card](https://huggingface.co/mosaicml/mpt-7b-chat).\n",
    "* **chatglm3-6b** - ChatGLM3-6B is the latest open-source model in the ChatGLM series. While retaining many excellent features such as smooth dialogue and low deployment threshold from the previous two generations, ChatGLM3-6B employs a more diverse training dataset, more sufficient training steps, and a more reasonable training strategy. ChatGLM3-6B adopts a newly designed [Prompt format](https://github.com/THUDM/ChatGLM3/blob/main/PROMPT_en.md), in addition to the normal multi-turn dialogue. You can find more details about model in the [model card](https://huggingface.co/THUDM/chatglm3-6b)\n",
    "* **mistral-7b** - The Mistral-7B-v0.1 Large Language Model (LLM) is a pretrained generative text model with 7 billion parameters. You can find more details about model in the [model card](https://huggingface.co/mistralai/Mistral-7B-v0.1), [paper](https://arxiv.org/abs/2310.06825) and [release blog post](https://mistral.ai/news/announcing-mistral-7b/).\n",
    "* **zephyr-7b-beta** - Zephyr is a series of language models that are trained to act as helpful assistants. Zephyr-7B-beta is the second model in the series, and is a fine-tuned version of [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) that was trained on on a mix of publicly available, synthetic datasets using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). You can find more details about model in [technical report](https://arxiv.org/abs/2310.16944) and [HuggingFace model card](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta).\n",
    "* **neural-chat-7b-v3-1** - Mistral-7b model fine-tuned using Intel Gaudi. The model fine-tuned on the open source dataset [Open-Orca/SlimOrca](https://huggingface.co/datasets/Open-Orca/SlimOrca) and aligned with [Direct Preference Optimization (DPO) algorithm](https://arxiv.org/abs/2305.18290). More details can be found in [model card](https://huggingface.co/Intel/neural-chat-7b-v3-1) and [blog post](https://medium.com/@NeuralCompressor/the-practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-gaudi2-a1197d8a3cd3).\n",
    "* **notus-7b-v1** - Notus is a collection of fine-tuned models using [Direct Preference Optimization (DPO)](https://arxiv.org/abs/2305.18290). and related [RLHF](https://huggingface.co/blog/rlhf) techniques. This model is the first version, fine-tuned with DPO over zephyr-7b-sft. Following a data-first approach, the only difference between Notus-7B-v1 and Zephyr-7B-beta is the preference dataset used for dDPO. Proposed approach for dataset creation helps to effectively fine-tune Notus-7b that surpasses Zephyr-7B-beta and Claude 2 on [AlpacaEval](https://tatsu-lab.github.io/alpaca_eval/). More details about model can be found in [model card](https://huggingface.co/argilla/notus-7b-v1).\n",
    "* **youri-7b-chat** - Youri-7b-chat is a Llama2 based model. [Rinna Co., Ltd.](https://rinna.co.jp/) conducted further pre-training for the Llama2 model with a mixture of English and Japanese datasets to improve Japanese task capability. The model is publicly released on Hugging Face hub. You can find detailed information at the [rinna/youri-7b-chat project page](https://huggingface.co/rinna/youri-7b). \n",
    "* **baichuan2-7b-chat** - Baichuan 2 is the new generation of large-scale open-source language models launched by [Baichuan Intelligence inc](https://www.baichuan-ai.com/home). It is trained on a high-quality corpus with 2.6 trillion tokens and has achieved the best performance in authoritative Chinese and English benchmarks of the same size.\n",
    "* **internlm2-chat-1.8b** - InternLM2 is the second generation InternLM series. Compared to the previous generation model, it shows significant improvements in various capabilities, including reasoning, mathematics, and coding. More details about model can be found in [model repository](https://huggingface.co/internlm).\n",
    "* **glm-4-9b-chat** - GLM-4-9B is the open-source version of the latest generation of pre-trained models in the GLM-4 series launched by Zhipu AI. In the evaluation of data sets in semantics, mathematics, reasoning, code, and knowledge, GLM-4-9B and its human preference-aligned version GLM-4-9B-Chat have shown superior performance beyond Llama-3-8B. In addition to multi-round conversations, GLM-4-9B-Chat also has advanced features such as web browsing, code execution, custom tool calls (Function Call), and long text reasoning (supporting up to 128K context). More details about model can be found in [model card](https://huggingface.co/THUDM/glm-4-9b-chat/blob/main/README_en.md), [technical report](https://arxiv.org/pdf/2406.12793) and [repository](https://github.com/THUDM/GLM-4)\n",
    "  \n",
    "  </detals>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f93282b6-f1f1-4153-84af-31aac79c3ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from llm_config import SUPPORTED_LLM_MODELS\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e02b34fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12241206783245f5a912abf0a46f4115",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model Language:', options=('English', 'Chinese', 'Japanese'), value='English')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_languages = list(SUPPORTED_LLM_MODELS)\n",
    "\n",
    "model_language = widgets.Dropdown(\n",
    "    options=model_languages,\n",
    "    value=model_languages[0],\n",
    "    description=\"Model Language:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d22fedb-d1f6-4306-b910-efac5b849c7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c3cef8f8e442c4aef26a999f56e3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model:', options=('qwen2-0.5b-instruct', 'tiny-llama-1b-chat', 'qwen2-1.5b-instruct', 'g…"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ids = list(SUPPORTED_LLM_MODELS[model_language.value])\n",
    "\n",
    "model_id = widgets.Dropdown(\n",
    "    options=model_ids,\n",
    "    value=model_ids[0],\n",
    "    description=\"Model:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906022ec-96bf-41a9-9447-789d2e875250",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model llama-3.1-8b-instruct\n"
     ]
    }
   ],
   "source": [
    "model_configuration = SUPPORTED_LLM_MODELS[model_language.value][model_id.value]\n",
    "print(f\"Selected model {model_id.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af3e8a-915a-49b4-8007-803777ba9eaf",
   "metadata": {},
   "source": [
    "## Convert model using Optimum-CLI tool\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "🤗 [Optimum Intel](https://huggingface.co/docs/optimum/intel/index) is the interface between the 🤗 [Transformers](https://huggingface.co/docs/transformers/index) and [Diffusers](https://huggingface.co/docs/diffusers/index) libraries and OpenVINO to accelerate end-to-end pipelines on Intel architectures. It provides ease-to-use cli interface for exporting models to [OpenVINO Intermediate Representation (IR)](https://docs.openvino.ai/2024/documentation/openvino-ir-format.html) format.\n",
    "\n",
    "The command bellow demonstrates basic command for model export with `optimum-cli`\n",
    "\n",
    "```\n",
    "optimum-cli export openvino --model <model_id_or_path> --task <task> <out_dir>\n",
    "```\n",
    "\n",
    "where `--model` argument is model id from HuggingFace Hub or local directory with model (saved using `.save_pretrained` method), `--task ` is one of [supported task](https://huggingface.co/docs/optimum/exporters/task_manager) that exported model should solve. For LLMs it will be `text-generation-with-past`. If model initialization requires to use remote code, `--trust-remote-code` flag additionally should be passed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13694bf8-ee7b-4186-a3e0-a8705be9733c",
   "metadata": {},
   "source": [
    "## Compress model weights\n",
    "\n",
    "\n",
    "The [Weights Compression](https://docs.openvino.ai/2024/openvino-workflow/model-optimization-guide/weight-compression.html) algorithm is aimed at compressing the weights of the models and can be used to optimize the model footprint and performance of large models where the size of weights is relatively larger than the size of activations, for example, Large Language Models (LLM). Compared to INT8 compression, INT4 compression improves performance even more, but introduces a minor drop in prediction quality.\n",
    "\n",
    "\n",
    "### Weights Compression using Optimum-CLI\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "You can also apply fp16, 8-bit or 4-bit weight compression on the Linear, Convolutional and Embedding layers when exporting your model with the CLI by setting `--weight-format` to respectively fp16, int8 or int4. This type of optimization allows to reduce the memory footprint and inference latency.\n",
    "By default the quantization scheme for int8/int4 will be [asymmetric](https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md#asymmetric-quantization), to make it [symmetric](https://github.com/openvinotoolkit/nncf/blob/develop/docs/compression_algorithms/Quantization.md#symmetric-quantization) you can add `--sym`.\n",
    "\n",
    "For INT4 quantization you can also specify the following arguments :\n",
    "- The `--group-size` parameter will define the group size to use for quantization, -1 it will results in per-column quantization.\n",
    "- The `--ratio` parameter controls the ratio between 4-bit and 8-bit quantization. If set to 0.9, it means that 90% of the layers will be quantized to int4 while 10% will be quantized to int8.\n",
    "\n",
    "Smaller group_size and ratio values usually improve accuracy at the sacrifice of the model size and inference latency.\n",
    "\n",
    ">**Note**: There may be no speedup for INT4/INT8 compressed models on dGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91eb2ccf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f00ea5106b44db889f4535407d03d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=True, description='Prepare INT4 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bdf48f4819476d939f054b2d355bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare INT8 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "579408abc137484e94678f9496a8b884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Prepare FP16 model')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "prepare_int4_model = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description=\"Prepare INT4 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_int8_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare INT8 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "prepare_fp16_model = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Prepare FP16 model\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "display(prepare_int4_model)\n",
    "display(prepare_int8_model)\n",
    "display(prepare_fp16_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a862a388",
   "metadata": {},
   "source": [
    "### Weight compression with AWQ\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "[Activation-aware Weight Quantization](https://arxiv.org/abs/2306.00978) (AWQ) is an algorithm that tunes model weights for more accurate INT4 compression. It slightly improves generation quality of compressed LLMs, but requires significant additional time for tuning weights on a calibration dataset. We use `wikitext-2-raw-v1/train` subset of the [Wikitext](https://huggingface.co/datasets/Salesforce/wikitext) dataset for calibration.\n",
    "\n",
    "Below you can enable AWQ to be additionally applied during model export with INT4 precision.\n",
    "\n",
    ">**Note**: Applying AWQ requires significant memory and time.\n",
    "\n",
    ">**Note**: It is possible that there will be no matching patterns in the model to apply AWQ, in such case it will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11a8473e509aa040",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8450de790a38411fa032babbeea0f1d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Enable AWQ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "enable_awq = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description=\"Enable AWQ\",\n",
    "    disabled=not prepare_int4_model.value,\n",
    ")\n",
    "display(enable_awq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a037a-7d98-4152-81ea-92ffb01da5a2",
   "metadata": {},
   "source": [
    "We can now save floating point and compressed model variants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ef9112",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Export command:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`optimum-cli export openvino --model meta-llama/Meta-Llama-3.1-8B-Instruct --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 1.0 --sym llama-3.1-8b-instruct/INT4_compressed_weights`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-24 13:00:41.469590: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-24 13:00:41.471480: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-24 13:00:41.506119: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-24 13:00:41.506464: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 13:00:42.233300: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.3.1+cpu)\n",
      "    Python  3.8.18 (you have 3.8.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "config.json: 100%|██████████████████████████████| 855/855 [00:00<00:00, 123kB/s]\n",
      "Framework not specified. Using pt to export the model.\n",
      "model.safetensors.index.json: 100%|█████████| 23.9k/23.9k [00:00<00:00, 714kB/s]\n",
      "Downloading shards:   0%|                                 | 0/4 [00:00<?, ?it/s]/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/huggingface_hub/file_download.py:982: UserWarning: Not enough free disk space to download the file. The expected file size is: 4976.70 MB. The target location /home/ea/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3.1-8B-Instruct/blobs only has 2149.95 MB free disk space.\n",
      "  warnings.warn(\n",
      "\n",
      "model-00001-of-00004.safetensors:   0%|             | 0.00/4.98G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 10.5M/4.98G [00:01<14:05, 5.87MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   0%|    | 21.0M/4.98G [00:02<08:34, 9.63MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 31.5M/4.98G [00:02<06:49, 12.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 41.9M/4.98G [00:03<06:03, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 52.4M/4.98G [00:04<05:35, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 62.9M/4.98G [00:04<05:17, 15.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   1%|    | 73.4M/4.98G [00:05<05:07, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|    | 83.9M/4.98G [00:06<04:59, 16.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|    | 94.4M/4.98G [00:06<04:55, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 105M/4.98G [00:07<04:50, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   2%|     | 115M/4.98G [00:07<04:48, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 126M/4.98G [00:08<04:46, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 136M/4.98G [00:09<04:44, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 147M/4.98G [00:09<04:44, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 157M/4.98G [00:10<04:45, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   3%|▏    | 168M/4.98G [00:11<05:15, 15.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 178M/4.98G [00:12<06:18, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 189M/4.98G [00:12<05:47, 13.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 199M/4.98G [00:14<07:04, 11.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 210M/4.98G [00:14<06:18, 12.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   4%|▏    | 220M/4.98G [00:15<05:46, 13.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▏    | 231M/4.98G [00:16<05:26, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▏    | 241M/4.98G [00:16<05:10, 15.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 252M/4.98G [00:17<04:59, 15.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 262M/4.98G [00:17<04:52, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   5%|▎    | 273M/4.98G [00:18<04:46, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 283M/4.98G [00:19<04:42, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 294M/4.98G [00:19<04:38, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 304M/4.98G [00:20<04:36, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   6%|▎    | 315M/4.98G [00:21<04:34, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 325M/4.98G [00:21<04:33, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 336M/4.98G [00:22<05:42, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 346M/4.98G [00:23<05:20, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 357M/4.98G [00:24<05:05, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   7%|▎    | 367M/4.98G [00:25<06:04, 12.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 377M/4.98G [00:25<05:35, 13.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 388M/4.98G [00:26<05:13, 14.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 398M/4.98G [00:26<04:59, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 409M/4.98G [00:27<04:49, 15.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   8%|▍    | 419M/4.98G [00:28<04:41, 16.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 430M/4.98G [00:28<04:36, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 440M/4.98G [00:29<04:32, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 451M/4.98G [00:30<04:29, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 461M/4.98G [00:30<04:27, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:   9%|▍    | 472M/4.98G [00:31<04:25, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍    | 482M/4.98G [00:31<04:23, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▍    | 493M/4.98G [00:32<04:22, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌    | 503M/4.98G [00:33<04:21, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  10%|▌    | 514M/4.98G [00:33<04:20, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 524M/4.98G [00:34<04:20, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 535M/4.98G [00:35<05:28, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 545M/4.98G [00:36<05:07, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 556M/4.98G [00:37<06:09, 12.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  11%|▌    | 566M/4.98G [00:37<05:35, 13.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 577M/4.98G [00:38<05:10, 14.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 587M/4.98G [00:39<04:53, 14.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 598M/4.98G [00:39<04:42, 15.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 608M/4.98G [00:40<04:33, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  12%|▌    | 619M/4.98G [00:40<04:26, 16.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 629M/4.98G [00:41<04:23, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 640M/4.98G [00:42<04:19, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 650M/4.98G [00:42<04:16, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 661M/4.98G [00:43<04:14, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  13%|▋    | 671M/4.98G [00:44<04:15, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 682M/4.98G [00:44<04:12, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 692M/4.98G [00:45<04:11, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 703M/4.98G [00:46<05:16, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  14%|▋    | 713M/4.98G [00:47<04:55, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 724M/4.98G [00:48<05:58, 11.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 734M/4.98G [00:49<05:46, 12.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▋    | 744M/4.98G [00:49<05:15, 13.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▊    | 755M/4.98G [00:50<04:53, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  15%|▊    | 765M/4.98G [00:50<04:39, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 776M/4.98G [00:51<04:29, 15.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 786M/4.98G [00:52<04:21, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 797M/4.98G [00:52<04:15, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 807M/4.98G [00:53<04:11, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  16%|▊    | 818M/4.98G [00:53<04:08, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 828M/4.98G [00:54<04:06, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 839M/4.98G [00:55<04:04, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 849M/4.98G [00:55<04:03, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 860M/4.98G [00:56<04:01, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  17%|▊    | 870M/4.98G [00:57<04:00, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 881M/4.98G [00:57<03:59, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 891M/4.98G [00:58<03:58, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 902M/4.98G [00:59<05:01, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  18%|▉    | 912M/4.98G [01:00<04:40, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 923M/4.98G [01:01<05:15, 12.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 933M/4.98G [01:01<05:16, 12.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 944M/4.98G [01:02<04:50, 13.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 954M/4.98G [01:03<04:33, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  19%|▉    | 965M/4.98G [01:03<04:21, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▉    | 975M/4.98G [01:04<04:36, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▉    | 986M/4.98G [01:05<04:24, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|█    | 996M/4.98G [01:05<04:14, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▊   | 1.01G/4.98G [01:06<04:06, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  20%|▊   | 1.02G/4.98G [01:07<04:02, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.03G/4.98G [01:07<03:57, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.04G/4.98G [01:08<03:54, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.05G/4.98G [01:08<03:53, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.06G/4.98G [01:09<03:51, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  21%|▊   | 1.07G/4.98G [01:10<03:49, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▊   | 1.08G/4.98G [01:10<03:48, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.09G/4.98G [01:11<03:47, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.10G/4.98G [01:12<04:46, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  22%|▉   | 1.11G/4.98G [01:13<04:32, 14.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.12G/4.98G [01:14<05:16, 12.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.13G/4.98G [01:15<05:10, 12.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.14G/4.98G [01:15<04:42, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.15G/4.98G [01:16<04:26, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  23%|▉   | 1.16G/4.98G [01:16<04:12, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.17G/4.98G [01:17<04:02, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.18G/4.98G [01:18<03:55, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.20G/4.98G [01:18<03:50, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.21G/4.98G [01:19<03:46, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  24%|▉   | 1.22G/4.98G [01:19<03:44, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|▉   | 1.23G/4.98G [01:20<03:42, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|▉   | 1.24G/4.98G [01:21<03:40, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.25G/4.98G [01:21<03:39, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.26G/4.98G [01:22<03:38, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  25%|█   | 1.27G/4.98G [01:23<03:36, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.28G/4.98G [01:23<03:36, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.29G/4.98G [01:24<04:33, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.30G/4.98G [01:25<04:14, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  26%|█   | 1.31G/4.98G [01:26<04:51, 12.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.32G/4.98G [01:27<04:47, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.33G/4.98G [01:27<04:23, 13.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.34G/4.98G [01:28<04:07, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.35G/4.98G [01:29<03:56, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  27%|█   | 1.36G/4.98G [01:29<03:48, 15.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.37G/4.98G [01:30<03:42, 16.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.38G/4.98G [01:30<03:38, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█   | 1.39G/4.98G [01:31<03:35, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▏  | 1.41G/4.98G [01:32<03:32, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  28%|█▏  | 1.42G/4.98G [01:32<03:30, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.43G/4.98G [01:33<03:29, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.44G/4.98G [01:34<03:28, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.45G/4.98G [01:34<03:26, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.46G/4.98G [01:35<03:25, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  29%|█▏  | 1.47G/4.98G [01:35<03:25, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.48G/4.98G [01:36<03:24, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.49G/4.98G [01:37<04:18, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.50G/4.98G [01:38<04:02, 14.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  30%|█▏  | 1.51G/4.98G [01:38<03:49, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.52G/4.98G [01:40<04:42, 12.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.53G/4.98G [01:40<04:17, 13.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.54G/4.98G [01:41<03:59, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▏  | 1.55G/4.98G [01:41<03:46, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  31%|█▎  | 1.56G/4.98G [01:42<03:38, 15.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.57G/4.98G [01:43<03:31, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.58G/4.98G [01:43<03:27, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.59G/4.98G [01:44<03:24, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.60G/4.98G [01:44<03:21, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  32%|█▎  | 1.61G/4.98G [01:45<03:20, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.63G/4.98G [01:46<03:17, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.64G/4.98G [01:46<03:16, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.65G/4.98G [01:47<03:15, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  33%|█▎  | 1.66G/4.98G [01:48<03:14, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.67G/4.98G [01:48<03:13, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.68G/4.98G [01:49<03:12, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.69G/4.98G [01:50<04:02, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.70G/4.98G [01:51<03:47, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  34%|█▎  | 1.71G/4.98G [01:52<04:36, 11.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.72G/4.98G [01:52<04:10, 13.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.73G/4.98G [01:53<03:50, 14.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.74G/4.98G [01:54<03:37, 14.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.75G/4.98G [01:54<03:28, 15.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  35%|█▍  | 1.76G/4.98G [01:55<03:21, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.77G/4.98G [01:55<03:16, 16.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.78G/4.98G [01:56<03:13, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.79G/4.98G [01:57<03:10, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.80G/4.98G [01:57<03:07, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  36%|█▍  | 1.81G/4.98G [01:58<03:06, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.82G/4.98G [01:59<03:05, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.84G/4.98G [01:59<03:03, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.85G/4.98G [02:00<03:03, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  37%|█▍  | 1.86G/4.98G [02:00<03:02, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.87G/4.98G [02:01<03:01, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.88G/4.98G [02:02<03:00, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.89G/4.98G [02:03<03:35, 14.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.90G/4.98G [02:03<03:23, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  38%|█▌  | 1.91G/4.98G [02:05<04:24, 11.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.92G/4.98G [02:05<03:59, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.93G/4.98G [02:06<03:41, 13.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.94G/4.98G [02:06<03:26, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.95G/4.98G [02:07<03:17, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  39%|█▌  | 1.96G/4.98G [02:08<03:10, 15.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.97G/4.98G [02:08<03:05, 16.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.98G/4.98G [02:09<03:01, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 1.99G/4.98G [02:09<02:59, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 2.00G/4.98G [02:10<02:56, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  40%|█▌  | 2.01G/4.98G [02:11<02:54, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.02G/4.98G [02:11<02:53, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.03G/4.98G [02:12<02:52, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.04G/4.98G [02:13<03:37, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  41%|█▋  | 2.06G/4.98G [02:14<03:23, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.07G/4.98G [02:14<03:13, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.08G/4.98G [02:15<03:50, 12.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.09G/4.98G [02:16<03:31, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.10G/4.98G [02:17<03:17, 14.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  42%|█▋  | 2.11G/4.98G [02:17<03:07, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.12G/4.98G [02:18<03:01, 15.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.13G/4.98G [02:19<02:56, 16.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.14G/4.98G [02:19<02:52, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.15G/4.98G [02:20<02:49, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  43%|█▋  | 2.16G/4.98G [02:20<02:47, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▋  | 2.17G/4.98G [02:21<02:45, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.18G/4.98G [02:22<02:44, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.19G/4.98G [02:22<02:43, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.20G/4.98G [02:23<02:42, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  44%|█▊  | 2.21G/4.98G [02:23<02:41, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.22G/4.98G [02:24<02:40, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.23G/4.98G [02:25<02:39, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.24G/4.98G [02:25<02:39, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  45%|█▊  | 2.25G/4.98G [02:26<02:39, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.26G/4.98G [02:26<02:37, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.28G/4.98G [02:27<02:37, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.29G/4.98G [02:28<02:37, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.30G/4.98G [02:28<02:36, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  46%|█▊  | 2.31G/4.98G [02:29<02:35, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▊  | 2.32G/4.98G [02:30<02:35, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▊  | 2.33G/4.98G [02:30<02:34, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.34G/4.98G [02:31<02:34, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.35G/4.98G [02:32<03:13, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  47%|█▉  | 2.36G/4.98G [02:32<03:00, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.37G/4.98G [02:34<03:56, 11.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.38G/4.98G [02:35<03:34, 12.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.39G/4.98G [02:35<03:15, 13.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.40G/4.98G [02:36<03:00, 14.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  48%|█▉  | 2.41G/4.98G [02:36<02:50, 15.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.42G/4.98G [02:37<02:43, 15.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.43G/4.98G [02:38<02:39, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.44G/4.98G [02:38<02:36, 16.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  49%|█▉  | 2.45G/4.98G [02:39<02:32, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.46G/4.98G [02:40<02:30, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.47G/4.98G [02:40<02:28, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|█▉  | 2.49G/4.98G [02:41<02:27, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.50G/4.98G [02:41<02:25, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  50%|██  | 2.51G/4.98G [02:42<02:25, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.52G/4.98G [02:43<02:23, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.53G/4.98G [02:44<03:00, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.54G/4.98G [02:44<02:48, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.55G/4.98G [02:45<03:14, 12.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  51%|██  | 2.56G/4.98G [02:46<02:59, 13.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.57G/4.98G [02:47<02:50, 14.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.58G/4.98G [02:48<03:07, 12.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.59G/4.98G [02:48<02:57, 13.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.60G/4.98G [02:50<03:34, 11.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  52%|██  | 2.61G/4.98G [02:50<03:13, 12.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.62G/4.98G [02:51<02:56, 13.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.63G/4.98G [02:52<02:43, 14.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██  | 2.64G/4.98G [02:52<02:34, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  53%|██▏ | 2.65G/4.98G [02:53<02:28, 15.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.66G/4.98G [02:54<02:24, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.67G/4.98G [02:54<02:20, 16.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.68G/4.98G [02:55<02:17, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.69G/4.98G [02:55<02:16, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  54%|██▏ | 2.71G/4.98G [02:56<02:49, 13.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.72G/4.98G [02:57<02:37, 14.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.73G/4.98G [02:58<03:14, 11.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.74G/4.98G [02:59<02:54, 12.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.75G/4.98G [03:00<02:40, 13.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  55%|██▏ | 2.76G/4.98G [03:00<02:30, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.77G/4.98G [03:01<02:23, 15.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.78G/4.98G [03:01<02:18, 15.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▏ | 2.79G/4.98G [03:02<02:14, 16.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▎ | 2.80G/4.98G [03:03<02:11, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  56%|██▎ | 2.81G/4.98G [03:03<02:09, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.82G/4.98G [03:04<02:07, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.83G/4.98G [03:05<02:06, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.84G/4.98G [03:05<02:05, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  57%|██▎ | 2.85G/4.98G [03:06<02:04, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.86G/4.98G [03:06<02:03, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.87G/4.98G [03:07<02:03, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.88G/4.98G [03:08<02:02, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.89G/4.98G [03:09<02:33, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  58%|██▎ | 2.90G/4.98G [03:09<02:23, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.92G/4.98G [03:10<02:20, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.93G/4.98G [03:11<02:41, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.94G/4.98G [03:12<02:27, 13.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▎ | 2.95G/4.98G [03:12<02:18, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  59%|██▍ | 2.96G/4.98G [03:13<02:12, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.97G/4.98G [03:14<02:07, 15.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.98G/4.98G [03:14<02:03, 16.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 2.99G/4.98G [03:15<02:00, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 3.00G/4.98G [03:15<01:58, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  60%|██▍ | 3.01G/4.98G [03:16<01:56, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.02G/4.98G [03:17<01:55, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.03G/4.98G [03:17<01:54, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.04G/4.98G [03:18<01:53, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  61%|██▍ | 3.05G/4.98G [03:18<01:52, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.06G/4.98G [03:20<02:20, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.07G/4.98G [03:20<02:10, 14.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.08G/4.98G [03:21<02:04, 15.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.09G/4.98G [03:22<02:28, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  62%|██▍ | 3.10G/4.98G [03:23<02:27, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.11G/4.98G [03:23<02:16, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.12G/4.98G [03:24<02:07, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.14G/4.98G [03:25<02:00, 15.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.15G/4.98G [03:25<01:56, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  63%|██▌ | 3.16G/4.98G [03:26<01:52, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.17G/4.98G [03:26<01:50, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.18G/4.98G [03:27<01:48, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.19G/4.98G [03:28<01:46, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.20G/4.98G [03:28<01:45, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  64%|██▌ | 3.21G/4.98G [03:29<01:44, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.22G/4.98G [03:30<01:43, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.23G/4.98G [03:30<01:42, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.24G/4.98G [03:31<01:41, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  65%|██▌ | 3.25G/4.98G [03:31<01:40, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▌ | 3.26G/4.98G [03:33<02:06, 13.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.27G/4.98G [03:33<01:57, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.28G/4.98G [03:34<01:51, 15.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.29G/4.98G [03:35<02:10, 12.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  66%|██▋ | 3.30G/4.98G [03:35<02:00, 13.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.31G/4.98G [03:36<01:52, 14.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.32G/4.98G [03:37<01:47, 15.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.33G/4.98G [03:37<01:43, 15.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.34G/4.98G [03:38<01:40, 16.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  67%|██▋ | 3.36G/4.98G [03:39<01:39, 16.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.37G/4.98G [03:39<01:37, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.38G/4.98G [03:40<01:35, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.39G/4.98G [03:40<01:34, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.40G/4.98G [03:41<01:33, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  68%|██▋ | 3.41G/4.98G [03:42<01:31, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▋ | 3.42G/4.98G [03:42<01:31, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.43G/4.98G [03:43<01:30, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.44G/4.98G [03:43<01:29, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  69%|██▊ | 3.45G/4.98G [03:44<01:29, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.46G/4.98G [03:45<01:28, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.47G/4.98G [03:45<01:27, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.48G/4.98G [03:46<01:50, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.49G/4.98G [03:47<01:42, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  70%|██▊ | 3.50G/4.98G [03:48<02:07, 11.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.51G/4.98G [03:49<01:55, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.52G/4.98G [03:50<01:45, 13.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.53G/4.98G [03:50<01:38, 14.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.54G/4.98G [03:51<01:33, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  71%|██▊ | 3.55G/4.98G [03:51<01:30, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▊ | 3.57G/4.98G [03:52<01:27, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▊ | 3.58G/4.98G [03:53<01:25, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.59G/4.98G [03:53<01:23, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.60G/4.98G [03:54<01:23, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  72%|██▉ | 3.61G/4.98G [03:55<01:21, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.62G/4.98G [03:55<01:20, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.63G/4.98G [03:56<01:19, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.64G/4.98G [03:56<01:18, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  73%|██▉ | 3.65G/4.98G [03:57<01:17, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.66G/4.98G [03:58<01:17, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.67G/4.98G [03:58<01:16, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.68G/4.98G [03:59<01:35, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.69G/4.98G [04:00<01:29, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  74%|██▉ | 3.70G/4.98G [04:01<01:24, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|██▉ | 3.71G/4.98G [04:01<01:20, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|██▉ | 3.72G/4.98G [04:02<01:18, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.73G/4.98G [04:02<01:15, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.74G/4.98G [04:03<01:14, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  75%|███ | 3.75G/4.98G [04:04<01:13, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.76G/4.98G [04:04<01:11, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.77G/4.98G [04:05<01:10, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.79G/4.98G [04:05<01:10, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.80G/4.98G [04:06<01:09, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  76%|███ | 3.81G/4.98G [04:07<01:08, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.82G/4.98G [04:07<01:07, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.83G/4.98G [04:08<01:07, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.84G/4.98G [04:09<01:06, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  77%|███ | 3.85G/4.98G [04:09<01:06, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.86G/4.98G [04:10<01:05, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.87G/4.98G [04:10<01:04, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███ | 3.88G/4.98G [04:11<01:04, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▏| 3.89G/4.98G [04:12<01:03, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  78%|███▏| 3.90G/4.98G [04:12<01:02, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.91G/4.98G [04:13<01:02, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.92G/4.98G [04:13<01:01, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.93G/4.98G [04:14<01:00, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.94G/4.98G [04:15<01:00, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  79%|███▏| 3.95G/4.98G [04:15<00:59, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.96G/4.98G [04:16<00:59, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.97G/4.98G [04:16<00:58, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 3.98G/4.98G [04:17<00:57, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 4.00G/4.98G [04:18<00:57, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  80%|███▏| 4.01G/4.98G [04:19<01:03, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.02G/4.98G [04:19<01:01, 15.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.03G/4.98G [04:20<00:59, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▏| 4.04G/4.98G [04:20<00:57, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  81%|███▎| 4.05G/4.98G [04:21<00:56, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.06G/4.98G [04:22<00:54, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.07G/4.98G [04:22<00:53, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.08G/4.98G [04:23<01:06, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.09G/4.98G [04:24<01:02, 14.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  82%|███▎| 4.10G/4.98G [04:25<00:58, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.11G/4.98G [04:25<00:55, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.12G/4.98G [04:26<00:53, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.13G/4.98G [04:26<00:51, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.14G/4.98G [04:27<00:50, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  83%|███▎| 4.15G/4.98G [04:28<00:49, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.16G/4.98G [04:28<00:48, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.17G/4.98G [04:29<00:47, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.18G/4.98G [04:30<00:46, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▎| 4.19G/4.98G [04:30<00:45, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  84%|███▍| 4.20G/4.98G [04:31<00:45, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.22G/4.98G [04:31<00:44, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.23G/4.98G [04:32<00:43, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.24G/4.98G [04:33<00:43, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  85%|███▍| 4.25G/4.98G [04:33<00:42, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.26G/4.98G [04:34<00:42, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.27G/4.98G [04:35<00:52, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.28G/4.98G [04:36<00:48, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.29G/4.98G [04:37<00:56, 12.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  86%|███▍| 4.30G/4.98G [04:37<00:53, 12.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.31G/4.98G [04:38<00:48, 13.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.32G/4.98G [04:39<00:44, 14.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.33G/4.98G [04:39<00:42, 15.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.34G/4.98G [04:40<00:40, 15.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  87%|███▍| 4.35G/4.98G [04:41<00:38, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.36G/4.98G [04:41<00:37, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.37G/4.98G [04:42<00:36, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.38G/4.98G [04:42<00:35, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.39G/4.98G [04:43<00:34, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  88%|███▌| 4.40G/4.98G [04:44<00:33, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.41G/4.98G [04:44<00:32, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.42G/4.98G [04:45<00:32, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.44G/4.98G [04:45<00:31, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  89%|███▌| 4.45G/4.98G [04:46<00:30, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.46G/4.98G [04:47<00:30, 17.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.47G/4.98G [04:48<00:37, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.48G/4.98G [04:48<00:34, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.49G/4.98G [04:49<00:35, 13.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  90%|███▌| 4.50G/4.98G [04:50<00:36, 13.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▌| 4.51G/4.98G [04:51<00:33, 14.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.52G/4.98G [04:51<00:30, 15.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.53G/4.98G [04:52<00:28, 15.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.54G/4.98G [04:53<00:27, 16.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  91%|███▋| 4.55G/4.98G [04:53<00:26, 16.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.56G/4.98G [04:54<00:25, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.57G/4.98G [04:54<00:24, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.58G/4.98G [04:55<00:23, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.59G/4.98G [04:56<00:22, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  92%|███▋| 4.60G/4.98G [04:56<00:21, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.61G/4.98G [04:57<00:21, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.62G/4.98G [04:57<00:20, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.63G/4.98G [04:58<00:22, 15.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  93%|███▋| 4.65G/4.98G [04:59<00:21, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▋| 4.66G/4.98G [05:00<00:19, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.67G/4.98G [05:00<00:18, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.68G/4.98G [05:01<00:18, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.69G/4.98G [05:02<00:21, 13.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  94%|███▊| 4.70G/4.98G [05:03<00:19, 14.4MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.71G/4.98G [05:03<00:17, 15.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.72G/4.98G [05:05<00:21, 11.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.73G/4.98G [05:05<00:19, 13.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.74G/4.98G [05:06<00:16, 14.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  95%|███▊| 4.75G/4.98G [05:06<00:15, 14.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.76G/4.98G [05:07<00:13, 15.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.77G/4.98G [05:08<00:12, 15.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.78G/4.98G [05:08<00:11, 16.3MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.79G/4.98G [05:09<00:11, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  96%|███▊| 4.80G/4.98G [05:09<00:10, 16.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▊| 4.81G/4.98G [05:10<00:09, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.82G/4.98G [05:11<00:09, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.83G/4.98G [05:11<00:08, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  97%|███▉| 4.84G/4.98G [05:12<00:07, 17.0MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.85G/4.98G [05:12<00:07, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.87G/4.98G [05:13<00:06, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.88G/4.98G [05:14<00:05, 17.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.89G/4.98G [05:15<00:06, 13.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  98%|███▉| 4.90G/4.98G [05:15<00:05, 14.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.91G/4.98G [05:16<00:04, 15.2MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.92G/4.98G [05:17<00:03, 15.7MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.93G/4.98G [05:17<00:03, 16.1MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.94G/4.98G [05:18<00:02, 16.5MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors:  99%|███▉| 4.95G/4.98G [05:18<00:01, 16.6MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|███▉| 4.96G/4.98G [05:19<00:01, 16.8MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|███▉| 4.97G/4.98G [05:20<00:00, 16.9MB/s]\u001b[A\n",
      "model-00001-of-00004.safetensors: 100%|████| 4.98G/4.98G [05:20<00:00, 15.5MB/s]\u001b[A\n",
      "Downloading shards:  25%|██████                  | 1/4 [05:22<16:07, 322.54s/it]\n",
      "model-00002-of-00004.safetensors:   0%|             | 0.00/5.00G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 10.5M/5.00G [00:00<07:13, 11.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   0%|    | 21.0M/5.00G [00:01<05:48, 14.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 31.5M/5.00G [00:02<05:19, 15.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 41.9M/5.00G [00:03<06:49, 12.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 52.4M/5.00G [00:03<06:04, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 62.9M/5.00G [00:05<07:15, 11.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   1%|    | 73.4M/5.00G [00:05<06:28, 12.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|    | 83.9M/5.00G [00:06<05:54, 13.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|    | 94.4M/5.00G [00:06<05:31, 14.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|     | 105M/5.00G [00:07<05:17, 15.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   2%|     | 115M/5.00G [00:08<05:07, 15.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 126M/5.00G [00:08<04:58, 16.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 136M/5.00G [00:09<04:54, 16.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 147M/5.00G [00:10<04:50, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 157M/5.00G [00:10<04:46, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   3%|▏    | 168M/5.00G [00:11<04:44, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 178M/5.00G [00:11<04:43, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 189M/5.00G [00:12<04:41, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 199M/5.00G [00:13<04:40, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 210M/5.00G [00:13<04:39, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   4%|▏    | 220M/5.00G [00:14<04:38, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▏    | 231M/5.00G [00:14<04:37, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▏    | 241M/5.00G [00:15<04:37, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎    | 252M/5.00G [00:16<04:36, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎    | 262M/5.00G [00:16<04:35, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   5%|▎    | 273M/5.00G [00:17<05:49, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 283M/5.00G [00:18<05:25, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 294M/5.00G [00:19<06:44, 11.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 304M/5.00G [00:20<06:04, 12.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   6%|▎    | 315M/5.00G [00:21<05:36, 13.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 325M/5.00G [00:21<05:16, 14.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 336M/5.00G [00:22<05:02, 15.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 346M/5.00G [00:22<04:53, 15.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 357M/5.00G [00:23<04:45, 16.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   7%|▎    | 367M/5.00G [00:24<04:40, 16.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 377M/5.00G [00:24<04:37, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 388M/5.00G [00:25<04:33, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 398M/5.00G [00:25<04:31, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 409M/5.00G [00:26<04:30, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   8%|▍    | 419M/5.00G [00:27<04:28, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 430M/5.00G [00:27<04:26, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 440M/5.00G [00:28<04:26, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 451M/5.00G [00:28<04:25, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 461M/5.00G [00:30<05:34, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:   9%|▍    | 472M/5.00G [00:30<05:14, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▍    | 482M/5.00G [00:31<04:57, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▍    | 493M/5.00G [00:32<05:54, 12.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌    | 503M/5.00G [00:33<05:24, 13.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌    | 514M/5.00G [00:33<05:05, 14.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  10%|▌    | 524M/5.00G [00:34<04:52, 15.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 535M/5.00G [00:34<04:41, 15.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 545M/5.00G [00:35<04:34, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 556M/5.00G [00:36<04:29, 16.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  11%|▌    | 566M/5.00G [00:36<04:25, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 577M/5.00G [00:37<04:22, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 587M/5.00G [00:37<04:21, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 598M/5.00G [00:38<04:19, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 608M/5.00G [00:39<04:17, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  12%|▌    | 619M/5.00G [00:39<04:17, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 629M/5.00G [00:40<04:15, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 640M/5.00G [00:41<04:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 650M/5.00G [00:41<04:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 661M/5.00G [00:42<04:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  13%|▋    | 671M/5.00G [00:42<04:13, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 682M/5.00G [00:43<04:13, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 692M/5.00G [00:44<05:18, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 703M/5.00G [00:45<04:58, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 713M/5.00G [00:46<06:01, 11.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  14%|▋    | 724M/5.00G [00:47<05:30, 13.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▋    | 734M/5.00G [00:47<05:05, 14.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▋    | 744M/5.00G [00:48<04:47, 14.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▊    | 755M/5.00G [00:48<04:34, 15.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  15%|▊    | 765M/5.00G [00:49<04:27, 15.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 776M/5.00G [00:50<04:20, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 786M/5.00G [00:50<04:15, 16.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 797M/5.00G [00:51<04:12, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 807M/5.00G [00:52<04:09, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  16%|▊    | 818M/5.00G [00:52<04:06, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 828M/5.00G [00:53<04:06, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 839M/5.00G [00:53<04:04, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 849M/5.00G [00:54<04:02, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 860M/5.00G [00:55<04:02, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  17%|▊    | 870M/5.00G [00:55<04:01, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 881M/5.00G [00:56<04:00, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 891M/5.00G [00:57<05:02, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 902M/5.00G [00:58<04:42, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 912M/5.00G [00:59<06:07, 11.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  18%|▉    | 923M/5.00G [01:00<05:29, 12.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 933M/5.00G [01:00<05:00, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 944M/5.00G [01:01<04:40, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 954M/5.00G [01:01<04:27, 15.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  19%|▉    | 965M/5.00G [01:02<04:16, 15.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 975M/5.00G [01:03<04:10, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 986M/5.00G [01:03<04:04, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▉    | 996M/5.00G [01:04<04:01, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▊   | 1.01G/5.00G [01:05<03:57, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  20%|▊   | 1.02G/5.00G [01:05<03:55, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.03G/5.00G [01:06<03:54, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.04G/5.00G [01:06<03:52, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.05G/5.00G [01:07<03:51, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.06G/5.00G [01:08<03:50, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  21%|▊   | 1.07G/5.00G [01:08<03:49, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▊   | 1.08G/5.00G [01:09<04:49, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▊   | 1.09G/5.00G [01:10<04:30, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.10G/5.00G [01:11<05:26, 11.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.11G/5.00G [01:12<04:56, 13.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  22%|▉   | 1.12G/5.00G [01:12<04:34, 14.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.13G/5.00G [01:13<04:35, 14.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.14G/5.00G [01:14<04:32, 14.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.15G/5.00G [01:15<04:18, 14.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.16G/5.00G [01:15<04:07, 15.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  23%|▉   | 1.17G/5.00G [01:16<03:59, 16.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.18G/5.00G [01:16<03:54, 16.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.20G/5.00G [01:17<03:50, 16.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.21G/5.00G [01:18<03:46, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  24%|▉   | 1.22G/5.00G [01:18<03:44, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|▉   | 1.23G/5.00G [01:19<03:42, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|▉   | 1.24G/5.00G [01:19<03:41, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|▉   | 1.25G/5.00G [01:20<03:40, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█   | 1.26G/5.00G [01:21<03:39, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  25%|█   | 1.27G/5.00G [01:22<04:35, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.28G/5.00G [01:22<04:16, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.29G/5.00G [01:23<04:03, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.30G/5.00G [01:24<03:55, 15.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.31G/5.00G [01:24<03:48, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  26%|█   | 1.32G/5.00G [01:25<03:43, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.33G/5.00G [01:25<03:40, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.34G/5.00G [01:26<03:38, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.35G/5.00G [01:27<03:35, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.36G/5.00G [01:27<03:34, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  27%|█   | 1.37G/5.00G [01:28<03:33, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.38G/5.00G [01:29<03:31, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.39G/5.00G [01:29<03:30, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█   | 1.41G/5.00G [01:30<03:30, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  28%|█▏  | 1.42G/5.00G [01:30<03:29, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.43G/5.00G [01:31<03:28, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.44G/5.00G [01:32<03:28, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.45G/5.00G [01:32<03:29, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.46G/5.00G [01:33<03:27, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  29%|█▏  | 1.47G/5.00G [01:33<03:27, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.48G/5.00G [01:34<03:26, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.49G/5.00G [01:35<03:25, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.50G/5.00G [01:35<03:24, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.51G/5.00G [01:36<03:24, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  30%|█▏  | 1.52G/5.00G [01:37<03:24, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.53G/5.00G [01:37<03:23, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.54G/5.00G [01:38<03:22, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.55G/5.00G [01:38<03:21, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▏  | 1.56G/5.00G [01:39<03:21, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  31%|█▎  | 1.57G/5.00G [01:40<03:20, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.58G/5.00G [01:40<03:19, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.59G/5.00G [01:41<03:19, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.60G/5.00G [01:41<03:18, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  32%|█▎  | 1.61G/5.00G [01:42<03:17, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.63G/5.00G [01:43<03:17, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.64G/5.00G [01:43<03:16, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.65G/5.00G [01:44<03:15, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.66G/5.00G [01:44<03:15, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  33%|█▎  | 1.67G/5.00G [01:45<03:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.68G/5.00G [01:46<04:04, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.69G/5.00G [01:47<03:49, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.70G/5.00G [01:47<03:37, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▎  | 1.71G/5.00G [01:49<04:19, 12.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  34%|█▍  | 1.72G/5.00G [01:49<03:58, 13.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.73G/5.00G [01:50<03:42, 14.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.74G/5.00G [01:50<03:33, 15.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.75G/5.00G [01:51<03:25, 15.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.76G/5.00G [01:52<03:19, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  35%|█▍  | 1.77G/5.00G [01:52<03:17, 16.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.78G/5.00G [01:53<03:13, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.79G/5.00G [01:53<03:11, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.80G/5.00G [01:54<03:09, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.81G/5.00G [01:55<03:07, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  36%|█▍  | 1.82G/5.00G [01:55<03:06, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.84G/5.00G [01:56<03:05, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.85G/5.00G [01:57<03:04, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.86G/5.00G [01:57<03:03, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  37%|█▍  | 1.87G/5.00G [01:58<03:03, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.88G/5.00G [01:59<03:50, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.89G/5.00G [02:00<03:36, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.90G/5.00G [02:00<03:24, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.91G/5.00G [02:01<04:04, 12.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  38%|█▌  | 1.92G/5.00G [02:02<03:44, 13.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.93G/5.00G [02:03<03:29, 14.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.94G/5.00G [02:03<03:20, 15.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.95G/5.00G [02:04<03:13, 15.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.96G/5.00G [02:04<03:07, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  39%|█▌  | 1.97G/5.00G [02:05<03:04, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.98G/5.00G [02:06<03:01, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 1.99G/5.00G [02:06<02:58, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.00G/5.00G [02:07<02:57, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.01G/5.00G [02:07<02:56, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  40%|█▌  | 2.02G/5.00G [02:08<02:55, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.03G/5.00G [02:09<02:53, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.04G/5.00G [02:09<02:53, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.06G/5.00G [02:10<02:52, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  41%|█▋  | 2.07G/5.00G [02:10<02:51, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.08G/5.00G [02:12<03:36, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.09G/5.00G [02:12<03:21, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.10G/5.00G [02:13<03:57, 12.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.11G/5.00G [02:14<03:35, 13.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  42%|█▋  | 2.12G/5.00G [02:15<03:20, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.13G/5.00G [02:15<03:10, 15.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.14G/5.00G [02:16<03:02, 15.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.15G/5.00G [02:16<02:57, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.16G/5.00G [02:17<02:53, 16.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  43%|█▋  | 2.17G/5.00G [02:18<02:50, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▋  | 2.18G/5.00G [02:18<02:48, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.19G/5.00G [02:19<02:46, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.20G/5.00G [02:20<02:45, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.21G/5.00G [02:20<02:43, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  44%|█▊  | 2.22G/5.00G [02:21<02:42, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.23G/5.00G [02:21<02:42, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.24G/5.00G [02:22<02:41, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.25G/5.00G [02:23<03:23, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  45%|█▊  | 2.26G/5.00G [02:24<03:09, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.28G/5.00G [02:24<02:59, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.29G/5.00G [02:26<03:38, 12.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.30G/5.00G [02:26<03:19, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.31G/5.00G [02:27<03:06, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  46%|█▊  | 2.32G/5.00G [02:27<02:56, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▊  | 2.33G/5.00G [02:28<02:50, 15.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▊  | 2.34G/5.00G [02:29<02:45, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.35G/5.00G [02:29<02:41, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.36G/5.00G [02:30<02:39, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  47%|█▉  | 2.37G/5.00G [02:30<02:36, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.38G/5.00G [02:31<02:35, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.39G/5.00G [02:32<02:33, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.40G/5.00G [02:32<02:32, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.41G/5.00G [02:33<02:31, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  48%|█▉  | 2.42G/5.00G [02:34<02:31, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.43G/5.00G [02:34<02:30, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.44G/5.00G [02:35<02:29, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.45G/5.00G [02:35<02:28, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.46G/5.00G [02:36<02:28, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  49%|█▉  | 2.47G/5.00G [02:37<02:27, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|█▉  | 2.49G/5.00G [02:37<02:26, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|█▉  | 2.50G/5.00G [02:38<02:26, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██  | 2.51G/5.00G [02:38<02:25, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  50%|██  | 2.52G/5.00G [02:40<03:03, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.53G/5.00G [02:40<02:51, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.54G/5.00G [02:41<03:18, 12.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.55G/5.00G [02:42<03:00, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.56G/5.00G [02:43<02:48, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  51%|██  | 2.57G/5.00G [02:43<02:40, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.58G/5.00G [02:44<02:33, 15.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.59G/5.00G [02:44<02:29, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.60G/5.00G [02:45<02:26, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.61G/5.00G [02:46<02:23, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  52%|██  | 2.62G/5.00G [02:46<02:21, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██  | 2.63G/5.00G [02:47<02:20, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██  | 2.64G/5.00G [02:47<02:18, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██  | 2.65G/5.00G [02:48<02:17, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▏ | 2.66G/5.00G [02:49<02:17, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  53%|██▏ | 2.67G/5.00G [02:49<02:16, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.68G/5.00G [02:50<02:15, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.69G/5.00G [02:50<02:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.71G/5.00G [02:51<02:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  54%|██▏ | 2.72G/5.00G [02:52<02:49, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.73G/5.00G [02:53<02:37, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.74G/5.00G [02:53<02:28, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.75G/5.00G [02:54<02:23, 15.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.76G/5.00G [02:55<02:19, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  55%|██▏ | 2.77G/5.00G [02:55<02:15, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.78G/5.00G [02:56<02:13, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.79G/5.00G [02:57<02:11, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.80G/5.00G [02:57<02:10, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▏ | 2.81G/5.00G [02:58<02:08, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  56%|██▎ | 2.82G/5.00G [02:58<02:07, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.83G/5.00G [02:59<02:07, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.84G/5.00G [03:00<02:06, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.85G/5.00G [03:00<02:05, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.86G/5.00G [03:01<02:04, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  57%|██▎ | 2.87G/5.00G [03:01<02:04, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.88G/5.00G [03:02<02:03, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.89G/5.00G [03:03<02:03, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.90G/5.00G [03:03<02:02, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  58%|██▎ | 2.92G/5.00G [03:04<02:01, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.93G/5.00G [03:04<02:01, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.94G/5.00G [03:06<02:31, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.95G/5.00G [03:06<02:21, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.96G/5.00G [03:07<02:38, 12.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  59%|██▎ | 2.97G/5.00G [03:08<02:32, 13.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 2.98G/5.00G [03:09<02:21, 14.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 2.99G/5.00G [03:09<02:13, 15.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 3.00G/5.00G [03:10<02:08, 15.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 3.01G/5.00G [03:10<02:03, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  60%|██▍ | 3.02G/5.00G [03:11<02:01, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.03G/5.00G [03:12<01:58, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.04G/5.00G [03:12<01:56, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.05G/5.00G [03:13<01:55, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.06G/5.00G [03:13<01:54, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  61%|██▍ | 3.07G/5.00G [03:14<01:53, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.08G/5.00G [03:15<01:52, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.09G/5.00G [03:15<01:51, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.10G/5.00G [03:16<01:50, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.11G/5.00G [03:17<01:50, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  62%|██▍ | 3.12G/5.00G [03:17<01:49, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.14G/5.00G [03:18<02:18, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.15G/5.00G [03:19<02:09, 14.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.16G/5.00G [03:20<02:13, 13.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  63%|██▌ | 3.17G/5.00G [03:21<02:31, 12.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.18G/5.00G [03:21<02:17, 13.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.19G/5.00G [03:22<02:06, 14.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.20G/5.00G [03:23<02:00, 15.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.21G/5.00G [03:23<01:55, 15.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  64%|██▌ | 3.22G/5.00G [03:24<01:51, 16.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.23G/5.00G [03:25<01:48, 16.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.24G/5.00G [03:25<01:46, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.25G/5.00G [03:26<01:44, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.26G/5.00G [03:26<01:43, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  65%|██▌ | 3.27G/5.00G [03:27<01:42, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.28G/5.00G [03:28<01:41, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.29G/5.00G [03:28<01:40, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.30G/5.00G [03:29<02:05, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.31G/5.00G [03:30<01:56, 14.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  66%|██▋ | 3.32G/5.00G [03:31<01:50, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.33G/5.00G [03:32<02:10, 12.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.34G/5.00G [03:32<01:59, 13.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.36G/5.00G [03:33<01:51, 14.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  67%|██▋ | 3.37G/5.00G [03:34<01:46, 15.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.38G/5.00G [03:34<01:42, 15.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.39G/5.00G [03:35<01:39, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.40G/5.00G [03:35<01:37, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.41G/5.00G [03:36<01:35, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  68%|██▋ | 3.42G/5.00G [03:37<01:33, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▋ | 3.43G/5.00G [03:37<01:33, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.44G/5.00G [03:38<01:31, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.45G/5.00G [03:38<01:31, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.46G/5.00G [03:39<01:30, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  69%|██▊ | 3.47G/5.00G [03:40<01:29, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.48G/5.00G [03:40<01:28, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.49G/5.00G [03:41<01:28, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.50G/5.00G [03:41<01:27, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.51G/5.00G [03:43<01:50, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  70%|██▊ | 3.52G/5.00G [03:43<01:42, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.53G/5.00G [03:44<01:57, 12.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.54G/5.00G [03:45<01:52, 12.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.55G/5.00G [03:46<01:43, 14.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  71%|██▊ | 3.57G/5.00G [03:46<01:37, 14.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.58G/5.00G [03:47<01:32, 15.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▊ | 3.59G/5.00G [03:48<01:28, 15.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.60G/5.00G [03:48<01:26, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.61G/5.00G [03:49<01:24, 16.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  72%|██▉ | 3.62G/5.00G [03:49<01:23, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.63G/5.00G [03:50<01:21, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.64G/5.00G [03:51<01:20, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.65G/5.00G [03:51<01:19, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.66G/5.00G [03:52<01:18, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  73%|██▉ | 3.67G/5.00G [03:52<01:17, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.68G/5.00G [03:53<01:17, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.69G/5.00G [03:54<01:16, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.70G/5.00G [03:54<01:15, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.71G/5.00G [03:55<01:15, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  74%|██▉ | 3.72G/5.00G [03:56<01:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|██▉ | 3.73G/5.00G [03:56<01:13, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|██▉ | 3.74G/5.00G [03:57<01:13, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███ | 3.75G/5.00G [03:58<01:32, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  75%|███ | 3.76G/5.00G [03:59<01:25, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.77G/5.00G [04:00<01:40, 12.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.79G/5.00G [04:00<01:31, 13.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.80G/5.00G [04:01<01:24, 14.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.81G/5.00G [04:02<01:19, 15.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  76%|███ | 3.82G/5.00G [04:02<01:16, 15.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.83G/5.00G [04:03<01:13, 16.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.84G/5.00G [04:03<01:11, 16.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.85G/5.00G [04:04<01:09, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.86G/5.00G [04:05<01:08, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  77%|███ | 3.87G/5.00G [04:05<01:07, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.88G/5.00G [04:06<01:06, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.89G/5.00G [04:06<01:05, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███ | 3.90G/5.00G [04:07<01:04, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▏| 3.91G/5.00G [04:08<01:03, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  78%|███▏| 3.92G/5.00G [04:08<01:03, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.93G/5.00G [04:09<01:02, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.94G/5.00G [04:10<01:01, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.95G/5.00G [04:11<01:17, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.96G/5.00G [04:11<01:12, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  79%|███▏| 3.97G/5.00G [04:13<01:25, 12.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 3.98G/5.00G [04:13<01:19, 12.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.00G/5.00G [04:14<01:12, 13.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.01G/5.00G [04:14<01:07, 14.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  80%|███▏| 4.02G/5.00G [04:15<01:04, 15.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.03G/5.00G [04:16<01:01, 15.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.04G/5.00G [04:16<00:59, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.05G/5.00G [04:17<00:57, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▏| 4.06G/5.00G [04:18<00:56, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  81%|███▎| 4.07G/5.00G [04:18<00:55, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.08G/5.00G [04:19<00:54, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.09G/5.00G [04:19<00:53, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.10G/5.00G [04:20<00:52, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.11G/5.00G [04:21<00:52, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  82%|███▎| 4.12G/5.00G [04:21<00:51, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.13G/5.00G [04:22<00:50, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.14G/5.00G [04:22<00:50, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.15G/5.00G [04:23<00:49, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.16G/5.00G [04:24<00:48, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  83%|███▎| 4.17G/5.00G [04:24<00:48, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.18G/5.00G [04:25<00:47, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.19G/5.00G [04:25<00:46, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.20G/5.00G [04:26<00:46, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  84%|███▎| 4.22G/5.00G [04:27<00:45, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.23G/5.00G [04:27<00:45, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.24G/5.00G [04:28<00:44, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.25G/5.00G [04:29<00:55, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.26G/5.00G [04:30<00:51, 14.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  85%|███▍| 4.27G/5.00G [04:30<00:48, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.28G/5.00G [04:32<01:06, 10.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.29G/5.00G [04:32<00:58, 12.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.30G/5.00G [04:33<00:52, 13.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.31G/5.00G [04:34<00:48, 14.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  86%|███▍| 4.32G/5.00G [04:34<00:45, 15.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.33G/5.00G [04:35<00:42, 15.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.34G/5.00G [04:36<00:41, 16.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.35G/5.00G [04:36<00:39, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.36G/5.00G [04:37<00:38, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  87%|███▍| 4.37G/5.00G [04:37<00:37, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.38G/5.00G [04:38<00:36, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.39G/5.00G [04:39<00:35, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.40G/5.00G [04:39<00:35, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  88%|███▌| 4.41G/5.00G [04:40<00:34, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.42G/5.00G [04:41<00:42, 13.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.44G/5.00G [04:42<00:39, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.45G/5.00G [04:42<00:36, 15.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.46G/5.00G [04:43<00:43, 12.5MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  89%|███▌| 4.47G/5.00G [04:44<00:39, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.48G/5.00G [04:45<00:35, 14.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.49G/5.00G [04:45<00:33, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.50G/5.00G [04:46<00:31, 15.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.51G/5.00G [04:46<00:30, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  90%|███▌| 4.52G/5.00G [04:47<00:29, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▌| 4.53G/5.00G [04:48<00:28, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.54G/5.00G [04:48<00:27, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.55G/5.00G [04:49<00:26, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.56G/5.00G [04:49<00:25, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  91%|███▋| 4.57G/5.00G [04:50<00:28, 15.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.58G/5.00G [04:51<00:28, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.59G/5.00G [04:52<00:27, 14.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.60G/5.00G [04:52<00:25, 15.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.61G/5.00G [04:54<00:30, 12.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  92%|███▋| 4.62G/5.00G [04:54<00:27, 13.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.63G/5.00G [04:55<00:24, 14.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.65G/5.00G [04:55<00:23, 15.3MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.66G/5.00G [04:56<00:21, 15.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  93%|███▋| 4.67G/5.00G [04:57<00:20, 16.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▋| 4.68G/5.00G [04:57<00:19, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▋| 4.69G/5.00G [04:58<00:18, 16.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.70G/5.00G [04:59<00:17, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.71G/5.00G [04:59<00:17, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  94%|███▊| 4.72G/5.00G [05:00<00:16, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.73G/5.00G [05:00<00:15, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.74G/5.00G [05:01<00:15, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.75G/5.00G [05:02<00:14, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.76G/5.00G [05:02<00:14, 17.0MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  95%|███▊| 4.77G/5.00G [05:03<00:13, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.78G/5.00G [05:03<00:12, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.79G/5.00G [05:04<00:12, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.80G/5.00G [05:05<00:11, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.81G/5.00G [05:05<00:10, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  96%|███▊| 4.82G/5.00G [05:06<00:10, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▊| 4.83G/5.00G [05:06<00:09, 17.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▉| 4.84G/5.00G [05:07<00:09, 17.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▉| 4.85G/5.00G [05:08<00:10, 13.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  97%|███▉| 4.87G/5.00G [05:09<00:09, 14.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.88G/5.00G [05:09<00:08, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.89G/5.00G [05:11<00:09, 12.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.90G/5.00G [05:11<00:07, 13.7MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.91G/5.00G [05:12<00:06, 14.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  98%|███▉| 4.92G/5.00G [05:12<00:05, 15.2MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.93G/5.00G [05:13<00:04, 15.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.94G/5.00G [05:14<00:03, 16.1MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.95G/5.00G [05:14<00:03, 16.4MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.96G/5.00G [05:15<00:02, 16.6MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors:  99%|███▉| 4.97G/5.00G [05:16<00:01, 16.8MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|███▉| 4.98G/5.00G [05:16<00:01, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|███▉| 4.99G/5.00G [05:17<00:00, 16.9MB/s]\u001b[A\n",
      "model-00002-of-00004.safetensors: 100%|████| 5.00G/5.00G [05:17<00:00, 15.7MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████            | 2/4 [10:42<10:42, 321.04s/it]\n",
      "model-00003-of-00004.safetensors:   0%|             | 0.00/4.92G [00:00<?, ?B/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 10.5M/4.92G [00:00<04:50, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   0%|    | 21.0M/4.92G [00:01<04:46, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 31.5M/4.92G [00:02<06:36, 12.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 41.9M/4.92G [00:02<05:50, 13.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 52.4M/4.92G [00:04<06:45, 12.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 62.9M/4.92G [00:04<06:03, 13.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   1%|    | 73.4M/4.92G [00:05<05:36, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|    | 83.9M/4.92G [00:05<05:17, 15.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|    | 94.4M/4.92G [00:06<05:06, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 105M/4.92G [00:07<04:57, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   2%|     | 115M/4.92G [00:07<04:51, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 126M/4.92G [00:08<04:48, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 136M/4.92G [00:08<04:44, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 147M/4.92G [00:09<04:41, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 157M/4.92G [00:10<04:40, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   3%|▏    | 168M/4.92G [00:10<04:38, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 178M/4.92G [00:11<04:36, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 189M/4.92G [00:11<04:36, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 199M/4.92G [00:12<04:34, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 210M/4.92G [00:13<04:33, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   4%|▏    | 220M/4.92G [00:13<04:35, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▏    | 231M/4.92G [00:14<05:44, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▏    | 241M/4.92G [00:15<05:23, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎    | 252M/4.92G [00:16<05:07, 15.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   5%|▎    | 262M/4.92G [00:16<04:55, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 273M/4.92G [00:17<04:48, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 283M/4.92G [00:18<04:41, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 294M/4.92G [00:18<04:37, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 304M/4.92G [00:19<04:35, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   6%|▎    | 315M/4.92G [00:19<04:32, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 325M/4.92G [00:20<04:30, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 336M/4.92G [00:21<04:29, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 346M/4.92G [00:21<04:27, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 357M/4.92G [00:22<04:26, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   7%|▎    | 367M/4.92G [00:22<04:25, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 377M/4.92G [00:23<04:25, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 388M/4.92G [00:24<04:24, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 398M/4.92G [00:24<04:23, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   8%|▍    | 409M/4.92G [00:25<05:32, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 419M/4.92G [00:26<05:12, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 430M/4.92G [00:27<06:03, 12.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 440M/4.92G [00:28<05:31, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 451M/4.92G [00:28<05:09, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:   9%|▍    | 461M/4.92G [00:29<04:54, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▍    | 472M/4.92G [00:30<04:43, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▍    | 482M/4.92G [00:30<04:35, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 493M/4.92G [00:31<04:29, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 503M/4.92G [00:31<04:26, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  10%|▌    | 514M/4.92G [00:32<04:22, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 524M/4.92G [00:33<04:19, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 535M/4.92G [00:33<04:18, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 545M/4.92G [00:34<04:18, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  11%|▌    | 556M/4.92G [00:35<04:16, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 566M/4.92G [00:35<04:15, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 577M/4.92G [00:36<04:14, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 587M/4.92G [00:36<04:13, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 598M/4.92G [00:38<05:19, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  12%|▌    | 608M/4.92G [00:38<04:58, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 619M/4.92G [00:39<04:45, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 629M/4.92G [00:40<05:46, 12.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 640M/4.92G [00:41<05:16, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 650M/4.92G [00:41<04:55, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  13%|▋    | 661M/4.92G [00:42<04:41, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 671M/4.92G [00:42<04:30, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 682M/4.92G [00:43<04:22, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 692M/4.92G [00:44<04:17, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  14%|▋    | 703M/4.92G [00:44<04:13, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 713M/4.92G [00:45<04:10, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 724M/4.92G [00:45<04:08, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▋    | 734M/4.92G [00:46<04:06, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▊    | 744M/4.92G [00:47<04:05, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  15%|▊    | 755M/4.92G [00:47<04:03, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 765M/4.92G [00:48<04:02, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 776M/4.92G [00:49<04:01, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 786M/4.92G [00:49<04:00, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 797M/4.92G [00:50<04:00, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  16%|▊    | 807M/4.92G [00:50<03:59, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 818M/4.92G [00:51<03:58, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 828M/4.92G [00:52<05:00, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 839M/4.92G [00:53<04:42, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 849M/4.92G [00:53<04:29, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  17%|▊    | 860M/4.92G [00:54<05:18, 12.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 870M/4.92G [00:55<04:53, 13.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 881M/4.92G [00:56<04:35, 14.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 891M/4.92G [00:56<04:21, 15.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  18%|▉    | 902M/4.92G [00:57<04:13, 15.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 912M/4.92G [00:58<04:06, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 923M/4.92G [00:58<04:01, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 933M/4.92G [00:59<03:59, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 944M/4.92G [00:59<03:56, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  19%|▉    | 954M/4.92G [01:00<03:53, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 965M/4.92G [01:01<03:52, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|▉    | 975M/4.92G [01:01<03:51, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|█    | 986M/4.92G [01:02<03:50, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|█    | 996M/4.92G [01:02<03:49, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  20%|▊   | 1.01G/4.92G [01:03<03:48, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.02G/4.92G [01:04<03:47, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.03G/4.92G [01:04<03:47, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.04G/4.92G [01:05<04:31, 14.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  21%|▊   | 1.05G/4.92G [01:06<04:16, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▊   | 1.06G/4.92G [01:07<05:10, 12.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▊   | 1.07G/4.92G [01:08<04:43, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.08G/4.92G [01:08<04:24, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.09G/4.92G [01:09<04:11, 15.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  22%|▉   | 1.10G/4.92G [01:09<04:02, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.11G/4.92G [01:10<03:55, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.12G/4.92G [01:11<03:50, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.13G/4.92G [01:11<03:47, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.14G/4.92G [01:12<03:44, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  23%|▉   | 1.15G/4.92G [01:13<03:42, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.16G/4.92G [01:13<03:40, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.17G/4.92G [01:14<03:41, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.18G/4.92G [01:14<03:39, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  24%|▉   | 1.20G/4.92G [01:15<03:37, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.21G/4.92G [01:16<03:37, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.22G/4.92G [01:16<03:36, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|▉   | 1.23G/4.92G [01:17<03:35, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.24G/4.92G [01:18<04:30, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  25%|█   | 1.25G/4.92G [01:19<04:12, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.26G/4.92G [01:19<04:00, 15.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.27G/4.92G [01:20<05:03, 12.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.28G/4.92G [01:21<04:35, 13.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.29G/4.92G [01:22<04:15, 14.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  26%|█   | 1.30G/4.92G [01:22<04:01, 15.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.31G/4.92G [01:23<03:52, 15.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.32G/4.92G [01:24<03:44, 16.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.33G/4.92G [01:24<03:38, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  27%|█   | 1.34G/4.92G [01:25<03:36, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.35G/4.92G [01:25<03:33, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.36G/4.92G [01:26<03:30, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█   | 1.37G/4.92G [01:27<03:29, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█▏  | 1.38G/4.92G [01:27<03:27, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  28%|█▏  | 1.39G/4.92G [01:28<03:26, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.41G/4.92G [01:28<03:25, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.42G/4.92G [01:29<03:24, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.43G/4.92G [01:30<03:25, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.44G/4.92G [01:30<03:24, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  29%|█▏  | 1.45G/4.92G [01:31<03:23, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.46G/4.92G [01:32<03:22, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.47G/4.92G [01:32<03:21, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.48G/4.92G [01:33<03:20, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  30%|█▏  | 1.49G/4.92G [01:33<03:19, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.50G/4.92G [01:34<03:19, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.51G/4.92G [01:35<03:18, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.52G/4.92G [01:35<03:17, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▏  | 1.53G/4.92G [01:36<03:17, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  31%|█▎  | 1.54G/4.92G [01:37<04:07, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.55G/4.92G [01:38<03:51, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.56G/4.92G [01:39<04:58, 11.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.57G/4.92G [01:40<04:27, 12.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.58G/4.92G [01:40<04:04, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  32%|█▎  | 1.59G/4.92G [01:41<03:47, 14.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.60G/4.92G [01:41<03:38, 15.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.61G/4.92G [01:42<03:29, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.63G/4.92G [01:43<03:24, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.64G/4.92G [01:43<03:19, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  33%|█▎  | 1.65G/4.92G [01:44<03:16, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.66G/4.92G [01:44<03:14, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.67G/4.92G [01:45<03:12, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.68G/4.92G [01:46<03:11, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  34%|█▎  | 1.69G/4.92G [01:46<03:09, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.70G/4.92G [01:47<03:08, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.71G/4.92G [01:48<03:08, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.72G/4.92G [01:48<03:06, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.73G/4.92G [01:49<03:55, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  35%|█▍  | 1.74G/4.92G [01:50<03:40, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.75G/4.92G [01:51<04:11, 12.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.76G/4.92G [01:52<03:52, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.77G/4.92G [01:52<03:37, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.78G/4.92G [01:53<03:25, 15.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  36%|█▍  | 1.79G/4.92G [01:53<03:18, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.80G/4.92G [01:54<03:13, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.81G/4.92G [01:55<03:08, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.82G/4.92G [01:55<03:05, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  37%|█▍  | 1.84G/4.92G [01:56<03:03, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.85G/4.92G [01:57<03:01, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.86G/4.92G [01:57<03:00, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.87G/4.92G [01:58<02:59, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.88G/4.92G [01:58<02:58, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  38%|█▌  | 1.89G/4.92G [01:59<02:56, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.90G/4.92G [02:00<02:56, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.91G/4.92G [02:00<02:55, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.92G/4.92G [02:01<02:54, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.93G/4.92G [02:01<02:54, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  39%|█▌  | 1.94G/4.92G [02:02<02:53, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.95G/4.92G [02:03<02:52, 17.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.96G/4.92G [02:04<03:37, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.97G/4.92G [02:04<03:23, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  40%|█▌  | 1.98G/4.92G [02:06<03:56, 12.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▌  | 1.99G/4.92G [02:06<03:35, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.00G/4.92G [02:07<03:20, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.01G/4.92G [02:07<03:11, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.02G/4.92G [02:08<03:04, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  41%|█▋  | 2.03G/4.92G [02:09<02:58, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.04G/4.92G [02:09<02:54, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.06G/4.92G [02:10<02:52, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.07G/4.92G [02:10<02:49, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.08G/4.92G [02:11<02:48, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  42%|█▋  | 2.09G/4.92G [02:12<02:46, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.10G/4.92G [02:12<02:45, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.11G/4.92G [02:13<02:44, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.12G/4.92G [02:13<02:44, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  43%|█▋  | 2.13G/4.92G [02:14<02:43, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▋  | 2.14G/4.92G [02:15<02:42, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▋  | 2.15G/4.92G [02:15<02:41, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.16G/4.92G [02:16<03:12, 14.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.17G/4.92G [02:17<03:01, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  44%|█▊  | 2.18G/4.92G [02:18<03:41, 12.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.19G/4.92G [02:19<03:22, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.20G/4.92G [02:19<03:07, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.21G/4.92G [02:20<02:58, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.22G/4.92G [02:21<02:51, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  45%|█▊  | 2.23G/4.92G [02:21<02:46, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.24G/4.92G [02:22<02:42, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.25G/4.92G [02:22<02:40, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.26G/4.92G [02:23<02:38, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.28G/4.92G [02:24<02:36, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  46%|█▊  | 2.29G/4.92G [02:24<02:35, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▊  | 2.30G/4.92G [02:25<02:33, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.31G/4.92G [02:25<02:33, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.32G/4.92G [02:26<02:32, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  47%|█▉  | 2.33G/4.92G [02:27<02:31, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.34G/4.92G [02:27<02:30, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.35G/4.92G [02:28<02:29, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.36G/4.92G [02:29<03:09, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.37G/4.92G [02:30<02:56, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  48%|█▉  | 2.38G/4.92G [02:31<03:17, 12.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.39G/4.92G [02:31<03:03, 13.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.40G/4.92G [02:32<02:52, 14.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.41G/4.92G [02:33<02:43, 15.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.42G/4.92G [02:33<02:37, 15.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  49%|█▉  | 2.43G/4.92G [02:34<02:46, 14.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|█▉  | 2.44G/4.92G [02:35<02:40, 15.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|█▉  | 2.45G/4.92G [02:35<02:35, 15.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|██  | 2.46G/4.92G [02:36<02:30, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  50%|██  | 2.47G/4.92G [02:36<02:27, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.49G/4.92G [02:37<02:25, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.50G/4.92G [02:38<02:23, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.51G/4.92G [02:38<02:22, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.52G/4.92G [02:39<02:21, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  51%|██  | 2.53G/4.92G [02:40<02:47, 14.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.54G/4.92G [02:41<02:38, 15.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.55G/4.92G [02:41<02:31, 15.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.56G/4.92G [02:42<03:05, 12.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.57G/4.92G [02:43<02:50, 13.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  52%|██  | 2.58G/4.92G [02:44<02:39, 14.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.59G/4.92G [02:44<02:31, 15.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.60G/4.92G [02:45<02:26, 15.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██  | 2.61G/4.92G [02:45<02:22, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  53%|██▏ | 2.62G/4.92G [02:46<02:19, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.63G/4.92G [02:47<02:17, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.64G/4.92G [02:47<02:15, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.65G/4.92G [02:48<02:13, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.66G/4.92G [02:48<02:12, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  54%|██▏ | 2.67G/4.92G [02:49<02:11, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.68G/4.92G [02:50<02:10, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.69G/4.92G [02:50<02:10, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.71G/4.92G [02:51<02:09, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.72G/4.92G [02:51<02:08, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  55%|██▏ | 2.73G/4.92G [02:53<02:41, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.74G/4.92G [02:53<02:30, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.75G/4.92G [02:55<03:03, 11.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▏ | 2.76G/4.92G [02:55<02:45, 13.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  56%|██▎ | 2.77G/4.92G [02:56<02:32, 14.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.78G/4.92G [02:56<02:23, 14.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.79G/4.92G [02:57<02:17, 15.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.80G/4.92G [02:58<02:12, 16.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.81G/4.92G [02:58<02:09, 16.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  57%|██▎ | 2.82G/4.92G [02:59<02:06, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.83G/4.92G [02:59<02:04, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.84G/4.92G [03:00<02:03, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.85G/4.92G [03:01<02:01, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.86G/4.92G [03:01<02:00, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  58%|██▎ | 2.87G/4.92G [03:02<01:59, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.88G/4.92G [03:02<01:59, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.89G/4.92G [03:03<01:58, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.90G/4.92G [03:04<01:57, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  59%|██▎ | 2.92G/4.92G [03:04<01:57, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.93G/4.92G [03:05<02:27, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.94G/4.92G [03:06<02:17, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.95G/4.92G [03:08<03:05, 10.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.96G/4.92G [03:08<02:44, 11.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  60%|██▍ | 2.97G/4.92G [03:09<02:28, 13.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 2.98G/4.92G [03:10<02:16, 14.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 2.99G/4.92G [03:10<02:09, 14.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.00G/4.92G [03:11<02:03, 15.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.01G/4.92G [03:11<01:59, 16.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  61%|██▍ | 3.02G/4.92G [03:12<01:56, 16.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.03G/4.92G [03:13<01:54, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.04G/4.92G [03:13<01:52, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.05G/4.92G [03:14<01:50, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.06G/4.92G [03:14<01:49, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  62%|██▍ | 3.07G/4.92G [03:15<01:48, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.08G/4.92G [03:16<01:47, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.09G/4.92G [03:16<01:46, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.10G/4.92G [03:17<01:46, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  63%|██▌ | 3.11G/4.92G [03:17<01:45, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.12G/4.92G [03:19<02:11, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.14G/4.92G [03:19<02:02, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.15G/4.92G [03:20<02:27, 12.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.16G/4.92G [03:21<02:13, 13.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  64%|██▌ | 3.17G/4.92G [03:22<02:03, 14.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.18G/4.92G [03:22<01:56, 14.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.19G/4.92G [03:23<01:51, 15.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.20G/4.92G [03:24<01:47, 16.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.21G/4.92G [03:24<01:44, 16.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  65%|██▌ | 3.22G/4.92G [03:25<01:42, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.23G/4.92G [03:25<01:40, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.24G/4.92G [03:26<01:39, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.25G/4.92G [03:27<01:38, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  66%|██▋ | 3.26G/4.92G [03:27<01:37, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.27G/4.92G [03:28<01:36, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.28G/4.92G [03:28<01:35, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.29G/4.92G [03:29<01:34, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.30G/4.92G [03:30<01:42, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  67%|██▋ | 3.31G/4.92G [03:31<01:59, 13.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.32G/4.92G [03:31<01:50, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.33G/4.92G [03:33<02:16, 11.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.34G/4.92G [03:33<02:03, 12.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.36G/4.92G [03:34<01:53, 13.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  68%|██▋ | 3.37G/4.92G [03:35<01:45, 14.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▋ | 3.38G/4.92G [03:35<01:40, 15.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▊ | 3.39G/4.92G [03:36<01:36, 15.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▊ | 3.40G/4.92G [03:36<01:33, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  69%|██▊ | 3.41G/4.92G [03:37<01:31, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.42G/4.92G [03:38<01:29, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.43G/4.92G [03:38<01:28, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.44G/4.92G [03:39<01:27, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.45G/4.92G [03:40<01:26, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  70%|██▊ | 3.46G/4.92G [03:40<01:25, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.47G/4.92G [03:41<01:24, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.48G/4.92G [03:41<01:23, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.49G/4.92G [03:42<01:23, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.50G/4.92G [03:43<01:22, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  71%|██▊ | 3.51G/4.92G [03:44<01:43, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▊ | 3.52G/4.92G [03:44<01:36, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▉ | 3.53G/4.92G [03:45<01:52, 12.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▉ | 3.54G/4.92G [03:46<01:47, 12.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  72%|██▉ | 3.55G/4.92G [03:47<01:38, 13.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.57G/4.92G [03:47<01:31, 14.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.58G/4.92G [03:48<01:27, 15.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.59G/4.92G [03:49<01:23, 15.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.60G/4.92G [03:49<01:21, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  73%|██▉ | 3.61G/4.92G [03:50<01:19, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.62G/4.92G [03:51<01:17, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.63G/4.92G [03:51<01:16, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.64G/4.92G [03:52<01:15, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.65G/4.92G [03:52<01:14, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  74%|██▉ | 3.66G/4.92G [03:53<01:13, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|██▉ | 3.67G/4.92G [03:54<01:12, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|██▉ | 3.68G/4.92G [03:54<01:12, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|███ | 3.69G/4.92G [03:55<01:11, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  75%|███ | 3.70G/4.92G [03:56<01:29, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.71G/4.92G [03:57<01:23, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.72G/4.92G [03:57<01:25, 13.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.73G/4.92G [03:58<01:28, 13.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.74G/4.92G [03:59<01:21, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  76%|███ | 3.75G/4.92G [03:59<01:16, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.76G/4.92G [04:00<01:13, 15.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.77G/4.92G [04:01<01:11, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.79G/4.92G [04:01<01:08, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.80G/4.92G [04:02<01:07, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  77%|███ | 3.81G/4.92G [04:03<01:06, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.82G/4.92G [04:03<01:05, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.83G/4.92G [04:04<01:04, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███ | 3.84G/4.92G [04:04<01:03, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███▏| 3.85G/4.92G [04:05<01:02, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  78%|███▏| 3.86G/4.92G [04:06<01:01, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.87G/4.92G [04:06<01:01, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.88G/4.92G [04:07<01:00, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.89G/4.92G [04:07<01:00, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  79%|███▏| 3.90G/4.92G [04:09<01:14, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.91G/4.92G [04:09<01:09, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.92G/4.92G [04:10<01:24, 11.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.93G/4.92G [04:11<01:16, 12.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.94G/4.92G [04:12<01:09, 13.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  80%|███▏| 3.95G/4.92G [04:12<01:05, 14.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 3.96G/4.92G [04:13<01:01, 15.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 3.97G/4.92G [04:14<00:59, 15.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▏| 3.98G/4.92G [04:14<00:57, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▎| 4.00G/4.92G [04:15<00:55, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  81%|███▎| 4.01G/4.92G [04:15<00:54, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.02G/4.92G [04:16<00:53, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.03G/4.92G [04:17<00:52, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.04G/4.92G [04:17<00:51, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  82%|███▎| 4.05G/4.92G [04:18<00:50, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.06G/4.92G [04:18<00:50, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.07G/4.92G [04:19<00:49, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.08G/4.92G [04:20<00:48, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.09G/4.92G [04:21<01:01, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  83%|███▎| 4.10G/4.92G [04:21<00:56, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.11G/4.92G [04:22<00:53, 15.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.12G/4.92G [04:24<01:26, 9.24MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.13G/4.92G [04:25<01:17, 10.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▎| 4.14G/4.92G [04:26<01:07, 11.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  84%|███▍| 4.15G/4.92G [04:26<00:59, 12.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.16G/4.92G [04:27<00:54, 13.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.17G/4.92G [04:27<00:50, 14.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.18G/4.92G [04:28<00:47, 15.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  85%|███▍| 4.19G/4.92G [04:29<00:45, 15.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.20G/4.92G [04:29<00:43, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.22G/4.92G [04:30<00:42, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.23G/4.92G [04:31<00:41, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.24G/4.92G [04:31<00:40, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  86%|███▍| 4.25G/4.92G [04:32<00:39, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.26G/4.92G [04:32<00:38, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.27G/4.92G [04:33<00:38, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.28G/4.92G [04:34<00:47, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.29G/4.92G [04:35<00:43, 14.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  87%|███▍| 4.30G/4.92G [04:35<00:41, 15.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.31G/4.92G [04:37<00:49, 12.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.32G/4.92G [04:37<00:44, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.33G/4.92G [04:38<00:40, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  88%|███▌| 4.34G/4.92G [04:38<00:38, 15.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.35G/4.92G [04:39<00:36, 15.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.36G/4.92G [04:40<00:34, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.37G/4.92G [04:40<00:33, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.38G/4.92G [04:41<00:32, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  89%|███▌| 4.39G/4.92G [04:41<00:31, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.40G/4.92G [04:42<00:30, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.41G/4.92G [04:43<00:29, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.42G/4.92G [04:43<00:28, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.44G/4.92G [04:44<00:28, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  90%|███▌| 4.45G/4.92G [04:45<00:27, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.46G/4.92G [04:45<00:26, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.47G/4.92G [04:46<00:26, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.48G/4.92G [04:47<00:32, 13.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  91%|███▋| 4.49G/4.92G [04:48<00:29, 14.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.50G/4.92G [04:48<00:27, 15.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.51G/4.92G [04:50<00:35, 11.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.52G/4.92G [04:50<00:30, 12.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.53G/4.92G [04:51<00:27, 13.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  92%|███▋| 4.54G/4.92G [04:51<00:25, 14.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.55G/4.92G [04:52<00:23, 15.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.56G/4.92G [04:53<00:22, 15.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.57G/4.92G [04:53<00:21, 16.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.58G/4.92G [04:54<00:20, 16.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  93%|███▋| 4.59G/4.92G [04:54<00:19, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▋| 4.60G/4.92G [04:55<00:18, 16.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.61G/4.92G [04:56<00:17, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.62G/4.92G [04:56<00:17, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.63G/4.92G [04:57<00:16, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  94%|███▊| 4.65G/4.92G [04:57<00:15, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.66G/4.92G [04:58<00:15, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.67G/4.92G [04:59<00:14, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.68G/4.92G [04:59<00:13, 17.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  95%|███▊| 4.69G/4.92G [05:00<00:16, 13.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.70G/4.92G [05:01<00:15, 14.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.71G/4.92G [05:03<00:18, 11.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.72G/4.92G [05:03<00:16, 12.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.73G/4.92G [05:04<00:14, 13.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  96%|███▊| 4.74G/4.92G [05:04<00:12, 14.3MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▊| 4.75G/4.92G [05:05<00:11, 15.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▊| 4.76G/4.92G [05:06<00:09, 15.5MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▉| 4.77G/4.92G [05:06<00:09, 16.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▉| 4.78G/4.92G [05:07<00:08, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  97%|███▉| 4.79G/4.92G [05:08<00:07, 16.6MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.80G/4.92G [05:08<00:06, 16.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.81G/4.92G [05:09<00:06, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.82G/4.92G [05:09<00:05, 16.9MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  98%|███▉| 4.83G/4.92G [05:10<00:04, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.84G/4.92G [05:11<00:04, 17.0MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.85G/4.92G [05:11<00:03, 15.7MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.87G/4.92G [05:12<00:03, 16.1MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.88G/4.92G [05:13<00:02, 16.4MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors:  99%|███▉| 4.89G/4.92G [05:14<00:02, 13.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|███▉| 4.90G/4.92G [05:14<00:01, 14.2MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|███▉| 4.91G/4.92G [05:15<00:00, 12.8MB/s]\u001b[A\n",
      "model-00003-of-00004.safetensors: 100%|████| 4.92G/4.92G [05:16<00:00, 15.5MB/s]\u001b[A\n",
      "Downloading shards:  75%|██████████████████      | 3/4 [16:00<05:19, 319.75s/it]\n",
      "model-00004-of-00004.safetensors:   0%|             | 0.00/1.17G [00:00<?, ?B/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   1%|    | 10.5M/1.17G [00:00<01:07, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   2%|    | 21.0M/1.17G [00:01<01:06, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   3%|    | 31.5M/1.17G [00:01<01:05, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   4%|▏   | 41.9M/1.17G [00:02<01:05, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   4%|▏   | 52.4M/1.17G [00:03<01:05, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   5%|▏   | 62.9M/1.17G [00:03<01:04, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   6%|▎   | 73.4M/1.17G [00:04<01:03, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   7%|▎   | 83.9M/1.17G [00:04<01:03, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   8%|▎   | 94.4M/1.17G [00:05<01:02, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:   9%|▍    | 105M/1.17G [00:06<01:02, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  10%|▍    | 115M/1.17G [00:06<01:01, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  11%|▌    | 126M/1.17G [00:07<01:00, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  12%|▌    | 136M/1.17G [00:07<01:00, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▋    | 147M/1.17G [00:09<01:15, 13.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  13%|▋    | 157M/1.17G [00:09<01:09, 14.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  14%|▋    | 168M/1.17G [00:10<01:19, 12.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  15%|▊    | 178M/1.17G [00:11<01:13, 13.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  16%|▊    | 189M/1.17G [00:12<01:08, 14.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  17%|▊    | 199M/1.17G [00:12<01:04, 15.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  18%|▉    | 210M/1.17G [00:13<01:01, 15.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  19%|▉    | 220M/1.17G [00:13<00:59, 16.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  20%|▉    | 231M/1.17G [00:14<00:57, 16.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  21%|█    | 241M/1.17G [00:15<00:55, 16.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|█    | 252M/1.17G [00:15<00:54, 16.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  22%|█    | 262M/1.17G [00:16<00:53, 16.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  23%|█▏   | 273M/1.17G [00:16<00:52, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  24%|█▏   | 283M/1.17G [00:17<00:51, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  25%|█▎   | 294M/1.17G [00:18<00:51, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  26%|█▎   | 304M/1.17G [00:18<00:50, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  27%|█▎   | 315M/1.17G [00:19<00:49, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  28%|█▍   | 325M/1.17G [00:20<00:49, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  29%|█▍   | 336M/1.17G [00:20<00:48, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  30%|█▍   | 346M/1.17G [00:21<00:48, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  31%|█▌   | 357M/1.17G [00:21<00:47, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  31%|█▌   | 367M/1.17G [00:22<00:46, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  32%|█▌   | 377M/1.17G [00:23<00:46, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  33%|█▋   | 388M/1.17G [00:23<00:45, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  34%|█▋   | 398M/1.17G [00:24<00:44, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  35%|█▊   | 409M/1.17G [00:25<00:55, 13.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  36%|█▊   | 419M/1.17G [00:26<00:51, 14.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  37%|█▊   | 430M/1.17G [00:26<00:48, 15.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  38%|█▉   | 440M/1.17G [00:27<00:46, 15.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  39%|█▉   | 451M/1.17G [00:27<00:44, 16.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  39%|█▉   | 461M/1.17G [00:28<00:43, 16.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  40%|██   | 472M/1.17G [00:29<00:41, 16.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  41%|██   | 482M/1.17G [00:29<00:40, 16.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  42%|██   | 493M/1.17G [00:30<00:39, 16.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  43%|██▏  | 503M/1.17G [00:30<00:39, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  44%|██▏  | 514M/1.17G [00:31<00:38, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  45%|██▏  | 524M/1.17G [00:32<00:37, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  46%|██▎  | 535M/1.17G [00:32<00:37, 16.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  47%|██▎  | 545M/1.17G [00:33<00:36, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|██▍  | 556M/1.17G [00:34<00:35, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  48%|██▍  | 566M/1.17G [00:34<00:35, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  49%|██▍  | 577M/1.17G [00:35<00:34, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  50%|██▌  | 587M/1.17G [00:35<00:33, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  51%|██▌  | 598M/1.17G [00:36<00:33, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  52%|██▌  | 608M/1.17G [00:37<00:32, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  53%|██▋  | 619M/1.17G [00:37<00:32, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  54%|██▋  | 629M/1.17G [00:38<00:31, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  55%|██▋  | 640M/1.17G [00:38<00:30, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  56%|██▊  | 650M/1.17G [00:39<00:30, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  57%|██▊  | 661M/1.17G [00:40<00:29, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  57%|██▊  | 671M/1.17G [00:40<00:28, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  58%|██▉  | 682M/1.17G [00:41<00:28, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  59%|██▉  | 692M/1.17G [00:41<00:27, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  60%|███  | 703M/1.17G [00:43<00:34, 13.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  61%|███  | 713M/1.17G [00:43<00:31, 14.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  62%|███  | 724M/1.17G [00:45<00:37, 11.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  63%|███▏ | 734M/1.17G [00:45<00:33, 13.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  64%|███▏ | 744M/1.17G [00:46<00:30, 14.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  65%|███▏ | 755M/1.17G [00:46<00:27, 14.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 765M/1.17G [00:47<00:26, 15.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  66%|███▎ | 776M/1.17G [00:48<00:24, 15.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  67%|███▎ | 786M/1.17G [00:48<00:23, 16.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  68%|███▍ | 797M/1.17G [00:49<00:22, 16.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  69%|███▍ | 807M/1.17G [00:49<00:21, 16.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  70%|███▌ | 818M/1.17G [00:50<00:20, 16.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  71%|███▌ | 828M/1.17G [00:51<00:20, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  72%|███▌ | 839M/1.17G [00:51<00:19, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  73%|███▋ | 849M/1.17G [00:52<00:18, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  74%|███▋ | 860M/1.17G [00:52<00:18, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  75%|███▋ | 870M/1.17G [00:53<00:17, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  75%|███▊ | 881M/1.17G [00:54<00:16, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  76%|███▊ | 891M/1.17G [00:54<00:16, 17.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  77%|███▊ | 902M/1.17G [00:55<00:15, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  78%|███▉ | 912M/1.17G [00:56<00:14, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  79%|███▉ | 923M/1.17G [00:57<00:18, 13.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  80%|███▉ | 933M/1.17G [00:57<00:16, 14.4MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  81%|████ | 944M/1.17G [00:58<00:14, 15.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  82%|████ | 954M/1.17G [00:59<00:17, 12.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  83%|████▏| 965M/1.17G [01:00<00:15, 13.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  83%|████▏| 975M/1.17G [01:00<00:13, 14.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  84%|████▏| 986M/1.17G [01:01<00:12, 14.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  85%|████▎| 996M/1.17G [01:02<00:11, 15.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  86%|███▍| 1.01G/1.17G [01:02<00:10, 16.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  87%|███▍| 1.02G/1.17G [01:03<00:09, 16.3MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  88%|███▌| 1.03G/1.17G [01:03<00:08, 16.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  89%|███▌| 1.04G/1.17G [01:04<00:07, 16.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  90%|███▌| 1.05G/1.17G [01:05<00:07, 16.8MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  91%|███▋| 1.06G/1.17G [01:05<00:06, 16.9MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  92%|███▋| 1.07G/1.17G [01:06<00:05, 17.0MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  92%|███▋| 1.08G/1.17G [01:07<00:05, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  93%|███▋| 1.09G/1.17G [01:07<00:04, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  94%|███▊| 1.10G/1.17G [01:08<00:03, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  95%|███▊| 1.11G/1.17G [01:08<00:03, 17.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  96%|███▊| 1.12G/1.17G [01:10<00:03, 13.6MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  97%|███▉| 1.13G/1.17G [01:10<00:02, 14.5MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  98%|███▉| 1.14G/1.17G [01:11<00:01, 15.2MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors:  99%|███▉| 1.15G/1.17G [01:11<00:00, 15.7MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|███▉| 1.16G/1.17G [01:12<00:00, 16.1MB/s]\u001b[A\n",
      "model-00004-of-00004.safetensors: 100%|████| 1.17G/1.17G [01:12<00:00, 16.1MB/s]\u001b[A\n",
      "Downloading shards: 100%|████████████████████████| 4/4 [17:15<00:00, 258.82s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 4/4 [00:03<00:00,  1.20it/s]\n",
      "generation_config.json: 100%|██████████████████| 184/184 [00:00<00:00, 36.2kB/s]\n",
      "tokenizer_config.json: 100%|████████████████| 50.9k/50.9k [00:00<00:00, 283kB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:01<00:00, 7.11MB/s]\n",
      "special_tokens_map.json: 100%|██████████████████| 296/296 [00:00<00:00, 184kB/s]\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> True\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:467: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if sequence_length != 1:\n",
      "graph(%self.1 : __torch__.transformers.models.llama.modeling_llama.LlamaForCausalLM,\n",
      "      %input_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),\n",
      "      %attention_mask : Long(2, 32, strides=[32, 1], requires_grad=0, device=cpu),\n",
      "      %position_ids : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu),\n",
      "      %past_key_values : ((Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)))):\n",
      "  %lm_head : __torch__.torch.nn.modules.linear.___torch_mangle_443.Linear = prim::GetAttr[name=\"lm_head\"](%self.1)\n",
      "  %model : __torch__.transformers.models.llama.modeling_llama.LlamaModel = prim::GetAttr[name=\"model\"](%self.1)\n",
      "  %7 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %8 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %9 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %10 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %11 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %12 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %13 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %14 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %15 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %16 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %17 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %18 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %19 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %20 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %21 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %22 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %23 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %24 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %25 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %26 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %27 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %28 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %29 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %30 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %31 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %32 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %33 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %34 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %35 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %36 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %37 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)), %38 : (Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu)) = prim::TupleUnpack(%past_key_values)\n",
      "  %key_states.1 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %40 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%7)\n",
      "  %41 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %42 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%8)\n",
      "  %43 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %44 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%9)\n",
      "  %45 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %46 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%10)\n",
      "  %47 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %48 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%11)\n",
      "  %49 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %50 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%12)\n",
      "  %51 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %52 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%13)\n",
      "  %53 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %54 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%14)\n",
      "  %55 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %56 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%15)\n",
      "  %57 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %58 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%16)\n",
      "  %59 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %60 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%17)\n",
      "  %61 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %62 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%18)\n",
      "  %63 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %64 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%19)\n",
      "  %65 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %66 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%20)\n",
      "  %67 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %68 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%21)\n",
      "  %69 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %70 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%22)\n",
      "  %71 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %72 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%23)\n",
      "  %73 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %74 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%24)\n",
      "  %75 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %76 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%25)\n",
      "  %77 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %78 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%26)\n",
      "  %79 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %80 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%27)\n",
      "  %81 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %82 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%28)\n",
      "  %83 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %84 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%29)\n",
      "  %85 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %86 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%30)\n",
      "  %87 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %88 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%31)\n",
      "  %89 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %90 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%32)\n",
      "  %91 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %92 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%33)\n",
      "  %93 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %94 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%34)\n",
      "  %95 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %96 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%35)\n",
      "  %97 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %98 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%36)\n",
      "  %99 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %100 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%37)\n",
      "  %101 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu), %102 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%38)\n",
      "  %209 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1e-05}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %210 : bool = prim::Constant[value=1](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %211 : float = prim::Constant[value=0.](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %212 : Long(requires_grad=0, device=cpu) = prim::Constant[value={4}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %213 : int = prim::Constant[value=4](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %214 : int = prim::Constant[value=-2](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %215 : Long(requires_grad=0, device=cpu) = prim::Constant[value={2}](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %216 : int = prim::Constant[value=8](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %217 : int = prim::Constant[value=128](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %218 : int = prim::Constant[value=32](), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %219 : Double(requires_grad=0, device=cpu) = prim::Constant[value={1}](), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:159:0\n",
      "  %220 : float = prim::Constant[value=-65504.](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %221 : int = prim::Constant[value=3](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %222 : int = prim::Constant[value=9223372036854775807](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %223 : int = prim::Constant[value=0](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %224 : Double(requires_grad=0, device=cpu) = prim::Constant[value={-65504}](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:464:0\n",
      "  %225 : int = prim::Constant[value=6](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %226 : Device = prim::Constant[value=\"cpu\"](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %227 : NoneType = prim::Constant(), scope: __module.model\n",
      "  %228 : int = prim::Constant[value=1](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:356:0\n",
      "  %229 : Long(requires_grad=0, device=cpu) = prim::Constant[value={0}](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:356:0\n",
      "  %230 : int = prim::Constant[value=2](), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:356:0\n",
      "  %231 : int = prim::Constant[value=-1](), scope: __module.model/__module.model.embed_tokens # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %232 : bool = prim::Constant[value=0](), scope: __module.model/__module.model.embed_tokens # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %norm : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_441.LlamaRMSNorm = prim::GetAttr[name=\"norm\"](%model)\n",
      "  %layers : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_31 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_440.LlamaDecoderLayer = prim::GetAttr[name=\"31\"](%layers)\n",
      "  %layers.63 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_30 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_426.LlamaDecoderLayer = prim::GetAttr[name=\"30\"](%layers.63)\n",
      "  %layers.61 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_29 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_412.LlamaDecoderLayer = prim::GetAttr[name=\"29\"](%layers.61)\n",
      "  %layers.59 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_28 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_398.LlamaDecoderLayer = prim::GetAttr[name=\"28\"](%layers.59)\n",
      "  %layers.57 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_27 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_384.LlamaDecoderLayer = prim::GetAttr[name=\"27\"](%layers.57)\n",
      "  %layers.55 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_26 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_370.LlamaDecoderLayer = prim::GetAttr[name=\"26\"](%layers.55)\n",
      "  %layers.53 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_25 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_356.LlamaDecoderLayer = prim::GetAttr[name=\"25\"](%layers.53)\n",
      "  %layers.51 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_24 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_342.LlamaDecoderLayer = prim::GetAttr[name=\"24\"](%layers.51)\n",
      "  %layers.49 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_23 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_328.LlamaDecoderLayer = prim::GetAttr[name=\"23\"](%layers.49)\n",
      "  %layers.47 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_22 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_314.LlamaDecoderLayer = prim::GetAttr[name=\"22\"](%layers.47)\n",
      "  %layers.45 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_21 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_300.LlamaDecoderLayer = prim::GetAttr[name=\"21\"](%layers.45)\n",
      "  %layers.43 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_20 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_286.LlamaDecoderLayer = prim::GetAttr[name=\"20\"](%layers.43)\n",
      "  %layers.41 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_19 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_272.LlamaDecoderLayer = prim::GetAttr[name=\"19\"](%layers.41)\n",
      "  %layers.39 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_18 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_258.LlamaDecoderLayer = prim::GetAttr[name=\"18\"](%layers.39)\n",
      "  %layers.37 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_17 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_244.LlamaDecoderLayer = prim::GetAttr[name=\"17\"](%layers.37)\n",
      "  %layers.35 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_16 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_230.LlamaDecoderLayer = prim::GetAttr[name=\"16\"](%layers.35)\n",
      "  %layers.33 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_15 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_216.LlamaDecoderLayer = prim::GetAttr[name=\"15\"](%layers.33)\n",
      "  %layers.31 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_14 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_202.LlamaDecoderLayer = prim::GetAttr[name=\"14\"](%layers.31)\n",
      "  %layers.29 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_13 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_188.LlamaDecoderLayer = prim::GetAttr[name=\"13\"](%layers.29)\n",
      "  %layers.27 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_12 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_174.LlamaDecoderLayer = prim::GetAttr[name=\"12\"](%layers.27)\n",
      "  %layers.25 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_11 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_160.LlamaDecoderLayer = prim::GetAttr[name=\"11\"](%layers.25)\n",
      "  %layers.23 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_10 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_146.LlamaDecoderLayer = prim::GetAttr[name=\"10\"](%layers.23)\n",
      "  %layers.21 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_9 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_132.LlamaDecoderLayer = prim::GetAttr[name=\"9\"](%layers.21)\n",
      "  %layers.19 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_8 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_118.LlamaDecoderLayer = prim::GetAttr[name=\"8\"](%layers.19)\n",
      "  %layers.17 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_7 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_104.LlamaDecoderLayer = prim::GetAttr[name=\"7\"](%layers.17)\n",
      "  %layers.15 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_6 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_90.LlamaDecoderLayer = prim::GetAttr[name=\"6\"](%layers.15)\n",
      "  %layers.13 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_5 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_76.LlamaDecoderLayer = prim::GetAttr[name=\"5\"](%layers.13)\n",
      "  %layers.11 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_4 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_62.LlamaDecoderLayer = prim::GetAttr[name=\"4\"](%layers.11)\n",
      "  %layers.9 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_3 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_48.LlamaDecoderLayer = prim::GetAttr[name=\"3\"](%layers.9)\n",
      "  %layers.7 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_2 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_34.LlamaDecoderLayer = prim::GetAttr[name=\"2\"](%layers.7)\n",
      "  %layers.5 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_1 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_20.LlamaDecoderLayer = prim::GetAttr[name=\"1\"](%layers.5)\n",
      "  %layers.3 : __torch__.torch.nn.modules.container.ModuleList = prim::GetAttr[name=\"layers\"](%model)\n",
      "  %_0 : __torch__.transformers.models.llama.modeling_llama.LlamaDecoderLayer = prim::GetAttr[name=\"0\"](%layers.3)\n",
      "  %rotary_emb : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_442.LlamaRotaryEmbedding = prim::GetAttr[name=\"rotary_emb\"](%model)\n",
      "  %embed_tokens : __torch__.torch.nn.modules.sparse.Embedding = prim::GetAttr[name=\"embed_tokens\"](%model)\n",
      "  %weight.1 : Tensor = prim::GetAttr[name=\"weight\"](%embed_tokens)\n",
      "  %inputs_embeds : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::embedding(%weight.1, %input_ids, %231, %232, %232), scope: __module.model/__module.model.embed_tokens # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %302 : int = aten::size(%key_states.1, %230), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:356:0\n",
      "  %303 : Long(device=cpu) = prim::NumToTensor(%302), scope: __module.model\n",
      "  %304 : Long(requires_grad=0, device=cpu) = aten::add(%303, %229, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:356:0\n",
      "  %305 : int = aten::size(%key_states.1, %230), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:373:0\n",
      "  %past_seen_tokens : Long(device=cpu) = prim::NumToTensor(%305), scope: __module.model\n",
      "  %307 : int = aten::size(%inputs_embeds, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:907:0\n",
      "  %308 : Long(device=cpu) = prim::NumToTensor(%307), scope: __module.model\n",
      "  %309 : Long(requires_grad=0, device=cpu) = aten::add(%past_seen_tokens, %308, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:907:0\n",
      "  %310 : Scalar = aten::ScalarImplicit(%309), scope: __module.model\n",
      "  %cache_position : Long(16, strides=[1], requires_grad=0, device=cpu) = aten::arange(%305, %310, %227, %227, %226, %232), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %312 : int = aten::size(%inputs_embeds, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:446:0\n",
      "  %313 : int = aten::size(%attention_mask, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:451:0\n",
      "  %314 : int[] = prim::ListConstruct(%312, %313), scope: __module.model\n",
      "  %315 : Float(16, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::full(%314, %228, %225, %227, %226, %232), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %316 : Float(16, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::mul(%315, %224), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:464:0\n",
      "  %causal_mask.1 : Float(16, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::triu(%316, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %318 : Long(32, strides=[1], requires_grad=0, device=cpu) = aten::arange(%313, %227, %227, %226, %232), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %319 : int[] = prim::ListConstruct(%231, %228), scope: __module.model\n",
      "  %320 : Long(16, 1, strides=[1, 1], requires_grad=0, device=cpu) = aten::reshape(%cache_position, %319), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:469:0\n",
      "  %321 : Bool(16, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::gt(%318, %320), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:469:0\n",
      "  %causal_mask.3 : Float(16, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::mul_(%causal_mask.1, %321), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:469:0\n",
      "  %323 : Float(1, 16, 32, strides=[512, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%causal_mask.3, %223), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %324 : Float(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%323, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %325 : Float(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%324, %230, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %326 : Float(1, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%325, %221, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %327 : int = aten::size(%inputs_embeds, %223), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %328 : int[] = prim::ListConstruct(%327, %228, %231, %231), scope: __module.model\n",
      "  %causal_mask.5 : Float(2, 1, 16, 32, strides=[0, 512, 32, 1], requires_grad=0, device=cpu) = aten::expand(%326, %328, %232), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:470:0\n",
      "  %causal_mask : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::clone(%causal_mask.5, %227), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:472:0\n",
      "  %331 : int = aten::size(%attention_mask, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:473:0\n",
      "  %332 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %333 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%332, %228, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %334 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%333, %230, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %335 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%334, %221, %223, %331, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %336 : Long(2, 32, strides=[32, 1], requires_grad=0, device=cpu) = aten::slice(%attention_mask, %223, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %337 : Long(2, 1, 32, strides=[32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%336, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %338 : Long(2, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%337, %230), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %339 : Long(2, 1, 1, 32, strides=[32, 32, 32, 1], requires_grad=0, device=cpu) = aten::slice(%338, %221, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %padding_mask.1 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::add(%335, %339, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:474:0\n",
      "  %padding_mask : Bool(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::eq(%padding_mask.1, %223), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:475:0\n",
      "  %342 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %343 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%342, %228, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %344 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%343, %230, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %345 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%344, %221, %223, %331, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %346 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::masked_fill(%345, %padding_mask, %220), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %347 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %348 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%347, %228, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %349 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%348, %230, %223, %222, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %350 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%349, %221, %223, %331, %228), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %351 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::copy_(%350, %346, %232), scope: __module.model # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/optimum/exporters/openvino/model_patcher.py:476:0\n",
      "  %inv_freq : Tensor = prim::GetAttr[name=\"inv_freq\"](%rotary_emb)\n",
      "  %353 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%inv_freq, %223), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:147:0\n",
      "  %354 : Float(1, 64, strides=[64, 1], requires_grad=0, device=cpu) = aten::slice(%353, %228, %223, %222, %228), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:147:0\n",
      "  %355 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%354, %230), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:147:0\n",
      "  %356 : Float(1, 64, 1, strides=[64, 1, 1], requires_grad=0, device=cpu) = aten::to(%355, %225, %232, %232, %227), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:147:0\n",
      "  %357 : int = aten::size(%position_ids, %223), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:147:0\n",
      "  %358 : int[] = prim::ListConstruct(%357, %231, %228), scope: __module.model/__module.model.rotary_emb\n",
      "  %inv_freq_expanded.1 : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::expand(%356, %358, %232), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:147:0\n",
      "  %360 : Long(2, 16, strides=[16, 1], requires_grad=0, device=cpu) = aten::slice(%position_ids, %223, %223, %222, %228), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:148:0\n",
      "  %361 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%360, %228), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:148:0\n",
      "  %362 : Long(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::slice(%361, %230, %223, %222, %228), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:148:0\n",
      "  %position_ids_expanded.1 : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%362, %225, %232, %232, %227), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:148:0\n",
      "  %inv_freq_expanded : Float(2, 64, 1, strides=[0, 1, 1], requires_grad=0, device=cpu) = aten::to(%inv_freq_expanded.1, %225, %232, %232, %227), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:153:0\n",
      "  %position_ids_expanded : Float(2, 1, 16, strides=[16, 16, 1], requires_grad=0, device=cpu) = aten::to(%position_ids_expanded.1, %225, %232, %232, %227), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:153:0\n",
      "  %366 : Float(2, 64, 16, strides=[1024, 16, 1], requires_grad=0, device=cpu) = aten::matmul(%inv_freq_expanded, %position_ids_expanded), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:153:0\n",
      "  %367 : Float(2, 16, 64, strides=[1024, 1, 16], requires_grad=0, device=cpu) = aten::transpose(%366, %228, %230), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:153:0\n",
      "  %368 : Tensor[] = prim::ListConstruct(%367, %367), scope: __module.model/__module.model.rotary_emb\n",
      "  %emb : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%368, %231), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %cos.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::cos(%emb), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:155:0\n",
      "  %sin.1 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::sin(%emb), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:156:0\n",
      "  %cos.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%cos.1, %219), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:159:0\n",
      "  %sin.3 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%sin.1, %219), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:160:0\n",
      "  %cos.5 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%cos.3, %225, %232, %232, %227), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:162:0\n",
      "  %sin.5 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = aten::to(%sin.3, %225, %232, %232, %227), scope: __module.model/__module.model.rotary_emb # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:162:0\n",
      "  %376 : (Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%cos.5, %sin.5)\n",
      "  %377 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu), %378 : Float(2, 16, 128, strides=[2048, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%376)\n",
      "  %mlp.1 : __torch__.transformers.models.llama.modeling_llama.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_0)\n",
      "  %post_attention_layernorm.1 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_6.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_0)\n",
      "  %self_attn.3 : __torch__.transformers.models.llama.modeling_llama.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_0)\n",
      "  %input_layernorm.1 : __torch__.transformers.models.llama.modeling_llama.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_0)\n",
      "  %weight.3 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.1)\n",
      "  %hidden_states.1 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%inputs_embeds, %225, %232, %232, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %385 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.1, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %386 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm\n",
      "  %variance.1 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%385, %386, %210, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %388 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.1, %209, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %389 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%388), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.3 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.1, %389), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.5 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.3, %225, %232, %232, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.7 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.3, %hidden_states.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %393 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.7, %hidden_states.1)\n",
      "  %394 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %395 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%393)\n",
      "  %o_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_2.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.3)\n",
      "  %v_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_1.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.3)\n",
      "  %k_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_0.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.3)\n",
      "  %q_proj.1 : __torch__.torch.nn.modules.linear.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.3)\n",
      "  %400 : int = aten::size(%394, %223), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %401 : int = aten::size(%394, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.5 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.1)\n",
      "  %query_states.1 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%394, %weight.5, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.7 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.1)\n",
      "  %key_states.3 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%394, %weight.7, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.9 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.1)\n",
      "  %value_states.1 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%394, %weight.9, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %408 : int[] = prim::ListConstruct(%400, %401, %218, %217), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %409 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.1, %408), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%409, %228, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %411 : int[] = prim::ListConstruct(%400, %401, %216, %217), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %412 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.3, %411), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.1 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%412, %228, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %414 : int[] = prim::ListConstruct(%400, %401, %216, %217), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %415 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.1, %414), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %416 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%415, %228, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.7 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %419 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.1, %cos.7), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %420 : int = aten::size(%q.1, %221), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %421 : Long(device=cpu) = prim::NumToTensor(%420), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %422 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%421, %215), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %423 : int = aten::Int(%422), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %424 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %221, %223, %423, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %425 : int = aten::size(%q.1, %221), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %426 : Long(device=cpu) = prim::NumToTensor(%425), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %427 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%426, %215), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %428 : int = aten::Int(%427), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %x2.1 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.1, %221, %428, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %430 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %431 : Tensor[] = prim::ListConstruct(%430, %424), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %432 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%431, %231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %433 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%432, %sin.7), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %434 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%419, %433, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %435 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.1, %cos.7), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %436 : int = aten::size(%k.1, %221), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %437 : Long(device=cpu) = prim::NumToTensor(%436), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %438 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%437, %215), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %439 : int = aten::Int(%438), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %440 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %221, %223, %439, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %441 : int = aten::size(%k.1, %221), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %442 : Long(device=cpu) = prim::NumToTensor(%441), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %443 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%442, %215), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %444 : int = aten::Int(%443), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %x2.3 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.1, %221, %444, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %446 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.3), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %447 : Tensor[] = prim::ListConstruct(%446, %440), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %448 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%447, %231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %449 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%448, %sin.7), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %key_states.5 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%435, %449, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %451 : int = aten::size(%key_states.5, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:356:0\n",
      "  %452 : Long(device=cpu) = prim::NumToTensor(%451), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %453 : Long(requires_grad=0, device=cpu) = aten::add_(%304, %452, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/cache_utils.py:356:0\n",
      "  %454 : Tensor[] = prim::ListConstruct(%key_states.1, %key_states.5), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %hidden_states.9 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%454, %214), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %456 : Tensor[] = prim::ListConstruct(%40, %416), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %hidden_states.13 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%456, %214), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %458 : int = aten::size(%hidden_states.9, %223), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %459 : int = aten::size(%hidden_states.9, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.1 : Long(device=cpu) = prim::NumToTensor(%459), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %461 : int = aten::size(%hidden_states.9, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %462 : int = aten::size(%hidden_states.9, %221), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %463 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.9, %223, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %464 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%463, %228, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %465 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%464, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %466 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%465, %221, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %467 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%466, %213, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %468 : int[] = prim::ListConstruct(%458, %459, %213, %461, %462), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %hidden_states.11 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%467, %468, %232), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %470 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.1, %212), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %471 : int = aten::Int(%470), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %472 : int[] = prim::ListConstruct(%458, %471, %461, %462), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %key_states.7 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.11, %472), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %474 : int = aten::size(%hidden_states.13, %223), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %475 : int = aten::size(%hidden_states.13, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.3 : Long(device=cpu) = prim::NumToTensor(%475), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %477 : int = aten::size(%hidden_states.13, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %478 : int = aten::size(%hidden_states.13, %221), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %479 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.13, %223, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %480 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%479, %228, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %481 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%480, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %482 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%481, %221, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %483 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%482, %213, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %484 : int[] = prim::ListConstruct(%474, %475, %213, %477, %478), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %hidden_states.15 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%483, %484, %232), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %486 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.3, %212), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %487 : int = aten::Int(%486), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %488 : int[] = prim::ListConstruct(%474, %487, %477, %478), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %489 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.15, %488), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %490 : int = aten::size(%key_states.7, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %491 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %492 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%491, %228, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %493 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%492, %230, %223, %222, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %494 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%493, %221, %223, %490, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.1 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%434, %key_states.7, %489, %494, %211, %232, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %496 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.1, %228, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.3 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%496, %223), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %498 : int[] = prim::ListConstruct(%400, %401, %231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn\n",
      "  %499 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.3, %498), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.11 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.1)\n",
      "  %hidden_states.17 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%499, %weight.11, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.self_attn/__module.model.layers.0.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %502 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.17, %hidden_states.9, %hidden_states.13)\n",
      "  %503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %504 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %505 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%502)\n",
      "  %hidden_states.19 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%395, %503, %228), scope: __module.model/__module.model.layers.0 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.13 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.1)\n",
      "  %hidden_states.21 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.19, %225, %232, %232, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %509 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.21, %230), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %510 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm\n",
      "  %variance.3 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%509, %510, %210, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %512 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.3, %209, %228), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %513 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%512), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.23 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.21, %513), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.25 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.23, %225, %232, %232, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %516 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.13, %hidden_states.25), scope: __module.model/__module.model.layers.0/__module.model.layers.0.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %517 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%516, %hidden_states.21)\n",
      "  %518 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %519 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%517)\n",
      "  %down_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_5.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.1)\n",
      "  %up_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_4.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.1)\n",
      "  %gate_proj.1 : __torch__.torch.nn.modules.linear.___torch_mangle_3.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.1)\n",
      "  %weight.15 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.1)\n",
      "  %input.1 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%518, %weight.15, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %525 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.1), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.17 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.1)\n",
      "  %527 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%518, %weight.17, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %528 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%525, %527), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.19 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.1)\n",
      "  %hidden_states.27 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%528, %weight.19, %227), scope: __module.model/__module.model.layers.0/__module.model.layers.0.mlp/__module.model.layers.0.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.29 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%519, %hidden_states.27, %228), scope: __module.model/__module.model.layers.0 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %532 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.29, %504, %505)\n",
      "  %533 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %534 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %535 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%532)\n",
      "  %mlp.3 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_17.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_1)\n",
      "  %post_attention_layernorm.3 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_19.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_1)\n",
      "  %self_attn.5 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_12.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_1)\n",
      "  %input_layernorm.3 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_18.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_1)\n",
      "  %weight.21 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.3)\n",
      "  %hidden_states.31 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%533, %225, %232, %232, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %542 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.31, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %543 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm\n",
      "  %variance.5 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%542, %543, %210, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %545 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.5, %209, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %546 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%545), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.33 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.31, %546), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.35 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.33, %225, %232, %232, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.37 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.21, %hidden_states.35), scope: __module.model/__module.model.layers.1/__module.model.layers.1.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %550 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.37, %hidden_states.31)\n",
      "  %551 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %552 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%550)\n",
      "  %o_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_10.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.5)\n",
      "  %v_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_9.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.5)\n",
      "  %k_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_8.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.5)\n",
      "  %q_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_7.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.5)\n",
      "  %557 : int = aten::size(%551, %223), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %558 : int = aten::size(%551, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.23 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.3)\n",
      "  %query_states.3 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%551, %weight.23, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.25 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.3)\n",
      "  %key_states.9 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%551, %weight.25, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.27 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.3)\n",
      "  %value_states.3 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%551, %weight.27, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %565 : int[] = prim::ListConstruct(%557, %558, %218, %217), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %566 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.3, %565), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.3 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%566, %228, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %568 : int[] = prim::ListConstruct(%557, %558, %216, %217), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %569 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.9, %568), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.3 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%569, %228, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %571 : int[] = prim::ListConstruct(%557, %558, %216, %217), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %572 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.3, %571), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %573 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%572, %228, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.9 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %576 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.3, %cos.9), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %577 : int = aten::size(%q.3, %221), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %578 : Long(device=cpu) = prim::NumToTensor(%577), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %579 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%578, %215), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %580 : int = aten::Int(%579), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %581 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %221, %223, %580, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %582 : int = aten::size(%q.3, %221), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %583 : Long(device=cpu) = prim::NumToTensor(%582), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %584 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%583, %215), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %585 : int = aten::Int(%584), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %x2.5 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.3, %221, %585, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %587 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.5), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %588 : Tensor[] = prim::ListConstruct(%587, %581), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %589 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%588, %231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %590 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%589, %sin.9), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %591 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%576, %590, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %592 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.3, %cos.9), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %593 : int = aten::size(%k.3, %221), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %594 : Long(device=cpu) = prim::NumToTensor(%593), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %595 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%594, %215), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %596 : int = aten::Int(%595), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %597 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %221, %223, %596, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %598 : int = aten::size(%k.3, %221), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %599 : Long(device=cpu) = prim::NumToTensor(%598), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %600 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%599, %215), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %601 : int = aten::Int(%600), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %x2.7 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.3, %221, %601, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %603 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.7), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %604 : Tensor[] = prim::ListConstruct(%603, %597), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %605 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%604, %231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %606 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%605, %sin.9), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %607 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%592, %606, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %608 : Tensor[] = prim::ListConstruct(%41, %607), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %hidden_states.39 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%608, %214), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %610 : Tensor[] = prim::ListConstruct(%42, %573), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %hidden_states.43 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%610, %214), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %612 : int = aten::size(%hidden_states.39, %223), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %613 : int = aten::size(%hidden_states.39, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.5 : Long(device=cpu) = prim::NumToTensor(%613), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %615 : int = aten::size(%hidden_states.39, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %616 : int = aten::size(%hidden_states.39, %221), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %617 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.39, %223, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %618 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%617, %228, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %619 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%618, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %620 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%619, %221, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %621 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%620, %213, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %622 : int[] = prim::ListConstruct(%612, %613, %213, %615, %616), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %hidden_states.41 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%621, %622, %232), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %624 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.5, %212), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %625 : int = aten::Int(%624), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %626 : int[] = prim::ListConstruct(%612, %625, %615, %616), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %key_states.11 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.41, %626), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %628 : int = aten::size(%hidden_states.43, %223), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %629 : int = aten::size(%hidden_states.43, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.7 : Long(device=cpu) = prim::NumToTensor(%629), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %631 : int = aten::size(%hidden_states.43, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %632 : int = aten::size(%hidden_states.43, %221), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %633 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.43, %223, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %634 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%633, %228, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %635 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%634, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %636 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%635, %221, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %637 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%636, %213, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %638 : int[] = prim::ListConstruct(%628, %629, %213, %631, %632), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %hidden_states.45 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%637, %638, %232), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %640 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.7, %212), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %641 : int = aten::Int(%640), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %642 : int[] = prim::ListConstruct(%628, %641, %631, %632), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %643 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.45, %642), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %644 : int = aten::size(%key_states.11, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %645 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %646 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%645, %228, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %647 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%646, %230, %223, %222, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %648 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%647, %221, %223, %644, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%591, %key_states.11, %643, %648, %211, %232, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %650 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.5, %228, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.7 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%650, %223), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %652 : int[] = prim::ListConstruct(%557, %558, %231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn\n",
      "  %653 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.7, %652), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.29 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.3)\n",
      "  %hidden_states.47 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%653, %weight.29, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.self_attn/__module.model.layers.1.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %656 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.47, %hidden_states.39, %hidden_states.43)\n",
      "  %657 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %658 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %659 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%656)\n",
      "  %hidden_states.49 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%552, %657, %228), scope: __module.model/__module.model.layers.1 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.31 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.3)\n",
      "  %hidden_states.51 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.49, %225, %232, %232, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %663 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.51, %230), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %664 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm\n",
      "  %variance.7 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%663, %664, %210, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %666 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.7, %209, %228), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %667 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%666), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.53 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.51, %667), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.55 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.53, %225, %232, %232, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %670 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.31, %hidden_states.55), scope: __module.model/__module.model.layers.1/__module.model.layers.1.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %671 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%670, %hidden_states.51)\n",
      "  %672 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %673 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%671)\n",
      "  %down_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_15.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.3)\n",
      "  %up_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_14.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.3)\n",
      "  %gate_proj.3 : __torch__.torch.nn.modules.linear.___torch_mangle_13.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.3)\n",
      "  %weight.33 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.3)\n",
      "  %input.3 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%672, %weight.33, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %679 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.3), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.35 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.3)\n",
      "  %681 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%672, %weight.35, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %682 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%679, %681), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.37 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.3)\n",
      "  %hidden_states.57 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%682, %weight.37, %227), scope: __module.model/__module.model.layers.1/__module.model.layers.1.mlp/__module.model.layers.1.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.59 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%673, %hidden_states.57, %228), scope: __module.model/__module.model.layers.1 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %686 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.59, %658, %659)\n",
      "  %687 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %688 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %689 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%686)\n",
      "  %mlp.5 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_31.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_2)\n",
      "  %post_attention_layernorm.5 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_33.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_2)\n",
      "  %self_attn.7 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_26.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_2)\n",
      "  %input_layernorm.5 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_32.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_2)\n",
      "  %weight.39 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.5)\n",
      "  %hidden_states.61 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%687, %225, %232, %232, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %696 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.61, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %697 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm\n",
      "  %variance.9 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%696, %697, %210, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %699 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.9, %209, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %700 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%699), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.63 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.61, %700), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.65 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.63, %225, %232, %232, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.67 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.39, %hidden_states.65), scope: __module.model/__module.model.layers.2/__module.model.layers.2.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %704 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.67, %hidden_states.61)\n",
      "  %705 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %706 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%704)\n",
      "  %o_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_24.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.7)\n",
      "  %v_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_23.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.7)\n",
      "  %k_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_22.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.7)\n",
      "  %q_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_21.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.7)\n",
      "  %711 : int = aten::size(%705, %223), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %712 : int = aten::size(%705, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.41 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.5)\n",
      "  %query_states.5 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%705, %weight.41, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.43 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.5)\n",
      "  %key_states.13 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%705, %weight.43, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.45 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.5)\n",
      "  %value_states.5 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%705, %weight.45, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %719 : int[] = prim::ListConstruct(%711, %712, %218, %217), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %720 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.5, %719), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.5 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%720, %228, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %722 : int[] = prim::ListConstruct(%711, %712, %216, %217), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %723 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.13, %722), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.5 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%723, %228, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %725 : int[] = prim::ListConstruct(%711, %712, %216, %217), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %726 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.5, %725), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %727 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%726, %228, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.11 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %730 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.5, %cos.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %731 : int = aten::size(%q.5, %221), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %732 : Long(device=cpu) = prim::NumToTensor(%731), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %733 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%732, %215), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %734 : int = aten::Int(%733), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %735 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %221, %223, %734, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %736 : int = aten::size(%q.5, %221), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %737 : Long(device=cpu) = prim::NumToTensor(%736), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %738 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%737, %215), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %739 : int = aten::Int(%738), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %x2.9 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.5, %221, %739, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %741 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.9), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %742 : Tensor[] = prim::ListConstruct(%741, %735), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %743 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%742, %231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %744 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%743, %sin.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %745 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%730, %744, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %746 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.5, %cos.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %747 : int = aten::size(%k.5, %221), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %748 : Long(device=cpu) = prim::NumToTensor(%747), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %749 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%748, %215), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %750 : int = aten::Int(%749), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %751 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %221, %223, %750, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %752 : int = aten::size(%k.5, %221), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %753 : Long(device=cpu) = prim::NumToTensor(%752), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %754 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%753, %215), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %755 : int = aten::Int(%754), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %x2.11 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.5, %221, %755, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %757 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %758 : Tensor[] = prim::ListConstruct(%757, %751), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %759 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%758, %231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %760 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%759, %sin.11), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %761 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%746, %760, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %762 : Tensor[] = prim::ListConstruct(%43, %761), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %hidden_states.69 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%762, %214), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %764 : Tensor[] = prim::ListConstruct(%44, %727), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %hidden_states.73 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%764, %214), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %766 : int = aten::size(%hidden_states.69, %223), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %767 : int = aten::size(%hidden_states.69, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.9 : Long(device=cpu) = prim::NumToTensor(%767), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %769 : int = aten::size(%hidden_states.69, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %770 : int = aten::size(%hidden_states.69, %221), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %771 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.69, %223, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %772 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%771, %228, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %773 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%772, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %774 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%773, %221, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %775 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%774, %213, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %776 : int[] = prim::ListConstruct(%766, %767, %213, %769, %770), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %hidden_states.71 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%775, %776, %232), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %778 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.9, %212), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %779 : int = aten::Int(%778), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %780 : int[] = prim::ListConstruct(%766, %779, %769, %770), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %key_states.15 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.71, %780), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %782 : int = aten::size(%hidden_states.73, %223), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %783 : int = aten::size(%hidden_states.73, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.11 : Long(device=cpu) = prim::NumToTensor(%783), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %785 : int = aten::size(%hidden_states.73, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %786 : int = aten::size(%hidden_states.73, %221), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %787 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.73, %223, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %788 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%787, %228, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %789 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%788, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %790 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%789, %221, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %791 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%790, %213, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %792 : int[] = prim::ListConstruct(%782, %783, %213, %785, %786), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %hidden_states.75 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%791, %792, %232), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %794 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.11, %212), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %795 : int = aten::Int(%794), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %796 : int[] = prim::ListConstruct(%782, %795, %785, %786), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %797 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.75, %796), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %798 : int = aten::size(%key_states.15, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %799 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %800 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%799, %228, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %801 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%800, %230, %223, %222, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %802 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%801, %221, %223, %798, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%745, %key_states.15, %797, %802, %211, %232, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %804 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.9, %228, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.11 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%804, %223), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %806 : int[] = prim::ListConstruct(%711, %712, %231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn\n",
      "  %807 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.11, %806), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.47 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.5)\n",
      "  %hidden_states.77 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%807, %weight.47, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.self_attn/__module.model.layers.2.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %810 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.77, %hidden_states.69, %hidden_states.73)\n",
      "  %811 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %812 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %813 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%810)\n",
      "  %hidden_states.79 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%706, %811, %228), scope: __module.model/__module.model.layers.2 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.49 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.5)\n",
      "  %hidden_states.81 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.79, %225, %232, %232, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %817 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.81, %230), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %818 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm\n",
      "  %variance.11 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%817, %818, %210, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %820 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.11, %209, %228), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %821 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%820), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.83 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.81, %821), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.85 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.83, %225, %232, %232, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %824 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.49, %hidden_states.85), scope: __module.model/__module.model.layers.2/__module.model.layers.2.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %825 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%824, %hidden_states.81)\n",
      "  %826 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %827 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%825)\n",
      "  %down_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_29.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.5)\n",
      "  %up_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_28.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.5)\n",
      "  %gate_proj.5 : __torch__.torch.nn.modules.linear.___torch_mangle_27.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.5)\n",
      "  %weight.51 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.5)\n",
      "  %input.5 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%826, %weight.51, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %833 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.5), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.53 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.5)\n",
      "  %835 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%826, %weight.53, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %836 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%833, %835), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.55 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.5)\n",
      "  %hidden_states.87 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%836, %weight.55, %227), scope: __module.model/__module.model.layers.2/__module.model.layers.2.mlp/__module.model.layers.2.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.89 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%827, %hidden_states.87, %228), scope: __module.model/__module.model.layers.2 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %840 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.89, %812, %813)\n",
      "  %841 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %842 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %843 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%840)\n",
      "  %mlp.7 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_45.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_3)\n",
      "  %post_attention_layernorm.7 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_47.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_3)\n",
      "  %self_attn.9 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_40.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_3)\n",
      "  %input_layernorm.7 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_46.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_3)\n",
      "  %weight.57 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.7)\n",
      "  %hidden_states.91 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%841, %225, %232, %232, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %850 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.91, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %851 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm\n",
      "  %variance.13 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%850, %851, %210, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %853 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.13, %209, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %854 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%853), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.93 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.91, %854), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.95 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.93, %225, %232, %232, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.97 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.57, %hidden_states.95), scope: __module.model/__module.model.layers.3/__module.model.layers.3.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %858 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.97, %hidden_states.91)\n",
      "  %859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %860 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%858)\n",
      "  %o_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_38.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.9)\n",
      "  %v_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_37.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.9)\n",
      "  %k_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_36.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.9)\n",
      "  %q_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_35.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.9)\n",
      "  %865 : int = aten::size(%859, %223), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %866 : int = aten::size(%859, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.59 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.7)\n",
      "  %query_states.7 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%859, %weight.59, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.61 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.7)\n",
      "  %key_states.17 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%859, %weight.61, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.63 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.7)\n",
      "  %value_states.7 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%859, %weight.63, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %873 : int[] = prim::ListConstruct(%865, %866, %218, %217), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %874 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.7, %873), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.7 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%874, %228, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %876 : int[] = prim::ListConstruct(%865, %866, %216, %217), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %877 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.17, %876), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.7 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%877, %228, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %879 : int[] = prim::ListConstruct(%865, %866, %216, %217), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %880 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.7, %879), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %881 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%880, %228, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.13 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %884 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.7, %cos.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %885 : int = aten::size(%q.7, %221), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %886 : Long(device=cpu) = prim::NumToTensor(%885), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %887 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%886, %215), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %888 : int = aten::Int(%887), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %889 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %221, %223, %888, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %890 : int = aten::size(%q.7, %221), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %891 : Long(device=cpu) = prim::NumToTensor(%890), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %892 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%891, %215), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %893 : int = aten::Int(%892), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %x2.13 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.7, %221, %893, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %895 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %896 : Tensor[] = prim::ListConstruct(%895, %889), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %897 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%896, %231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %898 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%897, %sin.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %899 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%884, %898, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %900 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.7, %cos.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %901 : int = aten::size(%k.7, %221), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %902 : Long(device=cpu) = prim::NumToTensor(%901), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %903 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%902, %215), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %904 : int = aten::Int(%903), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %905 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %221, %223, %904, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %906 : int = aten::size(%k.7, %221), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %907 : Long(device=cpu) = prim::NumToTensor(%906), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %908 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%907, %215), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %909 : int = aten::Int(%908), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %x2.15 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.7, %221, %909, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %911 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.15), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %912 : Tensor[] = prim::ListConstruct(%911, %905), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %913 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%912, %231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %914 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%913, %sin.13), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %915 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%900, %914, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %916 : Tensor[] = prim::ListConstruct(%45, %915), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %hidden_states.99 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%916, %214), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %918 : Tensor[] = prim::ListConstruct(%46, %881), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %hidden_states.103 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%918, %214), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %920 : int = aten::size(%hidden_states.99, %223), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %921 : int = aten::size(%hidden_states.99, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.13 : Long(device=cpu) = prim::NumToTensor(%921), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %923 : int = aten::size(%hidden_states.99, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %924 : int = aten::size(%hidden_states.99, %221), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %925 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.99, %223, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %926 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%925, %228, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %927 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%926, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %928 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%927, %221, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %929 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%928, %213, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %930 : int[] = prim::ListConstruct(%920, %921, %213, %923, %924), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %hidden_states.101 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%929, %930, %232), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %932 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.13, %212), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %933 : int = aten::Int(%932), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %934 : int[] = prim::ListConstruct(%920, %933, %923, %924), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %key_states.19 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.101, %934), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %936 : int = aten::size(%hidden_states.103, %223), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %937 : int = aten::size(%hidden_states.103, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.15 : Long(device=cpu) = prim::NumToTensor(%937), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %939 : int = aten::size(%hidden_states.103, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %940 : int = aten::size(%hidden_states.103, %221), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %941 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.103, %223, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %942 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%941, %228, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %943 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%942, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %944 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%943, %221, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %945 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%944, %213, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %946 : int[] = prim::ListConstruct(%936, %937, %213, %939, %940), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %hidden_states.105 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%945, %946, %232), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %948 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.15, %212), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %949 : int = aten::Int(%948), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %950 : int[] = prim::ListConstruct(%936, %949, %939, %940), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %951 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.105, %950), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %952 : int = aten::size(%key_states.19, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %953 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %954 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%953, %228, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %955 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%954, %230, %223, %222, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %956 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%955, %221, %223, %952, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%899, %key_states.19, %951, %956, %211, %232, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %958 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.13, %228, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.15 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%958, %223), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %960 : int[] = prim::ListConstruct(%865, %866, %231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn\n",
      "  %961 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.15, %960), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.65 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.7)\n",
      "  %hidden_states.107 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%961, %weight.65, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.self_attn/__module.model.layers.3.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %964 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.107, %hidden_states.99, %hidden_states.103)\n",
      "  %965 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %966 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %967 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%964)\n",
      "  %hidden_states.109 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%860, %965, %228), scope: __module.model/__module.model.layers.3 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.67 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.7)\n",
      "  %hidden_states.111 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.109, %225, %232, %232, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %971 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.111, %230), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %972 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm\n",
      "  %variance.15 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%971, %972, %210, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %974 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.15, %209, %228), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %975 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%974), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.113 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.111, %975), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.115 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.113, %225, %232, %232, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %978 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.67, %hidden_states.115), scope: __module.model/__module.model.layers.3/__module.model.layers.3.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %979 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%978, %hidden_states.111)\n",
      "  %980 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %981 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%979)\n",
      "  %down_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_43.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.7)\n",
      "  %up_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_42.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.7)\n",
      "  %gate_proj.7 : __torch__.torch.nn.modules.linear.___torch_mangle_41.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.7)\n",
      "  %weight.69 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.7)\n",
      "  %input.7 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%980, %weight.69, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %987 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.7), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.71 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.7)\n",
      "  %989 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%980, %weight.71, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %990 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%987, %989), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.73 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.7)\n",
      "  %hidden_states.117 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%990, %weight.73, %227), scope: __module.model/__module.model.layers.3/__module.model.layers.3.mlp/__module.model.layers.3.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.119 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%981, %hidden_states.117, %228), scope: __module.model/__module.model.layers.3 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %994 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.119, %966, %967)\n",
      "  %995 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %996 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %997 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%994)\n",
      "  %mlp.9 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_59.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_4)\n",
      "  %post_attention_layernorm.9 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_61.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_4)\n",
      "  %self_attn.11 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_54.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_4)\n",
      "  %input_layernorm.9 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_60.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_4)\n",
      "  %weight.75 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.9)\n",
      "  %hidden_states.121 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%995, %225, %232, %232, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1004 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.121, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1005 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm\n",
      "  %variance.17 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1004, %1005, %210, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1007 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.17, %209, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1008 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1007), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.123 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.121, %1008), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.125 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.123, %225, %232, %232, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.127 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.75, %hidden_states.125), scope: __module.model/__module.model.layers.4/__module.model.layers.4.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1012 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.127, %hidden_states.121)\n",
      "  %1013 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1014 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1012)\n",
      "  %o_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_52.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.11)\n",
      "  %v_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_51.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.11)\n",
      "  %k_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_50.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.11)\n",
      "  %q_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_49.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.11)\n",
      "  %1019 : int = aten::size(%1013, %223), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %1020 : int = aten::size(%1013, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.77 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.9)\n",
      "  %query_states.9 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1013, %weight.77, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.79 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.9)\n",
      "  %key_states.21 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1013, %weight.79, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.81 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.9)\n",
      "  %value_states.9 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1013, %weight.81, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1027 : int[] = prim::ListConstruct(%1019, %1020, %218, %217), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1028 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.9, %1027), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.9 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1028, %228, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %1030 : int[] = prim::ListConstruct(%1019, %1020, %216, %217), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1031 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.21, %1030), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.9 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1031, %228, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %1033 : int[] = prim::ListConstruct(%1019, %1020, %216, %217), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1034 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.9, %1033), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %1035 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1034, %228, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.15 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %1038 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.9, %cos.15), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1039 : int = aten::size(%q.9, %221), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1040 : Long(device=cpu) = prim::NumToTensor(%1039), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1041 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1040, %215), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1042 : int = aten::Int(%1041), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1043 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %221, %223, %1042, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1044 : int = aten::size(%q.9, %221), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1045 : Long(device=cpu) = prim::NumToTensor(%1044), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1046 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1045, %215), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1047 : int = aten::Int(%1046), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %x2.17 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.9, %221, %1047, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1049 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.17), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1050 : Tensor[] = prim::ListConstruct(%1049, %1043), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1051 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1050, %231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1052 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1051, %sin.15), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1053 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1038, %1052, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1054 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.9, %cos.15), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1055 : int = aten::size(%k.9, %221), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1056 : Long(device=cpu) = prim::NumToTensor(%1055), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1057 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1056, %215), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1058 : int = aten::Int(%1057), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1059 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %221, %223, %1058, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1060 : int = aten::size(%k.9, %221), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1061 : Long(device=cpu) = prim::NumToTensor(%1060), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1062 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1061, %215), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1063 : int = aten::Int(%1062), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %x2.19 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.9, %221, %1063, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1065 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.19), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1066 : Tensor[] = prim::ListConstruct(%1065, %1059), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1067 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1066, %231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1068 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1067, %sin.15), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1069 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1054, %1068, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1070 : Tensor[] = prim::ListConstruct(%47, %1069), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %hidden_states.129 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1070, %214), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1072 : Tensor[] = prim::ListConstruct(%48, %1035), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %hidden_states.133 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1072, %214), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1074 : int = aten::size(%hidden_states.129, %223), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1075 : int = aten::size(%hidden_states.129, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.17 : Long(device=cpu) = prim::NumToTensor(%1075), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1077 : int = aten::size(%hidden_states.129, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1078 : int = aten::size(%hidden_states.129, %221), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1079 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.129, %223, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1080 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1079, %228, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1081 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1080, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1082 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1081, %221, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1083 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1082, %213, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1084 : int[] = prim::ListConstruct(%1074, %1075, %213, %1077, %1078), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %hidden_states.131 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1083, %1084, %232), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1086 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.17, %212), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1087 : int = aten::Int(%1086), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1088 : int[] = prim::ListConstruct(%1074, %1087, %1077, %1078), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %key_states.23 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.131, %1088), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1090 : int = aten::size(%hidden_states.133, %223), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1091 : int = aten::size(%hidden_states.133, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.19 : Long(device=cpu) = prim::NumToTensor(%1091), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1093 : int = aten::size(%hidden_states.133, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1094 : int = aten::size(%hidden_states.133, %221), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1095 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.133, %223, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1096 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1095, %228, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1097 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1096, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1098 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1097, %221, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1099 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1098, %213, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1100 : int[] = prim::ListConstruct(%1090, %1091, %213, %1093, %1094), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %hidden_states.135 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1099, %1100, %232), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1102 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.19, %212), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1103 : int = aten::Int(%1102), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1104 : int[] = prim::ListConstruct(%1090, %1103, %1093, %1094), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1105 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.135, %1104), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1106 : int = aten::size(%key_states.23, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1107 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1108 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1107, %228, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1109 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1108, %230, %223, %222, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1110 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1109, %221, %223, %1106, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%1053, %key_states.23, %1105, %1110, %211, %232, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1112 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.17, %228, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.19 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1112, %223), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %1114 : int[] = prim::ListConstruct(%1019, %1020, %231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn\n",
      "  %1115 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.19, %1114), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.83 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.9)\n",
      "  %hidden_states.137 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1115, %weight.83, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.self_attn/__module.model.layers.4.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1118 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.137, %hidden_states.129, %hidden_states.133)\n",
      "  %1119 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1120 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1121 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1118)\n",
      "  %hidden_states.139 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1014, %1119, %228), scope: __module.model/__module.model.layers.4 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.85 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.9)\n",
      "  %hidden_states.141 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.139, %225, %232, %232, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1125 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.141, %230), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1126 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm\n",
      "  %variance.19 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1125, %1126, %210, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1128 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.19, %209, %228), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1129 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1128), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.143 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.141, %1129), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.145 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.143, %225, %232, %232, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1132 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.85, %hidden_states.145), scope: __module.model/__module.model.layers.4/__module.model.layers.4.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1133 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1132, %hidden_states.141)\n",
      "  %1134 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1135 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1133)\n",
      "  %down_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_57.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.9)\n",
      "  %up_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_56.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.9)\n",
      "  %gate_proj.9 : __torch__.torch.nn.modules.linear.___torch_mangle_55.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.9)\n",
      "  %weight.87 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.9)\n",
      "  %input.9 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1134, %weight.87, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1141 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.9), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.89 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.9)\n",
      "  %1143 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1134, %weight.89, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1144 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1141, %1143), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.91 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.9)\n",
      "  %hidden_states.147 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1144, %weight.91, %227), scope: __module.model/__module.model.layers.4/__module.model.layers.4.mlp/__module.model.layers.4.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.149 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1135, %hidden_states.147, %228), scope: __module.model/__module.model.layers.4 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %1148 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.149, %1120, %1121)\n",
      "  %1149 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1150 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1151 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1148)\n",
      "  %mlp.11 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_73.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_5)\n",
      "  %post_attention_layernorm.11 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_75.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_5)\n",
      "  %self_attn.13 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_68.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_5)\n",
      "  %input_layernorm.11 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_74.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_5)\n",
      "  %weight.93 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.11)\n",
      "  %hidden_states.151 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1149, %225, %232, %232, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1158 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.151, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1159 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm\n",
      "  %variance.21 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1158, %1159, %210, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1161 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.21, %209, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1162 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1161), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.153 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.151, %1162), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.155 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.153, %225, %232, %232, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.157 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.93, %hidden_states.155), scope: __module.model/__module.model.layers.5/__module.model.layers.5.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1166 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.157, %hidden_states.151)\n",
      "  %1167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1168 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1166)\n",
      "  %o_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_66.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.13)\n",
      "  %v_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_65.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.13)\n",
      "  %k_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_64.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.13)\n",
      "  %q_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_63.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.13)\n",
      "  %1173 : int = aten::size(%1167, %223), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %1174 : int = aten::size(%1167, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.95 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.11)\n",
      "  %query_states.11 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1167, %weight.95, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.97 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.11)\n",
      "  %key_states.25 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1167, %weight.97, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.99 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.11)\n",
      "  %value_states.11 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1167, %weight.99, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1181 : int[] = prim::ListConstruct(%1173, %1174, %218, %217), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1182 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.11, %1181), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.11 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1182, %228, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %1184 : int[] = prim::ListConstruct(%1173, %1174, %216, %217), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1185 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.25, %1184), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.11 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1185, %228, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %1187 : int[] = prim::ListConstruct(%1173, %1174, %216, %217), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1188 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.11, %1187), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %1189 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1188, %228, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.17 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %1192 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.11, %cos.17), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1193 : int = aten::size(%q.11, %221), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1194 : Long(device=cpu) = prim::NumToTensor(%1193), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1195 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1194, %215), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1196 : int = aten::Int(%1195), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1197 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %221, %223, %1196, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1198 : int = aten::size(%q.11, %221), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1199 : Long(device=cpu) = prim::NumToTensor(%1198), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1200 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1199, %215), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1201 : int = aten::Int(%1200), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %x2.21 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.11, %221, %1201, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1203 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.21), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1204 : Tensor[] = prim::ListConstruct(%1203, %1197), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1205 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1204, %231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1206 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1205, %sin.17), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1207 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1192, %1206, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1208 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.11, %cos.17), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1209 : int = aten::size(%k.11, %221), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1210 : Long(device=cpu) = prim::NumToTensor(%1209), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1211 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1210, %215), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1212 : int = aten::Int(%1211), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1213 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %221, %223, %1212, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1214 : int = aten::size(%k.11, %221), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1215 : Long(device=cpu) = prim::NumToTensor(%1214), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1216 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1215, %215), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1217 : int = aten::Int(%1216), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %x2.23 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.11, %221, %1217, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1219 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.23), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1220 : Tensor[] = prim::ListConstruct(%1219, %1213), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1221 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1220, %231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1222 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1221, %sin.17), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1223 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1208, %1222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1224 : Tensor[] = prim::ListConstruct(%49, %1223), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %hidden_states.159 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1224, %214), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1226 : Tensor[] = prim::ListConstruct(%50, %1189), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %hidden_states.163 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1226, %214), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1228 : int = aten::size(%hidden_states.159, %223), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1229 : int = aten::size(%hidden_states.159, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.21 : Long(device=cpu) = prim::NumToTensor(%1229), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1231 : int = aten::size(%hidden_states.159, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1232 : int = aten::size(%hidden_states.159, %221), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1233 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.159, %223, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1234 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1233, %228, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1235 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1234, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1236 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1235, %221, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1237 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1236, %213, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1238 : int[] = prim::ListConstruct(%1228, %1229, %213, %1231, %1232), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %hidden_states.161 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1237, %1238, %232), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1240 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.21, %212), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1241 : int = aten::Int(%1240), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1242 : int[] = prim::ListConstruct(%1228, %1241, %1231, %1232), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %key_states.27 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.161, %1242), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1244 : int = aten::size(%hidden_states.163, %223), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1245 : int = aten::size(%hidden_states.163, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.23 : Long(device=cpu) = prim::NumToTensor(%1245), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1247 : int = aten::size(%hidden_states.163, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1248 : int = aten::size(%hidden_states.163, %221), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1249 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.163, %223, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1250 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1249, %228, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1251 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1250, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1252 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1251, %221, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1253 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1252, %213, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1254 : int[] = prim::ListConstruct(%1244, %1245, %213, %1247, %1248), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %hidden_states.165 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1253, %1254, %232), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1256 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.23, %212), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1257 : int = aten::Int(%1256), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1258 : int[] = prim::ListConstruct(%1244, %1257, %1247, %1248), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1259 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.165, %1258), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1260 : int = aten::size(%key_states.27, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1261 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1262 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1261, %228, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1263 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1262, %230, %223, %222, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1264 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1263, %221, %223, %1260, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%1207, %key_states.27, %1259, %1264, %211, %232, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1266 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.21, %228, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.23 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1266, %223), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %1268 : int[] = prim::ListConstruct(%1173, %1174, %231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn\n",
      "  %1269 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.23, %1268), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.101 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.11)\n",
      "  %hidden_states.167 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1269, %weight.101, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.self_attn/__module.model.layers.5.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1272 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.167, %hidden_states.159, %hidden_states.163)\n",
      "  %1273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1274 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1275 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1272)\n",
      "  %hidden_states.169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1168, %1273, %228), scope: __module.model/__module.model.layers.5 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.103 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.11)\n",
      "  %hidden_states.171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.169, %225, %232, %232, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1279 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.171, %230), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1280 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm\n",
      "  %variance.23 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1279, %1280, %210, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1282 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.23, %209, %228), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1283 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1282), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.173 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.171, %1283), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.175 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.173, %225, %232, %232, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1286 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.103, %hidden_states.175), scope: __module.model/__module.model.layers.5/__module.model.layers.5.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1287 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1286, %hidden_states.171)\n",
      "  %1288 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1289 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1287)\n",
      "  %down_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_71.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.11)\n",
      "  %up_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_70.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.11)\n",
      "  %gate_proj.11 : __torch__.torch.nn.modules.linear.___torch_mangle_69.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.11)\n",
      "  %weight.105 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.11)\n",
      "  %input.11 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1288, %weight.105, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1295 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.11), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.107 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.11)\n",
      "  %1297 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1288, %weight.107, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1298 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1295, %1297), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.109 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.11)\n",
      "  %hidden_states.177 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1298, %weight.109, %227), scope: __module.model/__module.model.layers.5/__module.model.layers.5.mlp/__module.model.layers.5.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.179 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1289, %hidden_states.177, %228), scope: __module.model/__module.model.layers.5 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %1302 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.179, %1274, %1275)\n",
      "  %1303 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1304 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1305 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1302)\n",
      "  %mlp.13 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_87.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_6)\n",
      "  %post_attention_layernorm.13 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_89.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_6)\n",
      "  %self_attn.15 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_82.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_6)\n",
      "  %input_layernorm.13 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_88.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_6)\n",
      "  %weight.111 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.13)\n",
      "  %hidden_states.181 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1303, %225, %232, %232, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1312 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.181, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1313 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm\n",
      "  %variance.25 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1312, %1313, %210, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1315 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.25, %209, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1316 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1315), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.183 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.181, %1316), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.185 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.183, %225, %232, %232, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.187 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.111, %hidden_states.185), scope: __module.model/__module.model.layers.6/__module.model.layers.6.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1320 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.187, %hidden_states.181)\n",
      "  %1321 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1322 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1320)\n",
      "  %o_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_80.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.15)\n",
      "  %v_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_79.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.15)\n",
      "  %k_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_78.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.15)\n",
      "  %q_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_77.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.15)\n",
      "  %1327 : int = aten::size(%1321, %223), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %1328 : int = aten::size(%1321, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.113 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.13)\n",
      "  %query_states.13 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1321, %weight.113, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.115 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.13)\n",
      "  %key_states.29 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1321, %weight.115, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.117 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.13)\n",
      "  %value_states.13 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1321, %weight.117, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1335 : int[] = prim::ListConstruct(%1327, %1328, %218, %217), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1336 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.13, %1335), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.13 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1336, %228, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %1338 : int[] = prim::ListConstruct(%1327, %1328, %216, %217), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1339 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.29, %1338), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.13 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1339, %228, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %1341 : int[] = prim::ListConstruct(%1327, %1328, %216, %217), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1342 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.13, %1341), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %1343 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1342, %228, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.19 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %1346 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.13, %cos.19), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1347 : int = aten::size(%q.13, %221), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1348 : Long(device=cpu) = prim::NumToTensor(%1347), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1349 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1348, %215), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1350 : int = aten::Int(%1349), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1351 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %221, %223, %1350, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1352 : int = aten::size(%q.13, %221), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1353 : Long(device=cpu) = prim::NumToTensor(%1352), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1354 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1353, %215), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1355 : int = aten::Int(%1354), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %x2.25 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.13, %221, %1355, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1357 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.25), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1358 : Tensor[] = prim::ListConstruct(%1357, %1351), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1359 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1358, %231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1360 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1359, %sin.19), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1361 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1346, %1360, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1362 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.13, %cos.19), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1363 : int = aten::size(%k.13, %221), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1364 : Long(device=cpu) = prim::NumToTensor(%1363), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1365 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1364, %215), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1366 : int = aten::Int(%1365), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1367 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %221, %223, %1366, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1368 : int = aten::size(%k.13, %221), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1369 : Long(device=cpu) = prim::NumToTensor(%1368), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1370 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1369, %215), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1371 : int = aten::Int(%1370), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %x2.27 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.13, %221, %1371, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1373 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.27), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1374 : Tensor[] = prim::ListConstruct(%1373, %1367), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1375 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1374, %231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1376 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1375, %sin.19), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1377 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1362, %1376, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1378 : Tensor[] = prim::ListConstruct(%51, %1377), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %hidden_states.189 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1378, %214), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1380 : Tensor[] = prim::ListConstruct(%52, %1343), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %hidden_states.193 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1380, %214), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1382 : int = aten::size(%hidden_states.189, %223), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1383 : int = aten::size(%hidden_states.189, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.25 : Long(device=cpu) = prim::NumToTensor(%1383), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1385 : int = aten::size(%hidden_states.189, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1386 : int = aten::size(%hidden_states.189, %221), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1387 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.189, %223, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1388 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1387, %228, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1389 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1388, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1390 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1389, %221, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1391 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1390, %213, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1392 : int[] = prim::ListConstruct(%1382, %1383, %213, %1385, %1386), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %hidden_states.191 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1391, %1392, %232), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1394 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.25, %212), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1395 : int = aten::Int(%1394), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1396 : int[] = prim::ListConstruct(%1382, %1395, %1385, %1386), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %key_states.31 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.191, %1396), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1398 : int = aten::size(%hidden_states.193, %223), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1399 : int = aten::size(%hidden_states.193, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.27 : Long(device=cpu) = prim::NumToTensor(%1399), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1401 : int = aten::size(%hidden_states.193, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1402 : int = aten::size(%hidden_states.193, %221), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1403 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.193, %223, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1404 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1403, %228, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1405 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1404, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1406 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1405, %221, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1407 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1406, %213, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1408 : int[] = prim::ListConstruct(%1398, %1399, %213, %1401, %1402), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %hidden_states.195 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1407, %1408, %232), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1410 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.27, %212), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1411 : int = aten::Int(%1410), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1412 : int[] = prim::ListConstruct(%1398, %1411, %1401, %1402), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1413 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.195, %1412), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1414 : int = aten::size(%key_states.31, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1415 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1416 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1415, %228, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1417 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1416, %230, %223, %222, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1418 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1417, %221, %223, %1414, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%1361, %key_states.31, %1413, %1418, %211, %232, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1420 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.25, %228, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.27 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1420, %223), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %1422 : int[] = prim::ListConstruct(%1327, %1328, %231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn\n",
      "  %1423 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.27, %1422), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.119 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.13)\n",
      "  %hidden_states.197 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1423, %weight.119, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.self_attn/__module.model.layers.6.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1426 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.197, %hidden_states.189, %hidden_states.193)\n",
      "  %1427 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1428 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1429 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1426)\n",
      "  %hidden_states.199 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1322, %1427, %228), scope: __module.model/__module.model.layers.6 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.121 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.13)\n",
      "  %hidden_states.201 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.199, %225, %232, %232, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1433 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.201, %230), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1434 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm\n",
      "  %variance.27 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1433, %1434, %210, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1436 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.27, %209, %228), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1437 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1436), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.201, %1437), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.205 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.203, %225, %232, %232, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1440 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.121, %hidden_states.205), scope: __module.model/__module.model.layers.6/__module.model.layers.6.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1441 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1440, %hidden_states.201)\n",
      "  %1442 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1443 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1441)\n",
      "  %down_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_85.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.13)\n",
      "  %up_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_84.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.13)\n",
      "  %gate_proj.13 : __torch__.torch.nn.modules.linear.___torch_mangle_83.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.13)\n",
      "  %weight.123 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.13)\n",
      "  %input.13 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1442, %weight.123, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1449 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.13), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.125 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.13)\n",
      "  %1451 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1442, %weight.125, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1452 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1449, %1451), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.127 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.13)\n",
      "  %hidden_states.207 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1452, %weight.127, %227), scope: __module.model/__module.model.layers.6/__module.model.layers.6.mlp/__module.model.layers.6.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.209 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1443, %hidden_states.207, %228), scope: __module.model/__module.model.layers.6 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %1456 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.209, %1428, %1429)\n",
      "  %1457 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1458 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1459 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1456)\n",
      "  %mlp.15 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_101.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_7)\n",
      "  %post_attention_layernorm.15 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_103.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_7)\n",
      "  %self_attn.17 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_96.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_7)\n",
      "  %input_layernorm.15 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_102.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_7)\n",
      "  %weight.129 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.15)\n",
      "  %hidden_states.211 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1457, %225, %232, %232, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1466 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.211, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1467 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm\n",
      "  %variance.29 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1466, %1467, %210, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1469 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.29, %209, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1470 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1469), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.213 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.211, %1470), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.215 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.213, %225, %232, %232, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.217 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.129, %hidden_states.215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1474 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.217, %hidden_states.211)\n",
      "  %1475 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1476 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1474)\n",
      "  %o_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_94.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.17)\n",
      "  %v_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_93.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.17)\n",
      "  %k_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_92.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.17)\n",
      "  %q_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_91.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.17)\n",
      "  %1481 : int = aten::size(%1475, %223), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %1482 : int = aten::size(%1475, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.131 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.15)\n",
      "  %query_states.15 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1475, %weight.131, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.133 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.15)\n",
      "  %key_states.33 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1475, %weight.133, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.135 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.15)\n",
      "  %value_states.15 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1475, %weight.135, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1489 : int[] = prim::ListConstruct(%1481, %1482, %218, %217), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1490 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.15, %1489), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.15 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1490, %228, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %1492 : int[] = prim::ListConstruct(%1481, %1482, %216, %217), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1493 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.33, %1492), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.15 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1493, %228, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %1495 : int[] = prim::ListConstruct(%1481, %1482, %216, %217), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1496 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.15, %1495), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %1497 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1496, %228, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.21 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %1500 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.15, %cos.21), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1501 : int = aten::size(%q.15, %221), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1502 : Long(device=cpu) = prim::NumToTensor(%1501), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1503 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1502, %215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1504 : int = aten::Int(%1503), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1505 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %221, %223, %1504, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1506 : int = aten::size(%q.15, %221), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1507 : Long(device=cpu) = prim::NumToTensor(%1506), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1508 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1507, %215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1509 : int = aten::Int(%1508), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %x2.29 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.15, %221, %1509, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1511 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.29), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1512 : Tensor[] = prim::ListConstruct(%1511, %1505), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1513 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1512, %231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1514 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1513, %sin.21), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1515 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1500, %1514, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1516 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.15, %cos.21), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1517 : int = aten::size(%k.15, %221), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1518 : Long(device=cpu) = prim::NumToTensor(%1517), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1519 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1518, %215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1520 : int = aten::Int(%1519), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1521 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %221, %223, %1520, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1522 : int = aten::size(%k.15, %221), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1523 : Long(device=cpu) = prim::NumToTensor(%1522), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1524 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1523, %215), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1525 : int = aten::Int(%1524), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %x2.31 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.15, %221, %1525, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1527 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.31), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1528 : Tensor[] = prim::ListConstruct(%1527, %1521), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1529 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1528, %231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1530 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1529, %sin.21), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1531 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1516, %1530, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1532 : Tensor[] = prim::ListConstruct(%53, %1531), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %hidden_states.219 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1532, %214), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1534 : Tensor[] = prim::ListConstruct(%54, %1497), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %hidden_states.223 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1534, %214), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1536 : int = aten::size(%hidden_states.219, %223), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1537 : int = aten::size(%hidden_states.219, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.29 : Long(device=cpu) = prim::NumToTensor(%1537), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1539 : int = aten::size(%hidden_states.219, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1540 : int = aten::size(%hidden_states.219, %221), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1541 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.219, %223, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1542 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1541, %228, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1543 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1542, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1544 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1543, %221, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1545 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1544, %213, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1546 : int[] = prim::ListConstruct(%1536, %1537, %213, %1539, %1540), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %hidden_states.221 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1545, %1546, %232), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1548 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.29, %212), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1549 : int = aten::Int(%1548), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1550 : int[] = prim::ListConstruct(%1536, %1549, %1539, %1540), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %key_states.35 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.221, %1550), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1552 : int = aten::size(%hidden_states.223, %223), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1553 : int = aten::size(%hidden_states.223, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.31 : Long(device=cpu) = prim::NumToTensor(%1553), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1555 : int = aten::size(%hidden_states.223, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1556 : int = aten::size(%hidden_states.223, %221), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1557 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.223, %223, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1558 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1557, %228, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1559 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1558, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1560 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1559, %221, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1561 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1560, %213, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1562 : int[] = prim::ListConstruct(%1552, %1553, %213, %1555, %1556), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %hidden_states.225 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1561, %1562, %232), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1564 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.31, %212), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1565 : int = aten::Int(%1564), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1566 : int[] = prim::ListConstruct(%1552, %1565, %1555, %1556), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1567 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.225, %1566), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1568 : int = aten::size(%key_states.35, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1569 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1570 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1569, %228, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1571 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1570, %230, %223, %222, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1572 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1571, %221, %223, %1568, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%1515, %key_states.35, %1567, %1572, %211, %232, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1574 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.29, %228, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.31 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1574, %223), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %1576 : int[] = prim::ListConstruct(%1481, %1482, %231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn\n",
      "  %1577 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.31, %1576), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.137 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.15)\n",
      "  %hidden_states.227 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1577, %weight.137, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.self_attn/__module.model.layers.7.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1580 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.227, %hidden_states.219, %hidden_states.223)\n",
      "  %1581 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1582 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1583 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1580)\n",
      "  %hidden_states.229 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1476, %1581, %228), scope: __module.model/__module.model.layers.7 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.139 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.15)\n",
      "  %hidden_states.231 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.229, %225, %232, %232, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1587 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.231, %230), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1588 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm\n",
      "  %variance.31 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1587, %1588, %210, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1590 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.31, %209, %228), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1591 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1590), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.233 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.231, %1591), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.235 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.233, %225, %232, %232, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1594 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.139, %hidden_states.235), scope: __module.model/__module.model.layers.7/__module.model.layers.7.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1595 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1594, %hidden_states.231)\n",
      "  %1596 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1595)\n",
      "  %down_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_99.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.15)\n",
      "  %up_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_98.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.15)\n",
      "  %gate_proj.15 : __torch__.torch.nn.modules.linear.___torch_mangle_97.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.15)\n",
      "  %weight.141 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.15)\n",
      "  %input.15 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1596, %weight.141, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1603 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.15), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.143 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.15)\n",
      "  %1605 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1596, %weight.143, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1606 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1603, %1605), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.145 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.15)\n",
      "  %hidden_states.237 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1606, %weight.145, %227), scope: __module.model/__module.model.layers.7/__module.model.layers.7.mlp/__module.model.layers.7.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.239 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1597, %hidden_states.237, %228), scope: __module.model/__module.model.layers.7 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %1610 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.239, %1582, %1583)\n",
      "  %1611 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1612 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1613 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1610)\n",
      "  %mlp.17 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_115.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_8)\n",
      "  %post_attention_layernorm.17 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_117.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_8)\n",
      "  %self_attn.19 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_110.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_8)\n",
      "  %input_layernorm.17 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_116.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_8)\n",
      "  %weight.147 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.17)\n",
      "  %hidden_states.241 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1611, %225, %232, %232, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1620 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.241, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1621 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm\n",
      "  %variance.33 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1620, %1621, %210, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1623 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.33, %209, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1624 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1623), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.243 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.241, %1624), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.245 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.243, %225, %232, %232, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.247 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.147, %hidden_states.245), scope: __module.model/__module.model.layers.8/__module.model.layers.8.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1628 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.247, %hidden_states.241)\n",
      "  %1629 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1630 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1628)\n",
      "  %o_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_108.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.19)\n",
      "  %v_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_107.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.19)\n",
      "  %k_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_106.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.19)\n",
      "  %q_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_105.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.19)\n",
      "  %1635 : int = aten::size(%1629, %223), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %1636 : int = aten::size(%1629, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.149 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.17)\n",
      "  %query_states.17 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1629, %weight.149, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.151 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.17)\n",
      "  %key_states.37 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1629, %weight.151, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.153 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.17)\n",
      "  %value_states.17 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1629, %weight.153, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1643 : int[] = prim::ListConstruct(%1635, %1636, %218, %217), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1644 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.17, %1643), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.17 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1644, %228, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %1646 : int[] = prim::ListConstruct(%1635, %1636, %216, %217), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1647 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.37, %1646), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.17 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1647, %228, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %1649 : int[] = prim::ListConstruct(%1635, %1636, %216, %217), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1650 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.17, %1649), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %1651 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1650, %228, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.23 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %1654 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.17, %cos.23), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1655 : int = aten::size(%q.17, %221), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1656 : Long(device=cpu) = prim::NumToTensor(%1655), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1657 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1656, %215), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1658 : int = aten::Int(%1657), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1659 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %221, %223, %1658, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1660 : int = aten::size(%q.17, %221), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1661 : Long(device=cpu) = prim::NumToTensor(%1660), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1662 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1661, %215), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1663 : int = aten::Int(%1662), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %x2.33 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.17, %221, %1663, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1665 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.33), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1666 : Tensor[] = prim::ListConstruct(%1665, %1659), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1667 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1666, %231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1668 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1667, %sin.23), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1669 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1654, %1668, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1670 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.17, %cos.23), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1671 : int = aten::size(%k.17, %221), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1672 : Long(device=cpu) = prim::NumToTensor(%1671), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1673 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1672, %215), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1674 : int = aten::Int(%1673), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1675 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %221, %223, %1674, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1676 : int = aten::size(%k.17, %221), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1677 : Long(device=cpu) = prim::NumToTensor(%1676), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1678 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1677, %215), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1679 : int = aten::Int(%1678), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %x2.35 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.17, %221, %1679, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1681 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.35), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1682 : Tensor[] = prim::ListConstruct(%1681, %1675), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1683 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1682, %231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1684 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1683, %sin.23), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1685 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1670, %1684, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1686 : Tensor[] = prim::ListConstruct(%55, %1685), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %hidden_states.249 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1686, %214), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1688 : Tensor[] = prim::ListConstruct(%56, %1651), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %hidden_states.253 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1688, %214), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1690 : int = aten::size(%hidden_states.249, %223), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1691 : int = aten::size(%hidden_states.249, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.33 : Long(device=cpu) = prim::NumToTensor(%1691), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1693 : int = aten::size(%hidden_states.249, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1694 : int = aten::size(%hidden_states.249, %221), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1695 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.249, %223, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1696 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1695, %228, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1697 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1696, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1698 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1697, %221, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1699 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1698, %213, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1700 : int[] = prim::ListConstruct(%1690, %1691, %213, %1693, %1694), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %hidden_states.251 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1699, %1700, %232), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1702 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.33, %212), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1703 : int = aten::Int(%1702), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1704 : int[] = prim::ListConstruct(%1690, %1703, %1693, %1694), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %key_states.39 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.251, %1704), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1706 : int = aten::size(%hidden_states.253, %223), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1707 : int = aten::size(%hidden_states.253, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.35 : Long(device=cpu) = prim::NumToTensor(%1707), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1709 : int = aten::size(%hidden_states.253, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1710 : int = aten::size(%hidden_states.253, %221), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1711 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.253, %223, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1712 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1711, %228, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1713 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1712, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1714 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1713, %221, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1715 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1714, %213, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1716 : int[] = prim::ListConstruct(%1706, %1707, %213, %1709, %1710), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %hidden_states.255 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1715, %1716, %232), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1718 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.35, %212), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1719 : int = aten::Int(%1718), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1720 : int[] = prim::ListConstruct(%1706, %1719, %1709, %1710), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1721 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.255, %1720), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1722 : int = aten::size(%key_states.39, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1723 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1724 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1723, %228, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1725 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1724, %230, %223, %222, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1726 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1725, %221, %223, %1722, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%1669, %key_states.39, %1721, %1726, %211, %232, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1728 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.33, %228, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.35 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1728, %223), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %1730 : int[] = prim::ListConstruct(%1635, %1636, %231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn\n",
      "  %1731 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.35, %1730), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.155 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.17)\n",
      "  %hidden_states.257 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1731, %weight.155, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.self_attn/__module.model.layers.8.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1734 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.257, %hidden_states.249, %hidden_states.253)\n",
      "  %1735 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1736 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1737 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1734)\n",
      "  %hidden_states.259 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1630, %1735, %228), scope: __module.model/__module.model.layers.8 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.157 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.17)\n",
      "  %hidden_states.261 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.259, %225, %232, %232, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1741 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.261, %230), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1742 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm\n",
      "  %variance.35 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1741, %1742, %210, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1744 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.35, %209, %228), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1745 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1744), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.263 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.261, %1745), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.265 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.263, %225, %232, %232, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1748 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.157, %hidden_states.265), scope: __module.model/__module.model.layers.8/__module.model.layers.8.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1749 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1748, %hidden_states.261)\n",
      "  %1750 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1751 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1749)\n",
      "  %down_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_113.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.17)\n",
      "  %up_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_112.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.17)\n",
      "  %gate_proj.17 : __torch__.torch.nn.modules.linear.___torch_mangle_111.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.17)\n",
      "  %weight.159 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.17)\n",
      "  %input.17 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1750, %weight.159, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1757 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.17), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.161 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.17)\n",
      "  %1759 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1750, %weight.161, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1760 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1757, %1759), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.163 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.17)\n",
      "  %hidden_states.267 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1760, %weight.163, %227), scope: __module.model/__module.model.layers.8/__module.model.layers.8.mlp/__module.model.layers.8.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.269 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1751, %hidden_states.267, %228), scope: __module.model/__module.model.layers.8 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %1764 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.269, %1736, %1737)\n",
      "  %1765 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1766 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1767 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1764)\n",
      "  %mlp.19 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_129.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_9)\n",
      "  %post_attention_layernorm.19 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_131.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_9)\n",
      "  %self_attn.21 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_124.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_9)\n",
      "  %input_layernorm.19 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_130.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_9)\n",
      "  %weight.165 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.19)\n",
      "  %hidden_states.271 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1765, %225, %232, %232, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1774 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.271, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1775 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm\n",
      "  %variance.37 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1774, %1775, %210, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1777 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.37, %209, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1778 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1777), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.271, %1778), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.275 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.273, %225, %232, %232, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.277 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.165, %hidden_states.275), scope: __module.model/__module.model.layers.9/__module.model.layers.9.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1782 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.277, %hidden_states.271)\n",
      "  %1783 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1784 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1782)\n",
      "  %o_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_122.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.21)\n",
      "  %v_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_121.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.21)\n",
      "  %k_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_120.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.21)\n",
      "  %q_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_119.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.21)\n",
      "  %1789 : int = aten::size(%1783, %223), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %1790 : int = aten::size(%1783, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.167 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.19)\n",
      "  %query_states.19 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1783, %weight.167, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.169 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.19)\n",
      "  %key_states.41 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1783, %weight.169, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.171 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.19)\n",
      "  %value_states.19 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1783, %weight.171, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1797 : int[] = prim::ListConstruct(%1789, %1790, %218, %217), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1798 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.19, %1797), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.19 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1798, %228, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %1800 : int[] = prim::ListConstruct(%1789, %1790, %216, %217), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1801 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.41, %1800), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.19 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1801, %228, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %1803 : int[] = prim::ListConstruct(%1789, %1790, %216, %217), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1804 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.19, %1803), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %1805 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1804, %228, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.25 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %1808 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.19, %cos.25), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1809 : int = aten::size(%q.19, %221), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1810 : Long(device=cpu) = prim::NumToTensor(%1809), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1811 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1810, %215), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1812 : int = aten::Int(%1811), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1813 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %221, %223, %1812, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1814 : int = aten::size(%q.19, %221), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1815 : Long(device=cpu) = prim::NumToTensor(%1814), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1816 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1815, %215), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1817 : int = aten::Int(%1816), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %x2.37 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.19, %221, %1817, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1819 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.37), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1820 : Tensor[] = prim::ListConstruct(%1819, %1813), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1821 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1820, %231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1822 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1821, %sin.25), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1823 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1808, %1822, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1824 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.19, %cos.25), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1825 : int = aten::size(%k.19, %221), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1826 : Long(device=cpu) = prim::NumToTensor(%1825), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1827 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1826, %215), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1828 : int = aten::Int(%1827), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1829 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %221, %223, %1828, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1830 : int = aten::size(%k.19, %221), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1831 : Long(device=cpu) = prim::NumToTensor(%1830), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1832 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1831, %215), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1833 : int = aten::Int(%1832), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %x2.39 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.19, %221, %1833, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1835 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.39), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1836 : Tensor[] = prim::ListConstruct(%1835, %1829), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1837 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1836, %231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1838 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1837, %sin.25), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1839 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1824, %1838, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1840 : Tensor[] = prim::ListConstruct(%57, %1839), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %hidden_states.279 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1840, %214), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1842 : Tensor[] = prim::ListConstruct(%58, %1805), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %hidden_states.283 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1842, %214), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1844 : int = aten::size(%hidden_states.279, %223), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1845 : int = aten::size(%hidden_states.279, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.37 : Long(device=cpu) = prim::NumToTensor(%1845), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1847 : int = aten::size(%hidden_states.279, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1848 : int = aten::size(%hidden_states.279, %221), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1849 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.279, %223, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1850 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1849, %228, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1851 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1850, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1852 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1851, %221, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1853 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1852, %213, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1854 : int[] = prim::ListConstruct(%1844, %1845, %213, %1847, %1848), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %hidden_states.281 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1853, %1854, %232), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1856 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.37, %212), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1857 : int = aten::Int(%1856), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1858 : int[] = prim::ListConstruct(%1844, %1857, %1847, %1848), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %key_states.43 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.281, %1858), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1860 : int = aten::size(%hidden_states.283, %223), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1861 : int = aten::size(%hidden_states.283, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.39 : Long(device=cpu) = prim::NumToTensor(%1861), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1863 : int = aten::size(%hidden_states.283, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1864 : int = aten::size(%hidden_states.283, %221), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1865 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.283, %223, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1866 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1865, %228, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1867 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%1866, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1868 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1867, %221, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1869 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%1868, %213, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1870 : int[] = prim::ListConstruct(%1860, %1861, %213, %1863, %1864), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %hidden_states.285 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%1869, %1870, %232), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %1872 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.39, %212), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1873 : int = aten::Int(%1872), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1874 : int[] = prim::ListConstruct(%1860, %1873, %1863, %1864), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1875 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.285, %1874), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %1876 : int = aten::size(%key_states.43, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1877 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1878 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1877, %228, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1879 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1878, %230, %223, %222, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %1880 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%1879, %221, %223, %1876, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%1823, %key_states.43, %1875, %1880, %211, %232, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1882 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.37, %228, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.39 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%1882, %223), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %1884 : int[] = prim::ListConstruct(%1789, %1790, %231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn\n",
      "  %1885 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.39, %1884), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.173 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.19)\n",
      "  %hidden_states.287 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1885, %weight.173, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.self_attn/__module.model.layers.9.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1888 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.287, %hidden_states.279, %hidden_states.283)\n",
      "  %1889 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1890 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1891 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1888)\n",
      "  %hidden_states.289 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1784, %1889, %228), scope: __module.model/__module.model.layers.9 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.175 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.19)\n",
      "  %hidden_states.291 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.289, %225, %232, %232, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.291, %230), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1896 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm\n",
      "  %variance.39 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1895, %1896, %210, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1898 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.39, %209, %228), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1899 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1898), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.293 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.291, %1899), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.295 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.293, %225, %232, %232, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1902 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.175, %hidden_states.295), scope: __module.model/__module.model.layers.9/__module.model.layers.9.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1903 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%1902, %hidden_states.291)\n",
      "  %1904 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1905 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1903)\n",
      "  %down_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_127.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.19)\n",
      "  %up_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_126.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.19)\n",
      "  %gate_proj.19 : __torch__.torch.nn.modules.linear.___torch_mangle_125.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.19)\n",
      "  %weight.177 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.19)\n",
      "  %input.19 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1904, %weight.177, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1911 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.19), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.179 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.19)\n",
      "  %1913 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%1904, %weight.179, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1914 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%1911, %1913), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.181 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.19)\n",
      "  %hidden_states.297 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1914, %weight.181, %227), scope: __module.model/__module.model.layers.9/__module.model.layers.9.mlp/__module.model.layers.9.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.299 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1905, %hidden_states.297, %228), scope: __module.model/__module.model.layers.9 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %1918 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.299, %1890, %1891)\n",
      "  %1919 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1920 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %1921 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1918)\n",
      "  %mlp.21 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_143.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_10)\n",
      "  %post_attention_layernorm.21 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_145.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_10)\n",
      "  %self_attn.23 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_138.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_10)\n",
      "  %input_layernorm.21 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_144.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_10)\n",
      "  %weight.183 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.21)\n",
      "  %hidden_states.301 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%1919, %225, %232, %232, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %1928 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.301, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1929 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm\n",
      "  %variance.41 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%1928, %1929, %210, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %1931 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.41, %209, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %1932 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%1931), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.303 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.301, %1932), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.305 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.303, %225, %232, %232, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.183, %hidden_states.305), scope: __module.model/__module.model.layers.10/__module.model.layers.10.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %1936 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.307, %hidden_states.301)\n",
      "  %1937 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %1938 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%1936)\n",
      "  %o_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_136.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.23)\n",
      "  %v_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_135.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.23)\n",
      "  %k_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_134.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.23)\n",
      "  %q_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_133.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.23)\n",
      "  %1943 : int = aten::size(%1937, %223), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %1944 : int = aten::size(%1937, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.185 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.21)\n",
      "  %query_states.21 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%1937, %weight.185, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.187 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.21)\n",
      "  %key_states.45 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1937, %weight.187, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.189 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.21)\n",
      "  %value_states.21 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%1937, %weight.189, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1951 : int[] = prim::ListConstruct(%1943, %1944, %218, %217), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1952 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.21, %1951), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.21 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%1952, %228, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %1954 : int[] = prim::ListConstruct(%1943, %1944, %216, %217), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1955 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.45, %1954), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.21 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1955, %228, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %1957 : int[] = prim::ListConstruct(%1943, %1944, %216, %217), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1958 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.21, %1957), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %1959 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%1958, %228, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.27 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %1962 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.21, %cos.27), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1963 : int = aten::size(%q.21, %221), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1964 : Long(device=cpu) = prim::NumToTensor(%1963), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1965 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1964, %215), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1966 : int = aten::Int(%1965), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1967 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %221, %223, %1966, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1968 : int = aten::size(%q.21, %221), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1969 : Long(device=cpu) = prim::NumToTensor(%1968), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1970 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1969, %215), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1971 : int = aten::Int(%1970), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %x2.41 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.21, %221, %1971, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1973 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.41), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1974 : Tensor[] = prim::ListConstruct(%1973, %1967), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1975 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1974, %231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1976 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1975, %sin.27), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1977 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1962, %1976, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %1978 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.21, %cos.27), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1979 : int = aten::size(%k.21, %221), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1980 : Long(device=cpu) = prim::NumToTensor(%1979), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1981 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1980, %215), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1982 : int = aten::Int(%1981), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1983 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %221, %223, %1982, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %1984 : int = aten::size(%k.21, %221), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1985 : Long(device=cpu) = prim::NumToTensor(%1984), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1986 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%1985, %215), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1987 : int = aten::Int(%1986), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %x2.43 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.21, %221, %1987, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %1989 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.43), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %1990 : Tensor[] = prim::ListConstruct(%1989, %1983), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %1991 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1990, %231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1992 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%1991, %sin.27), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1993 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%1978, %1992, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %1994 : Tensor[] = prim::ListConstruct(%59, %1993), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %hidden_states.309 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1994, %214), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1996 : Tensor[] = prim::ListConstruct(%60, %1959), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %hidden_states.313 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%1996, %214), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %1998 : int = aten::size(%hidden_states.309, %223), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %1999 : int = aten::size(%hidden_states.309, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.41 : Long(device=cpu) = prim::NumToTensor(%1999), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %2001 : int = aten::size(%hidden_states.309, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2002 : int = aten::size(%hidden_states.309, %221), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2003 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.309, %223, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2004 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2003, %228, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2005 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2004, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2006 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2005, %221, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2007 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2006, %213, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2008 : int[] = prim::ListConstruct(%1998, %1999, %213, %2001, %2002), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %hidden_states.311 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2007, %2008, %232), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2010 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.41, %212), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2011 : int = aten::Int(%2010), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %2012 : int[] = prim::ListConstruct(%1998, %2011, %2001, %2002), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %key_states.47 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.311, %2012), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2014 : int = aten::size(%hidden_states.313, %223), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2015 : int = aten::size(%hidden_states.313, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.43 : Long(device=cpu) = prim::NumToTensor(%2015), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %2017 : int = aten::size(%hidden_states.313, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2018 : int = aten::size(%hidden_states.313, %221), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2019 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.313, %223, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2020 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2019, %228, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2021 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2020, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2022 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2021, %221, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2023 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2022, %213, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2024 : int[] = prim::ListConstruct(%2014, %2015, %213, %2017, %2018), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %hidden_states.315 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2023, %2024, %232), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2026 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.43, %212), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2027 : int = aten::Int(%2026), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %2028 : int[] = prim::ListConstruct(%2014, %2027, %2017, %2018), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %2029 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.315, %2028), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2030 : int = aten::size(%key_states.47, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2031 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2032 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2031, %228, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2033 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2032, %230, %223, %222, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2034 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2033, %221, %223, %2030, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%1977, %key_states.47, %2029, %2034, %211, %232, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2036 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.41, %228, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.43 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2036, %223), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %2038 : int[] = prim::ListConstruct(%1943, %1944, %231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn\n",
      "  %2039 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.43, %2038), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.191 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.21)\n",
      "  %hidden_states.317 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2039, %weight.191, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.self_attn/__module.model.layers.10.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2042 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.317, %hidden_states.309, %hidden_states.313)\n",
      "  %2043 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2044 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2045 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2042)\n",
      "  %hidden_states.319 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%1938, %2043, %228), scope: __module.model/__module.model.layers.10 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.193 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.21)\n",
      "  %hidden_states.321 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.319, %225, %232, %232, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2049 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.321, %230), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2050 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm\n",
      "  %variance.43 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2049, %2050, %210, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2052 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.43, %209, %228), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2053 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2052), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.321, %2053), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.325 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.323, %225, %232, %232, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2056 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.193, %hidden_states.325), scope: __module.model/__module.model.layers.10/__module.model.layers.10.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2057 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2056, %hidden_states.321)\n",
      "  %2058 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2059 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2057)\n",
      "  %down_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_141.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.21)\n",
      "  %up_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_140.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.21)\n",
      "  %gate_proj.21 : __torch__.torch.nn.modules.linear.___torch_mangle_139.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.21)\n",
      "  %weight.195 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.21)\n",
      "  %input.21 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2058, %weight.195, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2065 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.21), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.197 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.21)\n",
      "  %2067 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2058, %weight.197, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2068 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2065, %2067), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.199 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.21)\n",
      "  %hidden_states.327 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2068, %weight.199, %227), scope: __module.model/__module.model.layers.10/__module.model.layers.10.mlp/__module.model.layers.10.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.329 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2059, %hidden_states.327, %228), scope: __module.model/__module.model.layers.10 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %2072 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.329, %2044, %2045)\n",
      "  %2073 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2074 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2075 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2072)\n",
      "  %mlp.23 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_157.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_11)\n",
      "  %post_attention_layernorm.23 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_159.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_11)\n",
      "  %self_attn.25 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_152.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_11)\n",
      "  %input_layernorm.23 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_158.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_11)\n",
      "  %weight.201 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.23)\n",
      "  %hidden_states.331 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2073, %225, %232, %232, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2082 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.331, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2083 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm\n",
      "  %variance.45 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2082, %2083, %210, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2085 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.45, %209, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2086 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2085), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.333 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.331, %2086), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.335 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.333, %225, %232, %232, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.337 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.201, %hidden_states.335), scope: __module.model/__module.model.layers.11/__module.model.layers.11.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2090 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.337, %hidden_states.331)\n",
      "  %2091 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2092 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2090)\n",
      "  %o_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_150.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.25)\n",
      "  %v_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_149.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.25)\n",
      "  %k_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_148.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.25)\n",
      "  %q_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_147.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.25)\n",
      "  %2097 : int = aten::size(%2091, %223), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %2098 : int = aten::size(%2091, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.203 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.23)\n",
      "  %query_states.23 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2091, %weight.203, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.205 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.23)\n",
      "  %key_states.49 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2091, %weight.205, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.207 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.23)\n",
      "  %value_states.23 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2091, %weight.207, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2105 : int[] = prim::ListConstruct(%2097, %2098, %218, %217), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2106 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.23, %2105), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.23 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2106, %228, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %2108 : int[] = prim::ListConstruct(%2097, %2098, %216, %217), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2109 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.49, %2108), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.23 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2109, %228, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %2111 : int[] = prim::ListConstruct(%2097, %2098, %216, %217), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2112 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.23, %2111), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %2113 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2112, %228, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.29 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %2116 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.23, %cos.29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2117 : int = aten::size(%q.23, %221), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2118 : Long(device=cpu) = prim::NumToTensor(%2117), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2119 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2118, %215), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2120 : int = aten::Int(%2119), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %221, %223, %2120, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2122 : int = aten::size(%q.23, %221), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2123 : Long(device=cpu) = prim::NumToTensor(%2122), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2124 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2123, %215), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2125 : int = aten::Int(%2124), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %x2.45 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.23, %221, %2125, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2127 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.45), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2128 : Tensor[] = prim::ListConstruct(%2127, %2121), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2129 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2128, %231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2130 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2129, %sin.29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2131 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2116, %2130, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2132 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.23, %cos.29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2133 : int = aten::size(%k.23, %221), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2134 : Long(device=cpu) = prim::NumToTensor(%2133), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2135 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2134, %215), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2136 : int = aten::Int(%2135), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2137 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %221, %223, %2136, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2138 : int = aten::size(%k.23, %221), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2139 : Long(device=cpu) = prim::NumToTensor(%2138), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2140 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2139, %215), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2141 : int = aten::Int(%2140), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %x2.47 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.23, %221, %2141, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2143 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.47), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2144 : Tensor[] = prim::ListConstruct(%2143, %2137), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2145 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2144, %231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2146 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2145, %sin.29), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2147 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2132, %2146, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2148 : Tensor[] = prim::ListConstruct(%61, %2147), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %hidden_states.339 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2148, %214), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2150 : Tensor[] = prim::ListConstruct(%62, %2113), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %hidden_states.343 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2150, %214), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2152 : int = aten::size(%hidden_states.339, %223), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2153 : int = aten::size(%hidden_states.339, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.45 : Long(device=cpu) = prim::NumToTensor(%2153), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2155 : int = aten::size(%hidden_states.339, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2156 : int = aten::size(%hidden_states.339, %221), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2157 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.339, %223, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2158 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2157, %228, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2159 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2158, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2160 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2159, %221, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2161 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2160, %213, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2162 : int[] = prim::ListConstruct(%2152, %2153, %213, %2155, %2156), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %hidden_states.341 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2161, %2162, %232), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2164 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.45, %212), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2165 : int = aten::Int(%2164), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2166 : int[] = prim::ListConstruct(%2152, %2165, %2155, %2156), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %key_states.51 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.341, %2166), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2168 : int = aten::size(%hidden_states.343, %223), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2169 : int = aten::size(%hidden_states.343, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.47 : Long(device=cpu) = prim::NumToTensor(%2169), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2171 : int = aten::size(%hidden_states.343, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2172 : int = aten::size(%hidden_states.343, %221), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2173 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.343, %223, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2174 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2173, %228, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2175 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2174, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2176 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2175, %221, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2177 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2176, %213, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2178 : int[] = prim::ListConstruct(%2168, %2169, %213, %2171, %2172), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %hidden_states.345 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2177, %2178, %232), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2180 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.47, %212), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2181 : int = aten::Int(%2180), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2182 : int[] = prim::ListConstruct(%2168, %2181, %2171, %2172), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2183 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.345, %2182), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2184 : int = aten::size(%key_states.51, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2185 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2186 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2185, %228, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2187 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2186, %230, %223, %222, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2188 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2187, %221, %223, %2184, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%2131, %key_states.51, %2183, %2188, %211, %232, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2190 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.45, %228, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.47 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2190, %223), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %2192 : int[] = prim::ListConstruct(%2097, %2098, %231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn\n",
      "  %2193 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.47, %2192), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.209 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.23)\n",
      "  %hidden_states.347 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2193, %weight.209, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.self_attn/__module.model.layers.11.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2196 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.347, %hidden_states.339, %hidden_states.343)\n",
      "  %2197 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2198 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2199 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2196)\n",
      "  %hidden_states.349 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2092, %2197, %228), scope: __module.model/__module.model.layers.11 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.211 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.23)\n",
      "  %hidden_states.351 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.349, %225, %232, %232, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2203 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.351, %230), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2204 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm\n",
      "  %variance.47 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2203, %2204, %210, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2206 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.47, %209, %228), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2207 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2206), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.351, %2207), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.355 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.353, %225, %232, %232, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2210 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.211, %hidden_states.355), scope: __module.model/__module.model.layers.11/__module.model.layers.11.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2211 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2210, %hidden_states.351)\n",
      "  %2212 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2213 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2211)\n",
      "  %down_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_155.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.23)\n",
      "  %up_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_154.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.23)\n",
      "  %gate_proj.23 : __torch__.torch.nn.modules.linear.___torch_mangle_153.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.23)\n",
      "  %weight.213 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.23)\n",
      "  %input.23 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2212, %weight.213, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2219 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.23), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.215 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.23)\n",
      "  %2221 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2212, %weight.215, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2222 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2219, %2221), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.217 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.23)\n",
      "  %hidden_states.357 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2222, %weight.217, %227), scope: __module.model/__module.model.layers.11/__module.model.layers.11.mlp/__module.model.layers.11.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.359 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2213, %hidden_states.357, %228), scope: __module.model/__module.model.layers.11 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %2226 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.359, %2198, %2199)\n",
      "  %2227 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2228 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2229 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2226)\n",
      "  %mlp.25 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_171.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_12)\n",
      "  %post_attention_layernorm.25 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_173.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_12)\n",
      "  %self_attn.27 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_166.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_12)\n",
      "  %input_layernorm.25 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_172.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_12)\n",
      "  %weight.219 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.25)\n",
      "  %hidden_states.361 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2227, %225, %232, %232, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2236 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.361, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2237 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm\n",
      "  %variance.49 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2236, %2237, %210, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2239 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.49, %209, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2240 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2239), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.363 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.361, %2240), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.365 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.363, %225, %232, %232, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.367 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.219, %hidden_states.365), scope: __module.model/__module.model.layers.12/__module.model.layers.12.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2244 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.367, %hidden_states.361)\n",
      "  %2245 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2246 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2244)\n",
      "  %o_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_164.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.27)\n",
      "  %v_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_163.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.27)\n",
      "  %k_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_162.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.27)\n",
      "  %q_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_161.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.27)\n",
      "  %2251 : int = aten::size(%2245, %223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %2252 : int = aten::size(%2245, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.221 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.25)\n",
      "  %query_states.25 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2245, %weight.221, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.223 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.25)\n",
      "  %key_states.53 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2245, %weight.223, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.225 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.25)\n",
      "  %value_states.25 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2245, %weight.225, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2259 : int[] = prim::ListConstruct(%2251, %2252, %218, %217), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2260 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.25, %2259), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.25 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2260, %228, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %2262 : int[] = prim::ListConstruct(%2251, %2252, %216, %217), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2263 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.53, %2262), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.25 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2263, %228, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %2265 : int[] = prim::ListConstruct(%2251, %2252, %216, %217), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2266 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.25, %2265), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %2267 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2266, %228, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.31 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %2270 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.25, %cos.31), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2271 : int = aten::size(%q.25, %221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2272 : Long(device=cpu) = prim::NumToTensor(%2271), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2273 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2272, %215), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2274 : int = aten::Int(%2273), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2275 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %221, %223, %2274, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2276 : int = aten::size(%q.25, %221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2277 : Long(device=cpu) = prim::NumToTensor(%2276), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2278 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2277, %215), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2279 : int = aten::Int(%2278), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %x2.49 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.25, %221, %2279, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2281 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.49), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2282 : Tensor[] = prim::ListConstruct(%2281, %2275), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2283 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2282, %231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2284 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2283, %sin.31), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2285 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2270, %2284, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2286 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.25, %cos.31), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2287 : int = aten::size(%k.25, %221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2288 : Long(device=cpu) = prim::NumToTensor(%2287), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2289 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2288, %215), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2290 : int = aten::Int(%2289), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2291 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %221, %223, %2290, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2292 : int = aten::size(%k.25, %221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2293 : Long(device=cpu) = prim::NumToTensor(%2292), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2294 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2293, %215), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2295 : int = aten::Int(%2294), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %x2.51 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.25, %221, %2295, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2297 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.51), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2298 : Tensor[] = prim::ListConstruct(%2297, %2291), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2299 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2298, %231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2300 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2299, %sin.31), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2301 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2286, %2300, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2302 : Tensor[] = prim::ListConstruct(%63, %2301), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %hidden_states.369 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2302, %214), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2304 : Tensor[] = prim::ListConstruct(%64, %2267), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %hidden_states.373 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2304, %214), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2306 : int = aten::size(%hidden_states.369, %223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2307 : int = aten::size(%hidden_states.369, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.49 : Long(device=cpu) = prim::NumToTensor(%2307), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2309 : int = aten::size(%hidden_states.369, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2310 : int = aten::size(%hidden_states.369, %221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2311 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.369, %223, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2312 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2311, %228, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2313 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2312, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2314 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2313, %221, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2315 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2314, %213, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2316 : int[] = prim::ListConstruct(%2306, %2307, %213, %2309, %2310), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %hidden_states.371 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2315, %2316, %232), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2318 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.49, %212), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2319 : int = aten::Int(%2318), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2320 : int[] = prim::ListConstruct(%2306, %2319, %2309, %2310), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %key_states.55 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.371, %2320), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2322 : int = aten::size(%hidden_states.373, %223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2323 : int = aten::size(%hidden_states.373, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.51 : Long(device=cpu) = prim::NumToTensor(%2323), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2325 : int = aten::size(%hidden_states.373, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2326 : int = aten::size(%hidden_states.373, %221), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2327 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.373, %223, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2328 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2327, %228, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2329 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2328, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2330 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2329, %221, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2331 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2330, %213, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2332 : int[] = prim::ListConstruct(%2322, %2323, %213, %2325, %2326), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %hidden_states.375 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2331, %2332, %232), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2334 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.51, %212), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2335 : int = aten::Int(%2334), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2336 : int[] = prim::ListConstruct(%2322, %2335, %2325, %2326), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2337 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.375, %2336), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2338 : int = aten::size(%key_states.55, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2339 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2340 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2339, %228, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2341 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2340, %230, %223, %222, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2342 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2341, %221, %223, %2338, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%2285, %key_states.55, %2337, %2342, %211, %232, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2344 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.49, %228, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.51 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2344, %223), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %2346 : int[] = prim::ListConstruct(%2251, %2252, %231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn\n",
      "  %2347 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.51, %2346), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.227 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.25)\n",
      "  %hidden_states.377 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2347, %weight.227, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.self_attn/__module.model.layers.12.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2350 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.377, %hidden_states.369, %hidden_states.373)\n",
      "  %2351 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2352 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2353 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2350)\n",
      "  %hidden_states.379 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2246, %2351, %228), scope: __module.model/__module.model.layers.12 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.229 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.25)\n",
      "  %hidden_states.381 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.379, %225, %232, %232, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2357 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.381, %230), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2358 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm\n",
      "  %variance.51 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2357, %2358, %210, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2360 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.51, %209, %228), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2361 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2360), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.383 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.381, %2361), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.385 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.383, %225, %232, %232, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2364 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.229, %hidden_states.385), scope: __module.model/__module.model.layers.12/__module.model.layers.12.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2365 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2364, %hidden_states.381)\n",
      "  %2366 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2367 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2365)\n",
      "  %down_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_169.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.25)\n",
      "  %up_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_168.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.25)\n",
      "  %gate_proj.25 : __torch__.torch.nn.modules.linear.___torch_mangle_167.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.25)\n",
      "  %weight.231 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.25)\n",
      "  %input.25 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2366, %weight.231, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2373 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.25), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.233 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.25)\n",
      "  %2375 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2366, %weight.233, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2376 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2373, %2375), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.235 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.25)\n",
      "  %hidden_states.387 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2376, %weight.235, %227), scope: __module.model/__module.model.layers.12/__module.model.layers.12.mlp/__module.model.layers.12.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.389 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2367, %hidden_states.387, %228), scope: __module.model/__module.model.layers.12 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %2380 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.389, %2352, %2353)\n",
      "  %2381 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2382 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2383 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2380)\n",
      "  %mlp.27 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_185.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_13)\n",
      "  %post_attention_layernorm.27 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_187.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_13)\n",
      "  %self_attn.29 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_180.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_13)\n",
      "  %input_layernorm.27 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_186.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_13)\n",
      "  %weight.237 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.27)\n",
      "  %hidden_states.391 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2381, %225, %232, %232, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2390 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.391, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2391 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm\n",
      "  %variance.53 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2390, %2391, %210, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2393 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.53, %209, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2394 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2393), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.393 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.391, %2394), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.395 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.393, %225, %232, %232, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.397 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.237, %hidden_states.395), scope: __module.model/__module.model.layers.13/__module.model.layers.13.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2398 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.397, %hidden_states.391)\n",
      "  %2399 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2400 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2398)\n",
      "  %o_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_178.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.29)\n",
      "  %v_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_177.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.29)\n",
      "  %k_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_176.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.29)\n",
      "  %q_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_175.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.29)\n",
      "  %2405 : int = aten::size(%2399, %223), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %2406 : int = aten::size(%2399, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.239 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.27)\n",
      "  %query_states.27 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2399, %weight.239, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.241 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.27)\n",
      "  %key_states.57 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2399, %weight.241, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.243 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.27)\n",
      "  %value_states.27 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2399, %weight.243, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2413 : int[] = prim::ListConstruct(%2405, %2406, %218, %217), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2414 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.27, %2413), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.27 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2414, %228, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %2416 : int[] = prim::ListConstruct(%2405, %2406, %216, %217), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2417 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.57, %2416), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.27 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2417, %228, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %2419 : int[] = prim::ListConstruct(%2405, %2406, %216, %217), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2420 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.27, %2419), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %2421 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2420, %228, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.33 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %2424 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.27, %cos.33), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2425 : int = aten::size(%q.27, %221), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2426 : Long(device=cpu) = prim::NumToTensor(%2425), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2427 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2426, %215), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2428 : int = aten::Int(%2427), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2429 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %221, %223, %2428, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2430 : int = aten::size(%q.27, %221), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2431 : Long(device=cpu) = prim::NumToTensor(%2430), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2432 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2431, %215), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2433 : int = aten::Int(%2432), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %x2.53 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.27, %221, %2433, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2435 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.53), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2436 : Tensor[] = prim::ListConstruct(%2435, %2429), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2437 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2436, %231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2438 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2437, %sin.33), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2439 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2424, %2438, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2440 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.27, %cos.33), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2441 : int = aten::size(%k.27, %221), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2442 : Long(device=cpu) = prim::NumToTensor(%2441), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2443 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2442, %215), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2444 : int = aten::Int(%2443), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2445 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %221, %223, %2444, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2446 : int = aten::size(%k.27, %221), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2447 : Long(device=cpu) = prim::NumToTensor(%2446), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2448 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2447, %215), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2449 : int = aten::Int(%2448), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %x2.55 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.27, %221, %2449, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2451 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.55), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2452 : Tensor[] = prim::ListConstruct(%2451, %2445), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2453 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2452, %231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2454 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2453, %sin.33), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2455 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2440, %2454, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2456 : Tensor[] = prim::ListConstruct(%65, %2455), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %hidden_states.399 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2456, %214), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2458 : Tensor[] = prim::ListConstruct(%66, %2421), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %hidden_states.403 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2458, %214), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2460 : int = aten::size(%hidden_states.399, %223), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2461 : int = aten::size(%hidden_states.399, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.53 : Long(device=cpu) = prim::NumToTensor(%2461), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2463 : int = aten::size(%hidden_states.399, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2464 : int = aten::size(%hidden_states.399, %221), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2465 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.399, %223, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2466 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2465, %228, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2467 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2466, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2468 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2467, %221, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2469 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2468, %213, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2470 : int[] = prim::ListConstruct(%2460, %2461, %213, %2463, %2464), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %hidden_states.401 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2469, %2470, %232), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2472 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.53, %212), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2473 : int = aten::Int(%2472), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2474 : int[] = prim::ListConstruct(%2460, %2473, %2463, %2464), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %key_states.59 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.401, %2474), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2476 : int = aten::size(%hidden_states.403, %223), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2477 : int = aten::size(%hidden_states.403, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.55 : Long(device=cpu) = prim::NumToTensor(%2477), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2479 : int = aten::size(%hidden_states.403, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2480 : int = aten::size(%hidden_states.403, %221), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2481 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.403, %223, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2482 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2481, %228, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2483 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2482, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2484 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2483, %221, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2485 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2484, %213, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2486 : int[] = prim::ListConstruct(%2476, %2477, %213, %2479, %2480), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %hidden_states.405 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2485, %2486, %232), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2488 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.55, %212), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2489 : int = aten::Int(%2488), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2490 : int[] = prim::ListConstruct(%2476, %2489, %2479, %2480), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2491 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.405, %2490), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2492 : int = aten::size(%key_states.59, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2493 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2494 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2493, %228, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2495 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2494, %230, %223, %222, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2496 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2495, %221, %223, %2492, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%2439, %key_states.59, %2491, %2496, %211, %232, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2498 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.53, %228, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.55 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2498, %223), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %2500 : int[] = prim::ListConstruct(%2405, %2406, %231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn\n",
      "  %2501 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.55, %2500), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.245 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.27)\n",
      "  %hidden_states.407 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2501, %weight.245, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.self_attn/__module.model.layers.13.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2504 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.407, %hidden_states.399, %hidden_states.403)\n",
      "  %2505 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2506 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2507 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2504)\n",
      "  %hidden_states.409 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2400, %2505, %228), scope: __module.model/__module.model.layers.13 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.247 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.27)\n",
      "  %hidden_states.411 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.409, %225, %232, %232, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2511 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.411, %230), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2512 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm\n",
      "  %variance.55 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2511, %2512, %210, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2514 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.55, %209, %228), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2515 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2514), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.413 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.411, %2515), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.415 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.413, %225, %232, %232, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2518 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.247, %hidden_states.415), scope: __module.model/__module.model.layers.13/__module.model.layers.13.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2519 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2518, %hidden_states.411)\n",
      "  %2520 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2521 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2519)\n",
      "  %down_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_183.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.27)\n",
      "  %up_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_182.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.27)\n",
      "  %gate_proj.27 : __torch__.torch.nn.modules.linear.___torch_mangle_181.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.27)\n",
      "  %weight.249 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.27)\n",
      "  %input.27 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2520, %weight.249, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2527 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.27), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.251 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.27)\n",
      "  %2529 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2520, %weight.251, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2530 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2527, %2529), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.253 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.27)\n",
      "  %hidden_states.417 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2530, %weight.253, %227), scope: __module.model/__module.model.layers.13/__module.model.layers.13.mlp/__module.model.layers.13.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.419 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2521, %hidden_states.417, %228), scope: __module.model/__module.model.layers.13 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %2534 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.419, %2506, %2507)\n",
      "  %2535 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2536 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2537 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2534)\n",
      "  %mlp.29 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_199.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_14)\n",
      "  %post_attention_layernorm.29 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_201.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_14)\n",
      "  %self_attn.31 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_194.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_14)\n",
      "  %input_layernorm.29 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_200.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_14)\n",
      "  %weight.255 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.29)\n",
      "  %hidden_states.421 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2535, %225, %232, %232, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2544 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.421, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2545 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm\n",
      "  %variance.57 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2544, %2545, %210, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2547 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.57, %209, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2548 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2547), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.423 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.421, %2548), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.425 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.423, %225, %232, %232, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.427 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.255, %hidden_states.425), scope: __module.model/__module.model.layers.14/__module.model.layers.14.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2552 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.427, %hidden_states.421)\n",
      "  %2553 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2554 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2552)\n",
      "  %o_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_192.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.31)\n",
      "  %v_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_191.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.31)\n",
      "  %k_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_190.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.31)\n",
      "  %q_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_189.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.31)\n",
      "  %2559 : int = aten::size(%2553, %223), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %2560 : int = aten::size(%2553, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.257 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.29)\n",
      "  %query_states.29 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2553, %weight.257, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.259 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.29)\n",
      "  %key_states.61 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2553, %weight.259, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.261 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.29)\n",
      "  %value_states.29 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2553, %weight.261, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2567 : int[] = prim::ListConstruct(%2559, %2560, %218, %217), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2568 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.29, %2567), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.29 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2568, %228, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %2570 : int[] = prim::ListConstruct(%2559, %2560, %216, %217), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2571 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.61, %2570), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.29 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2571, %228, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %2573 : int[] = prim::ListConstruct(%2559, %2560, %216, %217), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2574 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.29, %2573), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %2575 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2574, %228, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.35 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %2578 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.29, %cos.35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2579 : int = aten::size(%q.29, %221), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2580 : Long(device=cpu) = prim::NumToTensor(%2579), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2581 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2580, %215), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2582 : int = aten::Int(%2581), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2583 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %221, %223, %2582, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2584 : int = aten::size(%q.29, %221), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2585 : Long(device=cpu) = prim::NumToTensor(%2584), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2586 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2585, %215), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2587 : int = aten::Int(%2586), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %x2.57 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.29, %221, %2587, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2589 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.57), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2590 : Tensor[] = prim::ListConstruct(%2589, %2583), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2591 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2590, %231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2592 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2591, %sin.35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2593 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2578, %2592, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2594 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.29, %cos.35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2595 : int = aten::size(%k.29, %221), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2596 : Long(device=cpu) = prim::NumToTensor(%2595), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2597 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2596, %215), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2598 : int = aten::Int(%2597), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2599 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %221, %223, %2598, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2600 : int = aten::size(%k.29, %221), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2601 : Long(device=cpu) = prim::NumToTensor(%2600), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2602 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2601, %215), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2603 : int = aten::Int(%2602), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %x2.59 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.29, %221, %2603, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2605 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.59), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2606 : Tensor[] = prim::ListConstruct(%2605, %2599), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2607 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2606, %231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2608 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2607, %sin.35), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2609 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2594, %2608, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2610 : Tensor[] = prim::ListConstruct(%67, %2609), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %hidden_states.429 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2610, %214), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2612 : Tensor[] = prim::ListConstruct(%68, %2575), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %hidden_states.433 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2612, %214), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2614 : int = aten::size(%hidden_states.429, %223), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2615 : int = aten::size(%hidden_states.429, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.57 : Long(device=cpu) = prim::NumToTensor(%2615), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2617 : int = aten::size(%hidden_states.429, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2618 : int = aten::size(%hidden_states.429, %221), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2619 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.429, %223, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2620 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2619, %228, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2621 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2620, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2622 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2621, %221, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2623 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2622, %213, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2624 : int[] = prim::ListConstruct(%2614, %2615, %213, %2617, %2618), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %hidden_states.431 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2623, %2624, %232), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2626 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.57, %212), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2627 : int = aten::Int(%2626), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2628 : int[] = prim::ListConstruct(%2614, %2627, %2617, %2618), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %key_states.63 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.431, %2628), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2630 : int = aten::size(%hidden_states.433, %223), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2631 : int = aten::size(%hidden_states.433, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.59 : Long(device=cpu) = prim::NumToTensor(%2631), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2633 : int = aten::size(%hidden_states.433, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2634 : int = aten::size(%hidden_states.433, %221), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2635 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.433, %223, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2636 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2635, %228, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2637 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2636, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2638 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2637, %221, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2639 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2638, %213, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2640 : int[] = prim::ListConstruct(%2630, %2631, %213, %2633, %2634), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %hidden_states.435 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2639, %2640, %232), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2642 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.59, %212), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2643 : int = aten::Int(%2642), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2644 : int[] = prim::ListConstruct(%2630, %2643, %2633, %2634), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2645 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.435, %2644), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2646 : int = aten::size(%key_states.63, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2647 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2648 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2647, %228, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2649 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2648, %230, %223, %222, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2650 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2649, %221, %223, %2646, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%2593, %key_states.63, %2645, %2650, %211, %232, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2652 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.57, %228, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.59 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2652, %223), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %2654 : int[] = prim::ListConstruct(%2559, %2560, %231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn\n",
      "  %2655 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.59, %2654), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.263 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.29)\n",
      "  %hidden_states.437 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2655, %weight.263, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.self_attn/__module.model.layers.14.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2658 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.437, %hidden_states.429, %hidden_states.433)\n",
      "  %2659 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2660 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2661 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2658)\n",
      "  %hidden_states.439 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2554, %2659, %228), scope: __module.model/__module.model.layers.14 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.265 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.29)\n",
      "  %hidden_states.441 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.439, %225, %232, %232, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2665 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.441, %230), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2666 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm\n",
      "  %variance.59 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2665, %2666, %210, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2668 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.59, %209, %228), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2669 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2668), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.443 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.441, %2669), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.445 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.443, %225, %232, %232, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2672 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.265, %hidden_states.445), scope: __module.model/__module.model.layers.14/__module.model.layers.14.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2673 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2672, %hidden_states.441)\n",
      "  %2674 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2675 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2673)\n",
      "  %down_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_197.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.29)\n",
      "  %up_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_196.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.29)\n",
      "  %gate_proj.29 : __torch__.torch.nn.modules.linear.___torch_mangle_195.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.29)\n",
      "  %weight.267 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.29)\n",
      "  %input.29 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2674, %weight.267, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2681 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.29), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.269 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.29)\n",
      "  %2683 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2674, %weight.269, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2684 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2681, %2683), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.271 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.29)\n",
      "  %hidden_states.447 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2684, %weight.271, %227), scope: __module.model/__module.model.layers.14/__module.model.layers.14.mlp/__module.model.layers.14.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.449 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2675, %hidden_states.447, %228), scope: __module.model/__module.model.layers.14 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %2688 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.449, %2660, %2661)\n",
      "  %2689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2690 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2691 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2688)\n",
      "  %mlp.31 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_213.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_15)\n",
      "  %post_attention_layernorm.31 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_215.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_15)\n",
      "  %self_attn.33 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_208.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_15)\n",
      "  %input_layernorm.31 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_214.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_15)\n",
      "  %weight.273 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.31)\n",
      "  %hidden_states.451 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2689, %225, %232, %232, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2698 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.451, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2699 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm\n",
      "  %variance.61 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2698, %2699, %210, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2701 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.61, %209, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2702 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2701), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.453 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.451, %2702), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.455 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.453, %225, %232, %232, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.457 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.273, %hidden_states.455), scope: __module.model/__module.model.layers.15/__module.model.layers.15.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2706 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.457, %hidden_states.451)\n",
      "  %2707 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2708 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2706)\n",
      "  %o_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_206.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.33)\n",
      "  %v_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_205.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.33)\n",
      "  %k_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_204.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.33)\n",
      "  %q_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_203.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.33)\n",
      "  %2713 : int = aten::size(%2707, %223), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %2714 : int = aten::size(%2707, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.275 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.31)\n",
      "  %query_states.31 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2707, %weight.275, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.277 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.31)\n",
      "  %key_states.65 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2707, %weight.277, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.279 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.31)\n",
      "  %value_states.31 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2707, %weight.279, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2721 : int[] = prim::ListConstruct(%2713, %2714, %218, %217), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2722 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.31, %2721), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.31 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2722, %228, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %2724 : int[] = prim::ListConstruct(%2713, %2714, %216, %217), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2725 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.65, %2724), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.31 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2725, %228, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %2727 : int[] = prim::ListConstruct(%2713, %2714, %216, %217), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2728 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.31, %2727), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %2729 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2728, %228, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.37 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %2732 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.31, %cos.37), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2733 : int = aten::size(%q.31, %221), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2734 : Long(device=cpu) = prim::NumToTensor(%2733), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2735 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2734, %215), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2736 : int = aten::Int(%2735), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2737 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %221, %223, %2736, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2738 : int = aten::size(%q.31, %221), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2739 : Long(device=cpu) = prim::NumToTensor(%2738), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2740 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2739, %215), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2741 : int = aten::Int(%2740), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %x2.61 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.31, %221, %2741, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2743 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.61), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2744 : Tensor[] = prim::ListConstruct(%2743, %2737), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2745 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2744, %231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2746 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2745, %sin.37), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2747 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2732, %2746, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2748 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.31, %cos.37), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2749 : int = aten::size(%k.31, %221), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2750 : Long(device=cpu) = prim::NumToTensor(%2749), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2751 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2750, %215), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2752 : int = aten::Int(%2751), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2753 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %221, %223, %2752, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2754 : int = aten::size(%k.31, %221), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2755 : Long(device=cpu) = prim::NumToTensor(%2754), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2756 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2755, %215), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2757 : int = aten::Int(%2756), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %x2.63 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.31, %221, %2757, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2759 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.63), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2760 : Tensor[] = prim::ListConstruct(%2759, %2753), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2761 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2760, %231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2762 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2761, %sin.37), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2763 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2748, %2762, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2764 : Tensor[] = prim::ListConstruct(%69, %2763), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %hidden_states.459 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2764, %214), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2766 : Tensor[] = prim::ListConstruct(%70, %2729), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %hidden_states.463 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2766, %214), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2768 : int = aten::size(%hidden_states.459, %223), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2769 : int = aten::size(%hidden_states.459, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.61 : Long(device=cpu) = prim::NumToTensor(%2769), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2771 : int = aten::size(%hidden_states.459, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2772 : int = aten::size(%hidden_states.459, %221), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2773 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.459, %223, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2774 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2773, %228, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2775 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2774, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2776 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2775, %221, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2777 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2776, %213, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2778 : int[] = prim::ListConstruct(%2768, %2769, %213, %2771, %2772), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %hidden_states.461 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2777, %2778, %232), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2780 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.61, %212), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2781 : int = aten::Int(%2780), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2782 : int[] = prim::ListConstruct(%2768, %2781, %2771, %2772), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %key_states.67 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.461, %2782), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2784 : int = aten::size(%hidden_states.463, %223), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2785 : int = aten::size(%hidden_states.463, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.63 : Long(device=cpu) = prim::NumToTensor(%2785), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2787 : int = aten::size(%hidden_states.463, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2788 : int = aten::size(%hidden_states.463, %221), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2789 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.463, %223, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2790 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2789, %228, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2791 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2790, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2792 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2791, %221, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2793 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2792, %213, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2794 : int[] = prim::ListConstruct(%2784, %2785, %213, %2787, %2788), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %hidden_states.465 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2793, %2794, %232), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2796 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.63, %212), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2797 : int = aten::Int(%2796), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2798 : int[] = prim::ListConstruct(%2784, %2797, %2787, %2788), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2799 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.465, %2798), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2800 : int = aten::size(%key_states.67, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2801 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2802 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2801, %228, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2803 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2802, %230, %223, %222, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2804 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2803, %221, %223, %2800, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%2747, %key_states.67, %2799, %2804, %211, %232, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2806 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.61, %228, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.63 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2806, %223), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %2808 : int[] = prim::ListConstruct(%2713, %2714, %231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn\n",
      "  %2809 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.63, %2808), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.281 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.31)\n",
      "  %hidden_states.467 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2809, %weight.281, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.self_attn/__module.model.layers.15.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2812 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.467, %hidden_states.459, %hidden_states.463)\n",
      "  %2813 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2814 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2815 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2812)\n",
      "  %hidden_states.469 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2708, %2813, %228), scope: __module.model/__module.model.layers.15 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.283 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.31)\n",
      "  %hidden_states.471 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.469, %225, %232, %232, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2819 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.471, %230), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2820 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm\n",
      "  %variance.63 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2819, %2820, %210, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2822 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.63, %209, %228), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2823 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2822), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.473 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.471, %2823), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.475 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.473, %225, %232, %232, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2826 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.283, %hidden_states.475), scope: __module.model/__module.model.layers.15/__module.model.layers.15.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2827 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2826, %hidden_states.471)\n",
      "  %2828 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2829 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2827)\n",
      "  %down_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_211.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.31)\n",
      "  %up_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_210.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.31)\n",
      "  %gate_proj.31 : __torch__.torch.nn.modules.linear.___torch_mangle_209.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.31)\n",
      "  %weight.285 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.31)\n",
      "  %input.31 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2828, %weight.285, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2835 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.31), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.287 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.31)\n",
      "  %2837 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2828, %weight.287, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2838 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2835, %2837), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.289 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.31)\n",
      "  %hidden_states.477 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2838, %weight.289, %227), scope: __module.model/__module.model.layers.15/__module.model.layers.15.mlp/__module.model.layers.15.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.479 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2829, %hidden_states.477, %228), scope: __module.model/__module.model.layers.15 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %2842 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.479, %2814, %2815)\n",
      "  %2843 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2844 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2845 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2842)\n",
      "  %mlp.33 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_227.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_16)\n",
      "  %post_attention_layernorm.33 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_229.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_16)\n",
      "  %self_attn.35 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_222.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_16)\n",
      "  %input_layernorm.33 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_228.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_16)\n",
      "  %weight.291 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.33)\n",
      "  %hidden_states.481 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2843, %225, %232, %232, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2852 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.481, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2853 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm\n",
      "  %variance.65 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2852, %2853, %210, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2855 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.65, %209, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2856 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2855), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.483 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.481, %2856), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.485 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.483, %225, %232, %232, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.487 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.291, %hidden_states.485), scope: __module.model/__module.model.layers.16/__module.model.layers.16.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2860 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.487, %hidden_states.481)\n",
      "  %2861 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2862 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2860)\n",
      "  %o_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_220.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.35)\n",
      "  %v_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_219.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.35)\n",
      "  %k_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_218.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.35)\n",
      "  %q_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_217.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.35)\n",
      "  %2867 : int = aten::size(%2861, %223), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %2868 : int = aten::size(%2861, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.293 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.33)\n",
      "  %query_states.33 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2861, %weight.293, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.295 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.33)\n",
      "  %key_states.69 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2861, %weight.295, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.297 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.33)\n",
      "  %value_states.33 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%2861, %weight.297, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2875 : int[] = prim::ListConstruct(%2867, %2868, %218, %217), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2876 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.33, %2875), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.33 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%2876, %228, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %2878 : int[] = prim::ListConstruct(%2867, %2868, %216, %217), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2879 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.69, %2878), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.33 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2879, %228, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %2881 : int[] = prim::ListConstruct(%2867, %2868, %216, %217), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2882 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.33, %2881), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %2883 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%2882, %228, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.39 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %2886 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.33, %cos.39), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2887 : int = aten::size(%q.33, %221), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2888 : Long(device=cpu) = prim::NumToTensor(%2887), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2889 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2888, %215), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2890 : int = aten::Int(%2889), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2891 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %221, %223, %2890, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2892 : int = aten::size(%q.33, %221), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2893 : Long(device=cpu) = prim::NumToTensor(%2892), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2894 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2893, %215), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2895 : int = aten::Int(%2894), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %x2.65 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.33, %221, %2895, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2897 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.65), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2898 : Tensor[] = prim::ListConstruct(%2897, %2891), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2899 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2898, %231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2900 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2899, %sin.39), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2901 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2886, %2900, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %2902 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.33, %cos.39), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2903 : int = aten::size(%k.33, %221), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2904 : Long(device=cpu) = prim::NumToTensor(%2903), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2905 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2904, %215), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2906 : int = aten::Int(%2905), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2907 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %221, %223, %2906, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %2908 : int = aten::size(%k.33, %221), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2909 : Long(device=cpu) = prim::NumToTensor(%2908), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2910 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%2909, %215), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2911 : int = aten::Int(%2910), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %x2.67 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.33, %221, %2911, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %2913 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.67), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %2914 : Tensor[] = prim::ListConstruct(%2913, %2907), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2915 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2914, %231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2916 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%2915, %sin.39), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2917 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%2902, %2916, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %2918 : Tensor[] = prim::ListConstruct(%71, %2917), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %hidden_states.489 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2918, %214), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2920 : Tensor[] = prim::ListConstruct(%72, %2883), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %hidden_states.493 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%2920, %214), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2922 : int = aten::size(%hidden_states.489, %223), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2923 : int = aten::size(%hidden_states.489, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.65 : Long(device=cpu) = prim::NumToTensor(%2923), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2925 : int = aten::size(%hidden_states.489, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2926 : int = aten::size(%hidden_states.489, %221), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2927 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.489, %223, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2928 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2927, %228, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2929 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2928, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2930 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2929, %221, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2931 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2930, %213, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2932 : int[] = prim::ListConstruct(%2922, %2923, %213, %2925, %2926), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %hidden_states.491 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2931, %2932, %232), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2934 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.65, %212), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2935 : int = aten::Int(%2934), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2936 : int[] = prim::ListConstruct(%2922, %2935, %2925, %2926), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %key_states.71 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.491, %2936), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2938 : int = aten::size(%hidden_states.493, %223), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2939 : int = aten::size(%hidden_states.493, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.67 : Long(device=cpu) = prim::NumToTensor(%2939), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2941 : int = aten::size(%hidden_states.493, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2942 : int = aten::size(%hidden_states.493, %221), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %2943 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.493, %223, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2944 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2943, %228, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2945 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%2944, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2946 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2945, %221, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2947 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%2946, %213, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2948 : int[] = prim::ListConstruct(%2938, %2939, %213, %2941, %2942), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %hidden_states.495 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%2947, %2948, %232), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %2950 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.67, %212), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2951 : int = aten::Int(%2950), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2952 : int[] = prim::ListConstruct(%2938, %2951, %2941, %2942), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2953 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.495, %2952), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %2954 : int = aten::size(%key_states.71, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2955 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2956 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2955, %228, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2957 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2956, %230, %223, %222, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %2958 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%2957, %221, %223, %2954, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.65 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%2901, %key_states.71, %2953, %2958, %211, %232, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2960 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.65, %228, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.67 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%2960, %223), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %2962 : int[] = prim::ListConstruct(%2867, %2868, %231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn\n",
      "  %2963 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.67, %2962), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.299 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.33)\n",
      "  %hidden_states.497 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2963, %weight.299, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.self_attn/__module.model.layers.16.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2966 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.497, %hidden_states.489, %hidden_states.493)\n",
      "  %2967 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2968 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2969 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2966)\n",
      "  %hidden_states.499 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2862, %2967, %228), scope: __module.model/__module.model.layers.16 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.301 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.33)\n",
      "  %hidden_states.501 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.499, %225, %232, %232, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %2973 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.501, %230), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2974 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm\n",
      "  %variance.67 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%2973, %2974, %210, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %2976 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.67, %209, %228), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %2977 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%2976), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.501, %2977), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.505 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.503, %225, %232, %232, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2980 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.301, %hidden_states.505), scope: __module.model/__module.model.layers.16/__module.model.layers.16.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %2981 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%2980, %hidden_states.501)\n",
      "  %2982 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2983 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2981)\n",
      "  %down_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_225.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.33)\n",
      "  %up_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_224.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.33)\n",
      "  %gate_proj.33 : __torch__.torch.nn.modules.linear.___torch_mangle_223.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.33)\n",
      "  %weight.303 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.33)\n",
      "  %input.33 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2982, %weight.303, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2989 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.33), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.305 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.33)\n",
      "  %2991 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%2982, %weight.305, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %2992 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%2989, %2991), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.307 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.33)\n",
      "  %hidden_states.507 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%2992, %weight.307, %227), scope: __module.model/__module.model.layers.16/__module.model.layers.16.mlp/__module.model.layers.16.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.509 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%2983, %hidden_states.507, %228), scope: __module.model/__module.model.layers.16 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %2996 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.509, %2968, %2969)\n",
      "  %2997 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %2998 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %2999 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%2996)\n",
      "  %mlp.35 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_241.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_17)\n",
      "  %post_attention_layernorm.35 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_243.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_17)\n",
      "  %self_attn.37 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_236.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_17)\n",
      "  %input_layernorm.35 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_242.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_17)\n",
      "  %weight.309 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.35)\n",
      "  %hidden_states.511 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%2997, %225, %232, %232, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3006 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.511, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3007 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm\n",
      "  %variance.69 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3006, %3007, %210, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3009 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.69, %209, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3010 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3009), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.513 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.511, %3010), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.515 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.513, %225, %232, %232, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.517 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.309, %hidden_states.515), scope: __module.model/__module.model.layers.17/__module.model.layers.17.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3014 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.517, %hidden_states.511)\n",
      "  %3015 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3016 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3014)\n",
      "  %o_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_234.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.37)\n",
      "  %v_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_233.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.37)\n",
      "  %k_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_232.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.37)\n",
      "  %q_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_231.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.37)\n",
      "  %3021 : int = aten::size(%3015, %223), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %3022 : int = aten::size(%3015, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.311 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.35)\n",
      "  %query_states.35 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3015, %weight.311, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.313 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.35)\n",
      "  %key_states.73 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3015, %weight.313, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.315 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.35)\n",
      "  %value_states.35 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3015, %weight.315, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3029 : int[] = prim::ListConstruct(%3021, %3022, %218, %217), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3030 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.35, %3029), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.35 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3030, %228, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %3032 : int[] = prim::ListConstruct(%3021, %3022, %216, %217), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3033 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.73, %3032), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.35 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3033, %228, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %3035 : int[] = prim::ListConstruct(%3021, %3022, %216, %217), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3036 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.35, %3035), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %3037 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3036, %228, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.41 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %3040 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.35, %cos.41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3041 : int = aten::size(%q.35, %221), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3042 : Long(device=cpu) = prim::NumToTensor(%3041), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3043 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3042, %215), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3044 : int = aten::Int(%3043), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3045 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %221, %223, %3044, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3046 : int = aten::size(%q.35, %221), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3047 : Long(device=cpu) = prim::NumToTensor(%3046), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3048 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3047, %215), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3049 : int = aten::Int(%3048), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %x2.69 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.35, %221, %3049, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3051 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.69), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3052 : Tensor[] = prim::ListConstruct(%3051, %3045), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3053 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3052, %231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3054 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3053, %sin.41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3055 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3040, %3054, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3056 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.35, %cos.41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3057 : int = aten::size(%k.35, %221), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3058 : Long(device=cpu) = prim::NumToTensor(%3057), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3059 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3058, %215), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3060 : int = aten::Int(%3059), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3061 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %221, %223, %3060, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3062 : int = aten::size(%k.35, %221), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3063 : Long(device=cpu) = prim::NumToTensor(%3062), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3064 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3063, %215), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3065 : int = aten::Int(%3064), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %x2.71 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.35, %221, %3065, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3067 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.71), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3068 : Tensor[] = prim::ListConstruct(%3067, %3061), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3069 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3068, %231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3070 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3069, %sin.41), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3071 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3056, %3070, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3072 : Tensor[] = prim::ListConstruct(%73, %3071), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %hidden_states.519 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3072, %214), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3074 : Tensor[] = prim::ListConstruct(%74, %3037), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %hidden_states.523 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3074, %214), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3076 : int = aten::size(%hidden_states.519, %223), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3077 : int = aten::size(%hidden_states.519, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.69 : Long(device=cpu) = prim::NumToTensor(%3077), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3079 : int = aten::size(%hidden_states.519, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3080 : int = aten::size(%hidden_states.519, %221), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3081 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.519, %223, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3082 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3081, %228, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3083 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3082, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3084 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3083, %221, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3085 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3084, %213, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3086 : int[] = prim::ListConstruct(%3076, %3077, %213, %3079, %3080), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %hidden_states.521 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3085, %3086, %232), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3088 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.69, %212), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3089 : int = aten::Int(%3088), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3090 : int[] = prim::ListConstruct(%3076, %3089, %3079, %3080), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %key_states.75 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.521, %3090), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3092 : int = aten::size(%hidden_states.523, %223), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3093 : int = aten::size(%hidden_states.523, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.71 : Long(device=cpu) = prim::NumToTensor(%3093), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3095 : int = aten::size(%hidden_states.523, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3096 : int = aten::size(%hidden_states.523, %221), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3097 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.523, %223, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3098 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3097, %228, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3099 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3098, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3100 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3099, %221, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3101 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3100, %213, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3102 : int[] = prim::ListConstruct(%3092, %3093, %213, %3095, %3096), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %hidden_states.525 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3101, %3102, %232), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3104 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.71, %212), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3105 : int = aten::Int(%3104), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3106 : int[] = prim::ListConstruct(%3092, %3105, %3095, %3096), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3107 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.525, %3106), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3108 : int = aten::size(%key_states.75, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3109 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3110 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3109, %228, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3111 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3110, %230, %223, %222, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3112 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3111, %221, %223, %3108, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.69 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%3055, %key_states.75, %3107, %3112, %211, %232, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3114 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.69, %228, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.71 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3114, %223), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %3116 : int[] = prim::ListConstruct(%3021, %3022, %231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn\n",
      "  %3117 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.71, %3116), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.317 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.35)\n",
      "  %hidden_states.527 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3117, %weight.317, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.self_attn/__module.model.layers.17.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3120 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.527, %hidden_states.519, %hidden_states.523)\n",
      "  %3121 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3122 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3123 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3120)\n",
      "  %hidden_states.529 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3016, %3121, %228), scope: __module.model/__module.model.layers.17 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.319 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.35)\n",
      "  %hidden_states.531 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.529, %225, %232, %232, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3127 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.531, %230), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3128 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm\n",
      "  %variance.71 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3127, %3128, %210, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3130 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.71, %209, %228), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3131 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3130), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.533 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.531, %3131), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.535 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.533, %225, %232, %232, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3134 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.319, %hidden_states.535), scope: __module.model/__module.model.layers.17/__module.model.layers.17.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3135 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3134, %hidden_states.531)\n",
      "  %3136 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3137 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3135)\n",
      "  %down_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_239.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.35)\n",
      "  %up_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_238.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.35)\n",
      "  %gate_proj.35 : __torch__.torch.nn.modules.linear.___torch_mangle_237.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.35)\n",
      "  %weight.321 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.35)\n",
      "  %input.35 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3136, %weight.321, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3143 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.35), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.323 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.35)\n",
      "  %3145 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3136, %weight.323, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3146 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3143, %3145), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.325 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.35)\n",
      "  %hidden_states.537 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3146, %weight.325, %227), scope: __module.model/__module.model.layers.17/__module.model.layers.17.mlp/__module.model.layers.17.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.539 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3137, %hidden_states.537, %228), scope: __module.model/__module.model.layers.17 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %3150 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.539, %3122, %3123)\n",
      "  %3151 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3152 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3153 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3150)\n",
      "  %mlp.37 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_255.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_18)\n",
      "  %post_attention_layernorm.37 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_257.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_18)\n",
      "  %self_attn.39 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_250.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_18)\n",
      "  %input_layernorm.37 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_256.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_18)\n",
      "  %weight.327 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.37)\n",
      "  %hidden_states.541 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3151, %225, %232, %232, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3160 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.541, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3161 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm\n",
      "  %variance.73 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3160, %3161, %210, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3163 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.73, %209, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3164 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3163), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.543 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.541, %3164), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.545 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.543, %225, %232, %232, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.547 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.327, %hidden_states.545), scope: __module.model/__module.model.layers.18/__module.model.layers.18.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3168 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.547, %hidden_states.541)\n",
      "  %3169 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3170 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3168)\n",
      "  %o_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_248.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.39)\n",
      "  %v_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_247.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.39)\n",
      "  %k_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_246.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.39)\n",
      "  %q_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_245.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.39)\n",
      "  %3175 : int = aten::size(%3169, %223), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %3176 : int = aten::size(%3169, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.329 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.37)\n",
      "  %query_states.37 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3169, %weight.329, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.331 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.37)\n",
      "  %key_states.77 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3169, %weight.331, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.333 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.37)\n",
      "  %value_states.37 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3169, %weight.333, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3183 : int[] = prim::ListConstruct(%3175, %3176, %218, %217), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3184 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.37, %3183), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.37 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3184, %228, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %3186 : int[] = prim::ListConstruct(%3175, %3176, %216, %217), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3187 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.77, %3186), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.37 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3187, %228, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %3189 : int[] = prim::ListConstruct(%3175, %3176, %216, %217), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3190 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.37, %3189), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %3191 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3190, %228, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.43 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %3194 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.37, %cos.43), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3195 : int = aten::size(%q.37, %221), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3196 : Long(device=cpu) = prim::NumToTensor(%3195), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3197 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3196, %215), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3198 : int = aten::Int(%3197), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3199 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %221, %223, %3198, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3200 : int = aten::size(%q.37, %221), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3201 : Long(device=cpu) = prim::NumToTensor(%3200), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3202 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3201, %215), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3203 : int = aten::Int(%3202), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %x2.73 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.37, %221, %3203, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3205 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.73), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3206 : Tensor[] = prim::ListConstruct(%3205, %3199), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3207 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3206, %231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3208 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3207, %sin.43), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3209 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3194, %3208, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3210 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.37, %cos.43), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3211 : int = aten::size(%k.37, %221), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3212 : Long(device=cpu) = prim::NumToTensor(%3211), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3213 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3212, %215), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3214 : int = aten::Int(%3213), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3215 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %221, %223, %3214, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3216 : int = aten::size(%k.37, %221), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3217 : Long(device=cpu) = prim::NumToTensor(%3216), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3218 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3217, %215), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3219 : int = aten::Int(%3218), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %x2.75 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.37, %221, %3219, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3221 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.75), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3222 : Tensor[] = prim::ListConstruct(%3221, %3215), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3223 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3222, %231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3224 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3223, %sin.43), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3225 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3210, %3224, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3226 : Tensor[] = prim::ListConstruct(%75, %3225), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %hidden_states.549 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3226, %214), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3228 : Tensor[] = prim::ListConstruct(%76, %3191), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %hidden_states.553 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3228, %214), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3230 : int = aten::size(%hidden_states.549, %223), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3231 : int = aten::size(%hidden_states.549, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.73 : Long(device=cpu) = prim::NumToTensor(%3231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3233 : int = aten::size(%hidden_states.549, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3234 : int = aten::size(%hidden_states.549, %221), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3235 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.549, %223, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3236 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3235, %228, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3237 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3236, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3238 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3237, %221, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3239 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3238, %213, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3240 : int[] = prim::ListConstruct(%3230, %3231, %213, %3233, %3234), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %hidden_states.551 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3239, %3240, %232), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3242 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.73, %212), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3243 : int = aten::Int(%3242), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3244 : int[] = prim::ListConstruct(%3230, %3243, %3233, %3234), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %key_states.79 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.551, %3244), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3246 : int = aten::size(%hidden_states.553, %223), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3247 : int = aten::size(%hidden_states.553, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.75 : Long(device=cpu) = prim::NumToTensor(%3247), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3249 : int = aten::size(%hidden_states.553, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3250 : int = aten::size(%hidden_states.553, %221), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3251 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.553, %223, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3252 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3251, %228, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3253 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3252, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3254 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3253, %221, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3255 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3254, %213, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3256 : int[] = prim::ListConstruct(%3246, %3247, %213, %3249, %3250), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %hidden_states.555 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3255, %3256, %232), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3258 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.75, %212), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3259 : int = aten::Int(%3258), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3260 : int[] = prim::ListConstruct(%3246, %3259, %3249, %3250), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3261 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.555, %3260), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3262 : int = aten::size(%key_states.79, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3263 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3264 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3263, %228, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3265 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3264, %230, %223, %222, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3266 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3265, %221, %223, %3262, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.73 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%3209, %key_states.79, %3261, %3266, %211, %232, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3268 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.73, %228, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.75 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3268, %223), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %3270 : int[] = prim::ListConstruct(%3175, %3176, %231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn\n",
      "  %3271 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.75, %3270), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.335 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.37)\n",
      "  %hidden_states.557 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3271, %weight.335, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.self_attn/__module.model.layers.18.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3274 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.557, %hidden_states.549, %hidden_states.553)\n",
      "  %3275 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3276 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3277 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3274)\n",
      "  %hidden_states.559 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3170, %3275, %228), scope: __module.model/__module.model.layers.18 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.337 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.37)\n",
      "  %hidden_states.561 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.559, %225, %232, %232, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3281 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.561, %230), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3282 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm\n",
      "  %variance.75 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3281, %3282, %210, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3284 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.75, %209, %228), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3285 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3284), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.563 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.561, %3285), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.565 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.563, %225, %232, %232, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3288 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.337, %hidden_states.565), scope: __module.model/__module.model.layers.18/__module.model.layers.18.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3289 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3288, %hidden_states.561)\n",
      "  %3290 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3291 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3289)\n",
      "  %down_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_253.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.37)\n",
      "  %up_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_252.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.37)\n",
      "  %gate_proj.37 : __torch__.torch.nn.modules.linear.___torch_mangle_251.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.37)\n",
      "  %weight.339 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.37)\n",
      "  %input.37 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3290, %weight.339, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3297 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.37), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.341 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.37)\n",
      "  %3299 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3290, %weight.341, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3300 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3297, %3299), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.343 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.37)\n",
      "  %hidden_states.567 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3300, %weight.343, %227), scope: __module.model/__module.model.layers.18/__module.model.layers.18.mlp/__module.model.layers.18.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.569 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3291, %hidden_states.567, %228), scope: __module.model/__module.model.layers.18 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %3304 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.569, %3276, %3277)\n",
      "  %3305 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3306 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3307 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3304)\n",
      "  %mlp.39 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_269.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_19)\n",
      "  %post_attention_layernorm.39 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_271.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_19)\n",
      "  %self_attn.41 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_264.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_19)\n",
      "  %input_layernorm.39 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_270.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_19)\n",
      "  %weight.345 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.39)\n",
      "  %hidden_states.571 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3305, %225, %232, %232, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3314 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.571, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3315 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm\n",
      "  %variance.77 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3314, %3315, %210, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3317 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.77, %209, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3318 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3317), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.573 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.571, %3318), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.575 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.573, %225, %232, %232, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.577 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.345, %hidden_states.575), scope: __module.model/__module.model.layers.19/__module.model.layers.19.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3322 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.577, %hidden_states.571)\n",
      "  %3323 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3324 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3322)\n",
      "  %o_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_262.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.41)\n",
      "  %v_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_261.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.41)\n",
      "  %k_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_260.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.41)\n",
      "  %q_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_259.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.41)\n",
      "  %3329 : int = aten::size(%3323, %223), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %3330 : int = aten::size(%3323, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.347 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.39)\n",
      "  %query_states.39 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3323, %weight.347, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.349 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.39)\n",
      "  %key_states.81 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3323, %weight.349, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.351 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.39)\n",
      "  %value_states.39 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3323, %weight.351, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3337 : int[] = prim::ListConstruct(%3329, %3330, %218, %217), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3338 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.39, %3337), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.39 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3338, %228, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %3340 : int[] = prim::ListConstruct(%3329, %3330, %216, %217), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3341 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.81, %3340), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.39 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3341, %228, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %3343 : int[] = prim::ListConstruct(%3329, %3330, %216, %217), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3344 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.39, %3343), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %3345 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3344, %228, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.45 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %3348 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.39, %cos.45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3349 : int = aten::size(%q.39, %221), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3350 : Long(device=cpu) = prim::NumToTensor(%3349), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3351 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3350, %215), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3352 : int = aten::Int(%3351), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3353 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %221, %223, %3352, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3354 : int = aten::size(%q.39, %221), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3355 : Long(device=cpu) = prim::NumToTensor(%3354), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3356 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3355, %215), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3357 : int = aten::Int(%3356), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %x2.77 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.39, %221, %3357, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3359 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.77), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3360 : Tensor[] = prim::ListConstruct(%3359, %3353), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3361 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3360, %231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3362 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3361, %sin.45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3363 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3348, %3362, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3364 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.39, %cos.45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3365 : int = aten::size(%k.39, %221), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3366 : Long(device=cpu) = prim::NumToTensor(%3365), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3367 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3366, %215), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3368 : int = aten::Int(%3367), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3369 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %221, %223, %3368, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3370 : int = aten::size(%k.39, %221), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3371 : Long(device=cpu) = prim::NumToTensor(%3370), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3372 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3371, %215), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3373 : int = aten::Int(%3372), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %x2.79 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.39, %221, %3373, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3375 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.79), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3376 : Tensor[] = prim::ListConstruct(%3375, %3369), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3377 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3376, %231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3378 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3377, %sin.45), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3379 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3364, %3378, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3380 : Tensor[] = prim::ListConstruct(%77, %3379), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %hidden_states.579 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3380, %214), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3382 : Tensor[] = prim::ListConstruct(%78, %3345), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %hidden_states.583 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3382, %214), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3384 : int = aten::size(%hidden_states.579, %223), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3385 : int = aten::size(%hidden_states.579, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.77 : Long(device=cpu) = prim::NumToTensor(%3385), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3387 : int = aten::size(%hidden_states.579, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3388 : int = aten::size(%hidden_states.579, %221), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3389 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.579, %223, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3390 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3389, %228, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3391 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3390, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3392 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3391, %221, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3393 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3392, %213, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3394 : int[] = prim::ListConstruct(%3384, %3385, %213, %3387, %3388), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %hidden_states.581 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3393, %3394, %232), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3396 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.77, %212), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3397 : int = aten::Int(%3396), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3398 : int[] = prim::ListConstruct(%3384, %3397, %3387, %3388), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %key_states.83 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.581, %3398), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3400 : int = aten::size(%hidden_states.583, %223), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3401 : int = aten::size(%hidden_states.583, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.79 : Long(device=cpu) = prim::NumToTensor(%3401), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3403 : int = aten::size(%hidden_states.583, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3404 : int = aten::size(%hidden_states.583, %221), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3405 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.583, %223, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3406 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3405, %228, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3407 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3406, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3408 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3407, %221, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3409 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3408, %213, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3410 : int[] = prim::ListConstruct(%3400, %3401, %213, %3403, %3404), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %hidden_states.585 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3409, %3410, %232), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3412 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.79, %212), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3413 : int = aten::Int(%3412), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3414 : int[] = prim::ListConstruct(%3400, %3413, %3403, %3404), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3415 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.585, %3414), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3416 : int = aten::size(%key_states.83, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3417 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3418 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3417, %228, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3419 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3418, %230, %223, %222, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3420 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3419, %221, %223, %3416, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.77 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%3363, %key_states.83, %3415, %3420, %211, %232, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3422 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.77, %228, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.79 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3422, %223), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %3424 : int[] = prim::ListConstruct(%3329, %3330, %231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn\n",
      "  %3425 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.79, %3424), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.353 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.39)\n",
      "  %hidden_states.587 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3425, %weight.353, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.self_attn/__module.model.layers.19.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3428 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.587, %hidden_states.579, %hidden_states.583)\n",
      "  %3429 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3430 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3431 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3428)\n",
      "  %hidden_states.589 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3324, %3429, %228), scope: __module.model/__module.model.layers.19 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.355 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.39)\n",
      "  %hidden_states.591 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.589, %225, %232, %232, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3435 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.591, %230), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3436 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm\n",
      "  %variance.79 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3435, %3436, %210, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3438 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.79, %209, %228), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3439 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3438), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.593 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.591, %3439), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.595 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.593, %225, %232, %232, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3442 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.355, %hidden_states.595), scope: __module.model/__module.model.layers.19/__module.model.layers.19.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3443 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3442, %hidden_states.591)\n",
      "  %3444 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3445 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3443)\n",
      "  %down_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_267.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.39)\n",
      "  %up_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_266.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.39)\n",
      "  %gate_proj.39 : __torch__.torch.nn.modules.linear.___torch_mangle_265.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.39)\n",
      "  %weight.357 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.39)\n",
      "  %input.39 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3444, %weight.357, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3451 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.39), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.359 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.39)\n",
      "  %3453 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3444, %weight.359, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3454 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3451, %3453), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.361 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.39)\n",
      "  %hidden_states.597 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3454, %weight.361, %227), scope: __module.model/__module.model.layers.19/__module.model.layers.19.mlp/__module.model.layers.19.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.599 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3445, %hidden_states.597, %228), scope: __module.model/__module.model.layers.19 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %3458 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.599, %3430, %3431)\n",
      "  %3459 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3460 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3461 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3458)\n",
      "  %mlp.41 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_283.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_20)\n",
      "  %post_attention_layernorm.41 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_285.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_20)\n",
      "  %self_attn.43 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_278.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_20)\n",
      "  %input_layernorm.41 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_284.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_20)\n",
      "  %weight.363 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.41)\n",
      "  %hidden_states.601 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3459, %225, %232, %232, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3468 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.601, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3469 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm\n",
      "  %variance.81 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3468, %3469, %210, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3471 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.81, %209, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3472 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3471), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.603 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.601, %3472), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.605 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.603, %225, %232, %232, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.607 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.363, %hidden_states.605), scope: __module.model/__module.model.layers.20/__module.model.layers.20.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3476 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.607, %hidden_states.601)\n",
      "  %3477 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3478 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3476)\n",
      "  %o_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_276.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.43)\n",
      "  %v_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_275.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.43)\n",
      "  %k_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_274.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.43)\n",
      "  %q_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_273.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.43)\n",
      "  %3483 : int = aten::size(%3477, %223), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %3484 : int = aten::size(%3477, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.365 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.41)\n",
      "  %query_states.41 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3477, %weight.365, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.367 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.41)\n",
      "  %key_states.85 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3477, %weight.367, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.369 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.41)\n",
      "  %value_states.41 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3477, %weight.369, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3491 : int[] = prim::ListConstruct(%3483, %3484, %218, %217), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3492 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.41, %3491), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.41 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3492, %228, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %3494 : int[] = prim::ListConstruct(%3483, %3484, %216, %217), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3495 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.85, %3494), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.41 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3495, %228, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %3497 : int[] = prim::ListConstruct(%3483, %3484, %216, %217), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3498 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.41, %3497), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %3499 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3498, %228, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.47 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %3502 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.41, %cos.47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3503 : int = aten::size(%q.41, %221), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3504 : Long(device=cpu) = prim::NumToTensor(%3503), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3505 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3504, %215), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3506 : int = aten::Int(%3505), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3507 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %221, %223, %3506, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3508 : int = aten::size(%q.41, %221), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3509 : Long(device=cpu) = prim::NumToTensor(%3508), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3510 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3509, %215), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3511 : int = aten::Int(%3510), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %x2.81 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.41, %221, %3511, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3513 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.81), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3514 : Tensor[] = prim::ListConstruct(%3513, %3507), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3515 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3514, %231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3516 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3515, %sin.47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3517 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3502, %3516, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3518 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.41, %cos.47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3519 : int = aten::size(%k.41, %221), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3520 : Long(device=cpu) = prim::NumToTensor(%3519), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3521 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3520, %215), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3522 : int = aten::Int(%3521), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3523 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %221, %223, %3522, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3524 : int = aten::size(%k.41, %221), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3525 : Long(device=cpu) = prim::NumToTensor(%3524), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3526 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3525, %215), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3527 : int = aten::Int(%3526), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %x2.83 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.41, %221, %3527, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3529 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.83), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3530 : Tensor[] = prim::ListConstruct(%3529, %3523), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3531 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3530, %231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3532 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3531, %sin.47), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3533 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3518, %3532, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3534 : Tensor[] = prim::ListConstruct(%79, %3533), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %hidden_states.609 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3534, %214), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3536 : Tensor[] = prim::ListConstruct(%80, %3499), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %hidden_states.613 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3536, %214), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3538 : int = aten::size(%hidden_states.609, %223), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3539 : int = aten::size(%hidden_states.609, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.81 : Long(device=cpu) = prim::NumToTensor(%3539), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3541 : int = aten::size(%hidden_states.609, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3542 : int = aten::size(%hidden_states.609, %221), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3543 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.609, %223, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3544 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3543, %228, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3545 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3544, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3546 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3545, %221, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3547 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3546, %213, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3548 : int[] = prim::ListConstruct(%3538, %3539, %213, %3541, %3542), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %hidden_states.611 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3547, %3548, %232), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3550 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.81, %212), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3551 : int = aten::Int(%3550), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3552 : int[] = prim::ListConstruct(%3538, %3551, %3541, %3542), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %key_states.87 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.611, %3552), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3554 : int = aten::size(%hidden_states.613, %223), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3555 : int = aten::size(%hidden_states.613, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.83 : Long(device=cpu) = prim::NumToTensor(%3555), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3557 : int = aten::size(%hidden_states.613, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3558 : int = aten::size(%hidden_states.613, %221), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3559 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.613, %223, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3560 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3559, %228, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3561 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3560, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3562 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3561, %221, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3563 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3562, %213, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3564 : int[] = prim::ListConstruct(%3554, %3555, %213, %3557, %3558), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %hidden_states.615 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3563, %3564, %232), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3566 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.83, %212), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3567 : int = aten::Int(%3566), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3568 : int[] = prim::ListConstruct(%3554, %3567, %3557, %3558), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3569 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.615, %3568), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3570 : int = aten::size(%key_states.87, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3571 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3572 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3571, %228, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3573 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3572, %230, %223, %222, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3574 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3573, %221, %223, %3570, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.81 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%3517, %key_states.87, %3569, %3574, %211, %232, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3576 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.81, %228, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.83 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3576, %223), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %3578 : int[] = prim::ListConstruct(%3483, %3484, %231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn\n",
      "  %3579 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.83, %3578), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.371 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.41)\n",
      "  %hidden_states.617 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3579, %weight.371, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.self_attn/__module.model.layers.20.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3582 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.617, %hidden_states.609, %hidden_states.613)\n",
      "  %3583 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3584 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3585 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3582)\n",
      "  %hidden_states.619 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3478, %3583, %228), scope: __module.model/__module.model.layers.20 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.373 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.41)\n",
      "  %hidden_states.621 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.619, %225, %232, %232, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3589 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.621, %230), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3590 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm\n",
      "  %variance.83 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3589, %3590, %210, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3592 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.83, %209, %228), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3593 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3592), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.623 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.621, %3593), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.625 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.623, %225, %232, %232, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3596 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.373, %hidden_states.625), scope: __module.model/__module.model.layers.20/__module.model.layers.20.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3597 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3596, %hidden_states.621)\n",
      "  %3598 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3599 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3597)\n",
      "  %down_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_281.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.41)\n",
      "  %up_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_280.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.41)\n",
      "  %gate_proj.41 : __torch__.torch.nn.modules.linear.___torch_mangle_279.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.41)\n",
      "  %weight.375 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.41)\n",
      "  %input.41 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3598, %weight.375, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3605 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.41), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.377 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.41)\n",
      "  %3607 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3598, %weight.377, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3608 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3605, %3607), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.379 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.41)\n",
      "  %hidden_states.627 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3608, %weight.379, %227), scope: __module.model/__module.model.layers.20/__module.model.layers.20.mlp/__module.model.layers.20.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.629 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3599, %hidden_states.627, %228), scope: __module.model/__module.model.layers.20 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %3612 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.629, %3584, %3585)\n",
      "  %3613 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3614 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3615 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3612)\n",
      "  %mlp.43 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_297.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_21)\n",
      "  %post_attention_layernorm.43 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_299.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_21)\n",
      "  %self_attn.45 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_292.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_21)\n",
      "  %input_layernorm.43 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_298.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_21)\n",
      "  %weight.381 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.43)\n",
      "  %hidden_states.631 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3613, %225, %232, %232, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3622 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.631, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3623 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm\n",
      "  %variance.85 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3622, %3623, %210, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3625 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.85, %209, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3626 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3625), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.633 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.631, %3626), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.635 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.633, %225, %232, %232, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.637 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.381, %hidden_states.635), scope: __module.model/__module.model.layers.21/__module.model.layers.21.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3630 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.637, %hidden_states.631)\n",
      "  %3631 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3632 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3630)\n",
      "  %o_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_290.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.45)\n",
      "  %v_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_289.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.45)\n",
      "  %k_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_288.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.45)\n",
      "  %q_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_287.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.45)\n",
      "  %3637 : int = aten::size(%3631, %223), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %3638 : int = aten::size(%3631, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.383 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.43)\n",
      "  %query_states.43 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3631, %weight.383, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.385 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.43)\n",
      "  %key_states.89 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3631, %weight.385, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.387 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.43)\n",
      "  %value_states.43 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3631, %weight.387, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3645 : int[] = prim::ListConstruct(%3637, %3638, %218, %217), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3646 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.43, %3645), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.43 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3646, %228, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %3648 : int[] = prim::ListConstruct(%3637, %3638, %216, %217), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3649 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.89, %3648), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.43 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3649, %228, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %3651 : int[] = prim::ListConstruct(%3637, %3638, %216, %217), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3652 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.43, %3651), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %3653 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3652, %228, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.49 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %3656 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.43, %cos.49), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3657 : int = aten::size(%q.43, %221), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3658 : Long(device=cpu) = prim::NumToTensor(%3657), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3659 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3658, %215), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3660 : int = aten::Int(%3659), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3661 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %221, %223, %3660, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3662 : int = aten::size(%q.43, %221), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3663 : Long(device=cpu) = prim::NumToTensor(%3662), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3664 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3663, %215), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3665 : int = aten::Int(%3664), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %x2.85 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.43, %221, %3665, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3667 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.85), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3668 : Tensor[] = prim::ListConstruct(%3667, %3661), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3669 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3668, %231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3670 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3669, %sin.49), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3671 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3656, %3670, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3672 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.43, %cos.49), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3673 : int = aten::size(%k.43, %221), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3674 : Long(device=cpu) = prim::NumToTensor(%3673), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3675 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3674, %215), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3676 : int = aten::Int(%3675), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3677 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %221, %223, %3676, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3678 : int = aten::size(%k.43, %221), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3679 : Long(device=cpu) = prim::NumToTensor(%3678), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3680 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3679, %215), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3681 : int = aten::Int(%3680), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %x2.87 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.43, %221, %3681, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3683 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.87), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3684 : Tensor[] = prim::ListConstruct(%3683, %3677), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3685 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3684, %231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3686 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3685, %sin.49), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3687 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3672, %3686, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3688 : Tensor[] = prim::ListConstruct(%81, %3687), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %hidden_states.639 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3688, %214), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3690 : Tensor[] = prim::ListConstruct(%82, %3653), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %hidden_states.643 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3690, %214), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3692 : int = aten::size(%hidden_states.639, %223), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3693 : int = aten::size(%hidden_states.639, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.85 : Long(device=cpu) = prim::NumToTensor(%3693), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3695 : int = aten::size(%hidden_states.639, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3696 : int = aten::size(%hidden_states.639, %221), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3697 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.639, %223, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3698 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3697, %228, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3699 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3698, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3700 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3699, %221, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3701 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3700, %213, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3702 : int[] = prim::ListConstruct(%3692, %3693, %213, %3695, %3696), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %hidden_states.641 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3701, %3702, %232), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3704 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.85, %212), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3705 : int = aten::Int(%3704), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3706 : int[] = prim::ListConstruct(%3692, %3705, %3695, %3696), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %key_states.91 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.641, %3706), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3708 : int = aten::size(%hidden_states.643, %223), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3709 : int = aten::size(%hidden_states.643, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.87 : Long(device=cpu) = prim::NumToTensor(%3709), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3711 : int = aten::size(%hidden_states.643, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3712 : int = aten::size(%hidden_states.643, %221), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3713 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.643, %223, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3714 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3713, %228, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3715 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3714, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3716 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3715, %221, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3717 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3716, %213, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3718 : int[] = prim::ListConstruct(%3708, %3709, %213, %3711, %3712), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %hidden_states.645 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3717, %3718, %232), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3720 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.87, %212), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3721 : int = aten::Int(%3720), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3722 : int[] = prim::ListConstruct(%3708, %3721, %3711, %3712), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3723 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.645, %3722), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3724 : int = aten::size(%key_states.91, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3725 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3726 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3725, %228, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3727 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3726, %230, %223, %222, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3728 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3727, %221, %223, %3724, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.85 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%3671, %key_states.91, %3723, %3728, %211, %232, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3730 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.85, %228, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.87 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3730, %223), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %3732 : int[] = prim::ListConstruct(%3637, %3638, %231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn\n",
      "  %3733 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.87, %3732), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.389 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.43)\n",
      "  %hidden_states.647 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3733, %weight.389, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.self_attn/__module.model.layers.21.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3736 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.647, %hidden_states.639, %hidden_states.643)\n",
      "  %3737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3738 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3739 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3736)\n",
      "  %hidden_states.649 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3632, %3737, %228), scope: __module.model/__module.model.layers.21 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.391 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.43)\n",
      "  %hidden_states.651 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.649, %225, %232, %232, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.651, %230), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3744 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm\n",
      "  %variance.87 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3743, %3744, %210, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3746 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.87, %209, %228), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3747 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3746), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.653 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.651, %3747), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.655 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.653, %225, %232, %232, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3750 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.391, %hidden_states.655), scope: __module.model/__module.model.layers.21/__module.model.layers.21.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3751 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3750, %hidden_states.651)\n",
      "  %3752 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3751)\n",
      "  %down_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_295.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.43)\n",
      "  %up_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_294.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.43)\n",
      "  %gate_proj.43 : __torch__.torch.nn.modules.linear.___torch_mangle_293.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.43)\n",
      "  %weight.393 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.43)\n",
      "  %input.43 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3752, %weight.393, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3759 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.43), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.395 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.43)\n",
      "  %3761 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3752, %weight.395, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3762 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3759, %3761), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.397 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.43)\n",
      "  %hidden_states.657 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3762, %weight.397, %227), scope: __module.model/__module.model.layers.21/__module.model.layers.21.mlp/__module.model.layers.21.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.659 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3753, %hidden_states.657, %228), scope: __module.model/__module.model.layers.21 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %3766 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.659, %3738, %3739)\n",
      "  %3767 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3768 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3769 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3766)\n",
      "  %mlp.45 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_311.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_22)\n",
      "  %post_attention_layernorm.45 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_313.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_22)\n",
      "  %self_attn.47 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_306.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_22)\n",
      "  %input_layernorm.45 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_312.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_22)\n",
      "  %weight.399 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.45)\n",
      "  %hidden_states.661 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3767, %225, %232, %232, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3776 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.661, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3777 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm\n",
      "  %variance.89 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3776, %3777, %210, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3779 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.89, %209, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3780 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3779), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.663 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.661, %3780), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.665 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.663, %225, %232, %232, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.667 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.399, %hidden_states.665), scope: __module.model/__module.model.layers.22/__module.model.layers.22.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3784 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.667, %hidden_states.661)\n",
      "  %3785 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3786 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3784)\n",
      "  %o_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_304.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.47)\n",
      "  %v_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_303.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.47)\n",
      "  %k_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_302.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.47)\n",
      "  %q_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_301.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.47)\n",
      "  %3791 : int = aten::size(%3785, %223), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %3792 : int = aten::size(%3785, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.401 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.45)\n",
      "  %query_states.45 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3785, %weight.401, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.403 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.45)\n",
      "  %key_states.93 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3785, %weight.403, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.405 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.45)\n",
      "  %value_states.45 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3785, %weight.405, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3799 : int[] = prim::ListConstruct(%3791, %3792, %218, %217), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3800 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.45, %3799), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.45 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3800, %228, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %3802 : int[] = prim::ListConstruct(%3791, %3792, %216, %217), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3803 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.93, %3802), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.45 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3803, %228, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %3805 : int[] = prim::ListConstruct(%3791, %3792, %216, %217), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3806 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.45, %3805), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %3807 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3806, %228, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.51 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %3810 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.45, %cos.51), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3811 : int = aten::size(%q.45, %221), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3812 : Long(device=cpu) = prim::NumToTensor(%3811), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3813 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3812, %215), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3814 : int = aten::Int(%3813), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3815 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %221, %223, %3814, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3816 : int = aten::size(%q.45, %221), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3817 : Long(device=cpu) = prim::NumToTensor(%3816), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3818 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3817, %215), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3819 : int = aten::Int(%3818), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %x2.89 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.45, %221, %3819, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3821 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.89), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3822 : Tensor[] = prim::ListConstruct(%3821, %3815), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3823 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3822, %231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3824 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3823, %sin.51), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3825 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3810, %3824, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3826 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.45, %cos.51), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3827 : int = aten::size(%k.45, %221), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3828 : Long(device=cpu) = prim::NumToTensor(%3827), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3829 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3828, %215), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3830 : int = aten::Int(%3829), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3831 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %221, %223, %3830, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3832 : int = aten::size(%k.45, %221), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3833 : Long(device=cpu) = prim::NumToTensor(%3832), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3834 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3833, %215), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3835 : int = aten::Int(%3834), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %x2.91 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.45, %221, %3835, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3837 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.91), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3838 : Tensor[] = prim::ListConstruct(%3837, %3831), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3839 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3838, %231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3840 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3839, %sin.51), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3841 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3826, %3840, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3842 : Tensor[] = prim::ListConstruct(%83, %3841), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %hidden_states.669 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3842, %214), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3844 : Tensor[] = prim::ListConstruct(%84, %3807), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %hidden_states.673 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3844, %214), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3846 : int = aten::size(%hidden_states.669, %223), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3847 : int = aten::size(%hidden_states.669, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.89 : Long(device=cpu) = prim::NumToTensor(%3847), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3849 : int = aten::size(%hidden_states.669, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3850 : int = aten::size(%hidden_states.669, %221), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3851 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.669, %223, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3852 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3851, %228, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3853 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3852, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3854 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3853, %221, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3855 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3854, %213, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3856 : int[] = prim::ListConstruct(%3846, %3847, %213, %3849, %3850), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %hidden_states.671 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3855, %3856, %232), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3858 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.89, %212), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3859 : int = aten::Int(%3858), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3860 : int[] = prim::ListConstruct(%3846, %3859, %3849, %3850), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %key_states.95 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.671, %3860), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3862 : int = aten::size(%hidden_states.673, %223), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3863 : int = aten::size(%hidden_states.673, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.91 : Long(device=cpu) = prim::NumToTensor(%3863), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3865 : int = aten::size(%hidden_states.673, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3866 : int = aten::size(%hidden_states.673, %221), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %3867 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.673, %223, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3868 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3867, %228, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3869 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%3868, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3870 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3869, %221, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3871 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%3870, %213, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3872 : int[] = prim::ListConstruct(%3862, %3863, %213, %3865, %3866), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %hidden_states.675 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%3871, %3872, %232), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %3874 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.91, %212), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3875 : int = aten::Int(%3874), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3876 : int[] = prim::ListConstruct(%3862, %3875, %3865, %3866), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3877 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.675, %3876), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %3878 : int = aten::size(%key_states.95, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3879 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3880 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3879, %228, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3881 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3880, %230, %223, %222, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %3882 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%3881, %221, %223, %3878, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.89 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%3825, %key_states.95, %3877, %3882, %211, %232, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3884 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.89, %228, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.91 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%3884, %223), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %3886 : int[] = prim::ListConstruct(%3791, %3792, %231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn\n",
      "  %3887 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.91, %3886), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.407 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.45)\n",
      "  %hidden_states.677 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3887, %weight.407, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.self_attn/__module.model.layers.22.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3890 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.677, %hidden_states.669, %hidden_states.673)\n",
      "  %3891 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3892 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3893 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3890)\n",
      "  %hidden_states.679 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3786, %3891, %228), scope: __module.model/__module.model.layers.22 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.409 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.45)\n",
      "  %hidden_states.681 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.679, %225, %232, %232, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3897 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.681, %230), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3898 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm\n",
      "  %variance.91 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3897, %3898, %210, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3900 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.91, %209, %228), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3901 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3900), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.683 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.681, %3901), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.685 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.683, %225, %232, %232, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3904 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.409, %hidden_states.685), scope: __module.model/__module.model.layers.22/__module.model.layers.22.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3905 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%3904, %hidden_states.681)\n",
      "  %3906 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3907 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3905)\n",
      "  %down_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_309.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.45)\n",
      "  %up_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_308.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.45)\n",
      "  %gate_proj.45 : __torch__.torch.nn.modules.linear.___torch_mangle_307.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.45)\n",
      "  %weight.411 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.45)\n",
      "  %input.45 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3906, %weight.411, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3913 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.45), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.413 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.45)\n",
      "  %3915 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%3906, %weight.413, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3916 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%3913, %3915), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.415 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.45)\n",
      "  %hidden_states.687 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3916, %weight.415, %227), scope: __module.model/__module.model.layers.22/__module.model.layers.22.mlp/__module.model.layers.22.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.689 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3907, %hidden_states.687, %228), scope: __module.model/__module.model.layers.22 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %3920 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.689, %3892, %3893)\n",
      "  %3921 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3922 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %3923 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3920)\n",
      "  %mlp.47 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_325.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_23)\n",
      "  %post_attention_layernorm.47 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_327.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_23)\n",
      "  %self_attn.49 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_320.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_23)\n",
      "  %input_layernorm.47 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_326.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_23)\n",
      "  %weight.417 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.47)\n",
      "  %hidden_states.691 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%3921, %225, %232, %232, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %3930 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.691, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3931 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm\n",
      "  %variance.93 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%3930, %3931, %210, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %3933 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.93, %209, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %3934 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%3933), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.693 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.691, %3934), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.695 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.693, %225, %232, %232, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.697 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.417, %hidden_states.695), scope: __module.model/__module.model.layers.23/__module.model.layers.23.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %3938 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.697, %hidden_states.691)\n",
      "  %3939 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %3940 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%3938)\n",
      "  %o_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_318.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.49)\n",
      "  %v_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_317.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.49)\n",
      "  %k_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_316.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.49)\n",
      "  %q_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_315.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.49)\n",
      "  %3945 : int = aten::size(%3939, %223), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %3946 : int = aten::size(%3939, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.419 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.47)\n",
      "  %query_states.47 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%3939, %weight.419, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.421 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.47)\n",
      "  %key_states.97 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3939, %weight.421, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.423 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.47)\n",
      "  %value_states.47 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%3939, %weight.423, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3953 : int[] = prim::ListConstruct(%3945, %3946, %218, %217), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3954 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.47, %3953), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.47 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%3954, %228, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %3956 : int[] = prim::ListConstruct(%3945, %3946, %216, %217), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3957 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.97, %3956), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.47 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3957, %228, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %3959 : int[] = prim::ListConstruct(%3945, %3946, %216, %217), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3960 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.47, %3959), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %3961 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%3960, %228, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.53 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %3964 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.47, %cos.53), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3965 : int = aten::size(%q.47, %221), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3966 : Long(device=cpu) = prim::NumToTensor(%3965), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3967 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3966, %215), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3968 : int = aten::Int(%3967), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3969 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %221, %223, %3968, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3970 : int = aten::size(%q.47, %221), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3971 : Long(device=cpu) = prim::NumToTensor(%3970), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3972 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3971, %215), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3973 : int = aten::Int(%3972), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %x2.93 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.47, %221, %3973, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3975 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.93), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3976 : Tensor[] = prim::ListConstruct(%3975, %3969), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3977 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3976, %231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3978 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3977, %sin.53), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3979 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3964, %3978, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %3980 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.47, %cos.53), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3981 : int = aten::size(%k.47, %221), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3982 : Long(device=cpu) = prim::NumToTensor(%3981), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3983 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3982, %215), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3984 : int = aten::Int(%3983), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3985 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %221, %223, %3984, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %3986 : int = aten::size(%k.47, %221), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3987 : Long(device=cpu) = prim::NumToTensor(%3986), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3988 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%3987, %215), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3989 : int = aten::Int(%3988), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %x2.95 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.47, %221, %3989, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %3991 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.95), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %3992 : Tensor[] = prim::ListConstruct(%3991, %3985), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %3993 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3992, %231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3994 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%3993, %sin.53), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3995 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%3980, %3994, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %3996 : Tensor[] = prim::ListConstruct(%85, %3995), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %hidden_states.699 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3996, %214), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %3998 : Tensor[] = prim::ListConstruct(%86, %3961), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %hidden_states.703 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%3998, %214), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4000 : int = aten::size(%hidden_states.699, %223), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4001 : int = aten::size(%hidden_states.699, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.93 : Long(device=cpu) = prim::NumToTensor(%4001), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %4003 : int = aten::size(%hidden_states.699, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4004 : int = aten::size(%hidden_states.699, %221), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4005 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.699, %223, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4006 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4005, %228, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4007 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4006, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4008 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4007, %221, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4009 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4008, %213, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4010 : int[] = prim::ListConstruct(%4000, %4001, %213, %4003, %4004), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %hidden_states.701 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4009, %4010, %232), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4012 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.93, %212), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4013 : int = aten::Int(%4012), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %4014 : int[] = prim::ListConstruct(%4000, %4013, %4003, %4004), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %key_states.99 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.701, %4014), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4016 : int = aten::size(%hidden_states.703, %223), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4017 : int = aten::size(%hidden_states.703, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.95 : Long(device=cpu) = prim::NumToTensor(%4017), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %4019 : int = aten::size(%hidden_states.703, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4020 : int = aten::size(%hidden_states.703, %221), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4021 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.703, %223, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4022 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4021, %228, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4023 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4022, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4024 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4023, %221, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4025 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4024, %213, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4026 : int[] = prim::ListConstruct(%4016, %4017, %213, %4019, %4020), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %hidden_states.705 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4025, %4026, %232), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4028 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.95, %212), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4029 : int = aten::Int(%4028), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %4030 : int[] = prim::ListConstruct(%4016, %4029, %4019, %4020), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %4031 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.705, %4030), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4032 : int = aten::size(%key_states.99, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4033 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4034 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4033, %228, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4035 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4034, %230, %223, %222, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4036 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4035, %221, %223, %4032, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.93 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%3979, %key_states.99, %4031, %4036, %211, %232, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4038 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.93, %228, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.95 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4038, %223), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %4040 : int[] = prim::ListConstruct(%3945, %3946, %231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn\n",
      "  %4041 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.95, %4040), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.425 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.47)\n",
      "  %hidden_states.707 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4041, %weight.425, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.self_attn/__module.model.layers.23.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4044 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.707, %hidden_states.699, %hidden_states.703)\n",
      "  %4045 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4046 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4047 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4044)\n",
      "  %hidden_states.709 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%3940, %4045, %228), scope: __module.model/__module.model.layers.23 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.427 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.47)\n",
      "  %hidden_states.711 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.709, %225, %232, %232, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4051 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.711, %230), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4052 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm\n",
      "  %variance.95 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4051, %4052, %210, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4054 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.95, %209, %228), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4055 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4054), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.713 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.711, %4055), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.715 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.713, %225, %232, %232, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4058 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.427, %hidden_states.715), scope: __module.model/__module.model.layers.23/__module.model.layers.23.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4059 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4058, %hidden_states.711)\n",
      "  %4060 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4061 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4059)\n",
      "  %down_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_323.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.47)\n",
      "  %up_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_322.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.47)\n",
      "  %gate_proj.47 : __torch__.torch.nn.modules.linear.___torch_mangle_321.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.47)\n",
      "  %weight.429 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.47)\n",
      "  %input.47 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4060, %weight.429, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4067 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.47), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.431 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.47)\n",
      "  %4069 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4060, %weight.431, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4070 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4067, %4069), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.433 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.47)\n",
      "  %hidden_states.717 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4070, %weight.433, %227), scope: __module.model/__module.model.layers.23/__module.model.layers.23.mlp/__module.model.layers.23.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.719 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4061, %hidden_states.717, %228), scope: __module.model/__module.model.layers.23 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %4074 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.719, %4046, %4047)\n",
      "  %4075 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4076 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4077 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4074)\n",
      "  %mlp.49 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_339.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_24)\n",
      "  %post_attention_layernorm.49 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_341.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_24)\n",
      "  %self_attn.51 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_334.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_24)\n",
      "  %input_layernorm.49 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_340.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_24)\n",
      "  %weight.435 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.49)\n",
      "  %hidden_states.721 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4075, %225, %232, %232, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4084 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.721, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4085 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm\n",
      "  %variance.97 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4084, %4085, %210, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4087 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.97, %209, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4088 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4087), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.723 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.721, %4088), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.725 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.723, %225, %232, %232, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.727 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.435, %hidden_states.725), scope: __module.model/__module.model.layers.24/__module.model.layers.24.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4092 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.727, %hidden_states.721)\n",
      "  %4093 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4094 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4092)\n",
      "  %o_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_332.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.51)\n",
      "  %v_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_331.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.51)\n",
      "  %k_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_330.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.51)\n",
      "  %q_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_329.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.51)\n",
      "  %4099 : int = aten::size(%4093, %223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %4100 : int = aten::size(%4093, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.437 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.49)\n",
      "  %query_states.49 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4093, %weight.437, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.439 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.49)\n",
      "  %key_states.101 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4093, %weight.439, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.441 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.49)\n",
      "  %value_states.49 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4093, %weight.441, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4107 : int[] = prim::ListConstruct(%4099, %4100, %218, %217), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4108 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.49, %4107), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.49 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4108, %228, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %4110 : int[] = prim::ListConstruct(%4099, %4100, %216, %217), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4111 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.101, %4110), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.49 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4111, %228, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %4113 : int[] = prim::ListConstruct(%4099, %4100, %216, %217), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4114 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.49, %4113), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %4115 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4114, %228, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.55 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %4118 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.49, %cos.55), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4119 : int = aten::size(%q.49, %221), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4120 : Long(device=cpu) = prim::NumToTensor(%4119), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4121 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4120, %215), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4122 : int = aten::Int(%4121), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4123 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %221, %223, %4122, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4124 : int = aten::size(%q.49, %221), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4125 : Long(device=cpu) = prim::NumToTensor(%4124), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4126 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4125, %215), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4127 : int = aten::Int(%4126), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %x2.97 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.49, %221, %4127, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4129 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.97), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4130 : Tensor[] = prim::ListConstruct(%4129, %4123), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4131 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4130, %231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4132 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4131, %sin.55), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4133 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4118, %4132, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4134 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.49, %cos.55), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4135 : int = aten::size(%k.49, %221), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4136 : Long(device=cpu) = prim::NumToTensor(%4135), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4137 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4136, %215), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4138 : int = aten::Int(%4137), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4139 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %221, %223, %4138, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4140 : int = aten::size(%k.49, %221), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4141 : Long(device=cpu) = prim::NumToTensor(%4140), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4142 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4141, %215), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4143 : int = aten::Int(%4142), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %x2.99 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.49, %221, %4143, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4145 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.99), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4146 : Tensor[] = prim::ListConstruct(%4145, %4139), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4147 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4146, %231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4148 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4147, %sin.55), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4149 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4134, %4148, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4150 : Tensor[] = prim::ListConstruct(%87, %4149), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %hidden_states.729 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4150, %214), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4152 : Tensor[] = prim::ListConstruct(%88, %4115), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %hidden_states.733 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4152, %214), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4154 : int = aten::size(%hidden_states.729, %223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4155 : int = aten::size(%hidden_states.729, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.97 : Long(device=cpu) = prim::NumToTensor(%4155), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4157 : int = aten::size(%hidden_states.729, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4158 : int = aten::size(%hidden_states.729, %221), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4159 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.729, %223, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4160 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4159, %228, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4161 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4160, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4162 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4161, %221, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4163 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4162, %213, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4164 : int[] = prim::ListConstruct(%4154, %4155, %213, %4157, %4158), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %hidden_states.731 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4163, %4164, %232), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4166 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.97, %212), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4167 : int = aten::Int(%4166), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4168 : int[] = prim::ListConstruct(%4154, %4167, %4157, %4158), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %key_states.103 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.731, %4168), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4170 : int = aten::size(%hidden_states.733, %223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4171 : int = aten::size(%hidden_states.733, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.99 : Long(device=cpu) = prim::NumToTensor(%4171), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4173 : int = aten::size(%hidden_states.733, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4174 : int = aten::size(%hidden_states.733, %221), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4175 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.733, %223, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4176 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4175, %228, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4177 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4176, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4178 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4177, %221, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4179 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4178, %213, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4180 : int[] = prim::ListConstruct(%4170, %4171, %213, %4173, %4174), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %hidden_states.735 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4179, %4180, %232), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4182 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.99, %212), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4183 : int = aten::Int(%4182), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4184 : int[] = prim::ListConstruct(%4170, %4183, %4173, %4174), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4185 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.735, %4184), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4186 : int = aten::size(%key_states.103, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4187 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4188 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4187, %228, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4189 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4188, %230, %223, %222, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4190 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4189, %221, %223, %4186, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.97 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%4133, %key_states.103, %4185, %4190, %211, %232, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4192 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.97, %228, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.99 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4192, %223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %4194 : int[] = prim::ListConstruct(%4099, %4100, %231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn\n",
      "  %4195 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.99, %4194), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.443 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.49)\n",
      "  %hidden_states.737 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4195, %weight.443, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.self_attn/__module.model.layers.24.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4198 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.737, %hidden_states.729, %hidden_states.733)\n",
      "  %4199 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4200 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4201 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4198)\n",
      "  %hidden_states.739 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4094, %4199, %228), scope: __module.model/__module.model.layers.24 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.445 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.49)\n",
      "  %hidden_states.741 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.739, %225, %232, %232, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4205 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.741, %230), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4206 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm\n",
      "  %variance.99 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4205, %4206, %210, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4208 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.99, %209, %228), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4209 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4208), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.743 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.741, %4209), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.745 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.743, %225, %232, %232, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4212 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.445, %hidden_states.745), scope: __module.model/__module.model.layers.24/__module.model.layers.24.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4213 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4212, %hidden_states.741)\n",
      "  %4214 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4215 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4213)\n",
      "  %down_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_337.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.49)\n",
      "  %up_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_336.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.49)\n",
      "  %gate_proj.49 : __torch__.torch.nn.modules.linear.___torch_mangle_335.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.49)\n",
      "  %weight.447 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.49)\n",
      "  %input.49 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4214, %weight.447, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4221 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.49), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.449 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.49)\n",
      "  %4223 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4214, %weight.449, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4224 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4221, %4223), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.451 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.49)\n",
      "  %hidden_states.747 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4224, %weight.451, %227), scope: __module.model/__module.model.layers.24/__module.model.layers.24.mlp/__module.model.layers.24.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.749 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4215, %hidden_states.747, %228), scope: __module.model/__module.model.layers.24 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %4228 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.749, %4200, %4201)\n",
      "  %4229 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4230 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4231 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4228)\n",
      "  %mlp.51 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_353.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_25)\n",
      "  %post_attention_layernorm.51 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_355.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_25)\n",
      "  %self_attn.53 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_348.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_25)\n",
      "  %input_layernorm.51 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_354.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_25)\n",
      "  %weight.453 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.51)\n",
      "  %hidden_states.751 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4229, %225, %232, %232, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4238 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.751, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4239 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm\n",
      "  %variance.101 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4238, %4239, %210, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4241 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.101, %209, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4242 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4241), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.753 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.751, %4242), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.755 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.753, %225, %232, %232, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.757 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.453, %hidden_states.755), scope: __module.model/__module.model.layers.25/__module.model.layers.25.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4246 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.757, %hidden_states.751)\n",
      "  %4247 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4248 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4246)\n",
      "  %o_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_346.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.53)\n",
      "  %v_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_345.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.53)\n",
      "  %k_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_344.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.53)\n",
      "  %q_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_343.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.53)\n",
      "  %4253 : int = aten::size(%4247, %223), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %4254 : int = aten::size(%4247, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.455 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.51)\n",
      "  %query_states.51 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4247, %weight.455, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.457 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.51)\n",
      "  %key_states.105 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4247, %weight.457, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.459 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.51)\n",
      "  %value_states.51 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4247, %weight.459, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4261 : int[] = prim::ListConstruct(%4253, %4254, %218, %217), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4262 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.51, %4261), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.51 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4262, %228, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %4264 : int[] = prim::ListConstruct(%4253, %4254, %216, %217), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4265 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.105, %4264), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.51 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4265, %228, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %4267 : int[] = prim::ListConstruct(%4253, %4254, %216, %217), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4268 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.51, %4267), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %4269 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4268, %228, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.57 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %4272 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.51, %cos.57), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4273 : int = aten::size(%q.51, %221), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4274 : Long(device=cpu) = prim::NumToTensor(%4273), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4275 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4274, %215), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4276 : int = aten::Int(%4275), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4277 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %221, %223, %4276, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4278 : int = aten::size(%q.51, %221), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4279 : Long(device=cpu) = prim::NumToTensor(%4278), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4280 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4279, %215), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4281 : int = aten::Int(%4280), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %x2.101 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.51, %221, %4281, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4283 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.101), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4284 : Tensor[] = prim::ListConstruct(%4283, %4277), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4285 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4284, %231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4286 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4285, %sin.57), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4287 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4272, %4286, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4288 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.51, %cos.57), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4289 : int = aten::size(%k.51, %221), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4290 : Long(device=cpu) = prim::NumToTensor(%4289), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4291 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4290, %215), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4292 : int = aten::Int(%4291), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4293 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %221, %223, %4292, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4294 : int = aten::size(%k.51, %221), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4295 : Long(device=cpu) = prim::NumToTensor(%4294), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4296 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4295, %215), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4297 : int = aten::Int(%4296), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %x2.103 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.51, %221, %4297, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4299 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.103), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4300 : Tensor[] = prim::ListConstruct(%4299, %4293), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4301 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4300, %231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4302 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4301, %sin.57), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4303 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4288, %4302, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4304 : Tensor[] = prim::ListConstruct(%89, %4303), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %hidden_states.759 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4304, %214), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4306 : Tensor[] = prim::ListConstruct(%90, %4269), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %hidden_states.763 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4306, %214), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4308 : int = aten::size(%hidden_states.759, %223), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4309 : int = aten::size(%hidden_states.759, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.101 : Long(device=cpu) = prim::NumToTensor(%4309), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4311 : int = aten::size(%hidden_states.759, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4312 : int = aten::size(%hidden_states.759, %221), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4313 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.759, %223, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4314 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4313, %228, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4315 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4314, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4316 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4315, %221, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4317 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4316, %213, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4318 : int[] = prim::ListConstruct(%4308, %4309, %213, %4311, %4312), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %hidden_states.761 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4317, %4318, %232), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4320 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.101, %212), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4321 : int = aten::Int(%4320), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4322 : int[] = prim::ListConstruct(%4308, %4321, %4311, %4312), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %key_states.107 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.761, %4322), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4324 : int = aten::size(%hidden_states.763, %223), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4325 : int = aten::size(%hidden_states.763, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.103 : Long(device=cpu) = prim::NumToTensor(%4325), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4327 : int = aten::size(%hidden_states.763, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4328 : int = aten::size(%hidden_states.763, %221), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4329 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.763, %223, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4330 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4329, %228, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4331 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4330, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4332 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4331, %221, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4333 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4332, %213, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4334 : int[] = prim::ListConstruct(%4324, %4325, %213, %4327, %4328), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %hidden_states.765 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4333, %4334, %232), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4336 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.103, %212), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4337 : int = aten::Int(%4336), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4338 : int[] = prim::ListConstruct(%4324, %4337, %4327, %4328), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4339 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.765, %4338), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4340 : int = aten::size(%key_states.107, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4341 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4342 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4341, %228, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4343 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4342, %230, %223, %222, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4344 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4343, %221, %223, %4340, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.101 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%4287, %key_states.107, %4339, %4344, %211, %232, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4346 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.101, %228, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.103 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4346, %223), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %4348 : int[] = prim::ListConstruct(%4253, %4254, %231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn\n",
      "  %4349 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.103, %4348), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.461 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.51)\n",
      "  %hidden_states.767 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4349, %weight.461, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.self_attn/__module.model.layers.25.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4352 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.767, %hidden_states.759, %hidden_states.763)\n",
      "  %4353 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4354 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4355 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4352)\n",
      "  %hidden_states.769 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4248, %4353, %228), scope: __module.model/__module.model.layers.25 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.463 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.51)\n",
      "  %hidden_states.771 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.769, %225, %232, %232, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4359 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.771, %230), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4360 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm\n",
      "  %variance.103 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4359, %4360, %210, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4362 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.103, %209, %228), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4363 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4362), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.773 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.771, %4363), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.775 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.773, %225, %232, %232, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4366 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.463, %hidden_states.775), scope: __module.model/__module.model.layers.25/__module.model.layers.25.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4367 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4366, %hidden_states.771)\n",
      "  %4368 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4369 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4367)\n",
      "  %down_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_351.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.51)\n",
      "  %up_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_350.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.51)\n",
      "  %gate_proj.51 : __torch__.torch.nn.modules.linear.___torch_mangle_349.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.51)\n",
      "  %weight.465 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.51)\n",
      "  %input.51 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4368, %weight.465, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4375 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.51), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.467 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.51)\n",
      "  %4377 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4368, %weight.467, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4378 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4375, %4377), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.469 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.51)\n",
      "  %hidden_states.777 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4378, %weight.469, %227), scope: __module.model/__module.model.layers.25/__module.model.layers.25.mlp/__module.model.layers.25.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.779 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4369, %hidden_states.777, %228), scope: __module.model/__module.model.layers.25 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %4382 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.779, %4354, %4355)\n",
      "  %4383 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4384 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4385 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4382)\n",
      "  %mlp.53 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_367.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_26)\n",
      "  %post_attention_layernorm.53 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_369.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_26)\n",
      "  %self_attn.55 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_362.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_26)\n",
      "  %input_layernorm.53 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_368.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_26)\n",
      "  %weight.471 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.53)\n",
      "  %hidden_states.781 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4383, %225, %232, %232, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4392 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.781, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4393 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm\n",
      "  %variance.105 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4392, %4393, %210, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4395 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.105, %209, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4396 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4395), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.783 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.781, %4396), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.785 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.783, %225, %232, %232, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.787 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.471, %hidden_states.785), scope: __module.model/__module.model.layers.26/__module.model.layers.26.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4400 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.787, %hidden_states.781)\n",
      "  %4401 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4402 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4400)\n",
      "  %o_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_360.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.55)\n",
      "  %v_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_359.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.55)\n",
      "  %k_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_358.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.55)\n",
      "  %q_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_357.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.55)\n",
      "  %4407 : int = aten::size(%4401, %223), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %4408 : int = aten::size(%4401, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.473 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.53)\n",
      "  %query_states.53 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4401, %weight.473, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.475 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.53)\n",
      "  %key_states.109 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4401, %weight.475, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.477 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.53)\n",
      "  %value_states.53 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4401, %weight.477, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4415 : int[] = prim::ListConstruct(%4407, %4408, %218, %217), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4416 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.53, %4415), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.53 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4416, %228, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %4418 : int[] = prim::ListConstruct(%4407, %4408, %216, %217), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4419 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.109, %4418), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.53 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4419, %228, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %4421 : int[] = prim::ListConstruct(%4407, %4408, %216, %217), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4422 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.53, %4421), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %4423 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4422, %228, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.59 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %4426 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.53, %cos.59), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4427 : int = aten::size(%q.53, %221), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4428 : Long(device=cpu) = prim::NumToTensor(%4427), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4429 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4428, %215), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4430 : int = aten::Int(%4429), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4431 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %221, %223, %4430, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4432 : int = aten::size(%q.53, %221), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4433 : Long(device=cpu) = prim::NumToTensor(%4432), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4434 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4433, %215), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4435 : int = aten::Int(%4434), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %x2.105 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.53, %221, %4435, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4437 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.105), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4438 : Tensor[] = prim::ListConstruct(%4437, %4431), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4439 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4438, %231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4440 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4439, %sin.59), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4441 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4426, %4440, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4442 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.53, %cos.59), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4443 : int = aten::size(%k.53, %221), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4444 : Long(device=cpu) = prim::NumToTensor(%4443), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4445 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4444, %215), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4446 : int = aten::Int(%4445), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4447 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %221, %223, %4446, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4448 : int = aten::size(%k.53, %221), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4449 : Long(device=cpu) = prim::NumToTensor(%4448), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4450 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4449, %215), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4451 : int = aten::Int(%4450), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %x2.107 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.53, %221, %4451, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4453 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.107), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4454 : Tensor[] = prim::ListConstruct(%4453, %4447), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4455 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4454, %231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4456 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4455, %sin.59), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4457 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4442, %4456, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4458 : Tensor[] = prim::ListConstruct(%91, %4457), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %hidden_states.789 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4458, %214), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4460 : Tensor[] = prim::ListConstruct(%92, %4423), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %hidden_states.793 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4460, %214), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4462 : int = aten::size(%hidden_states.789, %223), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4463 : int = aten::size(%hidden_states.789, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.105 : Long(device=cpu) = prim::NumToTensor(%4463), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4465 : int = aten::size(%hidden_states.789, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4466 : int = aten::size(%hidden_states.789, %221), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4467 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.789, %223, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4468 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4467, %228, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4469 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4468, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4470 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4469, %221, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4471 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4470, %213, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4472 : int[] = prim::ListConstruct(%4462, %4463, %213, %4465, %4466), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %hidden_states.791 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4471, %4472, %232), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4474 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.105, %212), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4475 : int = aten::Int(%4474), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4476 : int[] = prim::ListConstruct(%4462, %4475, %4465, %4466), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %key_states.111 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.791, %4476), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4478 : int = aten::size(%hidden_states.793, %223), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4479 : int = aten::size(%hidden_states.793, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.107 : Long(device=cpu) = prim::NumToTensor(%4479), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4481 : int = aten::size(%hidden_states.793, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4482 : int = aten::size(%hidden_states.793, %221), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4483 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.793, %223, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4484 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4483, %228, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4485 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4484, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4486 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4485, %221, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4487 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4486, %213, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4488 : int[] = prim::ListConstruct(%4478, %4479, %213, %4481, %4482), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %hidden_states.795 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4487, %4488, %232), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4490 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.107, %212), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4491 : int = aten::Int(%4490), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4492 : int[] = prim::ListConstruct(%4478, %4491, %4481, %4482), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4493 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.795, %4492), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4494 : int = aten::size(%key_states.111, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4495 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4496 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4495, %228, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4497 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4496, %230, %223, %222, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4498 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4497, %221, %223, %4494, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.105 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%4441, %key_states.111, %4493, %4498, %211, %232, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4500 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.105, %228, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.107 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4500, %223), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %4502 : int[] = prim::ListConstruct(%4407, %4408, %231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn\n",
      "  %4503 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.107, %4502), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.479 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.53)\n",
      "  %hidden_states.797 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4503, %weight.479, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.self_attn/__module.model.layers.26.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4506 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.797, %hidden_states.789, %hidden_states.793)\n",
      "  %4507 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4508 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4509 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4506)\n",
      "  %hidden_states.799 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4402, %4507, %228), scope: __module.model/__module.model.layers.26 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.481 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.53)\n",
      "  %hidden_states.801 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.799, %225, %232, %232, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4513 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.801, %230), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4514 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm\n",
      "  %variance.107 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4513, %4514, %210, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4516 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.107, %209, %228), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4517 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4516), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.803 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.801, %4517), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.805 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.803, %225, %232, %232, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4520 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.481, %hidden_states.805), scope: __module.model/__module.model.layers.26/__module.model.layers.26.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4521 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4520, %hidden_states.801)\n",
      "  %4522 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4523 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4521)\n",
      "  %down_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_365.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.53)\n",
      "  %up_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_364.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.53)\n",
      "  %gate_proj.53 : __torch__.torch.nn.modules.linear.___torch_mangle_363.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.53)\n",
      "  %weight.483 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.53)\n",
      "  %input.53 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4522, %weight.483, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4529 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.53), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.485 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.53)\n",
      "  %4531 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4522, %weight.485, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4532 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4529, %4531), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.487 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.53)\n",
      "  %hidden_states.807 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4532, %weight.487, %227), scope: __module.model/__module.model.layers.26/__module.model.layers.26.mlp/__module.model.layers.26.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.809 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4523, %hidden_states.807, %228), scope: __module.model/__module.model.layers.26 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %4536 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.809, %4508, %4509)\n",
      "  %4537 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4538 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4539 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4536)\n",
      "  %mlp.55 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_381.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_27)\n",
      "  %post_attention_layernorm.55 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_383.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_27)\n",
      "  %self_attn.57 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_376.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_27)\n",
      "  %input_layernorm.55 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_382.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_27)\n",
      "  %weight.489 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.55)\n",
      "  %hidden_states.811 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4537, %225, %232, %232, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4546 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.811, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4547 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm\n",
      "  %variance.109 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4546, %4547, %210, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4549 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.109, %209, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4550 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4549), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.813 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.811, %4550), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.815 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.813, %225, %232, %232, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.817 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.489, %hidden_states.815), scope: __module.model/__module.model.layers.27/__module.model.layers.27.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4554 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.817, %hidden_states.811)\n",
      "  %4555 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4556 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4554)\n",
      "  %o_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_374.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.57)\n",
      "  %v_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_373.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.57)\n",
      "  %k_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_372.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.57)\n",
      "  %q_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_371.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.57)\n",
      "  %4561 : int = aten::size(%4555, %223), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %4562 : int = aten::size(%4555, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.491 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.55)\n",
      "  %query_states.55 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4555, %weight.491, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.493 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.55)\n",
      "  %key_states.113 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4555, %weight.493, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.495 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.55)\n",
      "  %value_states.55 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4555, %weight.495, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4569 : int[] = prim::ListConstruct(%4561, %4562, %218, %217), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4570 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.55, %4569), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.55 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4570, %228, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %4572 : int[] = prim::ListConstruct(%4561, %4562, %216, %217), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4573 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.113, %4572), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.55 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4573, %228, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %4575 : int[] = prim::ListConstruct(%4561, %4562, %216, %217), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4576 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.55, %4575), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %4577 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4576, %228, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.61 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %4580 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.55, %cos.61), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4581 : int = aten::size(%q.55, %221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4582 : Long(device=cpu) = prim::NumToTensor(%4581), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4583 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4582, %215), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4584 : int = aten::Int(%4583), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4585 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %221, %223, %4584, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4586 : int = aten::size(%q.55, %221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4587 : Long(device=cpu) = prim::NumToTensor(%4586), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4588 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4587, %215), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4589 : int = aten::Int(%4588), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %x2.109 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.55, %221, %4589, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4591 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.109), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4592 : Tensor[] = prim::ListConstruct(%4591, %4585), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4593 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4592, %231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4594 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4593, %sin.61), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4595 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4580, %4594, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4596 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.55, %cos.61), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4597 : int = aten::size(%k.55, %221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4598 : Long(device=cpu) = prim::NumToTensor(%4597), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4599 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4598, %215), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4600 : int = aten::Int(%4599), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4601 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %221, %223, %4600, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4602 : int = aten::size(%k.55, %221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4603 : Long(device=cpu) = prim::NumToTensor(%4602), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4604 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4603, %215), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4605 : int = aten::Int(%4604), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %x2.111 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.55, %221, %4605, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4607 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.111), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4608 : Tensor[] = prim::ListConstruct(%4607, %4601), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4609 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4608, %231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4610 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4609, %sin.61), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4611 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4596, %4610, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4612 : Tensor[] = prim::ListConstruct(%93, %4611), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %hidden_states.819 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4612, %214), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4614 : Tensor[] = prim::ListConstruct(%94, %4577), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %hidden_states.823 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4614, %214), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4616 : int = aten::size(%hidden_states.819, %223), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4617 : int = aten::size(%hidden_states.819, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.109 : Long(device=cpu) = prim::NumToTensor(%4617), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4619 : int = aten::size(%hidden_states.819, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4620 : int = aten::size(%hidden_states.819, %221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4621 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.819, %223, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4622 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4621, %228, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4623 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4622, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4624 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4623, %221, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4625 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4624, %213, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4626 : int[] = prim::ListConstruct(%4616, %4617, %213, %4619, %4620), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %hidden_states.821 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4625, %4626, %232), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4628 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.109, %212), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4629 : int = aten::Int(%4628), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4630 : int[] = prim::ListConstruct(%4616, %4629, %4619, %4620), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %key_states.115 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.821, %4630), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4632 : int = aten::size(%hidden_states.823, %223), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4633 : int = aten::size(%hidden_states.823, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.111 : Long(device=cpu) = prim::NumToTensor(%4633), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4635 : int = aten::size(%hidden_states.823, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4636 : int = aten::size(%hidden_states.823, %221), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4637 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.823, %223, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4638 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4637, %228, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4639 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4638, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4640 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4639, %221, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4641 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4640, %213, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4642 : int[] = prim::ListConstruct(%4632, %4633, %213, %4635, %4636), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %hidden_states.825 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4641, %4642, %232), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4644 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.111, %212), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4645 : int = aten::Int(%4644), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4646 : int[] = prim::ListConstruct(%4632, %4645, %4635, %4636), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4647 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.825, %4646), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4648 : int = aten::size(%key_states.115, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4649 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4650 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4649, %228, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4651 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4650, %230, %223, %222, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4652 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4651, %221, %223, %4648, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.109 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%4595, %key_states.115, %4647, %4652, %211, %232, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4654 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.109, %228, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.111 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4654, %223), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %4656 : int[] = prim::ListConstruct(%4561, %4562, %231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn\n",
      "  %4657 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.111, %4656), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.497 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.55)\n",
      "  %hidden_states.827 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4657, %weight.497, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.self_attn/__module.model.layers.27.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4660 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.827, %hidden_states.819, %hidden_states.823)\n",
      "  %4661 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4662 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4663 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4660)\n",
      "  %hidden_states.829 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4556, %4661, %228), scope: __module.model/__module.model.layers.27 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.499 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.55)\n",
      "  %hidden_states.831 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.829, %225, %232, %232, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4667 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.831, %230), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4668 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm\n",
      "  %variance.111 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4667, %4668, %210, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4670 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.111, %209, %228), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4671 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4670), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.833 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.831, %4671), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.835 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.833, %225, %232, %232, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4674 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.499, %hidden_states.835), scope: __module.model/__module.model.layers.27/__module.model.layers.27.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4675 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4674, %hidden_states.831)\n",
      "  %4676 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4677 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4675)\n",
      "  %down_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_379.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.55)\n",
      "  %up_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_378.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.55)\n",
      "  %gate_proj.55 : __torch__.torch.nn.modules.linear.___torch_mangle_377.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.55)\n",
      "  %weight.501 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.55)\n",
      "  %input.55 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4676, %weight.501, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4683 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.55), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.503 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.55)\n",
      "  %4685 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4676, %weight.503, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4686 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4683, %4685), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.505 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.55)\n",
      "  %hidden_states.837 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4686, %weight.505, %227), scope: __module.model/__module.model.layers.27/__module.model.layers.27.mlp/__module.model.layers.27.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.839 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4677, %hidden_states.837, %228), scope: __module.model/__module.model.layers.27 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %4690 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.839, %4662, %4663)\n",
      "  %4691 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4692 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4693 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4690)\n",
      "  %mlp.57 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_395.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_28)\n",
      "  %post_attention_layernorm.57 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_397.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_28)\n",
      "  %self_attn.59 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_390.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_28)\n",
      "  %input_layernorm.57 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_396.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_28)\n",
      "  %weight.507 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.57)\n",
      "  %hidden_states.841 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4691, %225, %232, %232, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4700 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.841, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4701 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm\n",
      "  %variance.113 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4700, %4701, %210, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4703 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.113, %209, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4704 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4703), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.843 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.841, %4704), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.845 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.843, %225, %232, %232, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.847 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.507, %hidden_states.845), scope: __module.model/__module.model.layers.28/__module.model.layers.28.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4708 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.847, %hidden_states.841)\n",
      "  %4709 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4710 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4708)\n",
      "  %o_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_388.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.59)\n",
      "  %v_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_387.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.59)\n",
      "  %k_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_386.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.59)\n",
      "  %q_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_385.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.59)\n",
      "  %4715 : int = aten::size(%4709, %223), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %4716 : int = aten::size(%4709, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.509 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.57)\n",
      "  %query_states.57 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4709, %weight.509, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.511 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.57)\n",
      "  %key_states.117 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4709, %weight.511, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.513 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.57)\n",
      "  %value_states.57 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4709, %weight.513, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4723 : int[] = prim::ListConstruct(%4715, %4716, %218, %217), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4724 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.57, %4723), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.57 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4724, %228, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %4726 : int[] = prim::ListConstruct(%4715, %4716, %216, %217), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4727 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.117, %4726), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.57 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4727, %228, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %4729 : int[] = prim::ListConstruct(%4715, %4716, %216, %217), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4730 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.57, %4729), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %4731 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4730, %228, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.63 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %4734 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.57, %cos.63), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4735 : int = aten::size(%q.57, %221), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4736 : Long(device=cpu) = prim::NumToTensor(%4735), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4737 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4736, %215), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4738 : int = aten::Int(%4737), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4739 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %221, %223, %4738, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4740 : int = aten::size(%q.57, %221), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4741 : Long(device=cpu) = prim::NumToTensor(%4740), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4742 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4741, %215), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4743 : int = aten::Int(%4742), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %x2.113 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.57, %221, %4743, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4745 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.113), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4746 : Tensor[] = prim::ListConstruct(%4745, %4739), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4747 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4746, %231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4748 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4747, %sin.63), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4749 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4734, %4748, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4750 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.57, %cos.63), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4751 : int = aten::size(%k.57, %221), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4752 : Long(device=cpu) = prim::NumToTensor(%4751), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4753 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4752, %215), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4754 : int = aten::Int(%4753), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4755 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %221, %223, %4754, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4756 : int = aten::size(%k.57, %221), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4757 : Long(device=cpu) = prim::NumToTensor(%4756), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4758 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4757, %215), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4759 : int = aten::Int(%4758), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %x2.115 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.57, %221, %4759, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4761 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.115), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4762 : Tensor[] = prim::ListConstruct(%4761, %4755), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4763 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4762, %231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4764 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4763, %sin.63), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4765 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4750, %4764, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4766 : Tensor[] = prim::ListConstruct(%95, %4765), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %hidden_states.849 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4766, %214), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4768 : Tensor[] = prim::ListConstruct(%96, %4731), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %hidden_states.853 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4768, %214), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4770 : int = aten::size(%hidden_states.849, %223), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4771 : int = aten::size(%hidden_states.849, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.113 : Long(device=cpu) = prim::NumToTensor(%4771), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4773 : int = aten::size(%hidden_states.849, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4774 : int = aten::size(%hidden_states.849, %221), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4775 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.849, %223, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4776 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4775, %228, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4777 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4776, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4778 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4777, %221, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4779 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4778, %213, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4780 : int[] = prim::ListConstruct(%4770, %4771, %213, %4773, %4774), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %hidden_states.851 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4779, %4780, %232), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4782 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.113, %212), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4783 : int = aten::Int(%4782), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4784 : int[] = prim::ListConstruct(%4770, %4783, %4773, %4774), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %key_states.119 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.851, %4784), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4786 : int = aten::size(%hidden_states.853, %223), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4787 : int = aten::size(%hidden_states.853, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.115 : Long(device=cpu) = prim::NumToTensor(%4787), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4789 : int = aten::size(%hidden_states.853, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4790 : int = aten::size(%hidden_states.853, %221), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4791 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.853, %223, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4792 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4791, %228, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4793 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4792, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4794 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4793, %221, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4795 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4794, %213, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4796 : int[] = prim::ListConstruct(%4786, %4787, %213, %4789, %4790), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %hidden_states.855 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4795, %4796, %232), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4798 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.115, %212), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4799 : int = aten::Int(%4798), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4800 : int[] = prim::ListConstruct(%4786, %4799, %4789, %4790), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4801 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.855, %4800), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4802 : int = aten::size(%key_states.119, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4803 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4804 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4803, %228, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4805 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4804, %230, %223, %222, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4806 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4805, %221, %223, %4802, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.113 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%4749, %key_states.119, %4801, %4806, %211, %232, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4808 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.113, %228, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.115 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4808, %223), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %4810 : int[] = prim::ListConstruct(%4715, %4716, %231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn\n",
      "  %4811 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.115, %4810), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.515 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.57)\n",
      "  %hidden_states.857 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4811, %weight.515, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.self_attn/__module.model.layers.28.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4814 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.857, %hidden_states.849, %hidden_states.853)\n",
      "  %4815 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4816 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4817 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4814)\n",
      "  %hidden_states.859 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4710, %4815, %228), scope: __module.model/__module.model.layers.28 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.517 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.57)\n",
      "  %hidden_states.861 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.859, %225, %232, %232, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4821 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.861, %230), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4822 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm\n",
      "  %variance.115 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4821, %4822, %210, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4824 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.115, %209, %228), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4825 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4824), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.863 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.861, %4825), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.865 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.863, %225, %232, %232, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4828 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.517, %hidden_states.865), scope: __module.model/__module.model.layers.28/__module.model.layers.28.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4829 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4828, %hidden_states.861)\n",
      "  %4830 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4831 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4829)\n",
      "  %down_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_393.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.57)\n",
      "  %up_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_392.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.57)\n",
      "  %gate_proj.57 : __torch__.torch.nn.modules.linear.___torch_mangle_391.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.57)\n",
      "  %weight.519 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.57)\n",
      "  %input.57 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4830, %weight.519, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4837 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.57), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.521 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.57)\n",
      "  %4839 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4830, %weight.521, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4840 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4837, %4839), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.523 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.57)\n",
      "  %hidden_states.867 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4840, %weight.523, %227), scope: __module.model/__module.model.layers.28/__module.model.layers.28.mlp/__module.model.layers.28.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.869 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4831, %hidden_states.867, %228), scope: __module.model/__module.model.layers.28 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %4844 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.869, %4816, %4817)\n",
      "  %4845 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4846 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4847 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4844)\n",
      "  %mlp.59 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_409.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_29)\n",
      "  %post_attention_layernorm.59 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_411.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_29)\n",
      "  %self_attn.61 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_404.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_29)\n",
      "  %input_layernorm.59 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_410.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_29)\n",
      "  %weight.525 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.59)\n",
      "  %hidden_states.871 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4845, %225, %232, %232, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4854 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.871, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4855 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm\n",
      "  %variance.117 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4854, %4855, %210, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4857 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.117, %209, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4858 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4857), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.873 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.871, %4858), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.875 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.873, %225, %232, %232, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.877 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.525, %hidden_states.875), scope: __module.model/__module.model.layers.29/__module.model.layers.29.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4862 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.877, %hidden_states.871)\n",
      "  %4863 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4864 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4862)\n",
      "  %o_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_402.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.61)\n",
      "  %v_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_401.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.61)\n",
      "  %k_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_400.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.61)\n",
      "  %q_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_399.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.61)\n",
      "  %4869 : int = aten::size(%4863, %223), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %4870 : int = aten::size(%4863, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.527 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.59)\n",
      "  %query_states.59 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4863, %weight.527, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.529 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.59)\n",
      "  %key_states.121 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4863, %weight.529, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.531 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.59)\n",
      "  %value_states.59 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%4863, %weight.531, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4877 : int[] = prim::ListConstruct(%4869, %4870, %218, %217), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4878 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.59, %4877), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.59 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%4878, %228, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %4880 : int[] = prim::ListConstruct(%4869, %4870, %216, %217), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4881 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.121, %4880), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.59 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4881, %228, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %4883 : int[] = prim::ListConstruct(%4869, %4870, %216, %217), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4884 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.59, %4883), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %4885 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%4884, %228, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.65 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %4888 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.59, %cos.65), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4889 : int = aten::size(%q.59, %221), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4890 : Long(device=cpu) = prim::NumToTensor(%4889), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4891 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4890, %215), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4892 : int = aten::Int(%4891), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4893 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %221, %223, %4892, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4894 : int = aten::size(%q.59, %221), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4895 : Long(device=cpu) = prim::NumToTensor(%4894), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4896 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4895, %215), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4897 : int = aten::Int(%4896), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %x2.117 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.59, %221, %4897, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4899 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.117), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4900 : Tensor[] = prim::ListConstruct(%4899, %4893), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4901 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4900, %231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4902 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4901, %sin.65), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4903 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4888, %4902, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %4904 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.59, %cos.65), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4905 : int = aten::size(%k.59, %221), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4906 : Long(device=cpu) = prim::NumToTensor(%4905), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4907 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4906, %215), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4908 : int = aten::Int(%4907), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4909 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %221, %223, %4908, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %4910 : int = aten::size(%k.59, %221), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4911 : Long(device=cpu) = prim::NumToTensor(%4910), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4912 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%4911, %215), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4913 : int = aten::Int(%4912), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %x2.119 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.59, %221, %4913, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %4915 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.119), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %4916 : Tensor[] = prim::ListConstruct(%4915, %4909), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4917 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4916, %231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4918 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%4917, %sin.65), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4919 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%4904, %4918, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %4920 : Tensor[] = prim::ListConstruct(%97, %4919), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %hidden_states.879 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4920, %214), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4922 : Tensor[] = prim::ListConstruct(%98, %4885), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %hidden_states.883 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%4922, %214), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4924 : int = aten::size(%hidden_states.879, %223), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4925 : int = aten::size(%hidden_states.879, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.117 : Long(device=cpu) = prim::NumToTensor(%4925), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4927 : int = aten::size(%hidden_states.879, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4928 : int = aten::size(%hidden_states.879, %221), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4929 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.879, %223, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4930 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4929, %228, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4931 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4930, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4932 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4931, %221, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4933 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4932, %213, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4934 : int[] = prim::ListConstruct(%4924, %4925, %213, %4927, %4928), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %hidden_states.881 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4933, %4934, %232), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4936 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.117, %212), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4937 : int = aten::Int(%4936), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4938 : int[] = prim::ListConstruct(%4924, %4937, %4927, %4928), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %key_states.123 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.881, %4938), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4940 : int = aten::size(%hidden_states.883, %223), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4941 : int = aten::size(%hidden_states.883, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.119 : Long(device=cpu) = prim::NumToTensor(%4941), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4943 : int = aten::size(%hidden_states.883, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4944 : int = aten::size(%hidden_states.883, %221), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %4945 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.883, %223, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4946 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4945, %228, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4947 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%4946, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4948 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4947, %221, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4949 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%4948, %213, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4950 : int[] = prim::ListConstruct(%4940, %4941, %213, %4943, %4944), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %hidden_states.885 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%4949, %4950, %232), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %4952 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.119, %212), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4953 : int = aten::Int(%4952), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4954 : int[] = prim::ListConstruct(%4940, %4953, %4943, %4944), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4955 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.885, %4954), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %4956 : int = aten::size(%key_states.123, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4957 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4958 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4957, %228, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4959 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4958, %230, %223, %222, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %4960 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%4959, %221, %223, %4956, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.117 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%4903, %key_states.123, %4955, %4960, %211, %232, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4962 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.117, %228, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.119 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%4962, %223), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %4964 : int[] = prim::ListConstruct(%4869, %4870, %231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn\n",
      "  %4965 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.119, %4964), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.533 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.59)\n",
      "  %hidden_states.887 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4965, %weight.533, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.self_attn/__module.model.layers.29.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4968 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.887, %hidden_states.879, %hidden_states.883)\n",
      "  %4969 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4970 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %4971 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4968)\n",
      "  %hidden_states.889 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4864, %4969, %228), scope: __module.model/__module.model.layers.29 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.535 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.59)\n",
      "  %hidden_states.891 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.889, %225, %232, %232, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %4975 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.891, %230), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4976 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm\n",
      "  %variance.119 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%4975, %4976, %210, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %4978 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.119, %209, %228), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %4979 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%4978), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.893 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.891, %4979), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.895 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.893, %225, %232, %232, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4982 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.535, %hidden_states.895), scope: __module.model/__module.model.layers.29/__module.model.layers.29.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %4983 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%4982, %hidden_states.891)\n",
      "  %4984 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %4985 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4983)\n",
      "  %down_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_407.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.59)\n",
      "  %up_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_406.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.59)\n",
      "  %gate_proj.59 : __torch__.torch.nn.modules.linear.___torch_mangle_405.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.59)\n",
      "  %weight.537 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.59)\n",
      "  %input.59 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4984, %weight.537, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4991 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.59), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.539 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.59)\n",
      "  %4993 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%4984, %weight.539, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %4994 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%4991, %4993), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.541 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.59)\n",
      "  %hidden_states.897 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%4994, %weight.541, %227), scope: __module.model/__module.model.layers.29/__module.model.layers.29.mlp/__module.model.layers.29.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.899 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%4985, %hidden_states.897, %228), scope: __module.model/__module.model.layers.29 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %4998 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.899, %4970, %4971)\n",
      "  %4999 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5000 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5001 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%4998)\n",
      "  %mlp.61 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_423.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_30)\n",
      "  %post_attention_layernorm.61 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_425.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_30)\n",
      "  %self_attn.63 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_418.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_30)\n",
      "  %input_layernorm.61 : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_424.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_30)\n",
      "  %weight.543 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm.61)\n",
      "  %hidden_states.901 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%4999, %225, %232, %232, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %5008 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.901, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5009 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm\n",
      "  %variance.121 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5008, %5009, %210, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5011 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.121, %209, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %5012 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5011), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.903 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.901, %5012), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.905 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.903, %225, %232, %232, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.907 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.543, %hidden_states.905), scope: __module.model/__module.model.layers.30/__module.model.layers.30.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5016 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.907, %hidden_states.901)\n",
      "  %5017 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5018 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5016)\n",
      "  %o_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_416.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn.63)\n",
      "  %v_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_415.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn.63)\n",
      "  %k_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_414.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn.63)\n",
      "  %q_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_413.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn.63)\n",
      "  %5023 : int = aten::size(%5017, %223), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %5024 : int = aten::size(%5017, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.545 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj.61)\n",
      "  %query_states.61 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5017, %weight.545, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.547 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj.61)\n",
      "  %key_states.125 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5017, %weight.547, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.549 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj.61)\n",
      "  %value_states.61 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5017, %weight.549, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5031 : int[] = prim::ListConstruct(%5023, %5024, %218, %217), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5032 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states.61, %5031), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q.61 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%5032, %228, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %5034 : int[] = prim::ListConstruct(%5023, %5024, %216, %217), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5035 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.125, %5034), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k.61 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5035, %228, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %5037 : int[] = prim::ListConstruct(%5023, %5024, %216, %217), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5038 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states.61, %5037), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %5039 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5038, %228, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos.67 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin.67 : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %5042 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q.61, %cos.67), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %5043 : int = aten::size(%q.61, %221), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5044 : Long(device=cpu) = prim::NumToTensor(%5043), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5045 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5044, %215), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5046 : int = aten::Int(%5045), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5047 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %221, %223, %5046, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5048 : int = aten::size(%q.61, %221), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5049 : Long(device=cpu) = prim::NumToTensor(%5048), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5050 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5049, %215), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5051 : int = aten::Int(%5050), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %x2.121 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q.61, %221, %5051, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5053 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.121), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %5054 : Tensor[] = prim::ListConstruct(%5053, %5047), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5055 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5054, %231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5056 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5055, %sin.67), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %5057 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5042, %5056, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %5058 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k.61, %cos.67), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %5059 : int = aten::size(%k.61, %221), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5060 : Long(device=cpu) = prim::NumToTensor(%5059), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5061 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5060, %215), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5062 : int = aten::Int(%5061), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5063 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %221, %223, %5062, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5064 : int = aten::size(%k.61, %221), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5065 : Long(device=cpu) = prim::NumToTensor(%5064), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5066 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5065, %215), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5067 : int = aten::Int(%5066), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %x2.123 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k.61, %221, %5067, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5069 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2.123), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %5070 : Tensor[] = prim::ListConstruct(%5069, %5063), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5071 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5070, %231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5072 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5071, %sin.67), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %5073 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%5058, %5072, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %5074 : Tensor[] = prim::ListConstruct(%99, %5073), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %hidden_states.909 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5074, %214), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5076 : Tensor[] = prim::ListConstruct(%100, %5039), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %hidden_states.913 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5076, %214), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5078 : int = aten::size(%hidden_states.909, %223), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5079 : int = aten::size(%hidden_states.909, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.121 : Long(device=cpu) = prim::NumToTensor(%5079), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5081 : int = aten::size(%hidden_states.909, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5082 : int = aten::size(%hidden_states.909, %221), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5083 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.909, %223, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5084 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5083, %228, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5085 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5084, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5086 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5085, %221, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5087 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5086, %213, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5088 : int[] = prim::ListConstruct(%5078, %5079, %213, %5081, %5082), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %hidden_states.911 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5087, %5088, %232), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5090 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.121, %212), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5091 : int = aten::Int(%5090), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5092 : int[] = prim::ListConstruct(%5078, %5091, %5081, %5082), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %key_states.127 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.911, %5092), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5094 : int = aten::size(%hidden_states.913, %223), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5095 : int = aten::size(%hidden_states.913, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.123 : Long(device=cpu) = prim::NumToTensor(%5095), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5097 : int = aten::size(%hidden_states.913, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5098 : int = aten::size(%hidden_states.913, %221), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5099 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.913, %223, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5100 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5099, %228, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5101 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5100, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5102 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5101, %221, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5103 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5102, %213, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5104 : int[] = prim::ListConstruct(%5094, %5095, %213, %5097, %5098), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %hidden_states.915 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5103, %5104, %232), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5106 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.123, %212), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5107 : int = aten::Int(%5106), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5108 : int[] = prim::ListConstruct(%5094, %5107, %5097, %5098), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5109 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.915, %5108), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5110 : int = aten::size(%key_states.127, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5111 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5112 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5111, %228, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5113 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5112, %230, %223, %222, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5114 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5113, %221, %223, %5110, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.121 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%5057, %key_states.127, %5109, %5114, %211, %232, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5116 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.121, %228, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output.123 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5116, %223), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %5118 : int[] = prim::ListConstruct(%5023, %5024, %231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn\n",
      "  %5119 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output.123, %5118), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.551 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj.61)\n",
      "  %hidden_states.917 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5119, %weight.551, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.self_attn/__module.model.layers.30.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5122 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.917, %hidden_states.909, %hidden_states.913)\n",
      "  %5123 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5124 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5125 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5122)\n",
      "  %hidden_states.919 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5018, %5123, %228), scope: __module.model/__module.model.layers.30 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.553 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm.61)\n",
      "  %hidden_states.921 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.919, %225, %232, %232, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %5129 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.921, %230), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5130 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm\n",
      "  %variance.123 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5129, %5130, %210, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5132 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.123, %209, %228), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %5133 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5132), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.923 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.921, %5133), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.925 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.923, %225, %232, %232, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5136 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.553, %hidden_states.925), scope: __module.model/__module.model.layers.30/__module.model.layers.30.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5137 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%5136, %hidden_states.921)\n",
      "  %5138 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5139 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5137)\n",
      "  %down_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_421.Linear = prim::GetAttr[name=\"down_proj\"](%mlp.61)\n",
      "  %up_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_420.Linear = prim::GetAttr[name=\"up_proj\"](%mlp.61)\n",
      "  %gate_proj.61 : __torch__.torch.nn.modules.linear.___torch_mangle_419.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp.61)\n",
      "  %weight.555 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj.61)\n",
      "  %input.61 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%5138, %weight.555, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5145 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input.61), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.557 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj.61)\n",
      "  %5147 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%5138, %weight.557, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5148 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%5145, %5147), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.559 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj.61)\n",
      "  %hidden_states.927 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5148, %weight.559, %227), scope: __module.model/__module.model.layers.30/__module.model.layers.30.mlp/__module.model.layers.30.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.929 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5139, %hidden_states.927, %228), scope: __module.model/__module.model.layers.30 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %5152 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.929, %5124, %5125)\n",
      "  %5153 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5154 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5155 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5152)\n",
      "  %mlp : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_437.LlamaMLP = prim::GetAttr[name=\"mlp\"](%_31)\n",
      "  %post_attention_layernorm : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_439.LlamaRMSNorm = prim::GetAttr[name=\"post_attention_layernorm\"](%_31)\n",
      "  %self_attn : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_432.LlamaSdpaAttention = prim::GetAttr[name=\"self_attn\"](%_31)\n",
      "  %input_layernorm : __torch__.transformers.models.llama.modeling_llama.___torch_mangle_438.LlamaRMSNorm = prim::GetAttr[name=\"input_layernorm\"](%_31)\n",
      "  %weight.561 : Tensor = prim::GetAttr[name=\"weight\"](%input_layernorm)\n",
      "  %hidden_states.931 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%5153, %225, %232, %232, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %5162 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.931, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5163 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm\n",
      "  %variance.125 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5162, %5163, %210, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5165 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.125, %209, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %5166 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5165), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.933 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.931, %5166), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.935 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.933, %225, %232, %232, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %hidden_states.937 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.561, %hidden_states.935), scope: __module.model/__module.model.layers.31/__module.model.layers.31.input_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5170 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.937, %hidden_states.931)\n",
      "  %5171 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5172 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5170)\n",
      "  %o_proj : __torch__.torch.nn.modules.linear.___torch_mangle_430.Linear = prim::GetAttr[name=\"o_proj\"](%self_attn)\n",
      "  %v_proj : __torch__.torch.nn.modules.linear.___torch_mangle_429.Linear = prim::GetAttr[name=\"v_proj\"](%self_attn)\n",
      "  %k_proj : __torch__.torch.nn.modules.linear.___torch_mangle_428.Linear = prim::GetAttr[name=\"k_proj\"](%self_attn)\n",
      "  %q_proj : __torch__.torch.nn.modules.linear.___torch_mangle_427.Linear = prim::GetAttr[name=\"q_proj\"](%self_attn)\n",
      "  %5177 : int = aten::size(%5171, %223), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %5178 : int = aten::size(%5171, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:558:0\n",
      "  %weight.563 : Tensor = prim::GetAttr[name=\"weight\"](%q_proj)\n",
      "  %query_states : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5171, %weight.563, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.q_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.565 : Tensor = prim::GetAttr[name=\"weight\"](%k_proj)\n",
      "  %key_states.129 : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5171, %weight.565, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.k_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %weight.567 : Tensor = prim::GetAttr[name=\"weight\"](%v_proj)\n",
      "  %value_states : Float(2, 16, 1024, strides=[16384, 1024, 1], requires_grad=0, device=cpu) = aten::linear(%5171, %weight.567, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.v_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5185 : int[] = prim::ListConstruct(%5177, %5178, %218, %217), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5186 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::view(%query_states, %5185), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %q : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::transpose(%5186, %228, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:564:0\n",
      "  %5188 : int[] = prim::ListConstruct(%5177, %5178, %216, %217), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5189 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%key_states.129, %5188), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %k : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5189, %228, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:565:0\n",
      "  %5191 : int[] = prim::ListConstruct(%5177, %5178, %216, %217), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5192 : Float(2, 16, 8, 128, strides=[16384, 1024, 128, 1], requires_grad=0, device=cpu) = aten::view(%value_states, %5191), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %5193 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::transpose(%5192, %228, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:566:0\n",
      "  %cos : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%377, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:217:0\n",
      "  %sin : Float(2, 1, 16, 128, strides=[2048, 2048, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%378, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:218:0\n",
      "  %5196 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%q, %cos), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %5197 : int = aten::size(%q, %221), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5198 : Long(device=cpu) = prim::NumToTensor(%5197), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5199 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5198, %215), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5200 : int = aten::Int(%5199), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5201 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %221, %223, %5200, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5202 : int = aten::size(%q, %221), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5203 : Long(device=cpu) = prim::NumToTensor(%5202), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5204 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5203, %215), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5205 : int = aten::Int(%5204), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %x2.125 : Float(2, 32, 16, 64, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::slice(%q, %221, %5205, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5207 : Float(2, 32, 16, 64, strides=[32768, 64, 2048, 1], requires_grad=0, device=cpu) = aten::neg(%x2.125), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %5208 : Tensor[] = prim::ListConstruct(%5207, %5201), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5209 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5208, %231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5210 : Float(2, 32, 16, 128, strides=[65536, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5209, %sin), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %5211 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5196, %5210, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:219:0\n",
      "  %5212 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::mul(%k, %cos), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %5213 : int = aten::size(%k, %221), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5214 : Long(device=cpu) = prim::NumToTensor(%5213), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5215 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5214, %215), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5216 : int = aten::Int(%5215), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5217 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %221, %223, %5216, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:192:0\n",
      "  %5218 : int = aten::size(%k, %221), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5219 : Long(device=cpu) = prim::NumToTensor(%5218), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5220 : Long(requires_grad=0, device=cpu) = aten::floor_divide(%5219, %215), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5221 : int = aten::Int(%5220), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %x2 : Float(2, 8, 16, 64, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::slice(%k, %221, %5221, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:193:0\n",
      "  %5223 : Float(2, 8, 16, 64, strides=[8192, 64, 512, 1], requires_grad=0, device=cpu) = aten::neg(%x2), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:194:0\n",
      "  %5224 : Tensor[] = prim::ListConstruct(%5223, %5217), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5225 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5224, %231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5226 : Float(2, 8, 16, 128, strides=[16384, 2048, 128, 1], requires_grad=0, device=cpu) = aten::mul(%5225, %sin), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %5227 : Float(2, 8, 16, 128, strides=[16384, 128, 1024, 1], requires_grad=0, device=cpu) = aten::add(%5212, %5226, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:220:0\n",
      "  %5228 : Tensor[] = prim::ListConstruct(%101, %5227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %hidden_states.939 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5228, %214), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5230 : Tensor[] = prim::ListConstruct(%102, %5193), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %hidden_states.943 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::cat(%5230, %214), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5232 : int = aten::size(%hidden_states.939, %223), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5233 : int = aten::size(%hidden_states.939, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads.125 : Long(device=cpu) = prim::NumToTensor(%5233), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5235 : int = aten::size(%hidden_states.939, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5236 : int = aten::size(%hidden_states.939, %221), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5237 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.939, %223, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5238 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5237, %228, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5239 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5238, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5240 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5239, %221, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5241 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5240, %213, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5242 : int[] = prim::ListConstruct(%5232, %5233, %213, %5235, %5236), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %hidden_states.941 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5241, %5242, %232), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5244 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads.125, %212), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5245 : int = aten::Int(%5244), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5246 : int[] = prim::ListConstruct(%5232, %5245, %5235, %5236), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %key_states : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.941, %5246), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5248 : int = aten::size(%hidden_states.943, %223), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5249 : int = aten::size(%hidden_states.943, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %num_key_value_heads : Long(device=cpu) = prim::NumToTensor(%5249), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5251 : int = aten::size(%hidden_states.943, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5252 : int = aten::size(%hidden_states.943, %221), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:263:0\n",
      "  %5253 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%hidden_states.943, %223, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5254 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5253, %228, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5255 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::unsqueeze(%5254, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5256 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5255, %221, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5257 : Float(2, 8, 1, 32, 128, strides=[32768, 4096, 4096, 128, 1], requires_grad=0, device=cpu) = aten::slice(%5256, %213, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5258 : int[] = prim::ListConstruct(%5248, %5249, %213, %5251, %5252), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %hidden_states.945 : Float(2, 8, 4, 32, 128, strides=[32768, 4096, 0, 128, 1], requires_grad=0, device=cpu) = aten::expand(%5257, %5258, %232), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:266:0\n",
      "  %5260 : Long(requires_grad=0, device=cpu) = aten::mul(%num_key_value_heads, %212), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5261 : int = aten::Int(%5260), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5262 : int[] = prim::ListConstruct(%5248, %5261, %5251, %5252), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5263 : Float(2, 32, 32, 128, strides=[131072, 4096, 128, 1], requires_grad=0, device=cpu) = aten::reshape(%hidden_states.945, %5262), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:267:0\n",
      "  %5264 : int = aten::size(%key_states, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5265 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%causal_mask, %223, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5266 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5265, %228, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5267 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5266, %230, %223, %222, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %5268 : Float(2, 1, 16, 32, strides=[512, 512, 32, 1], requires_grad=0, device=cpu) = aten::slice(%5267, %221, %223, %5264, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:590:0\n",
      "  %attn_output.125 : Float(2, 32, 16, 128, strides=[65536, 128, 4096, 1], requires_grad=0, device=cpu) = aten::scaled_dot_product_attention(%5211, %key_states, %5263, %5268, %211, %232, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5270 : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::transpose(%attn_output.125, %228, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %attn_output : Float(2, 16, 32, 128, strides=[65536, 4096, 128, 1], requires_grad=0, device=cpu) = aten::contiguous(%5270, %223), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:612:0\n",
      "  %5272 : int[] = prim::ListConstruct(%5177, %5178, %231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn\n",
      "  %5273 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::view(%attn_output, %5272), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:613:0\n",
      "  %weight.569 : Tensor = prim::GetAttr[name=\"weight\"](%o_proj)\n",
      "  %hidden_states.947 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5273, %weight.569, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.self_attn/__module.model.layers.31.self_attn.o_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5276 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.947, %hidden_states.939, %hidden_states.943)\n",
      "  %5277 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5278 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5279 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5276)\n",
      "  %hidden_states.949 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5172, %5277, %228), scope: __module.model/__module.model.layers.31 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:688:0\n",
      "  %weight.571 : Tensor = prim::GetAttr[name=\"weight\"](%post_attention_layernorm)\n",
      "  %hidden_states.951 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.949, %225, %232, %232, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %5283 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.951, %230), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5284 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm\n",
      "  %variance.127 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5283, %5284, %210, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5286 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance.127, %209, %228), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %5287 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5286), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.953 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.951, %5287), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states.955 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.953, %225, %232, %232, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5290 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.571, %hidden_states.955), scope: __module.model/__module.model.layers.31/__module.model.layers.31.post_attention_layernorm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5291 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%5290, %hidden_states.951)\n",
      "  %5292 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5293 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5291)\n",
      "  %down_proj : __torch__.torch.nn.modules.linear.___torch_mangle_435.Linear = prim::GetAttr[name=\"down_proj\"](%mlp)\n",
      "  %up_proj : __torch__.torch.nn.modules.linear.___torch_mangle_434.Linear = prim::GetAttr[name=\"up_proj\"](%mlp)\n",
      "  %gate_proj : __torch__.torch.nn.modules.linear.___torch_mangle_433.Linear = prim::GetAttr[name=\"gate_proj\"](%mlp)\n",
      "  %weight.573 : Tensor = prim::GetAttr[name=\"weight\"](%gate_proj)\n",
      "  %input : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%5292, %weight.573, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.gate_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5299 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::silu(%input), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.act_fn # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torch/nn/functional.py:2102:0\n",
      "  %weight.575 : Tensor = prim::GetAttr[name=\"weight\"](%up_proj)\n",
      "  %5301 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::linear(%5292, %weight.575, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.up_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %5302 : Float(2, 16, 14336, strides=[229376, 14336, 1], requires_grad=0, device=cpu) = aten::mul(%5299, %5301), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:253:0\n",
      "  %weight.577 : Tensor = prim::GetAttr[name=\"weight\"](%down_proj)\n",
      "  %hidden_states.957 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::linear(%5302, %weight.577, %227), scope: __module.model/__module.model.layers.31/__module.model.layers.31.mlp/__module.model.layers.31.mlp.down_proj # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.959 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::add(%5293, %hidden_states.957, %228), scope: __module.model/__module.model.layers.31 # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:694:0\n",
      "  %5306 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%hidden_states.959, %5278, %5279)\n",
      "  %5307 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %5308 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %5309 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5306)\n",
      "  %weight.579 : Tensor = prim::GetAttr[name=\"weight\"](%norm)\n",
      "  %hidden_states.961 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%5307, %225, %232, %232, %227), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:69:0\n",
      "  %5312 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::pow(%hidden_states.961, %230), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5313 : int[] = prim::ListConstruct(%231), scope: __module.model/__module.model.norm\n",
      "  %variance : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::mean(%5312, %5313, %210, %227), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:70:0\n",
      "  %5315 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::add(%variance, %209, %228), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %5316 : Float(2, 16, 1, strides=[16, 1, 1], requires_grad=0, device=cpu) = aten::rsqrt(%5315), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %hidden_states.963 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%hidden_states.961, %5316), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:71:0\n",
      "  %hidden_states : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::to(%hidden_states.963, %225, %232, %232, %227), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5319 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu) = aten::mul(%weight.579, %hidden_states), scope: __module.model/__module.model.norm # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:72:0\n",
      "  %5320 : (Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%5319, %534, %535, %688, %689, %842, %843, %996, %997, %1150, %1151, %1304, %1305, %1458, %1459, %1612, %1613, %1766, %1767, %1920, %1921, %2074, %2075, %2228, %2229, %2382, %2383, %2536, %2537, %2690, %2691, %2844, %2845, %2998, %2999, %3152, %3153, %3306, %3307, %3460, %3461, %3614, %3615, %3768, %3769, %3922, %3923, %4076, %4077, %4230, %4231, %4384, %4385, %4538, %4539, %4692, %4693, %4846, %4847, %5000, %5001, %5154, %5155, %5308, %5309)\n",
      "  %104 : Float(2, 16, 4096, strides=[65536, 4096, 1], requires_grad=0, device=cpu), %105 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %106 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %107 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %108 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %109 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %110 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %111 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %112 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %113 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %114 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %115 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %116 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %117 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %118 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %119 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %120 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %121 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %122 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %123 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %124 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %125 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %126 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %127 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %128 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %129 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %130 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %131 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %132 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %133 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %134 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %135 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %136 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %137 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %138 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %139 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %140 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %141 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %142 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %143 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %144 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %145 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %146 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %147 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %148 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %149 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %150 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %151 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %152 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %153 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %154 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %155 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %156 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %157 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %158 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %159 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %160 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %161 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %162 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %163 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %164 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %165 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %166 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %167 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), %168 : Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu) = prim::TupleUnpack(%5320)\n",
      "  %5321 : NoneType = prim::Constant(), scope: __module.lm_head\n",
      "  %weight : Tensor = prim::GetAttr[name=\"weight\"](%lm_head)\n",
      "  %logits : Float(2, 16, 128256, strides=[2052096, 128256, 1], requires_grad=0, device=cpu) = aten::linear(%104, %weight, %5321), scope: __module.lm_head # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/nncf/torch/dynamic_graph/wrappers.py:86:0\n",
      "  %170 : int = prim::Constant[value=6]() # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1159:0\n",
      "  %171 : bool = prim::Constant[value=0]() # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1159:0\n",
      "  %172 : bool = prim::Constant[value=0]() # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1159:0\n",
      "  %173 : NoneType = prim::Constant()\n",
      "  %174 : Float(2, 16, 128256, strides=[2052096, 128256, 1], requires_grad=0, device=cpu) = aten::to(%logits, %170, %171, %172, %173) # /home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/transformers/models/llama/modeling_llama.py:1159:0\n",
      "  %175 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%105, %106)\n",
      "  %176 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%107, %108)\n",
      "  %177 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%109, %110)\n",
      "  %178 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%111, %112)\n",
      "  %179 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%113, %114)\n",
      "  %180 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%115, %116)\n",
      "  %181 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%117, %118)\n",
      "  %182 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%119, %120)\n",
      "  %183 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%121, %122)\n",
      "  %184 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%123, %124)\n",
      "  %185 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%125, %126)\n",
      "  %186 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%127, %128)\n",
      "  %187 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%129, %130)\n",
      "  %188 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%131, %132)\n",
      "  %189 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%133, %134)\n",
      "  %190 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%135, %136)\n",
      "  %191 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%137, %138)\n",
      "  %192 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%139, %140)\n",
      "  %193 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%141, %142)\n",
      "  %194 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%143, %144)\n",
      "  %195 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%145, %146)\n",
      "  %196 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%147, %148)\n",
      "  %197 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%149, %150)\n",
      "  %198 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%151, %152)\n",
      "  %199 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%153, %154)\n",
      "  %200 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%155, %156)\n",
      "  %201 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%157, %158)\n",
      "  %202 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%159, %160)\n",
      "  %203 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%161, %162)\n",
      "  %204 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%163, %164)\n",
      "  %205 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%165, %166)\n",
      "  %206 : (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)) = prim::TupleConstruct(%167, %168)\n",
      "  %207 : ((Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu))) = prim::TupleConstruct(%175, %176, %177, %178, %179, %180, %181, %182, %183, %184, %185, %186, %187, %188, %189, %190, %191, %192, %193, %194, %195, %196, %197, %198, %199, %200, %201, %202, %203, %204, %205, %206)\n",
      "  %208 : (Float(2, 16, 128256, strides=[2052096, 128256, 1], requires_grad=0, device=cpu), ((Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)), (Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu), Float(2, 8, 32, 128, strides=[32768, 4096, 128, 1], requires_grad=0, device=cpu)))) = prim::TupleConstruct(%174, %207)\n",
      "  return (%208)\n",
      "\n",
      "INFO:nncf:Statistics of the bitwidth distribution:\n",
      "┍━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┑\n",
      "│   Num bits (N) │ % all parameters (layers)   │ % ratio-defining parameters (layers)   │\n",
      "┝━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┿━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┥\n",
      "│              8 │ 13% (2 / 226)               │ 0% (0 / 224)                           │\n",
      "├────────────────┼─────────────────────────────┼────────────────────────────────────────┤\n",
      "│              4 │ 87% (224 / 226)             │ 100% (224 / 224)                       │\n",
      "┕━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┙\n",
      "\u001b[2KApplying Weight Compression \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[35m100%\u001b[0m \u001b[38;2;0;104;181m226/226\u001b[0m • \u001b[38;2;0;104;181m0:04:42\u001b[0m • \u001b[38;2;0;104;181m0:00:00\u001b[0m;0;104;181m0:00:01\u001b[0m181m0:00:05\u001b[0m\n",
      "\u001b[?25hReplacing `(?!\\S)` pattern to `(?:$|[^\\S])` in RegexSplit operation\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "pt_model_id = model_configuration[\"model_id\"]\n",
    "pt_model_name = model_id.value.split(\"-\")[0]\n",
    "fp16_model_dir = Path(model_id.value) / \"FP16\"\n",
    "int8_model_dir = Path(model_id.value) / \"INT8_compressed_weights\"\n",
    "int4_model_dir = Path(model_id.value) / \"INT4_compressed_weights\"\n",
    "\n",
    "\n",
    "def convert_to_fp16():\n",
    "    if (fp16_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format fp16\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(fp16_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int8():\n",
    "    if (int8_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    int8_model_dir.mkdir(parents=True, exist_ok=True)\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int8\".format(pt_model_id)\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int8_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "def convert_to_int4():\n",
    "    compression_configs = {\n",
    "        \"zephyr-7b-beta\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"mistral-7b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"minicpm-2b-dpo\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"gemma-2b-it\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"notus-7b-v1\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"neural-chat-7b-v3-1\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 64,\n",
    "            \"ratio\": 0.6,\n",
    "        },\n",
    "        \"llama-2-chat-7b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"llama-3-8b-instruct\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"llama-3.1-8b-instruct\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 1.0,\n",
    "        },\n",
    "        \"gemma-7b-it\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "        \"chatglm2-6b\": {\n",
    "            \"sym\": True,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.72,\n",
    "        },\n",
    "        \"qwen-7b-chat\": {\"sym\": True, \"group_size\": 128, \"ratio\": 0.6},\n",
    "        \"red-pajama-3b-chat\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.5,\n",
    "        },\n",
    "        \"default\": {\n",
    "            \"sym\": False,\n",
    "            \"group_size\": 128,\n",
    "            \"ratio\": 0.8,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    model_compression_params = compression_configs.get(model_id.value, compression_configs[\"default\"])\n",
    "    if (int4_model_dir / \"openvino_model.xml\").exists():\n",
    "        return\n",
    "    remote_code = model_configuration.get(\"remote_code\", False)\n",
    "    export_command_base = \"optimum-cli export openvino --model {} --task text-generation-with-past --weight-format int4\".format(pt_model_id)\n",
    "    int4_compression_args = \" --group-size {} --ratio {}\".format(model_compression_params[\"group_size\"], model_compression_params[\"ratio\"])\n",
    "    if model_compression_params[\"sym\"]:\n",
    "        int4_compression_args += \" --sym\"\n",
    "    if enable_awq.value:\n",
    "        int4_compression_args += \" --awq --dataset wikitext2 --num-samples 128\"\n",
    "    export_command_base += int4_compression_args\n",
    "    if remote_code:\n",
    "        export_command_base += \" --trust-remote-code\"\n",
    "    export_command = export_command_base + \" \" + str(int4_model_dir)\n",
    "    display(Markdown(\"**Export command:**\"))\n",
    "    display(Markdown(f\"`{export_command}`\"))\n",
    "    ! $export_command\n",
    "\n",
    "\n",
    "if prepare_fp16_model.value:\n",
    "    convert_to_fp16()\n",
    "if prepare_int8_model.value:\n",
    "    convert_to_int8()\n",
    "if prepare_int4_model.value:\n",
    "    convert_to_int4()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a17d4",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "Let's compare model size for different compression types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281f1d07-998e-4e13-ba95-0264564ede82",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of model with INT4 compressed weights is 4435.75 MB\n"
     ]
    }
   ],
   "source": [
    "fp16_weights = fp16_model_dir / \"openvino_model.bin\"\n",
    "int8_weights = int8_model_dir / \"openvino_model.bin\"\n",
    "int4_weights = int4_model_dir / \"openvino_model.bin\"\n",
    "\n",
    "if fp16_weights.exists():\n",
    "    print(f\"Size of FP16 model is {fp16_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "for precision, compressed_weights in zip([8, 4], [int8_weights, int4_weights]):\n",
    "    if compressed_weights.exists():\n",
    "        print(f\"Size of model with INT{precision} compressed weights is {compressed_weights.stat().st_size / 1024 / 1024:.2f} MB\")\n",
    "    if compressed_weights.exists() and fp16_weights.exists():\n",
    "        print(f\"Compression rate for INT{precision} model: {fp16_weights.stat().st_size / compressed_weights.stat().st_size:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d62f9f4-5434-4550-b372-c86b5a5089d5",
   "metadata": {},
   "source": [
    "## Select device for inference and model variant\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    ">**Note**: There may be no speedup for INT4/INT8 compressed models on dGPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "837b4a3b-ccc3-4004-9577-2b2c7b802dea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3adb981860e74737b283e4cb7862e4a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "support_devices = core.available_devices\n",
    "if \"NPU\" in support_devices:\n",
    "    support_devices.remove(\"NPU\")\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53001e7-615f-4eb5-b831-4e2b2ff32826",
   "metadata": {
    "tags": []
   },
   "source": [
    "The cell below demonstrates how to instantiate model based on selected variant of model weights and inference device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3536a1a7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94a64db8da184ee7a0ed1c7f5d7dae6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Model to run:', options=('INT4',), value='INT4')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_models = []\n",
    "if int4_model_dir.exists():\n",
    "    available_models.append(\"INT4\")\n",
    "if int8_model_dir.exists():\n",
    "    available_models.append(\"INT8\")\n",
    "if fp16_model_dir.exists():\n",
    "    available_models.append(\"FP16\")\n",
    "\n",
    "model_to_run = widgets.Dropdown(\n",
    "    options=available_models,\n",
    "    value=available_models[0],\n",
    "    description=\"Model to run:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "model_to_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f63327-f0f5-4e2d-bfc2-0f764f8c19a8",
   "metadata": {},
   "source": [
    "## Instantiate Model using Optimum Intel\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Optimum Intel can be used to load optimized models from the [Hugging Face Hub](https://huggingface.co/docs/optimum/intel/hf.co/models) and create pipelines to run an inference with OpenVINO Runtime using Hugging Face APIs. The Optimum Inference models are API compatible with Hugging Face Transformers models.  This means we just need to replace `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.\n",
    "\n",
    "Below is an example of the RedPajama model\n",
    "\n",
    "```diff\n",
    "-from transformers import AutoModelForCausalLM\n",
    "+from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"togethercomputer/RedPajama-INCITE-Chat-3B-v1\"\n",
    "-model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "+model = OVModelForCausalLM.from_pretrained(model_id, export=True)\n",
    "```\n",
    "\n",
    "Model class initialization starts with calling `from_pretrained` method. When downloading and converting Transformers model, the parameter `export=True` should be added (as we already converted model before, we do not need to provide this parameter). We can save the converted model for the next usage with the `save_pretrained` method.\n",
    "Tokenizer class and pipelines API are compatible with Optimum models.\n",
    "\n",
    "You can find more details about OpenVINO LLM inference using HuggingFace Optimum API in [LLM inference guide](https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a041101-7336-40fd-96c9-cd298015a0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-24 13:42:40.688513: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-07-24 13:42:40.690512: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-24 13:42:40.727974: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-07-24 13:42:40.728877: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-24 13:42:41.485799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.0.1+cu118 with CUDA 1108 (you have 2.3.1+cpu)\n",
      "    Python  3.8.18 (you have 3.8.10)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/utils/outputs.py:63: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n",
      "Loading model from llama-3.1-8b-instruct/INT4_compressed_weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM\n",
    "\n",
    "if model_to_run.value == \"INT4\":\n",
    "    model_dir = int4_model_dir\n",
    "elif model_to_run.value == \"INT8\":\n",
    "    model_dir = int8_model_dir\n",
    "else:\n",
    "    model_dir = fp16_model_dir\n",
    "print(f\"Loading model from {model_dir}\")\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "if \"GPU\" in device.value and \"qwen2-7b-instruct\" in model_id.value:\n",
    "    ov_config[\"GPU_ENABLE_SDPA_OPTIMIZATION\"] = \"NO\"\n",
    "\n",
    "# On a GPU device a model is executed in FP16 precision. For red-pajama-3b-chat model there known accuracy\n",
    "# issues caused by this, which we avoid by setting precision hint to \"f32\".\n",
    "if model_id.value == \"red-pajama-3b-chat\" and \"GPU\" in core.available_devices and device.value in [\"GPU\", \"AUTO\"]:\n",
    "    ov_config[\"INFERENCE_PRECISION_HINT\"] = \"f32\"\n",
    "\n",
    "model_name = model_configuration[\"model_id\"]\n",
    "tok = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "ov_model = OVModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device=device.value,\n",
    "    ov_config=ov_config,\n",
    "    config=AutoConfig.from_pretrained(model_dir, trust_remote_code=True),\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f6f7596-5677-4931-875b-aaabfa23cabc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 + 2 = 4\n"
     ]
    }
   ],
   "source": [
    "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
    "test_string = \"2 + 2 =\"\n",
    "input_tokens = tok(test_string, return_tensors=\"pt\", **tokenizer_kwargs)\n",
    "answer = ov_model.generate(**input_tokens, max_new_tokens=2)\n",
    "print(tok.batch_decode(answer, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d622d0-be46-47c0-a762-88cb50ab15a9",
   "metadata": {},
   "source": [
    "## Run Chatbot\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now, when model created, we can setup Chatbot interface using [Gradio](https://www.gradio.app/).\n",
    "The diagram below illustrates how the chatbot pipeline works\n",
    "\n",
    "![generation pipeline](https://user-images.githubusercontent.com/29454499/255523209-d9336491-c7ba-4dc1-98f0-07f23743ce89.png)\n",
    "\n",
    "As can be seen, the pipeline very similar to instruction-following with only changes that previous conversation history additionally passed as input with next user question for getting wider input context. On the first iteration, the user provided instructions joined to conversation history (if exists) converted to token ids using a tokenizer, then prepared input provided to the model. The model generates probabilities for all tokens in logits format  The way the next token will be selected over predicted probabilities is driven by the selected decoding methodology. You can find more information about the most popular decoding methods in this [blog](https://huggingface.co/blog/how-to-generate). The result generation updates conversation history for next conversation step. it makes stronger connection of next question with previously provided and allows user to make clarifications regarding previously provided answers.https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725544ea-05ec-40d7-bbbc-1dc87cf57d04",
   "metadata": {},
   "source": [
    "There are several parameters that can control text generation quality: \n",
    "  * `Temperature` is a parameter used to control the level of creativity in AI-generated text. By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.  \n",
    "  Consider the following example: The AI model has to complete the sentence \"The cat is ____.\" with the following token probabilities:  \n",
    "\n",
    "    playing: 0.5  \n",
    "    sleeping: 0.25  \n",
    "    eating: 0.15  \n",
    "    driving: 0.05  \n",
    "    flying: 0.05  \n",
    "\n",
    "    - **Low temperature** (e.g., 0.2): The AI model becomes more focused and deterministic, choosing tokens with the highest probability, such as \"playing.\"  \n",
    "    - **Medium temperature** (e.g., 1.0): The AI model maintains a balance between creativity and focus, selecting tokens based on their probabilities without significant bias, such as \"playing,\" \"sleeping,\" or \"eating.\"  \n",
    "    - **High temperature** (e.g., 2.0): The AI model becomes more adventurous, increasing the chances of selecting less likely tokens, such as \"driving\" and \"flying.\"\n",
    "  * `Top-p`, also known as nucleus sampling, is a parameter used to control the range of tokens considered by the AI model based on their cumulative probability. By adjusting the `top-p` value, you can influence the AI model's token selection, making it more focused or diverse.\n",
    "  Using the same example with the cat, consider the following top_p settings:  \n",
    "    - **Low top_p** (e.g., 0.5): The AI model considers only tokens with the highest cumulative probability, such as \"playing.\"  \n",
    "    - **Medium top_p** (e.g., 0.8): The AI model considers tokens with a higher cumulative probability, such as \"playing,\" \"sleeping,\" and \"eating.\"  \n",
    "    - **High top_p** (e.g., 1.0): The AI model considers all tokens, including those with lower probabilities, such as \"driving\" and \"flying.\" \n",
    "  * `Top-k` is an another popular sampling strategy. In comparison with Top-P, which chooses from the smallest possible set of words whose cumulative probability exceeds the probability P, in Top-K sampling K most likely next words are filtered and the probability mass is redistributed among only those K next words. In our example with cat, if k=3, then only \"playing\", \"sleeping\" and \"eating\" will be taken into account as possible next word.\n",
    "  * `Repetition Penalty` This parameter can help penalize tokens based on how frequently they occur in the text, including the input prompt. A token that has already appeared five times is penalized more heavily than a token that has appeared only one time. A value of 1 means that there is no penalty and values larger than 1 discourage repeated tokens.https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01f8f7f8-072e-45dc-b7c9-18d8c3c47754",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token.As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from threading import Event, Thread\n",
    "from uuid import uuid4\n",
    "from typing import List, Tuple\n",
    "import gradio as gr\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = model_configuration[\"model_id\"]\n",
    "start_message = model_configuration[\"start_message\"]\n",
    "history_template = model_configuration.get(\"history_template\")\n",
    "has_chat_template = model_configuration.get(\"has_chat_template\", history_template is None)\n",
    "current_message_template = model_configuration.get(\"current_message_template\")\n",
    "stop_tokens = model_configuration.get(\"stop_tokens\")\n",
    "tokenizer_kwargs = model_configuration.get(\"tokenizer_kwargs\", {})\n",
    "\n",
    "chinese_examples = [\n",
    "    [\"你好!\"],\n",
    "    [\"你是谁?\"],\n",
    "    [\"请介绍一下上海\"],\n",
    "    [\"请介绍一下英特尔公司\"],\n",
    "    [\"晚上睡不着怎么办？\"],\n",
    "    [\"给我讲一个年轻人奋斗创业最终取得成功的故事。\"],\n",
    "    [\"给这个故事起一个标题。\"],\n",
    "]\n",
    "\n",
    "english_examples = [\n",
    "    [\"Hello there! How are you doing?\"],\n",
    "    [\"What is OpenVINO?\"],\n",
    "    [\"Who are you?\"],\n",
    "    [\"Can you explain to me briefly what is Python programming language?\"],\n",
    "    [\"Explain the plot of Cinderella in a sentence.\"],\n",
    "    [\"What are some common mistakes to avoid when writing code?\"],\n",
    "    [\"Write a 100-word blog post on “Benefits of Artificial Intelligence and OpenVINO“\"],\n",
    "]\n",
    "\n",
    "japanese_examples = [\n",
    "    [\"こんにちは！調子はどうですか?\"],\n",
    "    [\"OpenVINOとは何ですか?\"],\n",
    "    [\"あなたは誰ですか?\"],\n",
    "    [\"Pythonプログラミング言語とは何か簡単に説明してもらえますか?\"],\n",
    "    [\"シンデレラのあらすじを一文で説明してください。\"],\n",
    "    [\"コードを書くときに避けるべきよくある間違いは何ですか?\"],\n",
    "    [\"人工知能と「OpenVINOの利点」について100語程度のブログ記事を書いてください。\"],\n",
    "]\n",
    "\n",
    "examples = chinese_examples if (model_language.value == \"Chinese\") else japanese_examples if (model_language.value == \"Japanese\") else english_examples\n",
    "\n",
    "max_new_tokens = 256\n",
    "\n",
    "\n",
    "class StopOnTokens(StoppingCriteria):\n",
    "    def __init__(self, token_ids):\n",
    "        self.token_ids = token_ids\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
    "        for stop_id in self.token_ids:\n",
    "            if input_ids[0][-1] == stop_id:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "\n",
    "if stop_tokens is not None:\n",
    "    if isinstance(stop_tokens[0], str):\n",
    "        stop_tokens = tok.convert_tokens_to_ids(stop_tokens)\n",
    "\n",
    "    stop_tokens = [StopOnTokens(stop_tokens)]\n",
    "\n",
    "\n",
    "def default_partial_text_processor(partial_text: str, new_text: str):\n",
    "    \"\"\"\n",
    "    helper for updating partially generated answer, used by default\n",
    "\n",
    "    Params:\n",
    "      partial_text: text buffer for storing previosly generated text\n",
    "      new_text: text update for the current step\n",
    "    Returns:\n",
    "      updated text string\n",
    "\n",
    "    \"\"\"\n",
    "    partial_text += new_text\n",
    "    return partial_text\n",
    "\n",
    "\n",
    "text_processor = model_configuration.get(\"partial_text_processor\", default_partial_text_processor)\n",
    "\n",
    "\n",
    "def convert_history_to_token(history: List[Tuple[str, str]]):\n",
    "    \"\"\"\n",
    "    function for conversion history stored as list pairs of user and assistant messages to tokens according to model expected conversation template\n",
    "    Params:\n",
    "      history: dialogue history\n",
    "    Returns:\n",
    "      history in token format\n",
    "    \"\"\"\n",
    "    if pt_model_name == \"baichuan2\":\n",
    "        system_tokens = tok.encode(start_message)\n",
    "        history_tokens = []\n",
    "        for old_query, response in history[:-1]:\n",
    "            round_tokens = []\n",
    "            round_tokens.append(195)\n",
    "            round_tokens.extend(tok.encode(old_query))\n",
    "            round_tokens.append(196)\n",
    "            round_tokens.extend(tok.encode(response))\n",
    "            history_tokens = round_tokens + history_tokens\n",
    "        input_tokens = system_tokens + history_tokens\n",
    "        input_tokens.append(195)\n",
    "        input_tokens.extend(tok.encode(history[-1][0]))\n",
    "        input_tokens.append(196)\n",
    "        input_token = torch.LongTensor([input_tokens])\n",
    "    elif history_template is None or has_chat_template:\n",
    "        messages = [{\"role\": \"system\", \"content\": start_message}]\n",
    "        for idx, (user_msg, model_msg) in enumerate(history):\n",
    "            if idx == len(history) - 1 and not model_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "                break\n",
    "            if user_msg:\n",
    "                messages.append({\"role\": \"user\", \"content\": user_msg})\n",
    "            if model_msg:\n",
    "                messages.append({\"role\": \"assistant\", \"content\": model_msg})\n",
    "\n",
    "        input_token = tok.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\")\n",
    "    else:\n",
    "        text = start_message + \"\".join(\n",
    "            [\"\".join([history_template.format(num=round, user=item[0], assistant=item[1])]) for round, item in enumerate(history[:-1])]\n",
    "        )\n",
    "        text += \"\".join(\n",
    "            [\n",
    "                \"\".join(\n",
    "                    [\n",
    "                        current_message_template.format(\n",
    "                            num=len(history) + 1,\n",
    "                            user=history[-1][0],\n",
    "                            assistant=history[-1][1],\n",
    "                        )\n",
    "                    ]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        input_token = tok(text, return_tensors=\"pt\", **tokenizer_kwargs).input_ids\n",
    "    return input_token\n",
    "\n",
    "\n",
    "def user(message, history):\n",
    "    \"\"\"\n",
    "    callback function for updating user messages in interface on submit button click\n",
    "\n",
    "    Params:\n",
    "      message: current message\n",
    "      history: conversation history\n",
    "    Returns:\n",
    "      None\n",
    "    \"\"\"\n",
    "    # Append the user's message to the conversation history\n",
    "    return \"\", history + [[message, \"\"]]\n",
    "\n",
    "\n",
    "def bot(history, temperature, top_p, top_k, repetition_penalty, conversation_id):\n",
    "    \"\"\"\n",
    "    callback function for running chatbot on submit button click\n",
    "\n",
    "    Params:\n",
    "      history: conversation history\n",
    "      temperature:  parameter for control the level of creativity in AI-generated text.\n",
    "                    By adjusting the `temperature`, you can influence the AI model's probability distribution, making the text more focused or diverse.\n",
    "      top_p: parameter for control the range of tokens considered by the AI model based on their cumulative probability.\n",
    "      top_k: parameter for control the range of tokens considered by the AI model based on their cumulative probability, selecting number of tokens with highest probability.\n",
    "      repetition_penalty: parameter for penalizing tokens based on how frequently they occur in the text.\n",
    "      conversation_id: unique conversation identifier.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the input message string for the model by concatenating the current system message and conversation history\n",
    "    # Tokenize the messages string\n",
    "    input_ids = convert_history_to_token(history)\n",
    "    if input_ids.shape[1] > 2000:\n",
    "        history = [history[-1]]\n",
    "        input_ids = convert_history_to_token(history)\n",
    "    streamer = TextIteratorStreamer(tok, timeout=30.0, skip_prompt=True, skip_special_tokens=True)\n",
    "    generate_kwargs = dict(\n",
    "        input_ids=input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        temperature=temperature,\n",
    "        do_sample=temperature > 0.0,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        streamer=streamer,\n",
    "    )\n",
    "    if stop_tokens is not None:\n",
    "        generate_kwargs[\"stopping_criteria\"] = StoppingCriteriaList(stop_tokens)\n",
    "\n",
    "    stream_complete = Event()\n",
    "\n",
    "    def generate_and_signal_complete():\n",
    "        \"\"\"\n",
    "        genration function for single thread\n",
    "        \"\"\"\n",
    "        global start_time\n",
    "        ov_model.generate(**generate_kwargs)\n",
    "        stream_complete.set()\n",
    "\n",
    "    t1 = Thread(target=generate_and_signal_complete)\n",
    "    t1.start()\n",
    "\n",
    "    # Initialize an empty string to store the generated text\n",
    "    partial_text = \"\"\n",
    "    for new_text in streamer:\n",
    "        partial_text = text_processor(partial_text, new_text)\n",
    "        history[-1][1] = partial_text\n",
    "        yield history\n",
    "\n",
    "\n",
    "def request_cancel():\n",
    "    ov_model.request.cancel()\n",
    "\n",
    "\n",
    "def get_uuid():\n",
    "    \"\"\"\n",
    "    universal unique identifier for thread\n",
    "    \"\"\"\n",
    "    return str(uuid4())\n",
    "\n",
    "\n",
    "with gr.Blocks(\n",
    "    theme=gr.themes.Soft(),\n",
    "    css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
    ") as demo:\n",
    "    conversation_id = gr.State(get_uuid)\n",
    "    gr.Markdown(f\"\"\"<h1><center>OpenVINO {model_id.value} Chatbot</center></h1>\"\"\")\n",
    "    chatbot = gr.Chatbot(height=500)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            msg = gr.Textbox(\n",
    "                label=\"Chat Message Box\",\n",
    "                placeholder=\"Chat Message Box\",\n",
    "                show_label=False,\n",
    "                container=False,\n",
    "            )\n",
    "        with gr.Column():\n",
    "            with gr.Row():\n",
    "                submit = gr.Button(\"Submit\")\n",
    "                stop = gr.Button(\"Stop\")\n",
    "                clear = gr.Button(\"Clear\")\n",
    "    with gr.Row():\n",
    "        with gr.Accordion(\"Advanced Options:\", open=False):\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        temperature = gr.Slider(\n",
    "                            label=\"Temperature\",\n",
    "                            value=0.1,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Higher values produce more diverse outputs\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_p = gr.Slider(\n",
    "                            label=\"Top-p (nucleus sampling)\",\n",
    "                            value=1.0,\n",
    "                            minimum=0.0,\n",
    "                            maximum=1,\n",
    "                            step=0.01,\n",
    "                            interactive=True,\n",
    "                            info=(\n",
    "                                \"Sample from the smallest possible set of tokens whose cumulative probability \"\n",
    "                                \"exceeds top_p. Set to 1 to disable and sample from all tokens.\"\n",
    "                            ),\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        top_k = gr.Slider(\n",
    "                            label=\"Top-k\",\n",
    "                            value=50,\n",
    "                            minimum=0.0,\n",
    "                            maximum=200,\n",
    "                            step=1,\n",
    "                            interactive=True,\n",
    "                            info=\"Sample from a shortlist of top-k tokens — 0 to disable and sample from all tokens.\",\n",
    "                        )\n",
    "                with gr.Column():\n",
    "                    with gr.Row():\n",
    "                        repetition_penalty = gr.Slider(\n",
    "                            label=\"Repetition Penalty\",\n",
    "                            value=1.1,\n",
    "                            minimum=1.0,\n",
    "                            maximum=2.0,\n",
    "                            step=0.1,\n",
    "                            interactive=True,\n",
    "                            info=\"Penalize repetition — 1.0 to disable.\",\n",
    "                        )\n",
    "    gr.Examples(examples, inputs=msg, label=\"Click on any example and press the 'Submit' button\")\n",
    "\n",
    "    submit_event = msg.submit(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    submit_click_event = submit.click(\n",
    "        fn=user,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot],\n",
    "        queue=False,\n",
    "    ).then(\n",
    "        fn=bot,\n",
    "        inputs=[\n",
    "            chatbot,\n",
    "            temperature,\n",
    "            top_p,\n",
    "            top_k,\n",
    "            repetition_penalty,\n",
    "            conversation_id,\n",
    "        ],\n",
    "        outputs=chatbot,\n",
    "        queue=True,\n",
    "    )\n",
    "    stop.click(\n",
    "        fn=request_cancel,\n",
    "        inputs=None,\n",
    "        outputs=None,\n",
    "        cancels=[submit_event, submit_click_event],\n",
    "        queue=False,\n",
    "    )\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "#  demo.launch(server_name='your server name', server_port='server port in int')\n",
    "# if you have any issue to launch on your platform, you can pass share=True to launch method:\n",
    "# demo.launch(share=True)\n",
    "# it creates a publicly shareable link for the interface. Read more in the docs: https://gradio.app/docs/\n",
    "try:\n",
    "    demo.launch()\n",
    "except Exception:\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b837f9e-4152-4a5c-880a-ed874aa64a74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# please uncomment and run this cell for stopping gradio interface\n",
    "# demo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69ca0a2",
   "metadata": {},
   "source": [
    "### Next Step\n",
    "\n",
    "Besides chatbot, we can use LangChain to augmenting LLM knowledge with additional data, which allow you to build AI applications that can reason about private data or data introduced after a model’s cutoff date. You can find this solution in [Retrieval-augmented generation (RAG) example](../llm-rag-langchain/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "openvino_notebooks": {
   "imageUrl": "https://user-images.githubusercontent.com/29454499/255799218-611e7189-8979-4ef5-8a80-5a75e0136b50.png",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation",
     "Conversational"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "06c3cef8f8e442c4aef26a999f56e3d4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "qwen2-0.5b-instruct",
        "tiny-llama-1b-chat",
        "qwen2-1.5b-instruct",
        "gemma-2b-it",
        "red-pajama-3b-chat",
        "qwen2-7b-instruct",
        "gemma-7b-it",
        "llama-2-chat-7b",
        "llama-3-8b-instruct",
        "llama-3.1-8b-instruct",
        "mpt-7b-chat",
        "mistral-7b",
        "zephyr-7b-beta",
        "notus-7b-v1",
        "neural-chat-7b-v3-1",
        "phi-3-mini-instruct"
       ],
       "description": "Model:",
       "index": 9,
       "layout": "IPY_MODEL_fd16971fa27c44c99f4bac159e2b54c4",
       "style": "IPY_MODEL_283799aa5882402fb8f497b9635bdc79"
      }
     },
     "12241206783245f5a912abf0a46f4115": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "English",
        "Chinese",
        "Japanese"
       ],
       "description": "Model Language:",
       "index": 0,
       "layout": "IPY_MODEL_63ecc77eba3646078da31fd5980de9f6",
       "style": "IPY_MODEL_e4441ff41ad64f2e9c3a1ea7b01e663c"
      }
     },
     "13bdf48f4819476d939f054b2d355bd0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxModel",
      "state": {
       "description": "Prepare INT8 model",
       "disabled": false,
       "layout": "IPY_MODEL_f6555b7e3f1d490fa0fcae19fffb52cf",
       "style": "IPY_MODEL_bca58aaf2cce4004bbf9961a5bc347dc",
       "value": false
      }
     },
     "1bc84c9449db4536a37a58fa8a81eca8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "283799aa5882402fb8f497b9635bdc79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "32a9fb61ac2b44c993d6c6c1e9529d6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3adb981860e74737b283e4cb7862e4a0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 0,
       "layout": "IPY_MODEL_bde2dd437b6746718707ef9e219bf841",
       "style": "IPY_MODEL_505445438c2540b1a452d10939d36778"
      }
     },
     "505445438c2540b1a452d10939d36778": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "5160a67343cf4288bdd84eda3b7635a2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "579408abc137484e94678f9496a8b884": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxModel",
      "state": {
       "description": "Prepare FP16 model",
       "disabled": false,
       "layout": "IPY_MODEL_1bc84c9449db4536a37a58fa8a81eca8",
       "style": "IPY_MODEL_5dc9b177085642fc965f407810bfd161",
       "value": false
      }
     },
     "5dc9b177085642fc965f407810bfd161": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "63ecc77eba3646078da31fd5980de9f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "76f00ea5106b44db889f4535407d03d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxModel",
      "state": {
       "description": "Prepare INT4 model",
       "disabled": false,
       "layout": "IPY_MODEL_af7e0ac65182473ab1adacdbbcda9624",
       "style": "IPY_MODEL_90b07973cf0e41cc8319ed0ae6dd437c",
       "value": true
      }
     },
     "8450de790a38411fa032babbeea0f1d2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxModel",
      "state": {
       "description": "Enable AWQ",
       "disabled": false,
       "layout": "IPY_MODEL_fd16b9e5dfcb4b36afd13ccdc914aaf5",
       "style": "IPY_MODEL_dfe1cd70e50545e5ad78fa78c7f1141c",
       "value": false
      }
     },
     "90b07973cf0e41cc8319ed0ae6dd437c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "94a64db8da184ee7a0ed1c7f5d7dae6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "INT4"
       ],
       "description": "Model to run:",
       "index": 0,
       "layout": "IPY_MODEL_32a9fb61ac2b44c993d6c6c1e9529d6a",
       "style": "IPY_MODEL_5160a67343cf4288bdd84eda3b7635a2"
      }
     },
     "af7e0ac65182473ab1adacdbbcda9624": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bca58aaf2cce4004bbf9961a5bc347dc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "bde2dd437b6746718707ef9e219bf841": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "dfe1cd70e50545e5ad78fa78c7f1141c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "e4441ff41ad64f2e9c3a1ea7b01e663c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "f6555b7e3f1d490fa0fcae19fffb52cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fd16971fa27c44c99f4bac159e2b54c4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "fd16b9e5dfcb4b36afd13ccdc914aaf5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
