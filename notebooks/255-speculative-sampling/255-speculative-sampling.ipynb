{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2ed242-3561-464c-8d1c-cc3862e23702",
   "metadata": {},
   "source": [
    "# Text Generation via Speculative Sampling, KV Caching, and OpenVINO™\n",
    "\n",
    "As model sizes grow, Generative AI implementations require significant inference resources. This not only increases the cost per generation from a prompt, but also increases the power consumption used to serve such requests.\n",
    "\n",
    "Inference optimizations for text generation are essential for reducing costs and power consumption. When optimizing the inference process, the amount of time and energy required to generate text can be significantly reduced. This can lead to cost savings in terms of hardware and software, as well as reduced power consumption. Additionally, inference optimizations can help improve the accuracy of text generation as well as the speed at which it can be generated. This can lead to an improved user experience and increased efficiency in text-generation tasks. In summary, inference optimizations for text generation are essential to reduce costs and power consumption, while also improving the accuracy and speed of text generation.\n",
    "\n",
    "Another necessary condition is that the optimizations are compatible with each other. That is, implementing a certain optimization should not preclude other optimizations. There are several levels of optimizations that can provide significant speedup without \"bumping into each other\" in a way that will compromise overall efficiency.\n",
    "\n",
    "For details on this method, please refer to the DeepMind paper by Chen et al, http://arxiv.org/abs/2302.01318"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c435a",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "### Table of content:\n",
    "\n",
    "- [Prerequisites](#1)\n",
    "    - [Select inference device](#2)\n",
    "- [Download and Convert Model](#3)\n",
    "- [Create autoregressive and speculative forms of sampling with KV Cache support](#4)\n",
    "    - [Setup imports](#5)\n",
    "    - [Prepare autoregressive sampling](#6)\n",
    "    - [Prepare speculative sampling](#7)\n",
    "    - [Main generation function](#8)\n",
    "    - [Download and convert model](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa16b1-d2f6-4a3a-abfb-5ec278133c80",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Prerequisites [&#8657;](#0)\n",
    "\n",
    "First, we must install the [Hugging Face Optimum](https://huggingface.co/docs/optimum/installation) library accelerated by OpenVINO integration.\n",
    "The Hugging Face Optimum Intel API is a high-level API that enables us to convert and quantize models from the Hugging Face Transformers library to the OpenVINO™ IR format. For more details, refer to the [Hugging Face Optimum Intel documentation](https://huggingface.co/docs/optimum/intel/inference).\n",
    "\n",
    "We will also need to install transformers (HuggingFace) and some other useful modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4421fc85-bed6-4a62-b8fa-19c7ba474891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m    WARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "dalle2-pytorch 1.10.5 requires einops>=0.4, but you have einops 0.3.2 which is incompatible.\n",
      "dalle2-pytorch 1.10.5 requires webdataset>=0.2.5, but you have webdataset 0.1.62 which is incompatible.\n",
      "detectron2 0.6 requires pycocotools>=2.0.2, but you have pycocotools 2.0 which is incompatible.\n",
      "pyannote-audio 2.0.1 requires torchaudio<1.0,>=0.10, but you have torchaudio 2.0.2+cpu which is incompatible.\n",
      "pytorch-lightning 1.6.5 requires protobuf<=3.20.1, but you have protobuf 3.20.3 which is incompatible.\n",
      "torchrec-nightly 2022.12.29 requires iopath>=0.1.10, but you have iopath 0.1.9 which is incompatible.\n",
      "torchrec-nightly 2022.12.29 requires ninja>=1.11.1, but you have ninja 1.10.2.4 which is incompatible.\n",
      "torchrec-nightly 2022.12.29 requires pyparsing>=3.0.9, but you have pyparsing 2.4.7 which is incompatible.\n",
      "wenetruntime 1.13.0 requires torch==1.13.0, but you have torch 2.0.1+cpu which is incompatible.\n",
      "ont-bonito 0.7.0 requires numpy~=1.24.2, but you have numpy 1.23.5 which is incompatible.\n",
      "ont-bonito 0.7.0 requires requests~=2.28.2, but you have requests 2.28.1 which is incompatible.\n",
      "effdet 0.4.0 requires pycocotools>=2.0.2, but you have pycocotools 2.0 which is incompatible.\n",
      "seamless-communication 1.0.0 requires fairseq2==0.1.0, but you have fairseq2 0.2.0+devel which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -orch (/home/ea/work/ov_venv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: pytorch-lightning 1.6.5 has a non-standard dependency specifier torch>=1.8.*. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q --upgrade transformers torch gradio openvino accelerate onnx onnxruntime ipywidgets\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f84f8-33e8-4ad6-bd40-e6fd41d2d703",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "### Select inference device [&#8657;](#0)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddd57de-9f41-403c-bccc-8d3118654a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49827d79d96c42ba8ab269a400c2bf93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GNA', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='CPU',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9c4a5-ef75-4076-9f1c-f45a2259ec46",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## Create autoregressive and speculative forms of sampling with KV Cache support [&#8657;](#0)\n",
    " \n",
    " blah, blah, blah\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5da4d-d2fd-440b-b204-7fbc6966dd1f",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "### Setup imports [&#8657;](#0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f976094-8603-42c4-8f18-a32ba6d7192e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n",
      "2023-09-26 10:09:55.700603: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-09-26 10:09:55.876688: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-26 10:09:56.870713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ea/work/ov_venv/lib/python3.8/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from threading import Thread\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58611d6-0a91-4efd-976e-4221acbb43cd",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "### Prepare autoregressive sampling [&#8657;](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ac10a5-3141-4227-8f0b-0617acd027c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_fn(x):\n",
    "    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    return x_max / torch.sum(x_max)\n",
    "\n",
    "\n",
    "# TODO: delete this before publication - it's only for reference\n",
    "def autoregressive_sampling(x, model, N):\n",
    "    n = len(x)\n",
    "    T = n + N\n",
    "\n",
    "    while n < T:\n",
    "        model_out = torch.softmax(model(x, attention_mask=torch.ones(x.size(), dtype=torch.long)).logits, dim=2)\n",
    "        x = torch.cat((x, torch.reshape(torch.argmax(model_out[-1][-1]), (1, 1))), dim=1)\n",
    "        n += 1\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def autoregressive_sampling_with_pkv(x, model, N):\n",
    "    n = len(x)\n",
    "    T = n + N\n",
    "    input = x\n",
    "    past_kv = None\n",
    "\n",
    "    while n < T:\n",
    "        res = model(input, attention_mask=torch.ones(input.size(), dtype=torch.long), past_key_values=past_kv)\n",
    "        model_out = torch.softmax(res.logits, dim=2)\n",
    "        past_kv = res.past_key_values\n",
    "        next_token = torch.reshape(torch.argmax(model_out[-1][-1]), (1, 1))\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "        n += 1\n",
    "        input = next_token\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a01739-1363-42ef-927f-6a340bdbe7ba",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "### Prepare speculative sampling [&#8657;](#0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "524e72f4-8750-48ff-b002-e558d03b3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: delete this before publication - it's only for reference\n",
    "def speculative_sampling(x, draft_model, target_model, N, K):\n",
    "    n = x.size(1)\n",
    "    T = n + N\n",
    "\n",
    "    while n < T:\n",
    "        # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "        x_draft = x\n",
    "        for _ in range(K):\n",
    "            p = torch.softmax(draft_model(x_draft, attention_mask=torch.ones(x_draft.size(), dtype=torch.long)).logits, dim=2)\n",
    "            x_draft = torch.cat((x_draft, torch.reshape(torch.argmax(p[-1][-1]), (1, 1))), dim=1)\n",
    "\n",
    "        # Step 2: target model forward passes on x_draft\n",
    "        q = torch.softmax(target_model(x_draft, attention_mask=torch.ones(x_draft.size(), dtype=torch.long)).logits, dim=2)\n",
    "\n",
    "        # Step 3: append draft tokens based on rejection criterion and resample\n",
    "        # a token on rejection\n",
    "        all_accepted = True\n",
    "        for _ in range(K):\n",
    "            i = n - 1\n",
    "            j = x_draft[-1][i + 1].item()\n",
    "\n",
    "            q_item = q[-1][i][j].detach().numpy()\n",
    "            p_item = p[-1][i][j].detach().numpy()\n",
    "\n",
    "            if np.random.random() < min(1, np.abs(q_item / p_item)):  # accepted\n",
    "                x = torch.cat((x, torch.tensor(j).reshape(1, 1)), dim=1)\n",
    "                n += 1\n",
    "            else:  # rejected\n",
    "                q_p = max_fn(q[0][i] - p[0][i])\n",
    "                # softmax isn't working here - q and p were reduced to arrays of numbers\n",
    "                resampled_output = torch.argmax(q_p)      \n",
    "                resampled_output = torch.reshape(resampled_output, (1, 1))\n",
    "                x = torch.cat((x, resampled_output), dim=1)\n",
    "                n += 1\n",
    "                all_accepted = False\n",
    "                break\n",
    "            \n",
    "        # Step 4: if all draft tokens were accepted, sample a final token\n",
    "        if all_accepted:\n",
    "            x = torch.cat((x, torch.reshape(torch.argmax(q[-1][-1]), (1, 1))), dim=1)\n",
    "            n += 1\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def speculative_sampling_with_pkv(x, draft_model, target_model, N, K):\n",
    "    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so\n",
    "    # we have to add an extra -1 term when indexing using n, T, or t\n",
    "    n = x.size(1)\n",
    "    T = n + N\n",
    "    target_past_kv = None\n",
    "    while n < T:\n",
    "        # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "        x_draft = None\n",
    "        draft_past_kv = None\n",
    "        x_draft_input = x\n",
    "        p_cum = None\n",
    "        q_cum = None\n",
    "        for _ in range(K):\n",
    "            past_len = 0 if draft_past_kv is None else draft_past_kv[0][0].shape[2]\n",
    "            res_draft = draft_model(x_draft_input, attention_mask=torch.ones((1, x_draft_input.shape[1] + past_len), dtype=torch.long), past_key_values=draft_past_kv, use_cache=True)\n",
    "            p = res_draft.logits\n",
    "            p = torch.softmax(p, dim=2)\n",
    "            draft_past_kv = res_draft.past_key_values\n",
    "            next_token = torch.reshape(torch.argmax(p[-1][-1]), (1, 1))\n",
    "            x_draft_input = next_token\n",
    "            if p_cum is None:\n",
    "                p_cum = p[:, -1].unsqueeze(1)\n",
    "                x_draft = next_token\n",
    "            else:\n",
    "                p_cum = torch.cat((p_cum, p), dim=1)\n",
    "                x_draft = torch.cat((x_draft, next_token), dim=1)\n",
    "        # Step 2: target model forward passes on x_draft\n",
    "        if target_past_kv is None:\n",
    "            x_draft_target_input = torch.cat((x, x_draft), dim=1)\n",
    "        else:\n",
    "            x_draft_target_input = x_draft\n",
    "        # in terms of performance - so both should be tested.\n",
    "        past_len = 0 if target_past_kv is None else target_past_kv[0][0].shape[2]\n",
    "        res = target_model(x_draft_target_input, attention_mask=torch.zeros((1, x_draft_target_input.shape[1] + past_len), dtype=torch.long), past_key_values=target_past_kv, use_cache=True)\n",
    "        q = res.logits\n",
    "        target_new_past_kv = res.past_key_values\n",
    "        if q_cum is None:\n",
    "            q_cum = q\n",
    "        else:\n",
    "            q_cum = torch.cat((q_cum, q), dim=1)\n",
    "        # Step 3: append draft tokens based on rejection criterion and resample\n",
    "        # a token on rejection\n",
    "        all_accepted = True\n",
    "        for k in range(K):\n",
    "            #i = n - 1\n",
    "            j = x_draft[0][k].item()\n",
    "            s_q = torch.softmax(q, dim=2)\n",
    "            s_p_cum = torch.softmax(p_cum, dim=2)\n",
    "            q_item = s_q[-1][k][j].detach().numpy()\n",
    "            p_item = s_p_cum[-1][k][j].detach().numpy()\n",
    "\n",
    "            if np.random.random() < min(1, (q_item / p_item)):  # accepted\n",
    "                x = torch.cat((x, torch.tensor(j).reshape(1,1)), dim=1)\n",
    "                n += 1\n",
    "            else:  # rejected\n",
    "                q_p = max_fn(q[0][k] - p_cum[0][k])\n",
    "                resampled_output = torch.argmax(q_p)      \n",
    "                resampled_output = torch.reshape(resampled_output, (1,1))\n",
    "                x = torch.cat((x, resampled_output), dim=1)\n",
    "                n += 1\n",
    "                all_accepted = False\n",
    "                break\n",
    "            \n",
    "        # Step 4: if all draft tokens were accepted, sample a final token\n",
    "        if all_accepted:\n",
    "            x = torch.cat((x, torch.reshape(torch.argmax(q[-1][-1]), (1,1))), dim=1)\n",
    "            n += 1\n",
    "            target_past_kv = target_new_past_kv\n",
    "        else:\n",
    "            target_past_kv = None\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583202d2-6d29-4729-af2e-232d3ee0bc2c",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## Main generation function [&#8657;](#0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6993840-e8b2-4c26-8da4-e1b046c3f3cc",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "### Download and Convert Model [&#8657;](#0)\n",
    "\n",
    "**TODO: I have not changed any of this yet and customized it for speculative sampling - but we should have an optimized model**\n",
    "\n",
    "Optimum Intel can be used to load optimized models from the [Hugging Face Hub](https://huggingface.co/docs/optimum/intel/hf.co/models) and create pipelines to run an inference with OpenVINO Runtime using Hugging Face APIs. The Optimum Inference models are API compatible with Hugging Face Transformers models.  This means we just need to replace `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.\n",
    "\n",
    "Below is an example of the Dolly model\n",
    "\n",
    "```diff\n",
    "-from transformers import AutoModelForCausalLM\n",
    "+from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "-model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "+model = OVModelForCausalLM.from_pretrained(model_id, from_transformers=True)\n",
    "```\n",
    "\n",
    "Model class initialization starts with calling `from_pretrained` method. When downloading and converting Transformers model, the parameter `from_transformers=True` should be added. We can save the converted model for the next usage with the `save_pretrained` method.\n",
    "Tokenizer class and pipelines API are compatible with Optimum models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67fb4f9d-5877-48d8-8eff-c30ff6974d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from openvino.runtime import Type, Tensor\n",
    "from typing import Optional, Tuple\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "class OVModelForCausalLMWithMultiTokenPKV(OVModelForCausalLM):\n",
    "       def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        self.compile()\n",
    "\n",
    "        inputs = {}\n",
    "        if past_key_values is not None:\n",
    "            if self._pkv_precision == Type.bf16:\n",
    "                # numpy does not support bf16, pretending f16, should change to bf16\n",
    "                past_key_values = tuple(\n",
    "                    Tensor(past_key_value, past_key_value.shape, Type.bf16)\n",
    "                    for pkv_per_layer in past_key_values\n",
    "                    for past_key_value in pkv_per_layer\n",
    "                )\n",
    "            else:\n",
    "                # Flatten the past_key_values\n",
    "                past_key_values = tuple(\n",
    "                    past_key_value for pkv_per_layer in past_key_values for past_key_value in pkv_per_layer\n",
    "                )\n",
    "            # Add the past_key_values to the decoder inputs\n",
    "            inputs = dict(zip(self.key_value_input_names, past_key_values))\n",
    "\n",
    "        # Create empty past_key_values for decoder_with_past first generation step\n",
    "        elif self.use_cache:\n",
    "            shape_input_ids = input_ids.shape\n",
    "            num_attention_heads = (\n",
    "                self.normalized_config.num_attention_heads if self.config.model_type == \"bloom\" else 1\n",
    "            )\n",
    "            for input_name in self.key_value_input_names:\n",
    "                model_inputs = self.model.input(input_name)\n",
    "                shape = model_inputs.get_partial_shape()\n",
    "                shape[0] = shape_input_ids[0] * num_attention_heads\n",
    "                if shape[2].is_dynamic:\n",
    "                    shape[2] = 0\n",
    "                if shape[1].is_dynamic:\n",
    "                    shape[1] = 0\n",
    "                inputs[input_name] = Tensor(model_inputs.get_element_type(), shape.get_shape())\n",
    "\n",
    "        inputs[\"input_ids\"] = np.array(input_ids)\n",
    "\n",
    "        # Add the attention_mask inputs when needed\n",
    "        if \"attention_mask\" in self.input_names and attention_mask is not None:\n",
    "            inputs[\"attention_mask\"] = np.array(attention_mask)\n",
    "\n",
    "        # Run inference\n",
    "        self.request.start_async(inputs, shared_memory=True)\n",
    "        self.request.wait()\n",
    "\n",
    "        logits = torch.from_numpy(self.request.get_tensor(\"logits\").data).to(self.device)\n",
    "\n",
    "        if self.use_cache:\n",
    "            # Tuple of length equal to : number of layer * number of past_key_value per decoder layer (2 corresponds to the self-attention layer)\n",
    "            past_key_values = tuple(self.request.get_tensor(key).data for key in self.key_value_output_names)\n",
    "            # Tuple of tuple of length `n_layers`, with each tuple of length equal to 2 (k/v of self-attention)\n",
    "            past_key_values = tuple(\n",
    "                past_key_values[i : i + self.num_pkv] for i in range(0, len(past_key_values), self.num_pkv)\n",
    "            )\n",
    "        else:\n",
    "            past_key_values = None\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=past_key_values)\n",
    "\n",
    "\n",
    "def main(\n",
    "    prompt: str = \"Alan Turing theorized that computers would one day become\",\n",
    "    n_tokens_to_generate: int = 40,\n",
    "    K: int = 5,\n",
    "    seed: int = 5555,\n",
    "):\n",
    "    # Consider a model selector here\n",
    "    #draft_model_id = \"databricks/dolly-v2-3b\"\n",
    "    #draft_model_path = Path(\"dolly-v2-3b\")\n",
    "    #target_model_id = \"databricks/dolly-v2-12b\"\n",
    "    #target_model_path = Path(\"dolly-v2-12b\")\n",
    "    ##facebook/opt-6.7b could be more interesting target model than facebook/opt-1.3b\n",
    "    # draft_model_id = \"facebook/opt-125m\"\n",
    "    # draft_model_path = Path(\"facebook/opt-125m-local\")\n",
    "    # target_model_id = \"facebook/opt-1.3b\"\n",
    "    # target_model_path = Path(\"facebook/opt-1.3b-local\")\n",
    "    draft_model_id = \"gpt2\"\n",
    "    draft_model_path = Path(\"gpt2-local\")\n",
    "    target_model_id = \"gpt2-xl\"\n",
    "    target_model_path = Path(\"gpt2-xl-local\")\n",
    "\n",
    "    target_tokenizer = AutoTokenizer.from_pretrained(target_model_id)\n",
    "\n",
    "    current_device = device.value\n",
    "\n",
    "    # Save local copies for subsequent runs\n",
    "    if draft_model_path.exists():\n",
    "        draft_ov_model = OVModelForCausalLM.from_pretrained(draft_model_path, device=current_device)\n",
    "    else:\n",
    "        draft_ov_model = OVModelForCausalLM.from_pretrained(draft_model_id, device=current_device, from_transformers=True)\n",
    "        draft_ov_model.save_pretrained(draft_model_path)\n",
    "    if target_model_path.exists():\n",
    "        target_ov_model = OVModelForCausalLM.from_pretrained(target_model_path, device=current_device)\n",
    "    else:\n",
    "        target_ov_model = OVModelForCausalLM.from_pretrained(target_model_id, device=current_device, from_transformers=True)\n",
    "        target_ov_model.save_pretrained(target_model_path)\n",
    "    \n",
    "    # seed numpy rng\n",
    "    np.random.seed(seed)    \n",
    "\n",
    "    input_ids = target_tokenizer(prompt, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "    def run_autoregressive_sampling_fn(decode_fn, input_ids, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        output_ids = decode_fn(x=input_ids, **kwargs)\n",
    "        text = target_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        return text, elapsed_time\n",
    "\n",
    "    def run_speculative_sampling_fn(decode_fn, input_ids, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        output_ids = decode_fn(x=input_ids, **kwargs)\n",
    "        text = target_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        return text, elapsed_time\n",
    "\n",
    "    autoregressive_text, autoregressive_time = run_autoregressive_sampling_fn(\n",
    "        autoregressive_sampling_with_pkv,\n",
    "        input_ids,\n",
    "        model=target_ov_model,\n",
    "        N=n_tokens_to_generate,\n",
    "    )\n",
    "    if target_model_path.exists():\n",
    "        target_ov_model = OVModelForCausalLMWithMultiTokenPKV.from_pretrained(target_model_path, device=current_device)\n",
    "    else:\n",
    "        target_ov_model = OVModelForCausalLMWithMultiTokenPKV.from_pretrained(target_model_id, device=current_device, from_transformers=True)\n",
    "        target_ov_model.save_pretrained(target_model_path)\n",
    "\n",
    "    speculative_text, speculative_time = run_speculative_sampling_fn(\n",
    "        speculative_sampling_with_pkv,\n",
    "        input_ids,\n",
    "        target_model=target_ov_model,\n",
    "        draft_model=draft_ov_model,\n",
    "        N=n_tokens_to_generate,\n",
    "        K=K,\n",
    "    )\n",
    "\n",
    "#   Print results for comparison of text and time\n",
    "    print()\n",
    "    print(\"Autoregressive Decode\")\n",
    "    print(\"---------------------\")\n",
    "    print(f\"Time = {autoregressive_time:.2f}s\")\n",
    "    print(f\"Text = {autoregressive_text}\")\n",
    "    print()\n",
    "    print(\"Speculative Decode\")\n",
    "    print(\"------------------\")\n",
    "    print(f\"Time = {speculative_time:.2f}s\")\n",
    "    print(f\"Text = {speculative_text}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Speculative Sampling Demo\n",
    "        ## The output will show a comparison of Autoregressive Sampling vs Speculative Sampling\n",
    "        - Target Model: gpt2-xl\n",
    "        - Draft Model: gpt2\n",
    "        - K = 5\n",
    "        > Some improvements can be made to acceptance criterion and adjusting temperature to improve text quality.\n",
    "        \"\"\")\n",
    "        with gr.Row():\n",
    "            inp = gr.Textbox(placeholder=\"THIS CANNOT BE EMPTY\", label=\"Input Prompt\")\n",
    "            out = gr.Textbox(label=\"Output\")\n",
    "        btn = gr.Button(\"Run\")\n",
    "        btn.click(fn=main, inputs=inp, outputs=out)\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19d22b0e-d671-4186-a5cf-d0457ccf7844",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "Set CACHE_DIR to gpt2-local/model_cache\n",
      "Compiling the model...\n",
      "Set CACHE_DIR to gpt2-xl-local/model_cache\n",
      "Compiling the model...\n",
      "Set CACHE_DIR to gpt2-xl-local/model_cache\n",
      "/tmp/ipykernel_1474833/3649586604.py:56: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25, 15, 64)\n",
      "(1, 25, 17, 64)\n",
      "(1, 25, 18, 64)\n",
      "(1, 25, 23, 64)\n",
      "(1, 25, 28, 64)\n",
      "(1, 25, 31, 64)\n",
      "(1, 25, 32, 64)\n",
      "(1, 25, 34, 64)\n",
      "(1, 25, 35, 64)\n",
      "(1, 25, 40, 64)\n",
      "(1, 25, 45, 64)\n",
      "(1, 25, 50, 64)\n",
      "\n",
      "Autoregressive Decode\n",
      "---------------------\n",
      "Time = 5.84s\n",
      "Text = Alan Turing theorized that computers would one day become so powerful that they would be able to think like humans.\n",
      "\n",
      "In the 1950s, he proposed a way to build a computer that could think like a human. He called it the \"T\n",
      "\n",
      "Speculative Decode\n",
      "------------------\n",
      "Time = 3.35s\n",
      "Text = Alan Turing theorized that computers would one day become the Turing, the Turing machine.\n",
      "\n",
      "The Turing machine is a theic, a Turing, a Turing machine.\n",
      " the Turing machine is a the aic, a Turing, the Turing machine.\n",
      "\n",
      " the\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e43e6-0823-4a50-9b34-0b30a861b34d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1143ff7dbf8e43f7a7991151056a6055": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49827d79d96c42ba8ab269a400c2bf93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "GNA",
        "AUTO"
       ],
       "description": "Device:",
       "index": 0,
       "layout": "IPY_MODEL_1143ff7dbf8e43f7a7991151056a6055",
       "style": "IPY_MODEL_e4e03c0919bf4b9ebd12edc8c8d16ae0"
      }
     },
     "e4e03c0919bf4b9ebd12edc8c8d16ae0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
