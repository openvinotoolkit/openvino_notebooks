{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2ed242-3561-464c-8d1c-cc3862e23702",
   "metadata": {},
   "source": [
    "# Text Generation via Speculative Sampling, KV Caching, and OpenVINO™\n",
    "\n",
    "As model sizes grow, Generative AI implementations require significant inference resources. This not only increases the cost per generation from a prompt, but also increases the power consumption used to serve such requests.\n",
    "\n",
    "Inference optimizations for text generation are essential for reducing costs and power consumption. When optimizing the inference process, the amount of time and energy required to generate text can be significantly reduced. This can lead to cost savings in terms of hardware and software, as well as reduced power consumption. Additionally, inference optimizations can help improve the accuracy of text generation as well as the speed at which it can be generated. This can lead to an improved user experience and increased efficiency in text-generation tasks. In summary, inference optimizations for text generation are essential to reduce costs and power consumption, while also improving the accuracy and speed of text generation.\n",
    "\n",
    "Another necessary condition is that the optimizations are compatible with each other. That is, implementing a certain optimization should not preclude other optimizations. There are several levels of optimizations that can provide significant speedup without \"bumping into each other\" in a way that will compromise overall efficiency.\n",
    "\n",
    "**TODO: point to openvino.ai blog article**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c435a",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "### Table of content:\n",
    "\n",
    "**TODO: Fix labels and anchors for this notebook**\n",
    "\n",
    "- [Prerequisites](#1)\n",
    "    - [Select inference device](#2)\n",
    "- [Download and Convert Model](#3)\n",
    "- [Create an instruction-following inference pipeline](#4)\n",
    "    - [Setup imports](#5)\n",
    "    - [Prepare template for user prompt](#6)\n",
    "    - [Helpers for output parsing](#7)\n",
    "    - [Main generation function](#8)\n",
    "    - [Helpers for application](#9)\n",
    "- [Run instruction-following pipeline](#10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa16b1-d2f6-4a3a-abfb-5ec278133c80",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Prerequisites [&#8657;](#0)\n",
    "\n",
    "First, we should install the [Hugging Face Optimum](https://huggingface.co/docs/optimum/installation) library accelerated by OpenVINO integration.\n",
    "The Hugging Face Optimum Intel API is a high-level API that enables us to convert and quantize models from the Hugging Face Transformers library to the OpenVINO™ IR format. For more details, refer to the [Hugging Face Optimum Intel documentation](https://huggingface.co/docs/optimum/intel/inference).\n",
    "\n",
    "We will also need to install transformers (HuggingFace) and some other useful modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4421fc85-bed6-4a62-b8fa-19c7ba474891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (23.2.1)\n",
      "Requirement already satisfied: transformers in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (4.33.2)\n",
      "Requirement already satisfied: torch in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (2.0.1)\n",
      "Requirement already satisfied: gradio in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (3.44.4)\n",
      "Requirement already satisfied: optimum-intel in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (1.11.0)\n",
      "Requirement already satisfied: openvino in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (2023.1.0)\n",
      "Requirement already satisfied: accelerate in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (0.23.0)\n",
      "Requirement already satisfied: onnx in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (1.14.1)\n",
      "Requirement already satisfied: onnxruntime in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (1.16.0)\n",
      "Requirement already satisfied: ipywidgets in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (8.1.1)\n",
      "Requirement already satisfied: fire in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (0.5.0)\n",
      "Requirement already satisfied: filelock in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (2.29.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from torch) (2.8.4)\n",
      "Requirement already satisfied: jinja2 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from torch) (2.11.3)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (23.1.0)\n",
      "Requirement already satisfied: altair<6.0,>=4.2.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (4.2.2)\n",
      "Requirement already satisfied: fastapi in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (0.95.1)\n",
      "Requirement already satisfied: ffmpy in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (0.3.0)\n",
      "Requirement already satisfied: gradio-client==0.5.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (0.5.1)\n",
      "Requirement already satisfied: httpx in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (0.24.0)\n",
      "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (6.0.1)\n",
      "Requirement already satisfied: markupsafe~=2.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (2.0.1)\n",
      "Requirement already satisfied: matplotlib~=3.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (3.6.2)\n",
      "Requirement already satisfied: orjson~=3.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (3.8.10)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (1.4.4)\n",
      "Requirement already satisfied: pillow<11.0,>=8.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (9.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,<3.0.0,>=1.7.4 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (1.10.2)\n",
      "Requirement already satisfied: pydub in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (0.0.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (0.21.1)\n",
      "Requirement already satisfied: websockets<12.0,>=10.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio) (11.0.1)\n",
      "Requirement already satisfied: fsspec in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from gradio-client==0.5.1->gradio) (2022.11.0)\n",
      "Requirement already satisfied: optimum>=1.13.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from optimum-intel) (1.13.1)\n",
      "Requirement already satisfied: datasets>=1.4.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from optimum-intel) (2.12.0)\n",
      "Requirement already satisfied: sentencepiece in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from optimum-intel) (0.1.98)\n",
      "Requirement already satisfied: scipy in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from optimum-intel) (1.10.0)\n",
      "Requirement already satisfied: openvino-telemetry>=2023.1.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from openvino) (2023.1.1)\n",
      "Requirement already satisfied: psutil in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from onnx) (3.20.3)\n",
      "Requirement already satisfied: coloredlogs in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from onnxruntime) (2.0.7)\n",
      "Requirement already satisfied: comm>=0.1.3 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipywidgets) (0.1.4)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipywidgets) (7.31.1)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipywidgets) (5.7.1)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.9 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipywidgets) (4.0.9)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.9 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipywidgets) (3.0.9)\n",
      "Requirement already satisfied: six in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from fire) (1.16.0)\n",
      "Requirement already satisfied: termcolor in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from fire) (2.2.0)\n",
      "Requirement already satisfied: entrypoints in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.16.0)\n",
      "Requirement already satisfied: toolz in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel) (12.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel) (0.3.6)\n",
      "Requirement already satisfied: xxhash in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel) (3.8.3)\n",
      "Requirement already satisfied: responses<0.19 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from datasets>=1.4.0->optimum-intel) (0.18.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (65.6.3)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: decorator in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.36)\n",
      "Requirement already satisfied: pygments in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.0)\n",
      "Requirement already satisfied: backcall in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2022.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: click>=7.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (8.0.4)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from coloredlogs->onnxruntime) (10.0)\n",
      "Requirement already satisfied: starlette<0.27.0,>=0.26.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from fastapi->gradio) (0.26.1)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from httpx->gradio) (0.17.0)\n",
      "Requirement already satisfied: sniffio in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from httpx->gradio) (1.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages/mpmath-1.2.1-py3.10.egg (from sympy->torch) (1.2.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel) (22.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from aiohttp->datasets>=1.4.0->optimum-intel) (1.2.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx->gradio) (3.5.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.18.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/barad/opt/anaconda3/lib/python3.10/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install --upgrade transformers torch gradio optimum-intel openvino accelerate onnx onnxruntime ipywidgets fire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f84f8-33e8-4ad6-bd40-e6fd41d2d703",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "### Select inference device [&#8657;](#0)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddd57de-9f41-403c-bccc-8d3118654a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device\n",
    "\n",
    "# DEBUG - force device to be CPU in any case\n",
    "device.value = 'CPU'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9c4a5-ef75-4076-9f1c-f45a2259ec46",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## Create autoregressive and speculative forms of sampling with KV Cache support [&#8657;](#0)\n",
    " \n",
    " blah, blah, blah\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5da4d-d2fd-440b-b204-7fbc6966dd1f",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "### Setup imports [&#8657;](#0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f976094-8603-42c4-8f18-a32ba6d7192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import gradio as gr\n",
    "\n",
    "from threading import Thread\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58611d6-0a91-4efd-976e-4221acbb43cd",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "### Prepare autoregressive sampling [&#8657;](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ac10a5-3141-4227-8f0b-0617acd027c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_fn(x):\n",
    "    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    return x_max / torch.sum(x_max)\n",
    "\n",
    "\n",
    "def autoregressive_sampling(x, model, N):\n",
    "    n = len(x)\n",
    "    T = n + N\n",
    "\n",
    "    while n < T:\n",
    "        #model_out = torch.softmax(model(x).logits, dim=2)\n",
    "        model_out = model(x, attention_mask=torch.ones(x.size()))\n",
    "        model_out = model_out.logits\n",
    "        model_out = torch.softmax(model_out, dim=2)\n",
    "        x = torch.cat((x, torch.reshape(torch.argmax(model_out[-1][-1]), (1, 1))), dim=1)\n",
    "        n += 1\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def autoregressive_sampling_with_pkv(x, model, N):\n",
    "    n = len(x)\n",
    "    T = n + N\n",
    "    input = x\n",
    "    past_kv = None\n",
    "\n",
    "    while n < T:\n",
    "        res = model(input, attention_mask=torch.ones(input.size()), past_key_values=past_kv)\n",
    "        model_out = torch.softmax(res.logits, dim=2)\n",
    "        past_kv = res.past_key_values\n",
    "        next_token = torch.reshape(torch.argmax(model_out[-1][-1]), (1, 1))\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "        n += 1\n",
    "        input = next_token\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a01739-1363-42ef-927f-6a340bdbe7ba",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "### Prepare speculative sampling [&#8657;](#0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524e72f4-8750-48ff-b002-e558d03b3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_sampling(x, draft_model, target_model, N, K):\n",
    "    n = x.size(1)\n",
    "    T = n + N\n",
    "\n",
    "    while n < T:\n",
    "        # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "        x_draft = x\n",
    "        for _ in range(K):\n",
    "            p = torch.softmax(draft_model(x_draft).logits, dim=2)\n",
    "            x_draft = torch.cat((x_draft, torch.reshape(torch.argmax(p[-1][-1]), (1, 1))), dim=1)\n",
    "\n",
    "        # Step 2: target model forward passes on x_draft\n",
    "        q = torch.softmax(target_model(x_draft).logits, dim=2)\n",
    "\n",
    "        # Step 3: append draft tokens based on rejection criterion and resample\n",
    "        # a token on rejection\n",
    "        all_accepted = True\n",
    "        for _ in range(K):\n",
    "            i = n - 1\n",
    "            j = x_draft[-1][i + 1].item()\n",
    "\n",
    "            q_item = q[-1][i][j].detach().numpy()\n",
    "            p_item = p[-1][i][j].detach().numpy()\n",
    "\n",
    "            if np.random.random() < min(1, (q_item / p_item)):  # accepted\n",
    "                x = torch.cat((x, torch.tensor(j).reshape(1, 1)), dim=1)\n",
    "                n += 1\n",
    "            else:  # rejected\n",
    "                q_p = max_fn(q[0][i] - p[0][i])\n",
    "                # softmax isn't working here - q and p were reduced to arrays of numbers\n",
    "                resampled_output = torch.argmax(q_p)      \n",
    "                resampled_output = torch.reshape(resampled_output, (1, 1))\n",
    "                x = torch.cat((x, resampled_output), dim=1)\n",
    "                n += 1\n",
    "                all_accepted = False\n",
    "                break\n",
    "            \n",
    "        # Step 4: if all draft tokens were accepted, sample a final token\n",
    "        if all_accepted:\n",
    "            x = torch.cat((x, torch.reshape(torch.argmax(q[-1][-1]), (1, 1))), dim=1)\n",
    "            n += 1\n",
    "\n",
    "        # just keeping my sanity\n",
    "        assert n == len(x.detach().numpy()[0]), f\"{n} {len(x.detach().numpy()[0])}\"\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def speculative_sampling_with_pkv(x, draft_model, target_model, N, K):\n",
    "    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so\n",
    "    # we have to add an extra -1 term when indexing using n, T, or t\n",
    "    n = x.size(1)\n",
    "    T = n + N\n",
    "    target_past_kv = None\n",
    "    while n < T:\n",
    "        # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "        x_draft = None\n",
    "        draft_past_kv = None\n",
    "        x_draft_input = x\n",
    "        p_cum = None\n",
    "        q_cum = None\n",
    "        for _ in range(K):\n",
    "            res_draft = draft_model(x_draft_input, attention_mask=torch.ones(x_draft_input.size()), past_key_values=draft_past_kv, use_cache=True)\n",
    "            p = res_draft.logits\n",
    "            p = torch.softmax(p, dim=2)\n",
    "            draft_past_kv = res_draft.past_key_values\n",
    "            next_token = torch.reshape(torch.argmax(p[-1][-1]), (1, 1))\n",
    "            x_draft_input = next_token\n",
    "            if p_cum is None:\n",
    "                p_cum = p[:, -1].unsqueeze(1)\n",
    "                x_draft = next_token\n",
    "            else:\n",
    "                p_cum = torch.cat((p_cum, p), dim=1)\n",
    "                x_draft = torch.cat((x_draft, next_token), dim=1)\n",
    "        # Step 2: target model forward passes on x_draft\n",
    "        if target_past_kv is None:\n",
    "            x_draft_target_input = torch.cat((x, x_draft), dim=1)\n",
    "        else:\n",
    "            x_draft_target_input = x_draft\n",
    "            #res = target_model(x_draft_target_input, attention_mask=torch.ones(x_draft_target_input.size()), past_key_values=target_past_kv, use_cache=True)\n",
    "        res = target_model(x_draft_target_input, attention_mask=torch.ones(x_draft_target_input.size()), use_cache=False)\n",
    "        #q = torch.softmax(res.logits, dim=2)    \"does this work and is it better?\"\n",
    "        q = res.logits\n",
    "        target_new_past_kv = res.past_key_values\n",
    "        if q_cum is None:\n",
    "            q_cum = q\n",
    "        else:\n",
    "            q_cum = torch.cat((q_cum, q), dim=1)\n",
    "        # Step 3: append draft tokens based on rejection criterion and resample\n",
    "        # a token on rejection\n",
    "        all_accepted = True\n",
    "        for k in range(K):\n",
    "            #i = n - 1\n",
    "            j = x_draft[0][k].item()\n",
    "\n",
    "            q_item = q[-1][k][j].detach().numpy()\n",
    "            p_item = p_cum[-1][k][j].detach().numpy()\n",
    "\n",
    "            if np.random.random() < min(1, (q_item / p_item)):  # accepted\n",
    "                x = torch.cat((x, torch.tensor(j).reshape(1,1)), dim=1)\n",
    "                n += 1\n",
    "                #target_past_kv = torch.cat((target_past_kv, target_new_past_kv[:, :, n, :]), dim=2)\n",
    "            else:  # rejected\n",
    "                q_p = max_fn(q[0][k] - p_cum[0][k])\n",
    "                # softmax isn't working here - q and p were reduced to arrays of numbers\n",
    "                #resampled_output = torch.softmax(torch.argmax(q_p), dim=2)\n",
    "                resampled_output = torch.argmax(q_p)      \n",
    "                resampled_output = torch.reshape(resampled_output, (1,1))\n",
    "                x = torch.cat((x, resampled_output), dim=1)\n",
    "                n += 1\n",
    "                all_accepted = False\n",
    "                break\n",
    "            \n",
    "        target_past_kv = target_new_past_kv\n",
    "        # Step 4: if all draft tokens were accepted, sample a final token\n",
    "        if all_accepted:\n",
    "            x = torch.cat((x, torch.reshape(torch.argmax(q[-1][-1]), (1,1))), dim=1)\n",
    "            n += 1\n",
    "\n",
    "        # just keeping my sanity\n",
    "        assert n == len(x.detach().numpy()[0]), f\"{n} {len(x.detach().numpy()[0])}\"\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583202d2-6d29-4729-af2e-232d3ee0bc2c",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## Main generation function [&#8657;](#0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6993840-e8b2-4c26-8da4-e1b046c3f3cc",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "### Download and Convert Model [&#8657;](#0)\n",
    "\n",
    "**TODO: I have not changed any of this yet and customized it for speculative sampling - but we should have an optimized model**\n",
    "\n",
    "Optimum Intel can be used to load optimized models from the [Hugging Face Hub](https://huggingface.co/docs/optimum/intel/hf.co/models) and create pipelines to run an inference with OpenVINO Runtime using Hugging Face APIs. The Optimum Inference models are API compatible with Hugging Face Transformers models.  This means we just need to replace `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.\n",
    "\n",
    "Below is an example of the Dolly model\n",
    "\n",
    "```diff\n",
    "-from transformers import AutoModelForCausalLM\n",
    "+from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "-model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "+model = OVModelForCausalLM.from_pretrained(model_id, from_transformers=True)\n",
    "```\n",
    "\n",
    "Model class initialization starts with calling `from_pretrained` method. When downloading and converting Transformers model, the parameter `from_transformers=True` should be added. We can save the converted model for the next usage with the `save_pretrained` method.\n",
    "Tokenizer class and pipelines API are compatible with Optimum models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fb4f9d-5877-48d8-8eff-c30ff6974d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "Set CACHE_DIR to facebook/opt-125m-local/model_cache\n",
      "Compiling the model...\n",
      "Set CACHE_DIR to facebook/opt-1.3b-local/model_cache\n",
      "/Users/barad/opt/anaconda3/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py:369: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Exception from src/inference/src/infer_request.cpp:249:\nException from src/inference/src/dev/converter_utils.cpp:706:\nException from src/bindings/python/src/pyopenvino/core/infer_request.hpp:54:\nCaught exception: [ GENERAL_ERROR ] Shape inference of Reshape node with name /decoder/layers.0/self_attn/Reshape_6 failed: Exception from src/plugins/intel_cpu/src/shape_inference/custom/reshape.cpp:61:\n[cpu]reshape: the shape of input data (32.0.11) conflicts with the reshape pattern (1.32.0.11)\n\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/l5/m569ndsx2dz5kbh1bp6m26d00000gp/T/ipykernel_89754/116250876.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mfire\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     \u001b[0mfire\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/fire/core.py\u001b[0m in \u001b[0;36mFire\u001b[0;34m(component, command, name, serialize)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaller_locals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0mcomponent_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Fire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparsed_flag_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mcomponent_trace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHasError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/fire/core.py\u001b[0m in \u001b[0;36m_Fire\u001b[0;34m(component, args, parsed_flag_args, context, name)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         component, remaining_args = _CallAndUpdateTrace(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mremaining_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/fire/core.py\u001b[0m in \u001b[0;36m_CallAndUpdateTrace\u001b[0;34m(component, args, component_trace, treatment, target)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m     \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mvarargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtreatment\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/l5/m569ndsx2dz5kbh1bp6m26d00000gp/T/ipykernel_89754/116250876.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(prompt, n_tokens_to_generate, K, seed)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     autoregressive_text, autoregressive_time = run_autoregressive_sampling_fn(\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mautoregressive_sampling_with_pkv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/l5/m569ndsx2dz5kbh1bp6m26d00000gp/T/ipykernel_89754/116250876.py\u001b[0m in \u001b[0;36mrun_autoregressive_sampling_fn\u001b[0;34m(decode_fn, input_ids, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_autoregressive_sampling_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;31m# this isn't right as speculative will have to switch between models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/l5/m569ndsx2dz5kbh1bp6m26d00000gp/T/ipykernel_89754/2579453343.py\u001b[0m in \u001b[0;36mautoregressive_sampling_with_pkv\u001b[0;34m(x, model, N)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#print(\"past_kv.size(): \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m#print(past_kv.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_kv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m#print(\"res.size(): \")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#print(res.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/optimum/modeling_base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, past_key_values, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshared_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logits\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Exception from src/inference/src/infer_request.cpp:249:\nException from src/inference/src/dev/converter_utils.cpp:706:\nException from src/bindings/python/src/pyopenvino/core/infer_request.hpp:54:\nCaught exception: [ GENERAL_ERROR ] Shape inference of Reshape node with name /decoder/layers.0/self_attn/Reshape_6 failed: Exception from src/plugins/intel_cpu/src/shape_inference/custom/reshape.cpp:61:\n[cpu]reshape: the shape of input data (32.0.11) conflicts with the reshape pattern (1.32.0.11)\n\n\n\n"
     ]
    }
   ],
   "source": [
    "def main(\n",
    "    prompt: str = \"Alan Turing theorized that computers would one day become\",\n",
    "    n_tokens_to_generate: int = 40,\n",
    "    K: int = 5,\n",
    "    seed: int = 223,\n",
    "):\n",
    "    from pathlib import Path\n",
    "    from transformers import AutoTokenizer\n",
    "    from optimum.intel.openvino import OVModelForCausalLM\n",
    "    import json\n",
    "\n",
    "    #draft_model_id = \"databricks/dolly-v2-3b\"\n",
    "    #draft_model_path = Path(\"dolly-v2-3b\")\n",
    "    #target_model_id = \"databricks/dolly-v2-12b\"\n",
    "    #target_model_path = Path(\"dolly-v2-12b\")\n",
    "    # facebook/opt-6.7b could be more interesting target model than facebook/opt-1.3b\n",
    "    draft_model_id = \"facebook/opt-125m\"\n",
    "    draft_model_path = Path(\"facebook/opt-125m-local\")\n",
    "    target_model_id = \"facebook/opt-1.3b\"\n",
    "    target_model_path = Path(\"facebook/opt-1.3b-local\")\n",
    "    #draft_model_id = \"gpt2\"\n",
    "    #draft_model_path = Path(\"gpt2-local\")\n",
    "    #target_model_id = \"gpt2-xl\"\n",
    "    #target_model_path = Path(\"gpt2-xl-local\")\n",
    "\n",
    "    target_tokenizer = AutoTokenizer.from_pretrained(target_model_id)\n",
    "\n",
    "    current_device = device.value\n",
    "\n",
    "    # Save local copies for subsequent runs\n",
    "    if draft_model_path.exists():\n",
    "        draft_ov_model = OVModelForCausalLM.from_pretrained(draft_model_path, device=current_device)\n",
    "    else:\n",
    "        draft_ov_model = OVModelForCausalLM.from_pretrained(draft_model_id, device=current_device, from_transformers=True)\n",
    "        draft_ov_model.save_pretrained(draft_model_path)\n",
    "    if target_model_path.exists():\n",
    "        target_ov_model = OVModelForCausalLM.from_pretrained(target_model_path, device=current_device)\n",
    "    else:\n",
    "        target_ov_model = OVModelForCausalLM.from_pretrained(target_model_id, device=current_device, from_transformers=True)\n",
    "        target_ov_model.save_pretrained(target_model_path)\n",
    "    \n",
    "    # seed numpy rng\n",
    "    np.random.seed(seed)\n",
    "    # Try eventually databricks/dolly-v2-3b and 12b\n",
    "    #draft_model = AutoModelForCausalLM.from_pretrained(draft_model_id, pad_token_id=target_tokenizer.eos_token_id)\n",
    "    #target_model = AutoModelForCausalLM.from_pretrained(target_model_id, pad_token_id=target_tokenizer.eos_token_id)\n",
    "    draft_model = draft_ov_model\n",
    "    target_model = target_ov_model\n",
    "    \n",
    "\n",
    "    input_ids = target_tokenizer(prompt, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "    def run_autoregressive_sampling_fn(decode_fn, input_ids, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        output_ids = decode_fn(x=input_ids, **kwargs)\n",
    "        # this isn't right as speculative will have to switch between models\n",
    "        text = target_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        return text, elapsed_time\n",
    "\n",
    "    def run_speculative_sampling_fn(decode_fn, input_ids, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        output_ids = decode_fn(x=input_ids, **kwargs)\n",
    "        # this isn't right as speculative will have to switch between models\n",
    "        text = target_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        return text, elapsed_time\n",
    "\n",
    "    autoregressive_text, autoregressive_time = run_autoregressive_sampling_fn(\n",
    "        autoregressive_sampling_with_pkv,\n",
    "        input_ids,\n",
    "        model=target_model,\n",
    "        N=n_tokens_to_generate,\n",
    "    )\n",
    "\n",
    "    speculative_text, speculative_time = run_speculative_sampling_fn(\n",
    "        speculative_sampling_with_pkv,\n",
    "        input_ids,\n",
    "        target_model=target_model,\n",
    "        draft_model=draft_model,\n",
    "        N=n_tokens_to_generate,\n",
    "        K=K,\n",
    "    )\n",
    "\n",
    "#   results\n",
    "    print()\n",
    "    print(\"Autoregressive Decode\")\n",
    "    print(\"---------------------\")\n",
    "    print(f\"Time = {autoregressive_time:.2f}s\")\n",
    "    print(f\"Text = {autoregressive_text}\")\n",
    "    print()\n",
    "    print(\"Speculative Decode\")\n",
    "    print(\"------------------\")\n",
    "    print(f\"Time = {speculative_time:.2f}s\")\n",
    "    print(f\"Text = {speculative_text}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #with gr.Blocks() as demo:\n",
    "    #    gr.Markdown(\n",
    "    #    \"\"\"\n",
    "    #    # Speculative Sampling Demo\n",
    "    #    ## The output will show a comparison of Autoregressive Sampling vs Speculative Sampling\n",
    "    #    - Target Model: gpt2-xl\n",
    "    #    - Draft Model: gpt2\n",
    "    #    - K = 5\n",
    "    #    > Some improvements can be made to acceptance criterion and adjusting temperature to improve text quality.\n",
    "    #    > Note: Have patience as it takes time run on free CPUs.\n",
    "    #    \"\"\")\n",
    "    #    with gr.Row():\n",
    "    #        inp = gr.Textbox(placeholder=\"THIS CANNOT BE EMPTY\", label=\"Input Prompt\")\n",
    "    #        out = gr.Textbox(label=\"Output\")\n",
    "    #    btn = gr.Button(\"Run\")\n",
    "    #    btn.click(fn=main, inputs=inp, outputs=out)\n",
    "    #demo.launch(share=True)\n",
    "\n",
    "    import fire\n",
    "\n",
    "    fire.Fire(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e996bc-eebb-4a5e-97f0-43c82499bf83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397c061b-f969-4111-b1bb-a6936cc892cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "5bc9f8fc615a4cf7af5cb987afd0211d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "GPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 2,
       "layout": "IPY_MODEL_f16aa744c427462abd7b791b52a88676",
       "style": "IPY_MODEL_ffce97ef6b86423d96b13e91a6dd913d"
      }
     },
     "f16aa744c427462abd7b791b52a88676": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ffce97ef6b86423d96b13e91a6dd913d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
