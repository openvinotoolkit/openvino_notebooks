{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef2ed242-3561-464c-8d1c-cc3862e23702",
   "metadata": {},
   "source": [
    "# Text Generation via Speculative Sampling, KV Caching, and OpenVINO™\n",
    "\n",
    "As model sizes grow, Generative AI implementations require significant inference resources. This not only increases the cost per generation from a prompt, but also increases the power consumption used to serve such requests.\n",
    "\n",
    "Inference optimizations for text generation are essential for reducing costs and power consumption. When optimizing the inference process, the amount of time and energy required to generate text can be significantly reduced. This can lead to cost savings in terms of hardware and software, as well as reduced power consumption. Additionally, inference optimizations can help improve the accuracy of text generation as well as the speed at which it can be generated. This can lead to an improved user experience and increased efficiency in text-generation tasks. In summary, inference optimizations for text generation are essential to reduce costs and power consumption, while also improving the accuracy and speed of text generation.\n",
    "\n",
    "Another necessary condition is that the optimizations are compatible with each other. That is, implementing a certain optimization should not preclude other optimizations. There are several levels of optimizations that can provide significant speedup without \"bumping into each other\" in a way that will compromise overall efficiency.\n",
    "\n",
    "For details on this method, please refer to the DeepMind paper by Chen et al, http://arxiv.org/abs/2302.01318\n",
    "\n",
    "Our paper describing this implementation with OpenVino is at: [TODO: Add link to blog arxiv here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97c435a",
   "metadata": {},
   "source": [
    "<a id=\"0\"></a>\n",
    "### Table of content:\n",
    "\n",
    "- [Prerequisites](#1)\n",
    "    - [Select inference device](#2)\n",
    "- [Download and Convert Model](#3)\n",
    "- [Create autoregressive and speculative forms of sampling with KV Cache support](#4)\n",
    "    - [Setup imports](#5)\n",
    "    - [Prepare autoregressive sampling](#6)\n",
    "    - [Prepare speculative sampling](#7)\n",
    "    - [Main generation function](#8)\n",
    "    - [Download and convert model](#9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa16b1-d2f6-4a3a-abfb-5ec278133c80",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "## Prerequisites [&#8657;](#0)\n",
    "\n",
    "First, we must install the [Hugging Face Optimum](https://huggingface.co/docs/optimum/installation) library accelerated by OpenVINO integration.\n",
    "The Hugging Face Optimum Intel API is a high-level API that enables us to convert and quantize models from the Hugging Face Transformers library to the OpenVINO™ IR format. For more details, refer to the [Hugging Face Optimum Intel documentation](https://huggingface.co/docs/optimum/intel/inference).\n",
    "\n",
    "We will also need to install transformers (HuggingFace) and some other useful modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4421fc85-bed6-4a62-b8fa-19c7ba474891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade pip\n",
    "%pip install -q --upgrade transformers torch gradio openvino accelerate onnx onnxruntime ipywidgets\n",
    "%pip install -q \"git+https://github.com/huggingface/optimum-intel.git\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367f84f8-33e8-4ad6-bd40-e6fd41d2d703",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "### Select inference device [&#8657;](#0)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ddd57de-9f41-403c-bccc-8d3118654a24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37fd9fb3e8946098ee21a53cac001cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='CPU',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d9c4a5-ef75-4076-9f1c-f45a2259ec46",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "## Create autoregressive and speculative forms of sampling with KV Cache support [&#8657;](#0)\n",
    " \n",
    " At this stage, we will create paths for both autoregressive and speculative sampling methods that incorporate their own KV caching mechanism. The key here\n",
    " is that the KV caches for the speculative sampling path must be separated (i.e. a separate cache for the draft model and one for the target model - this is necessary since the sizes of the logits do not match for the target and draft models). Another complexity is that since\n",
    " the target model is not called for each and every token (i.e. the tokens are often checked in batches of size K), then we need to do some bookkeeping to\n",
    " ensure that we have filled in the gaps that are created in the KV cache for the target model.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5da4d-d2fd-440b-b204-7fbc6966dd1f",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a>\n",
    "### Setup imports [&#8657;](#0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f976094-8603-42c4-8f18-a32ba6d7192e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import gradio as gr\n",
    "\n",
    "from threading import Thread\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "from optimum.intel.openvino import OVModelForCausalLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c58611d6-0a91-4efd-976e-4221acbb43cd",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a>\n",
    "### Prepare autoregressive sampling [&#8657;](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ac10a5-3141-4227-8f0b-0617acd027c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_fn(x):\n",
    "    x_max = torch.where(x > 0, x, torch.zeros_like(x))\n",
    "    return x_max / torch.sum(x_max)\n",
    "\n",
    "def autoregressive_sampling_with_pkv(x, model, N):\n",
    "    n = len(x)\n",
    "    T = n + N\n",
    "    input = x\n",
    "    past_kv = None\n",
    "\n",
    "    while n < T:\n",
    "        res = model(input, attention_mask=torch.ones(input.size(), dtype=torch.long), past_key_values=past_kv)\n",
    "        model_out = torch.softmax(res.logits, dim=2)\n",
    "        past_kv = res.past_key_values\n",
    "        next_token = torch.reshape(torch.argmax(model_out[-1][-1]), (1, 1))\n",
    "        x = torch.cat((x, next_token), dim=1)\n",
    "        n += 1\n",
    "        input = next_token\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a01739-1363-42ef-927f-6a340bdbe7ba",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a>\n",
    "### Prepare speculative sampling [&#8657;](#0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "524e72f4-8750-48ff-b002-e558d03b3302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speculative_sampling_with_pkv(x, draft_model, target_model, N, K):\n",
    "    # NOTE: paper indexes arrays starting from 1, python indexes from 0, so\n",
    "    # we have to add an extra -1 term when indexing using n, T, or t\n",
    "    n = x.size(1)\n",
    "    T = n + N\n",
    "    target_past_kv = None\n",
    "    while n < T:\n",
    "        # Step 1: auto-regressive decode K tokens from draft model and get final p\n",
    "        x_draft = None\n",
    "        draft_past_kv = None\n",
    "        x_draft_input = x\n",
    "        p_cum = None\n",
    "        q_cum = None\n",
    "        for _ in range(K):\n",
    "            past_len = 0 if draft_past_kv is None else draft_past_kv[0][0].shape[2]\n",
    "            res_draft = draft_model(x_draft_input, attention_mask=torch.ones((1, x_draft_input.shape[1] + past_len), dtype=torch.long), past_key_values=draft_past_kv, use_cache=True)\n",
    "            p = res_draft.logits\n",
    "            p = torch.softmax(p, dim=2)\n",
    "            draft_past_kv = res_draft.past_key_values\n",
    "            next_token = torch.reshape(torch.argmax(p[-1][-1]), (1, 1))\n",
    "            x_draft_input = next_token\n",
    "            if p_cum is None:\n",
    "                p_cum = p[:, -1].unsqueeze(1)\n",
    "                x_draft = next_token\n",
    "            else:\n",
    "                p_cum = torch.cat((p_cum, p), dim=1)\n",
    "                x_draft = torch.cat((x_draft, next_token), dim=1)\n",
    "        # Step 2: target model forward passes on x_draft\n",
    "        if target_past_kv is None:\n",
    "            x_draft_target_input = torch.cat((x, x_draft), dim=1)\n",
    "        else:\n",
    "            x_draft_target_input = x_draft\n",
    "        # in terms of performance - so both should be tested.\n",
    "        past_len = 0 if target_past_kv is None else target_past_kv[0][0].shape[2]\n",
    "        res = target_model(x_draft_target_input, attention_mask=torch.zeros((1, x_draft_target_input.shape[1] + past_len), dtype=torch.long), past_key_values=target_past_kv, use_cache=True)\n",
    "        q = res.logits\n",
    "        target_new_past_kv = res.past_key_values\n",
    "        if q_cum is None:\n",
    "            q_cum = q\n",
    "        else:\n",
    "            q_cum = torch.cat((q_cum, q), dim=1)\n",
    "        # Step 3: append draft tokens based on rejection criterion and resample\n",
    "        # a token on rejection\n",
    "        all_accepted = True\n",
    "        for k in range(K):\n",
    "            #i = n - 1\n",
    "            j = x_draft[0][k].item()\n",
    "            s_q = torch.softmax(q, dim=2)\n",
    "            s_p_cum = torch.softmax(p_cum, dim=2)\n",
    "            q_item = s_q[-1][k][j].detach().numpy()\n",
    "            p_item = s_p_cum[-1][k][j].detach().numpy()\n",
    "\n",
    "            if np.random.random() < min(1, (q_item / p_item)):  # accepted\n",
    "                x = torch.cat((x, torch.tensor(j).reshape(1,1)), dim=1)\n",
    "                n += 1\n",
    "            else:  # rejected\n",
    "                q_p = max_fn(q[0][k] - p_cum[0][k])\n",
    "                resampled_output = torch.argmax(q_p)      \n",
    "                resampled_output = torch.reshape(resampled_output, (1,1))\n",
    "                x = torch.cat((x, resampled_output), dim=1)\n",
    "                n += 1\n",
    "                all_accepted = False\n",
    "                break\n",
    "            \n",
    "        # Step 4: if all draft tokens were accepted, sample a final token\n",
    "        if all_accepted:\n",
    "            x = torch.cat((x, torch.reshape(torch.argmax(q[-1][-1]), (1,1))), dim=1)\n",
    "            n += 1\n",
    "            target_past_kv = target_new_past_kv\n",
    "        else:\n",
    "            target_past_kv = None\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583202d2-6d29-4729-af2e-232d3ee0bc2c",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a>\n",
    "## Main generation function [&#8657;](#0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6993840-e8b2-4c26-8da4-e1b046c3f3cc",
   "metadata": {},
   "source": [
    "Redefine the model class to accept multiple input_ids with past key values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fb4f9d-5877-48d8-8eff-c30ff6974d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple\n",
    "from openvino.runtime import Type, Tensor\n",
    "# TODO: Isn't this just a duplicate of the first line?\n",
    "from typing import Optional, Tuple\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "class OVModelForCausalLMWithMultiTokenPKV(OVModelForCausalLM):\n",
    "       def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n",
    "        **kwargs,\n",
    "    ) -> CausalLMOutputWithPast:\n",
    "        self.compile()\n",
    "\n",
    "        inputs = {}\n",
    "        if past_key_values is not None:\n",
    "            if self._pkv_precision == Type.bf16:\n",
    "                # numpy does not support bf16, pretending f16, should change to bf16\n",
    "                past_key_values = tuple(\n",
    "                    Tensor(past_key_value, past_key_value.shape, Type.bf16)\n",
    "                    for pkv_per_layer in past_key_values\n",
    "                    for past_key_value in pkv_per_layer\n",
    "                )\n",
    "            else:\n",
    "                # Flatten the past_key_values\n",
    "                past_key_values = tuple(\n",
    "                    past_key_value for pkv_per_layer in past_key_values for past_key_value in pkv_per_layer\n",
    "                )\n",
    "            # Add the past_key_values to the decoder inputs\n",
    "            inputs = dict(zip(self.key_value_input_names, past_key_values))\n",
    "\n",
    "        # Create empty past_key_values for decoder_with_past first generation step\n",
    "        elif self.use_cache:\n",
    "            shape_input_ids = input_ids.shape\n",
    "            num_attention_heads = (\n",
    "                self.normalized_config.num_attention_heads if self.config.model_type == \"bloom\" else 1\n",
    "            )\n",
    "            for input_name in self.key_value_input_names:\n",
    "                model_inputs = self.model.input(input_name)\n",
    "                shape = model_inputs.get_partial_shape()\n",
    "                shape[0] = shape_input_ids[0] * num_attention_heads\n",
    "                if shape[2].is_dynamic:\n",
    "                    shape[2] = 0\n",
    "                if shape[1].is_dynamic:\n",
    "                    shape[1] = 0\n",
    "                inputs[input_name] = Tensor(model_inputs.get_element_type(), shape.get_shape())\n",
    "\n",
    "        inputs[\"input_ids\"] = np.array(input_ids)\n",
    "\n",
    "        # Add the attention_mask inputs when needed\n",
    "        if \"attention_mask\" in self.input_names and attention_mask is not None:\n",
    "            inputs[\"attention_mask\"] = np.array(attention_mask)\n",
    "\n",
    "        # Run inference\n",
    "        self.request.start_async(inputs, shared_memory=True)\n",
    "        self.request.wait()\n",
    "\n",
    "        logits = torch.from_numpy(self.request.get_tensor(\"logits\").data).to(self.device)\n",
    "\n",
    "        if self.use_cache:\n",
    "            # Tuple of length equal to : number of layer * number of past_key_value per decoder layer (2 corresponds to the self-attention layer)\n",
    "            past_key_values = tuple(self.request.get_tensor(key).data for key in self.key_value_output_names)\n",
    "            # Tuple of tuple of length `n_layers`, with each tuple of length equal to 2 (k/v of self-attention)\n",
    "            past_key_values = tuple(\n",
    "                past_key_values[i : i + self.num_pkv] for i in range(0, len(past_key_values), self.num_pkv)\n",
    "            )\n",
    "        else:\n",
    "            past_key_values = None\n",
    "\n",
    "        return CausalLMOutputWithPast(logits=logits, past_key_values=past_key_values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f3ad17",
   "metadata": {},
   "source": [
    "<a id=\"9\"></a>\n",
    "### Download and Convert Model [&#8657;](#0)\n",
    "\n",
    "**TODO: I have not changed any of this yet and customized it for speculative sampling - but we should have an optimized model**\n",
    "\n",
    "Optimum Intel can be used to load optimized models from the [Hugging Face Hub](https://huggingface.co/docs/optimum/intel/hf.co/models) and create pipelines to run an inference with OpenVINO Runtime using Hugging Face APIs. The Optimum Inference models are API compatible with Hugging Face Transformers models.  This means we just need to replace `AutoModelForXxx` class with the corresponding `OVModelForXxx` class.\n",
    "\n",
    "Below is an example of the Dolly model\n",
    "\n",
    "```diff\n",
    "-from transformers import AutoModelForCausalLM\n",
    "+from optimum.intel.openvino import OVModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"databricks/dolly-v2-3b\"\n",
    "-model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "+model = OVModelForCausalLM.from_pretrained(model_id, from_transformers=True)\n",
    "```\n",
    "\n",
    "Model class initialization starts with calling `from_pretrained` method. When downloading and converting Transformers model, the parameter `from_transformers=True` should be added. We can save the converted model for the next usage with the `save_pretrained` method.\n",
    "Tokenizer class and pipelines API are compatible with Optimum models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adcacd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Set CACHE_DIR to gpt2-local/model_cache\n",
      "Compiling the model to CPU ...\n",
      "Set CACHE_DIR to gpt2-xl-local/model_cache\n",
      "Compiling the model to CPU ...\n",
      "Set CACHE_DIR to gpt2-local/model_cache\n",
      "/Users/barad/opt/anaconda3/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py:369: FutureWarning: `shared_memory` is deprecated and will be removed in 2024.0. Value of `shared_memory` is going to override `share_inputs` value. Please use only `share_inputs` explicitly.\n",
      "  self.request.start_async(inputs, shared_memory=True)\n",
      "Compiling the model to CPU ...\n",
      "Set CACHE_DIR to gpt2-xl-local/model_cache\n",
      "Compiling the model to CPU ...\n",
      "Set CACHE_DIR to gpt2-xl-local/model_cache\n",
      "Compiling the model to CPU ...\n",
      "Set CACHE_DIR to gpt2-xl-local/model_cache\n"
     ]
    }
   ],
   "source": [
    "def main(\n",
    "    prompt: str = \"Alan Turing theorized that computers would one day become\",\n",
    "    n_tokens_to_generate: int = 40,\n",
    "    K: int = 5,\n",
    "    seed: int = 5555,\n",
    "):\n",
    "    # Consider other models here\n",
    "    #draft_model_id = \"databricks/dolly-v2-3b\"\n",
    "    #draft_model_path = Path(\"dolly-v2-3b\")\n",
    "    #target_model_id = \"databricks/dolly-v2-12b\"\n",
    "    #target_model_path = Path(\"dolly-v2-12b\")\n",
    "    ##facebook/opt-6.7b could be more interesting target model than facebook/opt-1.3b\n",
    "    # draft_model_id = \"facebook/opt-125m\"\n",
    "    # draft_model_path = Path(\"facebook/opt-125m-local\")\n",
    "    # target_model_id = \"facebook/opt-1.3b\"\n",
    "    # target_model_path = Path(\"facebook/opt-1.3b-local\")\n",
    "    draft_model_id = \"gpt2\"\n",
    "    draft_model_path = Path(\"gpt2-local\")\n",
    "    target_model_id = \"gpt2-xl\"\n",
    "    target_model_path = Path(\"gpt2-xl-local\")\n",
    "\n",
    "    # note that the target and draft models must use the same tokenizer.\n",
    "    target_tokenizer = AutoTokenizer.from_pretrained(target_model_id)\n",
    "\n",
    "    current_device = device.value\n",
    "\n",
    "    # Save local copies for subsequent runs\n",
    "    if draft_model_path.exists():\n",
    "        draft_ov_model = OVModelForCausalLM.from_pretrained(draft_model_path, device=current_device)\n",
    "    else:\n",
    "        draft_ov_model = OVModelForCausalLM.from_pretrained(draft_model_id, device=current_device, from_transformers=True)\n",
    "        draft_ov_model.save_pretrained(draft_model_path)\n",
    "    if target_model_path.exists():\n",
    "        target_ov_model = OVModelForCausalLM.from_pretrained(target_model_path, device=current_device)\n",
    "    else:\n",
    "        target_ov_model = OVModelForCausalLM.from_pretrained(target_model_id, device=current_device, from_transformers=True)\n",
    "        target_ov_model.save_pretrained(target_model_path)\n",
    "    \n",
    "    # seed numpy rng\n",
    "    np.random.seed(seed)    \n",
    "\n",
    "    input_ids = target_tokenizer(prompt, return_tensors=\"pt\")['input_ids']\n",
    "\n",
    "    def run_autoregressive_sampling_fn(decode_fn, input_ids, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        output_ids = decode_fn(x=input_ids, **kwargs)\n",
    "        text = target_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        return text, elapsed_time\n",
    "\n",
    "    def run_speculative_sampling_fn(decode_fn, input_ids, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        output_ids = decode_fn(x=input_ids, **kwargs)\n",
    "        text = target_tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        elapsed_time = time.perf_counter() - start\n",
    "        return text, elapsed_time\n",
    "\n",
    "    autoregressive_text, autoregressive_time = run_autoregressive_sampling_fn(\n",
    "        autoregressive_sampling_with_pkv,\n",
    "        input_ids,\n",
    "        model=target_ov_model,\n",
    "        N=n_tokens_to_generate,\n",
    "    )\n",
    "    if target_model_path.exists():\n",
    "        target_ov_model = OVModelForCausalLMWithMultiTokenPKV.from_pretrained(target_model_path, device=current_device)\n",
    "    else:\n",
    "        target_ov_model = OVModelForCausalLMWithMultiTokenPKV.from_pretrained(target_model_id, device=current_device, from_transformers=True)\n",
    "        target_ov_model.save_pretrained(target_model_path)\n",
    "\n",
    "    speculative_text, speculative_time = run_speculative_sampling_fn(\n",
    "        speculative_sampling_with_pkv,\n",
    "        input_ids,\n",
    "        target_model=target_ov_model,\n",
    "        draft_model=draft_ov_model,\n",
    "        N=n_tokens_to_generate,\n",
    "        K=K,\n",
    "    )\n",
    "\n",
    "#   Print results for comparison of text and time\n",
    "    print()\n",
    "    print(\"Autoregressive Decode\")\n",
    "    print(\"---------------------\")\n",
    "    print(f\"Time = {autoregressive_time:.2f}s\")\n",
    "    print(f\"Text = {autoregressive_text}\")\n",
    "    print()\n",
    "    print(\"Speculative Decode\")\n",
    "    print(\"------------------\")\n",
    "    print(f\"Time = {speculative_time:.2f}s\")\n",
    "    print(f\"Text = {speculative_text}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Speculative Sampling Demo\n",
    "        ## The output will show a comparison of Autoregressive Sampling vs Speculative Sampling\n",
    "        - Target Model: gpt2-xl\n",
    "        - Draft Model: gpt2\n",
    "        - K = 5\n",
    "        > Some improvements can be made to acceptance criterion and adjusting temperature to improve text quality.\n",
    "        \"\"\")\n",
    "        with gr.Row():\n",
    "            inp = gr.Textbox(placeholder=\"THIS CANNOT BE EMPTY\", label=\"Input Prompt\")\n",
    "            out = gr.Textbox(label=\"Output\")\n",
    "        btn = gr.Button(\"Run\")\n",
    "        btn.click(fn=main, inputs=inp, outputs=out)\n",
    "    demo.launch()\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579516c2-ef7d-4b97-af15-07f119a0b4d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1143ff7dbf8e43f7a7991151056a6055": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "49827d79d96c42ba8ab269a400c2bf93": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "GNA",
        "AUTO"
       ],
       "description": "Device:",
       "index": 0,
       "layout": "IPY_MODEL_1143ff7dbf8e43f7a7991151056a6055",
       "style": "IPY_MODEL_e4e03c0919bf4b9ebd12edc8c8d16ae0"
      }
     },
     "e4e03c0919bf4b9ebd12edc8c8d16ae0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
