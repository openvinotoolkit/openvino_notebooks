{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98466560-036d-41d2-87f7-957af6f64626",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Monocular Visual-Inertial Depth Estimation using OpenVINO™\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img src=\"https://raw.githubusercontent.com/isl-org/VI-Depth/main/figures/methodology_diagram.png\"> \n",
    "    <figcaption>\n",
    "        <span class=\"caption\"> <i> The overall methodology.</i> </span>\n",
    "        <i class=\"photo-credit\"> Diagram taken from the VI-Depth repository.</i>\n",
    "    </figcaption>\n",
    "</p>\n",
    "\n",
    "A visual-inertial depth estimation pipeline that integrates monocular depth estimation and visual-inertial odometry to produce dense depth estimates with metric scale has been presented by the authors. The approach consists of three stages: \n",
    "\n",
    "1. input processing, where RGB and inertial measurement unit (IMU) data feed into monocular depth estimation alongside visual-inertial odometry,\n",
    "2. global scale and shift alignment, where monocular depth estimates are fitted to sparse depth from visual inertial odometry (VIO) in a least-squares manner and \n",
    "3. learning-based dense scale alignment, where globally-aligned depth is locally realigned using a dense scale map regressed by the ScaleMapLearner (SML). \n",
    "\n",
    "The images at the bottom in the diagram above illustrate a Visual Odometry with Inertial and Depth (VOID) sample being processed through our pipeline; from left to right: the input RGB, ground truth depth, sparse depth from VIO, globally-aligned depth, scale map scaffolding, dense scale map regressed by SML, final depth output.\n",
    "\n",
    "<p align=\"center\" width=\"100%\">\n",
    "    <img src=\"https://raw.githubusercontent.com/isl-org/VI-Depth/main/figures/teaser_figure.png\">\n",
    "    <figcaption>\n",
    "        <span class=\"caption\"> <i> An illustration of VOID samples being processed by the image pipeline.</i> </span>\n",
    "        <i class=\"photo-credit\"> Image taken from the VI-Depth repository.</i>\n",
    "    </figcaption>\n",
    "</p>\n",
    "\n",
    "We will be consulting the [VI-Depth repository](https://github.com/isl-org/VI-Depth) for the pre-processing, model transformations and basic utility code. A part of it has already been kept as it is in the [utils](utils) directory. At the same time we will learn how to perform [model conversion](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_PyTorch.html) for converting a model in a different format to the standard OpenVINO™ IR model representation *via* another format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2818c1d3-cd03-40f8-a03d-2101a14571fe",
   "metadata": {},
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918c6ee-8b3a-48a4-a5a7-3b32106173d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sys beforehand to inform of Python version <= 3.7 not being supported\n",
    "import sys\n",
    "\n",
    "if sys.version_info.minor < 8:\n",
    "    print('Python3.7 is not supported. Some features might not work as expected')\n",
    "    \n",
    "# Download the correct version of the PyTorch deep learning library associated with image models\n",
    "# alongside the lightning module\n",
    "!pip install -q lightning timm==0.6.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d669d1a-1e3d-4f00-9625-de361cff8492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import openvino\n",
    "import torch\n",
    "import torchvision\n",
    "from openvino.runtime import Core\n",
    "from pathlib import Path\n",
    "from shutil import rmtree\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "sys.path.append('../utils')\n",
    "from notebook_utils import download_file\n",
    "\n",
    "sys.path.append('vi_depth_utils')\n",
    "import data_loader\n",
    "import modules.midas.transforms as transforms\n",
    "import modules.midas.utils as utils\n",
    "from modules.estimator import LeastSquaresEstimator\n",
    "from modules.interpolator import Interpolator2D\n",
    "from modules.midas.midas_net_custom import MidasNet_small_videpth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6eeef-8eea-4954-93a0-e1d7d1879c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ability to display images inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7ff380-dc99-45b5-a5bc-18472dd7e99c",
   "metadata": {},
   "source": [
    "### Loading models and checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625575d8-7179-4c04-87a7-e678d82ab13c",
   "metadata": {},
   "source": [
    "The complete pipeline here requires only two models: one for depth estimation and a ScaleMapLearner model which is responsible for regressing a dense scale map. The table of models which has been given in the original [VI-Depth repo](https://github.com/isl-org/VI-Depth) has been presented as it is for the users to download from. [VOID](https://github.com/alexklwong/void-dataset) is the name of the original dataset from on which these models have been trained. The numbers after the word **VOID** represent the checkpoint in the model obtained after training samples for sparse dense maps corresponding to $150$, $500$ and $1500$ levels in the density map. Just *right-click* on any of the highlighted links and click on \"Copy link address\". We shall use this link in the next cell to download the ScaleMapLearner model. *Interestingly*, the ScaleMapLearner decides the depth prediction model as you will see.\n",
    " \n",
    " \n",
    "   | Depth Predictor   |  SML on VOID 150  |  SML on VOID 500  | SML on VOID 1500 |\n",
    "   | :---              |       :----:      |       :----:      |      :----:      |\n",
    "   | DPT-BEiT-Large    | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_beit_large_512.nsamples.150.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_beit_large_512.nsamples.500.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_beit_large_512.nsamples.1500.ckpt) |\n",
    "   | DPT-SwinV2-Large  | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_swin2_large_384.nsamples.150.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_swin2_large_384.nsamples.500.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_swin2_large_384.nsamples.1500.ckpt) |\n",
    "   | DPT-Large         | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_large.nsamples.150.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_large.nsamples.500.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_large.nsamples.1500.ckpt) |\n",
    "   | DPT-Hybrid        | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_hybrid.nsamples.150.ckpt)* | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_hybrid.nsamples.500.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_hybrid.nsamples.1500.ckpt) |\n",
    "   | DPT-SwinV2-Tiny   | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_swin2_tiny_256.nsamples.150.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_swin2_tiny_256.nsamples.500.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_swin2_tiny_256.nsamples.1500.ckpt) |\n",
    "   | DPT-LeViT         | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_levit_224.nsamples.150.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_levit_224.nsamples.500.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_levit_224.nsamples.1500.ckpt) |\n",
    "   | MiDaS-small       | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.midas_small.nsamples.150.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.midas_small.nsamples.500.ckpt) | [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.midas_small.nsamples.1500.ckpt) |\n",
    "\n",
    "\\*Also available with pre-training on TartanAir: [model](https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.dpt_hybrid.nsamples.150.pretrained.ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5dc086-e1f9-4d60-b406-0ebc0455a553",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base directory in which models would be stored as a pathlib.Path variable\n",
    "MODEL_DIR = Path('model')\n",
    "\n",
    "# Mapping between depth predictors and the corresponding scale map learners\n",
    "PREDICTOR_MODEL_MAP = {'dpt_beit_large_512': 'DPT_BEiT_L_512',\n",
    "                       'dpt_swin2_large_384': 'DPT_SwinV2_L_384',\n",
    "                       'dpt_large': 'DPT_Large',\n",
    "                       'dpt_hybrid': 'DPT_Hybrid',\n",
    "                       'dpt_swin2_tiny_256': 'DPT_SwinV2_T_256',\n",
    "                       'dpt_levit_224': 'DPT_LeViT_224',\n",
    "                       'midas_small': 'MiDaS_small'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf0c528-d5d0-416e-9a6d-c68ac1c2c38c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the model directory adjacent to the notebook and suppress errors if the directory already exists\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Here we will be downloading the SML model corresponding to the MiDaS-small depth predictor for \n",
    "# the checkpoint captured after training on 1500 points of the density level. Suppress errors if the file already exists\n",
    "download_file('https://github.com/isl-org/VI-Depth/releases/download/v1/sml_model.dpredictor.midas_small.nsamples.1500.ckpt', directory=MODEL_DIR, silent=True)\n",
    "\n",
    "# Take a note of the samples. It would be of major use later on\n",
    "NSAMPLES = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2a139e-910b-45ea-87b2-4a71a2f3a235",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the same model directory for downloading the depth predictor model which is available on\n",
    "# PyTorch hub\n",
    "torch.hub.set_dir(str(MODEL_DIR))\n",
    "\n",
    "\n",
    "# A utility function for utilising the mapping between depth predictors and \n",
    "# scale map learners so as to download the former\n",
    "def get_model_for_predictor(depth_predictor: str, remote_repo: str = 'intel-isl/MiDaS') -> str:    \n",
    "    \"\"\"\n",
    "    Download a model from the pre-validated 'isl-org/MiDaS:2.1' set of releases on the GitHub repo\n",
    "    while simultaneously trusting the repo permanently\n",
    "\n",
    "    :param: depth_predictor: Any depth estimation model amongst the ones given at https://github.com/isl-org/VI-Depth#setup\n",
    "    :param: remote_repo: The remote GitHub repo from where the models will be downloaded\n",
    "    :returns: A PyTorch model callable\n",
    "    \"\"\"    \n",
    "    \n",
    "    # Workaround for avoiding rate limit errors\n",
    "    torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n",
    "    \n",
    "    return torch.hub.load(remote_repo, PREDICTOR_MODEL_MAP[depth_predictor], skip_validation=True, trust_repo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12dc1de-47df-48ea-aabd-4845dcd2959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the above function so as to download the MiDaS-small model\n",
    "# and get the output of the model callable in return\n",
    "depth_model = get_model_for_predictor('midas_small')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8e0c34-015c-4da2-aee7-3c0c6234c46e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cleaning up the model directory\n",
    "\n",
    "From the verbose of the previous step it is obvious that [`torch.hub.load`](https://pytorch.org/docs/stable/hub.html#torch.hub.load) downloads a lot of unnecessary files. We shall move remove the unnecessary directories and files which were created during the download process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e194b-cfea-42a5-8ae8-c9bd785fae18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove unnecessary directories and files and suppress errors(if any)\n",
    "rmtree(path=str(MODEL_DIR / 'intel-isl_MiDaS_master'), ignore_errors=True)\n",
    "rmtree(path=str(MODEL_DIR / 'rwightman_gen-efficientnet-pytorch_master'), ignore_errors=True)\n",
    "rmtree(path=str(MODEL_DIR / 'checkpoints'), ignore_errors=True)\n",
    "\n",
    "# Check for the existence of the trusted list file and then remove\n",
    "list_file = Path(MODEL_DIR / 'trusted_list')\n",
    "if list_file.is_file():\n",
    "    list_file.unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38058667-3fd9-4178-aa51-79e0c3c41694",
   "metadata": {},
   "source": [
    "### Transformation of models\n",
    "\n",
    "Each of the models need an appropriate transformation which can be invoked by the `get_model_transforms` function. It needs only the `depth_predictor` parameter and NSAMPLES defined above to work. The reason being that the ScaleMapLearner and the depth estimation model are always in direct correspondence with each other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185ae95-20f6-45c2-8993-0a5bd125554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define important custom types\n",
    "type_transform_compose = torchvision.transforms.transforms.Compose\n",
    "type_compiled_model = openvino.runtime.ie_api.CompiledModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ebef89-2563-48c5-bb49-eb27b40e5cce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_transforms(depth_predictor: str, nsamples: int) -> Tuple[type_transform_compose, type_transform_compose]:\n",
    "    \"\"\"\n",
    "    Construct the transformation of the depth prediction model and the \n",
    "    associated ScaleMapLearner model\n",
    "\n",
    "    :param: depth_predictor: Any depth estimation model amongst the ones given at https://github.com/isl-org/VI-Depth#setup\n",
    "    :param: nsamples: The no. of density levels for the depth map\n",
    "    :returns: The transformed models as the resut of torchvision.transforms.Compose operations\n",
    "    \"\"\"    \n",
    "    model_transforms = transforms.get_transforms(depth_predictor, \"void\", str(nsamples))\n",
    "    return model_transforms['depth_model'], model_transforms['sml_model']    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f139b01-9b03-4685-865a-e54df566f04b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Obtain transforms of both the models here\n",
    "depth_model_transform, scale_map_learner_transform = get_model_transforms(depth_predictor='midas_small',\n",
    "                                                                          nsamples=NSAMPLES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca36f1-c98e-44a6-9a38-c38c8279e290",
   "metadata": {},
   "source": [
    "#### Dummy input creation\n",
    "\n",
    "Dummy inputs are necessary for [PyTorch to ONNX](https://docs.openvino.ai/latest/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_PyTorch.html#exporting-a-pytorch-model-to-onnx-format) conversion. Although [`torch.onnx.export`](https://pytorch.org/docs/stable/onnx.html) accepts any dummy input for a single pass through the model and thereby enabling model conversion, the pre-processing required for the actual inputs later at inference using compiled models would be substantial. So we have decided that even dummy inputs should go through the proper transformation process so that the reader gets the idea of a *transformed image* being compiled by a *transformed model*.\n",
    "\n",
    "Also note down the width and height of the image which would be used multiple times later. Do note that this is constant throughout the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3f5d9e-dfb3-46de-93db-226779b4a8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_H, IMAGE_W = 480, 640\n",
    "\n",
    "# Although you can always verify the same by uncommenting and running\n",
    "# the following lines\n",
    "# img = cv2.imread('data/image/dummy_img.png')\n",
    "# print(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ac917-e114-412f-8a41-91c0e447bd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base directory in which data would be stored as a pathlib.Path variable\n",
    "DATA_DIR = Path('data')\n",
    "\n",
    "# Create the data directory tree adjacent to the notebook and suppress errors if the directory already exists\n",
    "# Create a directory each for the images and their corresponding depth maps\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "Path(DATA_DIR / 'image').mkdir(exist_ok=True)\n",
    "Path(DATA_DIR / 'sparse_depth').mkdir(exist_ok=True)\n",
    "\n",
    "# Download the dummy image and its depth scale from the dev branch of a fork\n",
    "# On the fly download is being done to avoid unnecessary memory/data load during testing and \n",
    "# creation of PRs\n",
    "download_file('https://github.com/pronoym99/openvino_notebooks/blob/dev/notebooks/243-depth-estimation-videpth/data/image/dummy_img.png?raw=true', directory=Path(DATA_DIR / 'image'), silent=True)\n",
    "download_file('https://github.com/pronoym99/openvino_notebooks/blob/dev/notebooks/243-depth-estimation-videpth/data/sparse_depth/dummy_depth.png?raw=true', directory=Path(DATA_DIR / 'sparse_depth'), silent=True)\n",
    "\n",
    "# Load the dummy image and its depth scale\n",
    "dummy_input = data_loader.load_input_image('data/image/dummy_img.png')\n",
    "dummy_depth = data_loader.load_sparse_depth('data/sparse_depth/dummy_depth.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102c2ddc-a3d2-47c3-8e1f-88ef5d9d7e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_image_for_depth(input_image: np.ndarray, depth_model_transform: np.ndarray, device: torch.device = 'cpu') -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Transform the input_image for processing by a PyTorch depth estimation model\n",
    "\n",
    "    :param: input_image: The input image obtained as a result of data_loader.load_input_image\n",
    "    :param: depth_model_transform: The transformed depth model\n",
    "    :param: device: The device on which the image would be allocated\n",
    "    :returns: The transformed image suitable to be used as an input to the depth estimation model\n",
    "    \"\"\"\n",
    "    input_height, input_width = np.shape(input_image)[:2]\n",
    "        \n",
    "    sample = {'image' : input_image}\n",
    "    sample = depth_model_transform(sample)\n",
    "    im = sample['image'].to(device)\n",
    "    return im.unsqueeze(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f085d4-bfb3-4ea8-8641-f03bf525d195",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform the dummy input image for the depth model\n",
    "transformed_dummy_image = transform_image_for_depth(input_image=dummy_input, depth_model_transform=depth_model_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab725795-45fa-4253-b927-3cdbfd0a4e62",
   "metadata": {},
   "source": [
    "#### Conversion of depth model to OpenVINO™ IR format\n",
    "\n",
    "The OpenVINO™ toolkit doesn't provide any direct method of converting PyTorch models to the intermediate representation format. To have a depth estimation model in the OpenVINO™ IR format and then compile it, we shall follow the following steps:\n",
    "\n",
    "1. Use the `depth_model` callable to our advantage from the *Loading models and checkpoints* stage.\n",
    "2. Export the model to `.onnx` format using the transformed dummy input created earlier.\n",
    "3. Use the serialize function from OpenVINO to create equivalent `.xml` and `.bin` files and obtain compiled models in the same step. Alternatively serialization procedure may be avoided and compiled model may be obtained by directly using OpenVINO's `compile` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01880b-1e92-4873-99a5-9a2dc53967e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model to switch some operations from training mode to inference.\n",
    "depth_model.eval()\n",
    "\n",
    "# Call the export function via the transformed dummy image obtained from the \n",
    "# previous step. It is absolutely not a case of worry if multiple warnings pop up \n",
    "# in this step. They can be safely ignored.\n",
    "torch.onnx.export(model=depth_model, args=(transformed_dummy_image, ), f=str(MODEL_DIR / 'depth_model.onnx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960d51a2-e918-49a2-b607-41442cc7f987",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Compilation of depth model\n",
    "\n",
    "Now we can go ahead and compile our depth models from the `.onnx` file path. We will not perform serialization because we don't plan to re-read the file again within this tutorial. Therefore we will use the compiled depth estimation model as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efafdcb-0624-4306-86a6-d94490cb5a25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize OpenVINO Runtime.\n",
    "ie_core = Core()\n",
    "depth_model = ie_core.read_model(MODEL_DIR / 'depth_model.onnx')\n",
    "\n",
    "# In the situation where you are unaware of the correct device to compile your\n",
    "# model in, just set device_name='AUTO' and let OpenVINO decide for you\n",
    "compiled_depth_model = ie_core.compile_model(model=depth_model, device_name='AUTO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c18cb1a-c6df-45bd-bb79-76b5f2c87f62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_depth_model(input_image_h: int, input_image_w: int, \n",
    "                    transformed_image: torch.Tensor, compiled_depth_model: type_compiled_model) -> np.ndarray:\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the compiled_depth_model on the transformed_image of dimensions \n",
    "    input_image_w x input_image_h \n",
    "\n",
    "    :param: input_image_h: The height of the input image \n",
    "    :param: input_image_w: The width of the input image \n",
    "    :param: transformed_image: The transformed image suitable to be used as an input to the depth estimation model\n",
    "    :returns:\n",
    "             depth_pred: The depth prediction on the image as an np.ndarray type \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain the last output layer separately\n",
    "    output_layer_depth_model = compiled_depth_model.output(0)    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Perform computation like a standard OpenVINO compiled model\n",
    "        depth_pred = torch.from_numpy(compiled_depth_model([transformed_image])[output_layer_depth_model])\n",
    "        depth_pred = (\n",
    "            torch.nn.functional.interpolate(\n",
    "                depth_pred.unsqueeze(1),\n",
    "                size=(input_image_h, input_image_w),\n",
    "                mode='bicubic',\n",
    "                align_corners=False,\n",
    "            )\n",
    "            .squeeze()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "    \n",
    "    return depth_pred    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164798b4-bdc1-496a-9d01-532191081b96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the compiled depth model using the dummy input \n",
    "# It will be used to compute the metrics associated with the ScaleMapLearner model\n",
    "# and hence obtain a compiled version of the same later\n",
    "depth_pred_dummy = run_depth_model(input_image_h=IMAGE_H, input_image_w=IMAGE_W,\n",
    "                                   transformed_image=transformed_dummy_image, compiled_depth_model=compiled_depth_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9946594b-6990-4511-abca-8859af7cbd2a",
   "metadata": {},
   "source": [
    "##### Computation of scale and shift parameters\n",
    "\n",
    "Computation of these parameters required the depth estimation model output from the previous step. These are the regression based parameters the ScaleMapLearner model deals with. An utility function for the purpose has already been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f964685-e0bf-4428-aaa6-c5ea716efdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_scale_and_shift(input_sparse_depth: np.ndarray, validity_map: Optional[np.ndarray], \n",
    "                                   depth_pred: np.ndarray,\n",
    "                                   min_pred: float = 0.1, max_pred: float = 8.0,\n",
    "                                   min_depth: float = 0.2, max_depth: float = 5.0) -> Tuple[np.ndarray, np.ndarray]:    \n",
    "    \n",
    "    \"\"\"\n",
    "    Compute the global scale and shift alignment required for SML model to work on\n",
    "    with the input_sparse_depth map being provided and the depth estimation output depth_pred\n",
    "    being provided with an optional validity_map\n",
    "\n",
    "    :param: input_sparse_depth: The depth map of the input image \n",
    "    :param: validity_map: An optional depth map associated with the original input image \n",
    "    :param: depth_pred: The depth estimate obtained after running the depth model on the input image\n",
    "    :param: min_pred: Lower bound for predicted depth values \n",
    "    :param: max_pred: Upper bound for predicted depth values \n",
    "    :param: min_depth: Min valid depth when evaluating\n",
    "    :param: max_depth: Max valid depth when evaluating\n",
    "    :returns:\n",
    "             int_depth: The depth estimate for the SML regression model\n",
    "             int_scales: The scale to be used for the SML regression model\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    input_sparse_depth_valid = (input_sparse_depth < max_depth) * (input_sparse_depth > min_depth)\n",
    "    if validity_map is not None:\n",
    "        input_sparse_depth_valid *= validity_map.astype(np.bool)\n",
    "\n",
    "    input_sparse_depth_valid = input_sparse_depth_valid.astype(bool)\n",
    "    input_sparse_depth[~input_sparse_depth_valid] = np.inf  # set invalid depth\n",
    "    input_sparse_depth = 1.0 / input_sparse_depth    \n",
    "    \n",
    "    # global scale and shift alignment\n",
    "    GlobalAlignment = LeastSquaresEstimator(\n",
    "        estimate=depth_pred,\n",
    "        target=input_sparse_depth,\n",
    "        valid=input_sparse_depth_valid\n",
    "    )\n",
    "    GlobalAlignment.compute_scale_and_shift()\n",
    "    GlobalAlignment.apply_scale_and_shift()\n",
    "    GlobalAlignment.clamp_min_max(clamp_min=min_pred, clamp_max=max_pred)\n",
    "    int_depth = GlobalAlignment.output.astype(np.float32)    \n",
    "\n",
    "    # interpolation of scale map\n",
    "    assert (np.sum(input_sparse_depth_valid) >= 3), 'not enough valid sparse points'    \n",
    "    ScaleMapInterpolator = Interpolator2D(\n",
    "        pred_inv=int_depth,\n",
    "        sparse_depth_inv=input_sparse_depth,\n",
    "        valid=input_sparse_depth_valid,\n",
    "    )\n",
    "    ScaleMapInterpolator.generate_interpolated_scale_map(\n",
    "        interpolate_method='linear', \n",
    "        fill_corners=False\n",
    "    )\n",
    "    \n",
    "    int_scales = ScaleMapInterpolator.interpolated_scale_map.astype(np.float32)\n",
    "    int_scales = utils.normalize_unit_range(int_scales)\n",
    "    \n",
    "    return int_depth, int_scales    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19daea36-177f-431d-b8eb-e303d4ee0ddb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Call the function on the dummy depth map we loaded in the dummy_depth variable\n",
    "# with all default settings and store in appropriate variables\n",
    "d_depth, d_scales = compute_global_scale_and_shift(input_sparse_depth=dummy_depth, validity_map=None, depth_pred=depth_pred_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5e094d-2baa-4a99-9cb8-60ca6aa310f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_image_for_depth_scale(input_image: np.ndarray, scale_map_learner_transform: type_transform_compose, \n",
    "                                    int_depth: np.ndarray, int_scales: np.ndarray, \n",
    "                                    device: torch.device = 'cpu') -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Transform the input_image for processing by a PyTorch SML model\n",
    "\n",
    "    :param: input_image: The input image obtained as a result of data_loader.load_input_image\n",
    "    :param: scale_map_learner_transform: The transformed SML model\n",
    "    :param: int_depth: The depth estimate for the SML regression model\n",
    "    :param: int_scales: he scale to be used for the SML regression model\n",
    "    :param: device: The device on which the image would be allocated\n",
    "    :returns: The transformed tensor inputs suitable to be used with an SML model\n",
    "    \"\"\"\n",
    "    \n",
    "    sample = {'image' : input_image, 'int_depth' : int_depth, 'int_scales' : int_scales, 'int_depth_no_tf' : int_depth}\n",
    "    sample = scale_map_learner_transform(sample)\n",
    "    x = torch.cat([sample['int_depth'], sample['int_scales']], 0)\n",
    "    x = x.to(device)\n",
    "    d = sample['int_depth_no_tf'].to(device)\n",
    "    \n",
    "    return x.unsqueeze(0), d.unsqueeze(0)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e5a65b-f332-4b35-a005-58ee7a780732",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Transform the dummy input image for the ScaleMapLearner model\n",
    "# Note that this will lead to a tuple as an output. Both the elements\n",
    "# which is fed to ScaleMapLearner during the conversion process to onxx\n",
    "transformed_dummy_image_scale = transform_image_for_depth_scale(input_image=dummy_input,\n",
    "                                                                scale_map_learner_transform=scale_map_learner_transform,\n",
    "                                                                int_depth=d_depth, int_scales=d_scales)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d08c4-0ccf-428e-ac83-bc2ee9835efc",
   "metadata": {},
   "source": [
    "#### Conversion of ScaleMApLearner model to OpenVINO™ IR format\n",
    "\n",
    "The OpenVINO™ toolkit doesn't provide any direct method of converting PyTorch models to the intermediate representation format. To have the associated ScaleMapLearner in the OpenVINO™ IR format and then compile it, we shall follow the following steps:\n",
    "\n",
    "1. Load the model in memory via instantiating the `modules.midas.midas_net_custom.MidasNet_small_videpth` class and passing the downloaded checkpoint earlier as an argument.\n",
    "2. Export the model to `.onnx` format using the transformed dummy inputs created earlier.\n",
    "3. Use the serialize function from OpenVINO to create equivalent `.xml` and `.bin` files and obtain compiled models in the same step. Alternatively serialization procedure may be avoided and compiled model may be obtained by directly using OpenVINO's `compile` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b5fba3-9d2d-4499-940c-8e3e196e5d4d",
   "metadata": {},
   "source": [
    "If the name of the `.ckpt` file is too much to handle, here is the common format of all checkpoint files from the model releases.\n",
    "\n",
    "> - sml_model.dpredictor.\\<DEPTH_PREDICTOR\\>.nsamples.\\<NSAMPLES\\>.ckpt\n",
    "> - Replace \\<DEPTH_PREDICTOR\\> and \\<NSAMPLES\\> with the depth estimation model name and the no. of levels of depth density the SML model has been trained on\n",
    "> - E.g. sml_model.dpredictor.dpt_hybrid.nsamples.500.ckpt will be the file name corresponding to the SML model based on the dpt_hybrid depth predictor and has been trained on 500 points of the density level on the depth map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af78cb35-fcc9-45d2-ae94-f56f50ff599d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run with the same min_pred and max_pred arguments which were used to compute\n",
    "# global scale and shift alignment\n",
    "scale_map_learner = MidasNet_small_videpth(path=str(MODEL_DIR / 'sml_model.dpredictor.midas_small.nsamples.1500.ckpt'),\n",
    "                                           min_pred=0.1, max_pred=8.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243b664-11ed-4beb-a6af-a5e907a09938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As usual, since the MidasNet_small_videpthc class internally downloads a repo again from torch hub\n",
    "# we shall clean the same since the model callable is now available to us\n",
    "# Remove unnecessary directories and files and suppress errors(if any)\n",
    "rmtree(path=str(MODEL_DIR / 'rwightman_gen-efficientnet-pytorch_master'), ignore_errors=True)\n",
    "\n",
    "# Check for the existence of the trusted list file and then remove\n",
    "list_file = Path(MODEL_DIR / 'trusted_list')\n",
    "if list_file.is_file():\n",
    "    list_file.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d5ca7f-d9b3-4636-ae67-47dd1a56b4db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the model to switch some operations from training mode to inference.\n",
    "scale_map_learner.eval()\n",
    "\n",
    "# Store the tuple of dummy variables into separate variables for easier reference\n",
    "x_dummy, d_dummy = transformed_dummy_image_scale\n",
    "\n",
    "# Call the export function via the transformed dummy image obtained from the \n",
    "# earlier steps. It is absolutely not a case of worry if multiple warnings pop up \n",
    "# in this step. They can be safely ignored.\n",
    "torch.onnx.export(model=scale_map_learner, args=(x_dummy, d_dummy), f=str(MODEL_DIR / 'scale_map_learner.onnx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783f5ad-78ff-4beb-9386-db55811220c0",
   "metadata": {},
   "source": [
    "##### Compilation of the ScaleMapLearner(SML) model\n",
    "\n",
    "Now we can go ahead and compile our SML model from the `.onnx` file path. We will not perform serialization because we don't plan to re-read the file again within this tutorial. Therefore we will use the compiled SML model as it is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6d8473-3f33-414f-8a22-f3cfb3895fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scale_map_learner = ie_core.read_model(MODEL_DIR / 'scale_map_learner.onnx')\n",
    "\n",
    "# In the situation where you are unaware of the correct device to compile your\n",
    "# model in, just set device_name='AUTO' and let OpenVINO decide for you\n",
    "compiled_scale_map_learner = ie_core.compile_model(model=scale_map_learner, device_name='AUTO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0147c65-3630-4990-8cc5-77aadc1a13bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_depth_scale_model(input_image_h: int, input_image_w: int, \n",
    "                          transformed_image_for_depth_scale: Tuple[torch.Tensor, torch.Tensor],\n",
    "                          compiled_scale_map_learner: type_compiled_model) -> np.ndarray:\n",
    "    \n",
    "    \"\"\"\n",
    "    Run the compiled_scale_map_learner on the transformed image of dimensions \n",
    "    input_image_w x input_image_h suitable to be used on such a model\n",
    "\n",
    "    :param: input_image_h: The height of the input image \n",
    "    :param: input_image_w: The width of the input image \n",
    "    :param: transformed_image_for_depth_scale: The transformed image inputs suitable to be used as an input to the SML model\n",
    "    :returns:\n",
    "             sml_pred: The regression based prediction of the SML model \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtain the last output layer separately\n",
    "    output_layer_scale_map_learner = compiled_scale_map_learner.output(0)\n",
    "    x_transform, d_transform = transformed_image_for_depth_scale\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Perform computation like a standard OpenVINO compiled model\n",
    "        sml_pred = torch.from_numpy(compiled_scale_map_learner([x_transform, d_transform])[output_layer_scale_map_learner])\n",
    "        sml_pred = (\n",
    "            torch.nn.functional.interpolate(\n",
    "                sml_pred,\n",
    "                size=(input_image_h, input_image_w),\n",
    "                mode='bicubic',\n",
    "                align_corners=False,\n",
    "            )\n",
    "            .squeeze()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        \n",
    "    return sml_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4dec16-eb27-4507-b8ad-6c4da243fdc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run the compiled SML model using the set of dummy inputs \n",
    "sml_pred_dummy = run_depth_scale_model(input_image_h=IMAGE_H, input_image_w=IMAGE_W,\n",
    "                                       transformed_image_for_depth_scale=transformed_dummy_image_scale,\n",
    "                                       compiled_scale_map_learner=compiled_scale_map_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b870c7b4-d3d0-4eea-86dd-21bd21567a1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Storing and visualizing dummy results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f11a30b-d33d-4111-aa47-4988700a9232",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Base directory in which outputs would be stored as a pathlib.Path variable\n",
    "OUTPUT_DIR = Path('output')\n",
    "\n",
    "# Create the output directory adjacent to the notebook and suppress errors if the directory already exists\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Utility functions are directly available in modules.midas.utils\n",
    "# Provide path names without any extension and let the write_depth\n",
    "# function provide them for you. Take note of the arguments.\n",
    "utils.write_depth(path=str(OUTPUT_DIR / 'dummy_input'), depth=d_depth, bits=2)\n",
    "utils.write_depth(path=str(OUTPUT_DIR / 'dummy_input_sml'), depth=sml_pred_dummy, bits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97632703-0429-4fd4-87cf-4fc03c593da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "img_dummy_in = mpimg.imread('data/image/dummy_img.png')\n",
    "img_dummy_out = mpimg.imread(OUTPUT_DIR / 'dummy_input.png')\n",
    "img_dummy_sml_out = mpimg.imread(OUTPUT_DIR / 'dummy_input_sml.png')\n",
    "\n",
    "f, axes = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(right=2.0)\n",
    "\n",
    "axes[0].imshow(img_dummy_in)\n",
    "axes[1].imshow(img_dummy_out)\n",
    "axes[2].imshow(img_dummy_sml_out)\n",
    "\n",
    "axes[0].set_title('dummy input')\n",
    "axes[1].set_title('depth prediction on dummy input')\n",
    "axes[2].set_title('SML on depth estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a79770e-fd46-4719-a240-065c8add229d",
   "metadata": {},
   "source": [
    "### Running inference on a test image\n",
    "\n",
    "Now role of both the dummy inputs i.e. the dummy image as well as its associated depth map is now over. Since we have access to the compiled models now, we can load the *one* image available to us for pure inferencing purposes and run all the above steps one by one till plotting of the depth map.\n",
    "\n",
    "If you haven't noticed already the data directory of this tutorial has been arranged as follows. This allows us to comply to these [rules](https://github.com/pronoym99/openvino_notebooks/blob/main/CONTRIBUTING.md#file-structure).\n",
    "\n",
    " ```bash\n",
    "    data\n",
    "    ├── image                   \n",
    "    │   ├── dummy_img.png       # RGB images\n",
    "    │   └── <timestamp>.png\n",
    "    └── sparse_depth            \n",
    "        ├── dummy_img.png       # sparse metric depth maps\n",
    "        └── <timestamp>.png     # as 16b PNG files\n",
    " ```\n",
    " \n",
    "At the same time, the depth storage method [used in the VOID dataset](https://github.com/alexklwong/void-dataset/blob/master/src/data_utils.py) is assumed. \n",
    "\n",
    "If you are thinking of the file name format of the image for inference, here is the reasoning.\n",
    " \n",
    "The dataset was collected using the Intel [RealSense D435i camera](https://realsense.intel.com/depth-camera), which was configured to produce synchronized accelerometer and gyroscope measurements at 400 Hz, along with synchronized VGA-size (640 x 480) RGB and depth streams at 30 Hz. The depth frames are acquired using active stereo and is aligned to the RGB frame using the sensor factory calibration. The frequency of sensor and depth stream input run at certain fixed frequencies and hence time stamping every frame captured is beneficial for maintaining structure as well as for debugging purposes later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a97b51-5d3e-4903-8c73-53aa881d7c35",
   "metadata": {},
   "source": [
    "*The image for inference and it sparse depth map is taken from the compressed dataset present [here](https://drive.google.com/uc?id=1bbN46kR_hcH3GG8-jGRqAI433uddYrnc)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e669615-733b-491e-9a79-48472ca5448f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# As before download the sample images for inference from the dedicated dev branch\n",
    "download_file('https://github.com/pronoym99/openvino_notebooks/blob/dev/notebooks/243-depth-estimation-videpth/data/image/1552097950.2672.png?raw=true', directory=Path(DATA_DIR / 'image'), silent=True)\n",
    "download_file('https://github.com/pronoym99/openvino_notebooks/blob/dev/notebooks/243-depth-estimation-videpth/data/sparse_depth/1552097950.2672.png?raw=true', directory=Path(DATA_DIR / 'sparse_depth'), silent=True)\n",
    "\n",
    "# Load the image and its depth scale  \n",
    "img_input = data_loader.load_input_image('data/image/1552097950.2672.png')\n",
    "img_depth_input = data_loader.load_sparse_depth('data/sparse_depth/1552097950.2672.png')\n",
    "\n",
    "# Transform the input image for the depth model\n",
    "transformed_image = transform_image_for_depth(input_image=img_input, depth_model_transform=depth_model_transform)\n",
    "\n",
    "# Run the depth model on the transformed input\n",
    "depth_pred = run_depth_model(input_image_h=IMAGE_H, input_image_w=IMAGE_W,\n",
    "                             transformed_image=transformed_image, compiled_depth_model=compiled_depth_model)\n",
    "\n",
    "\n",
    "# Call the function on the sparse depth map\n",
    "# with all default settings and store in appropriate variables\n",
    "int_depth, int_scales = compute_global_scale_and_shift(input_sparse_depth=img_depth_input, validity_map=None, depth_pred=depth_pred)\n",
    "\n",
    "# Transform the input image for the ScaleMapLearner model\n",
    "transformed_image_scale = transform_image_for_depth_scale(input_image=img_input,\n",
    "                                                          scale_map_learner_transform=scale_map_learner_transform,\n",
    "                                                          int_depth=int_depth, int_scales=int_scales)\n",
    "\n",
    "# Run the SML model using the set of inputs \n",
    "sml_pred = run_depth_scale_model(input_image_h=IMAGE_H, input_image_w=IMAGE_W,\n",
    "                                 transformed_image_for_depth_scale=transformed_image_scale,\n",
    "                                 compiled_scale_map_learner=compiled_scale_map_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102713e-fd0b-4e08-858b-a3d7fd50d51f",
   "metadata": {},
   "source": [
    "### Store and visualize Inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e284b-b681-4a82-8aa2-0687a4e99dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Store the depth and SML predictions\n",
    "utils.write_depth(path=str(OUTPUT_DIR / '1552097950.2672'), depth=int_depth, bits=2)\n",
    "utils.write_depth(path=str(OUTPUT_DIR / '1552097950.2672_sml'), depth=sml_pred, bits=2)\n",
    "\n",
    "\n",
    "# Display result\n",
    "plt.figure()\n",
    "\n",
    "img_in = mpimg.imread('data/image/1552097950.2672.png')\n",
    "img_out = mpimg.imread(OUTPUT_DIR / '1552097950.2672.png')\n",
    "img_sml_out = mpimg.imread(OUTPUT_DIR / '1552097950.2672_sml.png')\n",
    "\n",
    "f, axes = plt.subplots(1, 3)\n",
    "plt.subplots_adjust(right=2.0)\n",
    "\n",
    "axes[0].imshow(img_in)\n",
    "axes[1].imshow(img_out)\n",
    "axes[2].imshow(img_sml_out)\n",
    "\n",
    "axes[0].set_title('Input image')\n",
    "axes[1].set_title('Depth prediction on input')\n",
    "axes[2].set_title('SML on depth estimate')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3825aa74-b825-4502-999f-21f46bf86c50",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Cleaning up the data directory\n",
    "\n",
    "We will _follow suit_ for the directory in which we downloaded images and depth maps from another repo. We shall move remove the unnecessary directories and files which were created during the download process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8e5c09-f3e6-489d-96d1-5c90fe5a91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the data directory and suppress errors(if any)\n",
    "rmtree(path=str(DATA_DIR), ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19806ccf-34f5-44c5-8b57-ec000f558601",
   "metadata": {},
   "source": [
    "### Concluding notes\n",
    "\n",
    "> 1. The code for this tutorial is adapted from the [VI-Depth repository](https://github.com/isl-org/VI-Depth).\n",
    "> 2. Users may choose to download the original and raw datasets from the [VOID dataset](https://github.com/alexklwong/void-dataset/).\n",
    "> 3. The [isl-org/VI-Depth](https://github.com/isl-org/VI-Depth) works on a slightly older version of released model assets from its [MiDaS sibling repository](https://github.com/isl-org/MiDaS). However, the new releases beginning from [v3.1](https://github.com/isl-org/MiDaS/releases/tag/v3_1) directly have OpenVINO™ `.xml` and `.bin` model files as their assets thereby rendering the **major pre-processing and model compilation step irrelevant**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
