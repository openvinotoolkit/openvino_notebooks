{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and Optimize YOLOv7 with OpenVINO‚Ñ¢\n",
    "\n",
    "The YOLOv7 algorithm is making big waves in the computer vision and machine learning communities.\n",
    "It is real-time object detection algorithm that performs image recognition tasks by taking an image as input \n",
    "and then predicting bounding boxes and class probabilities for each object in the image.\n",
    "\n",
    "YOLO stands for ‚ÄúYou Only Look Once‚Äù, it is a popular family of real-time object detection algorithms. \n",
    "The original YOLO object detector was first released in 2016. Since then, different versions and variants of YOLO have been proposed, each providing a significant increase in performance and efficiency.\n",
    "YOLOv7 is next stage of evalution of YOLO models family which provides a greatly improved real-time object detection accuracy without increasing the inference costs.\n",
    "More details about its realization can be found in original model [paper](https://arxiv.org/abs/2207.02696) and [repository](https://github.com/WongKinYiu/yolov7)\n",
    "\n",
    "Real-time object detection is often used as a key component in computer vision systems. \n",
    "Applications that use real-time object detection models include video analytics, robotics, autonomous vehicles, multi-object tracking and object counting, medical image analysis, and many others.\n",
    "\n",
    "This short tutorial demonstrates step-by-step instruction how to convert Pytorch Yolo V7 to OpenVINO IR and optimize it using NNCF PTQ API.\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Prepare PyTorch model\n",
    "- Download and prepare dataset\n",
    "- Validate original model\n",
    "- Convert PyTorch model to ONNX\n",
    "- Convert ONNX model to OpenVINO IR\n",
    "- Validate converted model\n",
    "- Prepare and run optimization pipeline\n",
    "- Compare accuracy of the FP32 and quantized models.\n",
    "- Compare performance of the FP32 and quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pytorch model\n",
    "\n",
    "Generally, PyTorch model represents instance of torch.nn.Module class, iniatilized by state dictionary with model weights.\n",
    "We will use YOLOv7 tiny model pretrained on COCO dataset, which available in this [repo](https://github.com/WongKinYiu/yolov7).\n",
    "Typical steps for getting pretrained model:\n",
    "1. Create instance of model class\n",
    "2. Load checkpoint state dict, which contains pretrained model weights\n",
    "3. Turn model to evaluation for switching some operations to inference mode\n",
    "\n",
    "In our case, model authors already provide tool which allow to convert model to ONNX, so it is not necessary to do these steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download YOLOv7 code\n",
    "if not Path('yolov7').exists():\n",
    "    !git clone https://github.com/WongKinYiu/yolov7\n",
    "%cd yolov7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model weights\n",
    "MODEL_LINK = \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\"\n",
    "DATA_DIR = Path(\"data/\")\n",
    "MODEL_DIR = Path(\"model/\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(MODEL_LINK, directory=MODEL_DIR, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model inference\n",
    "\n",
    "`detect.py` script run pytorch model inference and save image as result,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights model/yolov7-tiny.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction result\n",
    "from PIL import Image\n",
    "Image.open('runs/detect/exp/horses.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain ONNX model, we will use `export.py` script. Let's check it's arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python export.py - -help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important parameters:\n",
    "* `--weights` - path to model weigths checkpoint\n",
    "* `--img-size` - size of input image for onnx tracing\n",
    "\n",
    "As ONNX is less flexible format then PyTorch, there is also opportunity to setup configurable parameters for results postprocessing included in model:\n",
    "* `--end2end` - export full model to onnx including postporcessing\n",
    "* `--grid` - export Detect layer as part of model\n",
    "* `--topk-all` - topk elements for all images\n",
    "* `--iou-thres` - intersection over union threshold for NMS\n",
    "* `--conf-thres` - minimal confidence threshold\n",
    "* `--max-wh` - max bounding box width and height for NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import onnx_graphsurgeon failure: No module named 'onnx_graphsurgeon'\n",
      "Namespace(batch_size=1, conf_thres=0.25, device='cpu', dynamic=False, dynamic_batch=False, end2end=False, fp16=False, grid=True, img_size=[640, 640], include_nms=False, int8=False, iou_thres=0.45, max_wh=None, simplify=False, topk_all=100, weights='model/yolov7-tiny.pt')\n",
      "YOLOR üöÄ v0.1-115-g072f76c torch 1.13.0+cu117 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model Summary: 200 layers, 6219709 parameters, 6219709 gradients\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "\n",
      "Starting TorchScript export with torch 1.13.0+cu117...\n",
      "/home/ea/work/openvino_notebooks/notebooks/225-yolov7-optimization/yolov7/models/yolo.py:52: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if self.grid[i].shape[2:4] != x[i].shape[2:4]:\n",
      "TorchScript export success, saved as model/yolov7-tiny.torchscript.pt\n",
      "CoreML export failure: No module named 'coremltools'\n",
      "\n",
      "Starting TorchScript-Lite export with torch 1.13.0+cu117...\n",
      "TorchScript-Lite export success, saved as model/yolov7-tiny.torchscript.ptl\n",
      "\n",
      "Starting ONNX export with onnx 1.11.0...\n",
      "/home/ea/work/openvino_notebooks/notebooks/225-yolov7-optimization/yolov7/models/yolo.py:582: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if augment:\n",
      "/home/ea/work/openvino_notebooks/notebooks/225-yolov7-optimization/yolov7/models/yolo.py:614: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "/home/ea/work/openvino_notebooks/notebooks/225-yolov7-optimization/yolov7/models/yolo.py:629: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if profile:\n",
      "ONNX export success, saved as model/yolov7-tiny.onnx\n",
      "\n",
      "Export complete (2.64s). Visualize with https://github.com/lutzroeder/netron.\n"
     ]
    }
   ],
   "source": [
    "!python export.py --weights model/yolov7-tiny.pt --grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO Intermideate Representation\n",
    "While ONNX models are directly supported by OpenVINO‚Ñ¢, it can be useful to convert them to IR format to take advantage of OpenVINO optimization tools and features.\n",
    "`mo.convert` function can be used for converting model using OpenVINO Model Optimizer capabilities. \n",
    "It returns of instance OpenVINO Model class, which is ready to use in python interface and can be serialized to IR for future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "model = mo.convert(input_model='model/yolov7-tiny.onnx')\n",
    "# serialize model for saving IR\n",
    "serialize(model, 'model/yolov7-tiny.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.dataset import letterbox\n",
    "from detect import non_maximum_supression\n",
    "from utils.general import scale_coords\n",
    "from utils.plots import plot_one_box\n",
    "\n",
    "def preprocess_image(img0):\n",
    "    img = letterbox(img, img_size, stride)\n",
    "    \n",
    "    # Convert\n",
    "    img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
    "    img = np.ascontiguousarray(img)\n",
    "    return img, img0\n",
    "\n",
    "\n",
    "def prepare_input_tensor(image):\n",
    "    img = torch.from_numpy(image)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "         'hair drier', 'toothbrush']\n",
    "colors = {name: [np.random.randint(0, 255) for _ in range(3)]\n",
    "          for i, name in enumerate(names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(output, dwdh, ratio, flatten=False):\n",
    "    batch_ids = int(np.max(output[:, 0]))\n",
    "    total_boxes, total_labels, total_scores = [], [], []\n",
    "    total_predictions = []\n",
    "    for batch_id in range(batch_ids + 1):\n",
    "        boxes, scores, labels = [], [], []\n",
    "        predictions = []\n",
    "        batch_elem_out = output[output[:, 0] == batch_id]\n",
    "        for (_, x0, y0, x1, y1, cls_id, score) in output:\n",
    "            box = np.array([x0,y0,x1,y1])\n",
    "            box -= np.array(dwdh*2)\n",
    "            box /= ratio\n",
    "            score = float(score)\n",
    "            if not flatten:\n",
    "                boxes.append(box)\n",
    "                scores.append(score)\n",
    "                labels.append(cls_id)\n",
    "            else:\n",
    "                predictions.append(np.array([*box, score, cls_id]))\n",
    "        total_boxes.append(boxes)\n",
    "        total_labels.append(labels)\n",
    "        total_scores.append(scores)\n",
    "        total_predictions.append(predictions)\n",
    "    if not flatten:\n",
    "        return total_boxes, total_scores, total_labels\n",
    "    return total_predictions\n",
    "\n",
    "\n",
    "def draw_boxes(images, total_boxes, total_scores, total_labels):\n",
    "    images_with_boxes = []\n",
    "    for img, boxes, scores, labels in zip(images, total_boxes, total_scores, total_labels):\n",
    "        if not boxes:\n",
    "            continue\n",
    "        for box, score, cls_id in zip(boxes, scores, labels):\n",
    "            box = box.round().astype(np.int32).tolist()\n",
    "            cls_id = int(cls_id)\n",
    "            score = round(float(score),3)\n",
    "            name = names[cls_id]\n",
    "            color = colors[name]\n",
    "            name += ' '+str(score)\n",
    "            img = cv2.rectangle(img, box[:2], box[2:], color, 2)\n",
    "            img = cv2.putText(img, name,(box[0], box[1] - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.75, [225, 255, 255], thickness=2)\n",
    "        images_with_boxes.append(img)\n",
    "        return images_with_boxes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "core = Core()\n",
    "# read converted model\n",
    "model = core.read_model('model/yolov7-tiny.xml')\n",
    "# load model on CPU device\n",
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "# prepare output blob for getting results\n",
    "output = compiled_model.output(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data example\n",
    "img = cv2.imread('inference/images/horses.jpg')\n",
    "# preprocess input data\n",
    "im, ratio, dwdh = preprocess_image(img)\n",
    "# run inference and got result for desired output\n",
    "result = compiled_model([im])[output]\n",
    "# postprocess result\n",
    "boxes, scores, labels = get_boxes(result, dwdh, ratio)\n",
    "# draw boxes on image\n",
    "images_with_boxes = draw_boxes([img], boxes, scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results\n",
    "Image.fromarray(images_with_boxes[0][:, :, ::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "\n",
    "Yolo V7 tiny pretrained on COCO dataset, in order to evaluate model accuracy we need to download it. According to instruction, provided in model repo, we also need to download annotation in prepared by model author format for using original model evaluation scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "sys.path.append(\"../../utils\")\n",
    "from notebook_utils import download_file\n",
    "\n",
    "data_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "labels_url = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip\"\n",
    "\n",
    "out_dir = Path('.')\n",
    "\n",
    "download_file(data_url, directory=out_dir, show_progress=True)\n",
    "download_file(labels_url, directory=out_dir, show_progress=True)\n",
    "with ZipFile('coco2017labels-segments.zip' , \"r\") as zip_ref:\n",
    "    zip_ref.extractall(out_dir)\n",
    "with ZipFile('val2017.zip' , \"r\") as zip_ref:\n",
    "    zip_ref.extractall(out_dir / 'coco/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import yaml\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import (\n",
    "    coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements,\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
    ")\n",
    "\n",
    "# read dataset config\n",
    "data = 'data/coco.yaml'\n",
    "with open(data) as f:\n",
    "    data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "# Dataloader\n",
    "task = 'val'  # path to train/val/test images\n",
    "Option = namedtuple('Options', ['single_cls']) # imitation of commandline provided options for single class evaluation\n",
    "opt = Option(False)\n",
    "dataloader = create_dataloader(data[task], 640, 1, 32, opt, pad=0.5, rect=True,\n",
    "                                       prefix=colorstr(f'{task}: '))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define validation function\n",
    "\n",
    "We will reuse validation metrics provided in model repo with adoption to our case (removing extra steps). Original model evaluation procedure can be found in this [file](https://github.com/WongKinYiu/yolov7/blob/main/test.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from utils.metrics import ap_per_class, ConfusionMatrix\n",
    "\n",
    "\n",
    "# test function        \n",
    "def test(data,\n",
    "         model,\n",
    "         dataloader,\n",
    "         single_cls=False,\n",
    "         save_dir=Path(''),  # for saving images\n",
    "         is_coco=False,\n",
    "         v5_metric=False):\n",
    "\n",
    "    check_dataset(data)  # check\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "    opt = Option(False)\n",
    "    \n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        img, ratio, dwdh = preprocess_image(np.transpose(img[0].numpy(), (1, 2, 0)))\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "        # Run model\n",
    "        out = model(img)[output]  # inference outputs\n",
    "        out = get_boxes(out, dwdh, ratio, flatten=True)\n",
    "        out = torch.from_numpy(np.array(out))\n",
    "        targets[:, 2:] *= torch.Tensor([width, height, width, height])  # to pixels\n",
    "        lb = []\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            predn = pred.clone()\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])\n",
    "                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        detected_set = set()\n",
    "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d.item() not in detected_set:\n",
    "                                detected_set.add(d.item())\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if len(detected) == nl:  # all targets already located in image\n",
    "                                    break\n",
    "\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*stats, v5_metric=v5_metric, save_dir=save_dir, names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    return mp, mr, map50, map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(data=data, model=compiled_model, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Postrainging Quantization API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "\n",
    "def transform_fn(data_batch):\n",
    "    img = data_batch[0]\n",
    "    img, _, _ = preprocess_image(np.transpose(img[0].numpy()))\n",
    "    return img\n",
    "\n",
    "quantization_dataset = nncf.Dataset(dataloader, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = nncf.quantize(model, quantization_dataset, preset=nncf.QuantizationPreset.MIXED)\n",
    "\n",
    "serialize(quantized_model, 'model/yolov7-tiny_int8.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Quantized model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_int8_model = core.compile_model(quantized_model, 'CPU')\n",
    "result = compiled_int8_model([im])[compiled_int8_model.output(0)]\n",
    "boxes, scores, labels = get_boxes(result, dwdh, ratio)\n",
    "images_with_boxes = draw_boxes([img.copy()], boxes, scores, labels)\n",
    "Image.fromarray(images_with_boxes[0][:, :, ::-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate quantized model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_result = test(data=data, model=compiled_model, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "Finally, use [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance of the `FP16` and `INT8` models.\n",
    "\n",
    "> NOTE: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP16 model (OpenVINO IR)\n",
    "!benchmark_app -m model/yolov7-tiny.xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Inference FP16 model (OpenVINO IR)\n",
    "!benchmark_app -m model/yolov7-tiny_int8.xml -d CPU -api async"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0404472fd7b5b63117a9fa5c50283296e2708c2449c6090d2cdf8903f95897f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
