{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Yolo V7 to ONNX and OpenVINO™ IR\n",
    "\n",
    "This short tutorial demonstrates step-by-step instruction how to convert Pytorch Yolo V7 to OpenVINO IR. \n",
    "\n",
    "This tutorial consists of the following steps:\n",
    "- Prepare PyTorch model\n",
    "- Download and prepare dataset\n",
    "- Validate original model\n",
    "- Convert PyTorch model to ONNX\n",
    "- Convert ONNX model to OpenVINO IR\n",
    "- Validate converted model\n",
    "- Prepare and run optimization pipeline\n",
    "- Compare accuracy of the FP32 and quantized models.\n",
    "- Compare performance of the FP32 and quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pytorch model\n",
    "\n",
    "Generally, PyTorch model represents instance of torch.nn.Module class, iniatilized by state dictionary with model weights.\n",
    "We will use YOLOv7 tiny model pretrained on COCO dataset, which available in this [repo](https://github.com/WongKinYiu/yolov7).\n",
    "Typical steps for getting pretrained model:\n",
    "1. Create instance of model class\n",
    "2. Load checkpoint state dict, which contains pretrained model weights\n",
    "3. Turn model to evaluation for switching some operations to inference mode\n",
    "\n",
    "In our case, model authors already provide tool which allow to convert model to ONNX, so it is not necessary to do these steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download YOLOv7 code\n",
    "!git clone https://github.com/WongKinYiu/yolov7\n",
    "%cd yolov7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model weights\n",
    "MODEL_LINK = \"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\"\n",
    "DATA_DIR = Path(\"data/\")\n",
    "MODEL_DIR = Path(\"model/\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(MODEL_LINK, directory=MODEL_DIR, show_progress=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model inference\n",
    "\n",
    "`detect.py` script run pytorch model inference and save image as result,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python detect.py --weights model/yolov7-tiny.pt --conf 0.25 --img-size 640 --source inference/images/horses.jpg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize prediction result\n",
    "from PIL import Image\n",
    "Image.open('runs/detect/exp/horses.jpg')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain ONNX model, we will use `export.py` script. Let's check it's arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python export.py - -help\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important parameters:\n",
    "* `--weights` - path to model weigths checkpoint\n",
    "* `--img-size` - size of input image for onnx tracing\n",
    "\n",
    "As ONNX is less flexible format then PyTorch, there is also opportunity to setup configurable parameters for results postprocessing included in model:\n",
    "* `--end2end` - export full model to onnx including postporcessing\n",
    "* `--grid` - export Detect layer as part of model\n",
    "* `--topk-all` - topk elements for all images\n",
    "* `--iou-thres` - intersection over union threshold for NMS\n",
    "* `--conf-thres` - minimal confidence threshold\n",
    "* `--max-wh` - max bounding box width and height for NMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python export.py --weights model/yolov7-tiny.pt \\\n",
    "    --grid --end2end --simplify \\\n",
    "    --topk-all 100 --iou-thres 0.65 --conf-thres 0.35 \\\n",
    "    --img-size 640 640 --max-wh 640\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert ONNX Model to OpenVINO Intermideate Representation\n",
    "While ONNX models are directly supported by OpenVINO™, it can be useful to convert them to IR format to take advantage of OpenVINO optimization tools and features.\n",
    "`mo.convert` function can be used for converting model using OpenVINO Model Optimizer capabilities. \n",
    "It returns of instance OpenVINO Model class, which is ready to use in python interface and can be serialized to IR for future execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "model = mo.convert(input_model='model/yolov7-tiny.onnx')\n",
    "# serialize model for saving IR\n",
    "serialize(model, 'model/yolov7-tiny.xml')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# define preprocessing\n",
    "\n",
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - \\\n",
    "        new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right,\n",
    "                            cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, r, (dw, dh)\n",
    "\n",
    "\n",
    "names = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',\n",
    "         'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "         'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee',\n",
    "         'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',\n",
    "         'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',\n",
    "         'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch',\n",
    "         'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "         'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear',\n",
    "         'hair drier', 'toothbrush']\n",
    "colors = {name: [np.random.randint(0, 255) for _ in range(3)]\n",
    "          for i, name in enumerate(names)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxes(output, dwdh, ratio, flatten=False):\n",
    "    batch_ids = int(np.max(output[:, 0]))\n",
    "    total_boxes, total_labels, total_scores = [], [], []\n",
    "    total_predictions = []\n",
    "    for batch_id in range(batch_ids + 1):\n",
    "        boxes, scores, labels = [], [], []\n",
    "        predictions = []\n",
    "        batch_elem_out = output[output[:, 0] == batch_id]\n",
    "        for (_, x0, y0, x1, y1, cls_id, score) in output:\n",
    "            box = np.array([x0,y0,x1,y1])\n",
    "            box -= np.array(dwdh*2)\n",
    "            box /= ratio\n",
    "            score = float(score)\n",
    "            if not flatten:\n",
    "                boxes.append(box)\n",
    "                scores.append(score)\n",
    "                labels.append(cls_id)\n",
    "            else:\n",
    "                predictions.append(np.array([*box, score, cls_id]))\n",
    "        total_boxes.append(boxes)\n",
    "        total_labels.append(labels)\n",
    "        total_scores.append(scores)\n",
    "        total_predictions.append(predictions)\n",
    "    if not flatten:\n",
    "        return total_boxes, total_scores, total_labels\n",
    "    return total_predictions\n",
    "\n",
    "\n",
    "def draw_boxes(images, total_boxes, total_scores, total_labels):\n",
    "    images_with_boxes = []\n",
    "    for img, boxes, scores, labels in zip(images, total_boxes, total_scores, total_labels):\n",
    "        if not boxes:\n",
    "            continue\n",
    "        for box, score, cls_id in zip(boxes, scores, labels):\n",
    "            box = box.round().astype(np.int32).tolist()\n",
    "            cls_id = int(cls_id)\n",
    "            score = round(float(score),3)\n",
    "            name = names[cls_id]\n",
    "            color = colors[name]\n",
    "            name += ' '+str(score)\n",
    "            img = cv2.rectangle(img, box[:2], box[2:], color, 2)\n",
    "            img = cv2.putText(img, name,(box[0], box[1] - 2), cv2.FONT_HERSHEY_SIMPLEX, 0.75, [225, 255, 255], thickness=2)\n",
    "        images_with_boxes.append(img)\n",
    "        return images_with_boxes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img):\n",
    "    # Model expect input with RGB channels order, while opencv reads in BGR, we need to swap channels for getting correct results\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image = img.copy()\n",
    "    # Resize image using letterbox resize approach defined above\n",
    "    image, ratio, dwdh = letterbox(image, auto=False)\n",
    "\n",
    "    # transpose layout from HWC to CHW\n",
    "    image = image.transpose((2, 0, 1))\n",
    "\n",
    "    # expand dims to get batch of images\n",
    "    image = np.expand_dims(image, 0)\n",
    "    image = np.ascontiguousarray(image)\n",
    "\n",
    "    im = image.astype(np.float32)\n",
    "    # normalize image to range [0, 1]\n",
    "    im /= 255\n",
    "    return im, ratio, dwdh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "core = Core()\n",
    "# read converted model\n",
    "model = core.read_model('model/yolov7-tiny.xml')\n",
    "# load model on CPU device\n",
    "compiled_model = core.compile_model(model, 'CPU')\n",
    "# prepare output blob for getting results\n",
    "output = compiled_model.output(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read input data example\n",
    "img = cv2.imread('inference/images/horses.jpg')\n",
    "# preprocess input data\n",
    "im, ratio, dwdh = preprocess_image(img)\n",
    "# run inference and got result for desired output\n",
    "result = compiled_model([im])[output]\n",
    "# postprocess result\n",
    "boxes, scores, labels = get_boxes(result, dwdh, ratio)\n",
    "# draw boxes on image\n",
    "images_with_boxes = draw_boxes([img], boxes, scores, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results\n",
    "Image.fromarray(images_with_boxes[0][:, :, ::-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download dataset\n",
    "\n",
    "Yolo V7 tiny pretrained on COCO dataset, in order to evaluate model accuracy we need to download it. According to instruction, provided in model repo, we also need to download annotation in prepared by model author format for using original model evaluation scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipfile import ZipFile\n",
    "\n",
    "sys.path.append(\"../../utils\")\n",
    "from notebook_utils import download_file\n",
    "\n",
    "data_url = \"http://images.cocodataset.org/zips/val2017.zip\"\n",
    "labels_url = \"https://github.com/ultralytics/yolov5/releases/download/v1.0/coco2017labels-segments.zip\"\n",
    "\n",
    "out_dir = Path('.')\n",
    "\n",
    "download_file(data_url, directory=out_dir, show_progress=True)\n",
    "download_file(labels_url, directory=out_dir, show_progress=True)\n",
    "with ZipFile('coco2017labels-segments.zip' , \"r\") as zip_ref:\n",
    "    zip_ref.extractall(out_dir)\n",
    "with ZipFile('val2017.zip' , \"r\") as zip_ref:\n",
    "    zip_ref.extractall(out_dir / 'coco/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import yaml\n",
    "from utils.datasets import create_dataloader\n",
    "from utils.general import (\n",
    "    coco80_to_coco91_class, check_dataset, check_file, check_img_size, check_requirements,\n",
    "    box_iou, non_max_suppression, scale_coords, xyxy2xywh, xywh2xyxy, set_logging, increment_path, colorstr\n",
    ")\n",
    "\n",
    "# read dataset config\n",
    "data = 'data/coco.yaml'\n",
    "with open(data) as f:\n",
    "    data = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "# Dataloader\n",
    "task = 'val'  # path to train/val/test images\n",
    "Option = namedtuple('Options', ['single_cls']) # imitation of commandline provided options for single class evaluation\n",
    "opt = Option(False)\n",
    "dataloader = create_dataloader(data[task], 640, 1, 32, opt, pad=0.5, rect=True,\n",
    "                                       prefix=colorstr(f'{task}: '))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define validation function\n",
    "\n",
    "We will reuse validation metrics provided in model repo with adoption to our case (removing extra steps). Original model evaluation procedure can be found in this [file](https://github.com/WongKinYiu/yolov7/blob/main/test.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "from utils.metrics import ap_per_class, ConfusionMatrix\n",
    "\n",
    "\n",
    "# test function        \n",
    "def test(data,\n",
    "         model,\n",
    "         dataloader,\n",
    "         single_cls=False,\n",
    "         save_dir=Path(''),  # for saving images\n",
    "         is_coco=False,\n",
    "         v5_metric=False):\n",
    "\n",
    "    check_dataset(data)  # check\n",
    "    nc = 1 if single_cls else int(data['nc'])  # number of classes\n",
    "    iouv = torch.linspace(0.5, 0.95, 10)  # iou vector for mAP@0.5:0.95\n",
    "    niou = iouv.numel()\n",
    "    opt = Option(False)\n",
    "    \n",
    "    seen = 0\n",
    "    confusion_matrix = ConfusionMatrix(nc=nc)\n",
    "    coco91class = coco80_to_coco91_class()\n",
    "    s = ('%20s' + '%12s' * 6) % ('Class', 'Images', 'Labels', 'P', 'R', 'mAP@.5', 'mAP@.5:.95')\n",
    "    p, r, f1, mp, mr, map50, map, t0, t1 = 0., 0., 0., 0., 0., 0., 0., 0., 0.\n",
    "    jdict, stats, ap, ap_class, wandb_images = [], [], [], [], []\n",
    "    for batch_i, (img, targets, paths, shapes) in enumerate(tqdm(dataloader, desc=s)):\n",
    "        img, ratio, dwdh = preprocess_image(np.transpose(img[0].numpy(), (1, 2, 0)))\n",
    "        nb, _, height, width = img.shape  # batch size, channels, height, width\n",
    "        # Run model\n",
    "        out = model(img)[output]  # inference outputs\n",
    "        out = get_boxes(out, dwdh, ratio, flatten=True)\n",
    "        out = torch.from_numpy(np.array(out))\n",
    "        targets[:, 2:] *= torch.Tensor([width, height, width, height])  # to pixels\n",
    "        lb = []\n",
    "\n",
    "        # Statistics per image\n",
    "        for si, pred in enumerate(out):\n",
    "            labels = targets[targets[:, 0] == si, 1:]\n",
    "            nl = len(labels)\n",
    "            tcls = labels[:, 0].tolist() if nl else []  # target class\n",
    "            seen += 1\n",
    "\n",
    "            if len(pred) == 0:\n",
    "                if nl:\n",
    "                    stats.append((torch.zeros(0, niou, dtype=torch.bool), torch.Tensor(), torch.Tensor(), tcls))\n",
    "                continue\n",
    "\n",
    "            # Predictions\n",
    "            predn = pred.clone()\n",
    "\n",
    "            # Assign all predictions as incorrect\n",
    "            correct = torch.zeros(pred.shape[0], niou, dtype=torch.bool)\n",
    "            if nl:\n",
    "                detected = []  # target indices\n",
    "                tcls_tensor = labels[:, 0]\n",
    "\n",
    "                # target boxes\n",
    "                tbox = xywh2xyxy(labels[:, 1:5])\n",
    "                scale_coords(img[si].shape[1:], tbox, shapes[si][0], shapes[si][1])  # native-space labels\n",
    "\n",
    "                # Per target class\n",
    "                for cls in torch.unique(tcls_tensor):\n",
    "                    ti = (cls == tcls_tensor).nonzero(as_tuple=False).view(-1)  # prediction indices\n",
    "                    pi = (cls == pred[:, 5]).nonzero(as_tuple=False).view(-1)  # target indices\n",
    "\n",
    "                    # Search for detections\n",
    "                    if pi.shape[0]:\n",
    "                        # Prediction to target ious\n",
    "                        ious, i = box_iou(predn[pi, :4], tbox[ti]).max(1)  # best ious, indices\n",
    "\n",
    "                        # Append detections\n",
    "                        detected_set = set()\n",
    "                        for j in (ious > iouv[0]).nonzero(as_tuple=False):\n",
    "                            d = ti[i[j]]  # detected target\n",
    "                            if d.item() not in detected_set:\n",
    "                                detected_set.add(d.item())\n",
    "                                detected.append(d)\n",
    "                                correct[pi[j]] = ious[j] > iouv  # iou_thres is 1xn\n",
    "                                if len(detected) == nl:  # all targets already located in image\n",
    "                                    break\n",
    "\n",
    "            # Append statistics (correct, conf, pcls, tcls)\n",
    "            stats.append((correct.cpu(), pred[:, 4].cpu(), pred[:, 5].cpu(), tcls))\n",
    "\n",
    "    # Compute statistics\n",
    "    stats = [np.concatenate(x, 0) for x in zip(*stats)]  # to numpy\n",
    "    if len(stats) and stats[0].any():\n",
    "        p, r, ap, f1, ap_class = ap_per_class(*stats, v5_metric=v5_metric, save_dir=save_dir, names=names)\n",
    "        ap50, ap = ap[:, 0], ap.mean(1)  # AP@0.5, AP@0.5:0.95\n",
    "        mp, mr, map50, map = p.mean(), r.mean(), ap50.mean(), ap.mean()\n",
    "        nt = np.bincount(stats[3].astype(np.int64), minlength=nc)  # number of targets per class\n",
    "    else:\n",
    "        nt = torch.zeros(1)\n",
    "\n",
    "    return mp, mr, map50, map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = test(data=data, model=compiled_model, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize model using NNCF Postrainging Quantization API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_fn(data_batch):\n",
    "    img = data_batch[0]\n",
    "    img, _, _ = preprocess_image(np.transpose(img[0].numpy()))\n",
    "    return img\n",
    "\n",
    "quantization_dataset = nncf.create_dataloader(dataloader, transform_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = nncf.quantize(ov_model, quantization_dataset, preset='mixed')\n",
    "\n",
    "serialize(quantized_model. 'model/yolov7-tiny_int8.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Quantized model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_int8_model = core.compile_model(quantized_model, 'CPU')\n",
    "result = compiled_int8_model([im])[compiled_int8_model.output(0)]\n",
    "boxes, scores, labels = get_boxes(result)\n",
    "images_with_boxes = draw_boxes([img.copy()], boxes, scores, labels)\n",
    "Image.fromarray(images_with_boxes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate quantized model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int8_result = test(data=data, model=compiled_model, dataloader=dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Performance of the Original and Quantized Models\n",
    "Finally, use [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance of the `FP16` and `INT8` models.\n",
    "\n",
    "> NOTE: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP16 model (OpenVINO IR)\n",
    "!benchmark_app -m model/yolov7-tiny.xml -d CPU -api async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference FP16 model (OpenVINO IR)\n",
    "!benchmark_app -m model/yolov7-tiny_int8.xml -d CPU -api async"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0404472fd7b5b63117a9fa5c50283296e2708c2449c6090d2cdf8903f95897f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
