{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Handwritten Chinese and Japanese OCR\n",
    "\n",
    "In this tutorial optical character recognition for handwritten Chinese (simplified) and Japanese is presented. Roman alphabet OCR can be find in [notebook 208](../208-optical-character-recognition). This model is capable of doing only one line of symbols each time. \n",
    "\n",
    "Models used for this notebooks are [handwritten-japanese-recognition](https://docs.openvinotoolkit.org/latest/omz_models_model_handwritten_japanese_recognition_0001.html) and [handwritten-simplified-chinese](https://docs.openvinotoolkit.org/latest/omz_models_model_handwritten_simplified_chinese_recognition_0001.html). To decode models output to readable text [kondate_nakayosi](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/dataset_classes/kondate_nakayosi.txt) and [scut_ept](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/dataset_classes/scut_ept.txt) charlists are used. Both model are from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Imports modules required to run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from openvino.inference_engine import IECore\n",
    "from os import path, makedirs\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Settings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class Files:\n",
    "    def __init__(self, model: str, charlist_link: str, charlist_name: str = None):\n",
    "        self.model = model\n",
    "        self.charlist_link = charlist_link\n",
    "        self.charlist_name = charlist_name if charlist_name is not None else self.charlist_link.split('/')[-1]\n",
    "\n",
    "\n",
    "ie = IECore()\n",
    "\n",
    "model_folder = \"model\"\n",
    "data_folder = \"data\"\n",
    "charlist_folder = f\"{model_folder}/charlists\"\n",
    "\n",
    "precision = \"FP16\"\n",
    "\n",
    "model_extensions = (\"bin\", \"xml\")\n",
    "\n",
    "handwritten_japanese_model_name = \"handwritten-japanese-recognition-0001\"\n",
    "handwritten_simplified_chinese_model_name = \"handwritten-simplified-chinese-recognition-0001\"\n",
    "\n",
    "japaneses_charlist_link = \"https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/data/dataset_classes/kondate_nakayosi.txt\"\n",
    "simplified_chinese_charlist_link = \"https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/data/dataset_classes/scut_ept.txt\"\n",
    "\n",
    "chinese_image_link = 'https://github.com/openvinotoolkit/open_model_zoo/raw/master/demos/handwritten_text_recognition_demo/python/data/handwritten_simplified_chinese_test.png'\n",
    "japaneses_image_link = 'https://github.com/openvinotoolkit/open_model_zoo/raw/master/demos/handwritten_text_recognition_demo/python/data/handwritten_japanese_test.png'\n",
    "\n",
    "japaneses_files = Files(handwritten_japanese_model_name, japaneses_charlist_link)\n",
    "chinese_files = Files(handwritten_simplified_chinese_model_name, simplified_chinese_charlist_link)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Download models and convert public model\n",
    "\n",
    "If it is your first run models will download and convert here. It might take up to ten minutes. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "makedirs(model_folder, exist_ok=True)\n",
    "\n",
    "# Define which models to use, based on this line there will be downloaded nececery files used for OCR\n",
    "used_files = (chinese_files, japaneses_files)\n",
    "\n",
    "# Download models\n",
    "for file_name in used_files:\n",
    "    for extension in model_extensions:\n",
    "        if not path.isfile(f'{model_folder}/intel/{file_name.model}/{precision}/{file_name.model}.{extension}'):\n",
    "            download_command = f'omz_downloader --name {file_name.model} --output_dir {model_folder} --precision {precision}'\n",
    "            ! $download_command\n",
    "\n",
    "# Download charlists            \n",
    "for file_name in used_files:\n",
    "    if not path.isfile(f'{charlist_folder}/{file_name.charlist_name}'):\n",
    "        download_file(file_name.charlist_link, directory=charlist_folder, show_progress=False)    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the network"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Choose which model to use\n",
    "\n",
    "# Uncomment line below for Japaneses \n",
    "# currently_used_model = japaneses_files.model\n",
    "\n",
    "# Uncomment line below for Chinese\n",
    "currently_used_model = chinese_files.model\n",
    "\n",
    "net = ie.read_network(\n",
    "    model=f\"{model_folder}/intel/{currently_used_model}/{precision}/{currently_used_model}.xml\"\n",
    ")\n",
    "\n",
    "exec_net = ie.load_network(net, \"CPU\")\n",
    "\n",
    "recognition_output_layer = next(iter(exec_net.outputs))\n",
    "recognition_input_layer = next(iter(exec_net.input_info))"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load an Image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create data folder\n",
    "makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Download image\n",
    "image_link = chinese_image_link if currently_used_model == chinese_files.model else japaneses_image_link\n",
    "file_name = image_link.split('/')[-1]\n",
    "download_file(image_link, directory=data_folder, show_progress=False)\n",
    "\n",
    "# Text detection models expects image in grayscale format\n",
    "# IMPORTANT!!! This model allows to read only one line at time\n",
    "\n",
    "# Read image\n",
    "image = cv2.imread(f\"{data_folder}/{file_name}\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Fetch shape\n",
    "image_height, image_width = image.shape\n",
    "\n",
    "# Calculate aspect ratio to keep text readable\n",
    "aspect_ratio = image_width / image_height\n",
    "\n",
    "# B,C,H,W = batch size, number of channels, height, width\n",
    "_, _, H, W = net.input_info[recognition_input_layer].input_data.shape\n",
    "\n",
    "# Resize image to meet network expected input sizes\n",
    "resized_image = cv2.resize(image, (int(H * aspect_ratio), H), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Pad image to meet input size\n",
    "resized_image = np.pad(resized_image, ((0, 0), (0, W - int(H * aspect_ratio))), mode='edge')\n",
    "\n",
    "# Reshape to network input shape\n",
    "input_image = resized_image[None, None, :, :]\n",
    "\n",
    "plt.figure(figsize=(20, 1))\n",
    "plt.axis('off')\n",
    "plt.imshow(resized_image, cmap='gray', vmin=0, vmax=255);"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare charlist"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get dictionary to encode output, based on model documentation\n",
    "used_charlist = chinese_files.charlist_name if currently_used_model == handwritten_simplified_chinese_model_name else japaneses_files.charlist_name\n",
    "\n",
    "# With both models, there should be blank symbol added at index 0 of each charlists\n",
    "blank_char = '~'\n",
    "\n",
    "with open(f\"{charlist_folder}/{used_charlist}\", 'r', encoding='utf-8') as charlist:\n",
    "    letters = blank_char + ''.join(line.strip() for line in charlist)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Run inference on model\n",
    "predictions = exec_net.infer(inputs={recognition_input_layer: input_image})[recognition_output_layer]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Process infered data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def threshold(prob, cnt, mult):\n",
    "    return (prob / cnt) * mult\n",
    "\n",
    "\n",
    "# Remove unnececery dimension\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Run argsort to pick two most possible symbols\n",
    "predictions_indexes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Define counter and probability to calculate avg probability difference between predicted symbol and blank symbol. \n",
    "cnt, prob = 0, 0\n",
    "for i, p in enumerate(predictions_indexes):\n",
    "    if p != 0:\n",
    "        cnt += 1\n",
    "        prob += predictions[i][p] - predictions[i][0]\n",
    "\n",
    "output_text = []"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Print output"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create list of letters that difference between its calculated probability and probability of blank symbol is greater than given threshold. \n",
    "# WARNING! Threshold multiplier might be different for different models!\n",
    "# To change output (e.g. if letters are missing, or duplicates are visible) modify threshold_multiplier\n",
    "\n",
    "threshold_multiplier = 1 / 2\n",
    "\n",
    "for i, p in enumerate(predictions_indexes):\n",
    "    if p != 0:\n",
    "        if predictions[i][p] - predictions[i][0] > threshold(prob, cnt, threshold_multiplier):\n",
    "            output_text.append(letters[p])\n",
    "            \n",
    "plt.figure(figsize=(20, 1))\n",
    "plt.axis('off')\n",
    "plt.imshow(resized_image, cmap='gray', vmin=0, vmax=255)\n",
    "print(''.join(output_text))\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}