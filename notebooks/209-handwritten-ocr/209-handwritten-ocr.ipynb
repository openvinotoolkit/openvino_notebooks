{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten Chinese and Japanese OCR\n",
    "\n",
    "In this tutorial optical character recognition for handwritten Chinese (simplified) and Japanese is presented. Roman alphabet OCR can be find in [notebook 208](../208-optical-character-recognition). This model is capable of doing only one line of symbols each time. \n",
    "\n",
    "Models used for this notebooks are [handwritten-japanese-recognition](https://docs.openvinotoolkit.org/latest/omz_models_model_handwritten_japanese_recognition_0001.html) and [handwritten-simplified-chinese](https://docs.openvinotoolkit.org/latest/omz_models_model_handwritten_simplified_chinese_recognition_0001.html). To decode models output to readable text [kondate_nakayosi](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/dataset_classes/kondate_nakayosi.txt) and [scut_ept](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/dataset_classes/scut_ept.txt) charlists are used. Both model are from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports modules required to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "from itertools import groupby\n",
    "from openvino.inference_engine import IECore\n",
    "from os import path, makedirs\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper class\n",
    "\n",
    "To run OCR we need both model and charlist. Class ```LanguageFiles``` is created to group all necesery files for single language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageFiles:\n",
    "    def __init__(self, model: str, charlist_link: str, demo_image_name: str):\n",
    "        self.model = model\n",
    "        self.charlist_link = charlist_link\n",
    "        self.demo_image_name = demo_image_name\n",
    "        self.charlist_name = self.charlist_link.split('/')[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Set up all consts and folders used in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories where data will be placed\n",
    "model_folder = \"model\"\n",
    "data_folder = \"data\"\n",
    "charlist_folder = f\"{model_folder}/charlists\"\n",
    "\n",
    "# Precision used by model\n",
    "precision = \"FP16\"\n",
    "\n",
    "model_extensions = (\"bin\", \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Names and links used for Japanese and Chinese\n",
    "\n",
    "To run OCR you need model, charlist and image. All of those are defined in **Japanese** and **Chinese** subsections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Japanese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of model that will be used\n",
    "handwritten_japanese_model_name = \"handwritten-japanese-recognition-0001\"\n",
    "\n",
    "# Link to charlist\n",
    "japanese_charlist_link = \"https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/data/dataset_classes/kondate_nakayosi.txt\"\n",
    "\n",
    "# Link to demo image\n",
    "japanese_image_link = 'https://github.com/openvinotoolkit/open_model_zoo/raw/master/demos/handwritten_text_recognition_demo/python/data/handwritten_japanese_test.png'\n",
    "\n",
    "# Extract image name from demo link\n",
    "japanese_image_name = 'handwritten_japanese_test.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of model that will be used\n",
    "handwritten_simplified_chinese_model_name = \"handwritten-simplified-chinese-recognition-0001\"\n",
    "\n",
    "# Link to charlist\n",
    "simplified_chinese_charlist_link = \"https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/data/dataset_classes/scut_ept.txt\"\n",
    "\n",
    "# Link to demo image\n",
    "chinese_image_link = 'https://user-images.githubusercontent.com/36741649/140065813-1970cd70-53c6-4d6c-b403-7e6974df34f7.jpg'\n",
    "\n",
    "# Extract image name from demo link\n",
    "chinese_image_name = 'handwritten_chinese_test.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create directories for data and model\n",
    "\n",
    "Charlists doesn't require creating an additional folder as the download function itself creates a subfolder inside the model folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(data_folder, exist_ok=True)\n",
    "makedirs(model_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images\n",
    "\n",
    "You need to download images to run inference on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(url=chinese_image_link, filename=chinese_image_name, directory=data_folder, show_progress=False)\n",
    "download_file(url=japanese_image_link, filename=japanese_image_name, directory=data_folder, show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group files used by languages to classes\n",
    "\n",
    "To make language files easier to group, we use previously defined ```LanguageFiles``` to group up model, charlist, and demo image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Japanese files grouped as a class\n",
    "japanese_files = LanguageFiles(model=handwritten_japanese_model_name, charlist_link=japanese_charlist_link, demo_image_name=japanese_image_name)\n",
    "\n",
    "# Chinese files grouped as a class\n",
    "chinese_files = LanguageFiles(model=handwritten_simplified_chinese_model_name, charlist_link=simplified_chinese_charlist_link, demo_image_name=chinese_image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models and convert public model\n",
    "\n",
    "As you already have images to run on, now you are missing the model and charlist. In the sections below there are cells for both downloading Chinese and Japanese.\n",
    " \n",
    "If it is your first run models will download and convert here. It might take up to ten minutes. \n",
    "\n",
    "We use `omz_downloader`, which is a command-line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_files(language_files: LanguageFiles):\n",
    "    # Download model\n",
    "    for extension in model_extensions:\n",
    "        path_to_model = f'{model_folder}/intel/{language_files.model}/{precision}/{language_files.model}.{extension}'\n",
    "        if not path.isfile(path_to_model):\n",
    "            download_command = f'omz_downloader --name {language_files.model} --output_dir {model_folder} --precision {precision}'\n",
    "            print(download_command)\n",
    "            ! $download_command\n",
    "\n",
    "    # Download charlist            \n",
    "    if not path.isfile(f'{charlist_folder}/{language_files.charlist_name}'):\n",
    "        download_file(language_files.charlist_link, directory=charlist_folder, show_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Japanese files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_files(language_files=japanese_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Chinese files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_files(language_files=chinese_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Select language\n",
    "\n",
    "All required files are downloaded, now you need to select which language you want to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_language(language: str) -> LanguageFiles:\n",
    "    languages = {\n",
    "        \"chinese\": chinese_files,\n",
    "        \"japanese\": japanese_files\n",
    "    }\n",
    "    if language not in languages.keys():\n",
    "        raise KeyError(f\"Invalid language choosen! Please pick one of those: {', '.join(languages.keys())}\")\n",
    "    return languages.get(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on your choice you will need to change a line of code in the cell below.\n",
    "\n",
    "In case you want to use Japanese OCR this line should be ```language = 'japanese'``` otherwise if you want to use Chinese ```language = 'chinese'```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select language by using either language='chinese' or language='japanese'\n",
    "language = 'japanese'\n",
    "\n",
    "selected_language = use_language(language=language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load network and execute it\n",
    "\n",
    "When all files are downloaded and language is selected, you need to read and load the network to run inference. The path to the model is defined based on the selected language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "\n",
    "path_to_model = f\"{model_folder}/intel/{selected_language.model}/{precision}/{selected_language.model}.xml\"\n",
    "\n",
    "net = ie.read_network(\n",
    "    model=path_to_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select device name\n",
    "\n",
    "You may choose to run the network on multiple devices by default it will load the model on the CPU (you can choose manually CPU, GPU, MYRIAD, etc.) or let the engine choose the best available device (AUTO).\n",
    "\n",
    "To list all available devices that you can use, uncomment and run line ```print(ie.available_devices)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check available device names run line below\n",
    "# print(ie.available_devices)\n",
    "\n",
    "exec_net = ie.load_network(network=net, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch information about input and output layers \n",
    "\n",
    "The model is loaded, now you need to fetch information about input and output layers. This allows you to properly pass input and read the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_output_layer = next(iter(exec_net.outputs))\n",
    "recognition_input_layer = next(iter(exec_net.input_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Load an Image\n",
    "\n",
    "As mentioned previously, to run OCR inference we need a model, charlist, and image. You already have loaded the model, downloaded the charlist, and downloaded the image. The next step is loading the image. \n",
    "\n",
    "Model input expects a single-channel image, for that reason we read the image in grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read file name of demo file based on used model\n",
    "\n",
    "file_name = selected_language.demo_image_name\n",
    "\n",
    "# Text detection models expects image in grayscale format\n",
    "# IMPORTANT!!! This model allows to read only one line at time\n",
    "\n",
    "# Read image\n",
    "image = cv2.imread(filename=f\"{data_folder}/{file_name}\", flags=cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch information about image and input layer shape\n",
    "\n",
    "Now you loaded image, the next step is getting information that you will use for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch shape\n",
    "image_height, _ = image.shape\n",
    "\n",
    "# B,C,H,W = batch size, number of channels, height, width\n",
    "_, _, H, W = net.input_info[recognition_input_layer].input_data.shape\n",
    "\n",
    "# Calculate scale ratio between input shape height and image height to resize image\n",
    "scale_ratio = H / image_height"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess input\n",
    "\n",
    "Now you use scale ratio, which describes the ratio between required by input layer height and current image height. In the cell below image will be resized and padded to keep letters proportions and meet input shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize image to meet network expected input sizes\n",
    "resized_image = cv2.resize(image, None, fx=scale_ratio, fy=scale_ratio, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# Pad image to meet input size\n",
    "resized_image = np.pad(resized_image, ((0, 0), (0, W - resized_image.shape[1])), mode='edge')\n",
    "\n",
    "# Reshape to network input shape\n",
    "input_image = resized_image[None, None, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise input image\n",
    "\n",
    "After preprocessing you can display how the current image looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 1))\n",
    "plt.axis('off')\n",
    "plt.imshow(resized_image, cmap='gray', vmin=0, vmax=255);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare charlist\n",
    "\n",
    "The model is loaded, image is ready to go. The only element left is charlist. It is downloaded but before you will use it, there is one more thing. You need to add a blank symbol at the beginning of the charlist. It is expected by both languages models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dictionary to encode output, based on model documentation\n",
    "used_charlist = selected_language.charlist_name\n",
    "\n",
    "# With both models, there should be blank symbol added at index 0 of each charlists\n",
    "blank_char = '~'\n",
    "\n",
    "with open(f\"{charlist_folder}/{used_charlist}\", 'r', encoding='utf-8') as charlist:\n",
    "    letters = blank_char + ''.join(line.strip() for line in charlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "Now, when everything is ready to go, run inference. As input argument you need to use previously fetched information about input layer and preprocessed input image, and to read output predictions you need informations from output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on model\n",
    "predictions = exec_net.infer(inputs={recognition_input_layer: input_image})[recognition_output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process infered data\n",
    "\n",
    "Output of model format is W, B, L, where:\n",
    "\n",
    "* W - output sequence length\n",
    "* B - batch size\n",
    "* L - confidence distribution across the supported symbols in Kondate and Nakayosi.\n",
    "\n",
    "You need to make it in a more human-readable format. To do this you need to get a symbol with the highest probability. When you hold a list of indexes that are predicted to have the highest probability, due to limitations given by [CTC Decoding](https://towardsdatascience.com/beam-search-decoding-in-ctc-trained-neural-networks-5a889a3d85a7) you will remove concurrent symbols and then remove all the blanks.\n",
    "\n",
    "The last step is getting symbols from corresponding indexes in charlist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unnececery dimension\n",
    "predictions = np.squeeze(predictions)\n",
    "\n",
    "# Run argmax to pick most possible symbols\n",
    "predictions_indexes = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use groupby to remove concurrent letters, as required by CTC greedy decoding\n",
    "output_text_indexes = list(groupby(predictions_indexes))\n",
    "\n",
    "# Remove grouper objects\n",
    "output_text_indexes, _ = np.transpose(output_text_indexes, (1, 0))\n",
    "\n",
    "# Remove blank symbols \n",
    "output_text_indexes = output_text_indexes[output_text_indexes != 0] \n",
    "\n",
    "# Assign letters to indexes from output array\n",
    "output_text = [letters[letter_index] for letter_index in output_text_indexes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print output\n",
    "\n",
    "Now you have a list of letters predicted by the model. The only thing left to do is display the picture and predicted text below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 1))\n",
    "plt.axis('off')\n",
    "plt.imshow(resized_image, cmap='gray', vmin=0, vmax=255)\n",
    "\n",
    "print(''.join(output_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
