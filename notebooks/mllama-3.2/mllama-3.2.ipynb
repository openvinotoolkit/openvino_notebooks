{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install \"transformers>=4.45.0\" \"torch>=2.1\" \"torchvision\" \"Pillow\" \"tqdm\" \"datasets>=2.14.6\" \"gradio>=4.36\" \"nncf>=2.13.0\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -U --pre \"openvino>=2024.4.0\" --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "if not Path(\"ov_mllama_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/mllama3.2/ov_mllama_helper.py\")\n",
    "    open(\"ov_mllama_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"gradio_helper.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/flux.1-image-generation/gradio_helper.py\")\n",
    "    open(\"gradio_helper.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"ov_mllama_compression.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/mllama3.2/ov_mllama_compression.py\")\n",
    "    open(\"ov_mllama_compression.py\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"data_preprocessing.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/mllama3.2/data_preprocessing.py\")\n",
    "    open(\"data_preprocessing\", \"w\").write(r.text)\n",
    "\n",
    "if not Path(\"notebook_utils.py\").exists():\n",
    "    r = requests.get(url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py\")\n",
    "    open(\"notebook_utils.py\", \"w\").write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-23 14:24:20.729672: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-23 14:24:20.731581: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-23 14:24:20.767644: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-23 14:24:21.417611: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/models/vq_model.py:20: FutureWarning: `VQEncoderOutput` is deprecated and will be removed in version 0.31. Importing `VQEncoderOutput` from `diffusers.models.vq_model` is deprecated and this will be removed in a future version. Please use `from diffusers.models.autoencoders.vq_model import VQEncoderOutput`, instead.\n",
      "  deprecate(\"VQEncoderOutput\", \"0.31\", deprecation_message)\n",
      "/home/ea/work/my_optimum_intel/optimum_env/lib/python3.8/site-packages/diffusers/models/vq_model.py:25: FutureWarning: `VQModel` is deprecated and will be removed in version 0.31. Importing `VQModel` from `diffusers.models.vq_model` is deprecated and this will be removed in a future version. Please use `from diffusers.models.autoencoders.vq_model import VQModel`, instead.\n",
      "  deprecate(\"VQModel\", \"0.31\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ov_mllama_helper import convert_mllama\n",
    "\n",
    "model_id = \"Llama-3.2-11B-Vision-Instruct\"\n",
    "model_dir = Path(model_id.split(\"/\")[-1]) / \"OV\"\n",
    "\n",
    "# uncomment the line to see model conversion code \n",
    "# convert_mllama??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model already converted and can be found in Llama-3.2-11B-Vision-Instruct/OV\n"
     ]
    }
   ],
   "source": [
    "convert_mllama(model_id, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10ec8c0b1054cd794b98e130143e01e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from notebook_utils import device_widget\n",
    "\n",
    "device = device_widget()\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:nncf:NNCF initialized successfully. Supported frameworks detected: torch, tensorflow, onnx, openvino\n"
     ]
    }
   ],
   "source": [
    "from ov_mllama_compression import compress\n",
    "# uncomment the line to see compression code\n",
    "# compress??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed model already exists and can be found in Llama-3.2-11B-Vision-Instruct/OV/llm_int4_asym_r10_gs64_max_activation_variance_all_layers.xml\n"
     ]
    }
   ],
   "source": [
    "language_model_path = compress(model_dir, awq=False, scale_estimation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "import nncf\n",
    "import openvino as ov\n",
    "import gc\n",
    "\n",
    "from data_preprocessing import prepare_dataset_vision\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_dir)\n",
    "core = ov.Core()\n",
    "\n",
    "vision_encoder_path = model_dir / \"openvino_vision_encoder.xml\"\n",
    "int8_vision_encoder_path = model_dir / vision_encoder_path.name.replace('.xml', '_int8.xml')\n",
    "\n",
    "if not int8_vision_encoder_path.exists() and device.value != \"GPU\":\n",
    "    calibration_data = prepare_dataset_vision(processor, 100)\n",
    "    ov_model = core.read_model(vision_encoder_path)\n",
    "    calibration_dataset = nncf.Dataset(calibration_data)\n",
    "    quantized_model = nncf.quantize(\n",
    "        model=ov_model,\n",
    "        calibration_dataset=calibration_dataset,\n",
    "        model_type=nncf.ModelType.TRANSFORMER,\n",
    "        advanced_parameters=nncf.AdvancedQuantizationParameters(smooth_quant_alpha=0.6)\n",
    "    )\n",
    "    ov.save_model(quantized_model, int8_vision_encoder_path)\n",
    "    del quantized_model\n",
    "    del ov_model\n",
    "    del calibration_dataset\n",
    "    del calibration_data\n",
    "    gc.collect()\n",
    "\n",
    "vision_encoder_path = int8_vision_encoder_path if device.value != \"GPU\" else vision_encoder_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applied slice for lm head\n"
     ]
    }
   ],
   "source": [
    "from ov_mllama_helper import OVMLlamaForConditionalGeneration\n",
    "\n",
    "# Uncomment this line to see model inference code\n",
    "# OVMLlamaForConditionalGeneration??\n",
    "\n",
    "ov_model = OVMLlamaForConditionalGeneration(model_dir, device=device.value, language_model_name=language_model_path.name, image_encoder_name=vision_encoder_path.name)\n",
    "processor = AutoProcessor.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ea/llama3.2/Llama-3.2-11B-Vision-Early/transformers/transformers/generation/configuration_utils.py:593: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/ea/llama3.2/Llama-3.2-11B-Vision-Early/transformers/transformers/generation/configuration_utils.py:598: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a serene lake scene, featuring a long wooden dock that extends from the foreground into the distance, with a mountain range in the background and a cloudy sky above.\n",
      "\n",
      "The dock is constructed from wooden planks and features metal railings along its\n",
      "Visual encoder time 19400.806584046222 ms\n",
      "First token latency 2414.042363059707ms, Second token latency 423.10670324677255ms\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from transformers import TextStreamer\n",
    "import numpy as np\n",
    "\n",
    "messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"image\"},\n",
    "                {\"type\": \"text\", \"text\": \"Describe image in two sentences\"}\n",
    "            ]\n",
    "        },\n",
    "]\n",
    "text = processor.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "url = \"https://llava-vl.github.io/static/images/view.jpg\"\n",
    "raw_image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=text, images=[raw_image], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(processor.tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "output = ov_model.generate(**inputs, do_sample=False, max_new_tokens=50, streamer=streamer)\n",
    "print(f\"Visual encoder time {ov_model.vision_encoder_infer_time[0] * 1000} ms\")\n",
    "print(f\"First token latency {ov_model.llm_infer_time[0] * 1000}ms, Second token latency {np.mean(np.array(ov_model.llm_infer_time[1:])) * 1000}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gradio_helper import make_demo\n",
    "\n",
    "processor.chat_template = processor.tokenizer.chat_template\n",
    "demo = make_demo(ov_model, processor)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "aa686863102b46d78fcec8f1a96ee256": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ac8b90b1515542b0b4923025d1d4fef2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c10ec8c0b1054cd794b98e130143e01e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 1,
       "layout": "IPY_MODEL_aa686863102b46d78fcec8f1a96ee256",
       "style": "IPY_MODEL_ac8b90b1515542b0b4923025d1d4fef2"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
