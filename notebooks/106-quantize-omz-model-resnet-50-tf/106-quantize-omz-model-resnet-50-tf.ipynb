{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the Open Model Zoo resnet-50-tf model\n",
    "Quantizing a model accelerates a trained model by reducing the precision necessary for its calculations.  Acceleration comes from lower-precision calculations being faster as well as less memory needed and less data to transfer since the data type itself is smaller along with the model weights data.  Though lower-precision may reduce model accuracy, typically a model using 32-bit floating-point precision (FP32) can be quantized to use lower-precision 8-bit integers (INT8) giving good results that are worth the trade off between accuracy and speed.  To see how quantization can accelerate models, see [INT8 vs FP32 Comparison on Select Networks and Platforms](https://docs.openvino.ai/latest/openvino_docs_performance_int8_vs_fp32.html#doxid-openvino-docs-performance-int8-vs-fp32) for some benchmarking results.\n",
    "\n",
    "[Intel Distribution of OpenVINO toolkit](https://software.intel.com/openvino-toolkit) includes the [Post-Training Optimization Tool (POT)](https://docs.openvino.ai/latest/pot_README.html) to automate quantization.  For models available from the [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), the [`omz_quantizer`](../104-model-tools/104-model-tools.ipynb) tool is available to automate running POT using its [DefaultQuantization](https://docs.openvino.ai/latest/pot_compression_algorithms_quantization_default_README.html#doxid-pot-compression-algorithms-quantization-default-r-e-a-d-m-e) 8-bit quantization algorithm to quantize models down to INT8 precision.\n",
    "\n",
    "This Jupyter* Notebook will go step-by-step through the workflow of downloading the [resnet-50-tf](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/resnet-50-tf) model from the Open Model Zoo through quantization and then checking and benchmarking the results.  The workflow consists of following the steps:\n",
    "1. Download and set up the the [Imagenette](https://github.com/fastai/imagenette) (subset of [ImageNet](http://www.image-net.org/)) validation dataset to be used by omz_quantize\n",
    "2. Download model from the Open Model Zoo\n",
    "3. Convert model to FP32 IR files\n",
    "4. Quantize FP32 model to create INT8 IR files\n",
    "5. Run inference on original and quantized model\n",
    "6. Check accuracy before and after quantization\n",
    "7. Benchmark before and after quantization\n",
    "\n",
    "While performing the steps above, the following [OpenVINO tools](../104-model-tools/104-model-tools.ipynb) will be used to download, convert, quantize, check accuracy, and benchmark the model:\n",
    "- `omz_downloader` - Download model from the Open Model Zoo\n",
    "- `omz_converter` - Convert an Open Model Zoo model\n",
    "- `omz_quantizer` - Quantize an Open Model Zoo model\n",
    "- `accuracy_check` - Check the accuracy of models using a validation dataset\n",
    "- `benchmark_app` - Benchmark models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the model\n",
    "This notebook uses the resnet-50-tf model which is a TensorFlow* implementation of ResNet-50, an image classification model that has been trained on the ImageNet dataset. The input to the converted model is a 224x224 BGR image.  The output of the model is 1001 prediction probabilities in the range of 0.0-1.0 for each of the 1000 classes, plus one for background.\n",
    "\n",
    "For details more details on the resnet-50-tf model, see the Open Model Zoo [model](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/resnet-50-tf), the [paper](https://arxiv.org/abs/1512.03385) and the [repository](https://github.com/tensorflow/models/tree/v2.2.0/official/r1/resnet)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from subprocess import PIPE, STDOUT, Popen\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as nbutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "By default, this notebook downloads the model, dataset, etc. to subdirectories where this notebook is located.  The following variables may be used to set file locations:\n",
    "* `OMZ_MODEL_NAME`: Model name as it appears on the Open Model Zoo\n",
    "* `DATA_DIR`: Directory where dataset will be downloaded and set up\n",
    "* `MODEL_DIR`: Models will be downloaded into the `intel` and `public` folders in this directory\n",
    "* `OUTPUT_DIR`: Directory used to store any output and other downloaded files (e.g. configuration files for running accuracy_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base settings\n",
    "OMZ_MODEL_NAME = \"resnet-50-tf\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATASET_DIR = DATA_DIR / \"imagenette\"\n",
    "LABELS_PATH = DATASET_DIR / \"imagenet_2012.txt\"\n",
    "\n",
    "# different model precisions location\n",
    "MODEL_PUBLIC_DIR = MODEL_DIR / \"public\" / OMZ_MODEL_NAME\n",
    "MODEL_FP32_DIR = MODEL_PUBLIC_DIR / \"FP32\"\n",
    "MODEL_FP32INT8_DIR = MODEL_PUBLIC_DIR / \"FP32-INT8\"\n",
    "\n",
    "# create directories if they do not already exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The `run_command_line()` helper function is provided to aid filtering the output of some of the commands that will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command_line(cmd: str, filter=None):\n",
    "    \"\"\"\n",
    "    runs the given command-line outputting lines as they become available to show progress in ~realtime.\n",
    "    If a filter is provided, it will be called with each line before printing the result from calling the filter\n",
    "    :param cmd: String containing complete command-line to run\n",
    "    :param filter: Optional filter called per-line before printing\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    proc = Popen(cmd.split(), stdout=PIPE, stderr=STDOUT, universal_newlines=True)\n",
    "    while proc.poll() is None:\n",
    "        line = proc.stdout.readline()\n",
    "        if filter is not None:\n",
    "            line = filter(line)\n",
    "        if line is not None:\n",
    "            sys.stdout.write(\"%s\" % (line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and set up the validation dataset\n",
    "Instead of using the very large [ImageNet](http://www.image-net.org/) dataset, the smaller [Imagenette 320px](https://github.com/fastai/imagenette) dataset containing 10 classes with lower-resolution images will be used by this notebook.  The Imagenette dataset will be downloaded and arranged to look just like ImageNet so that it can be used by the `omz_quantizer` and `accuracy_check` tools.  Any ImageNet (or subset of ImageNet) dataset may be used when following the steps in the notebook, however all must be set up as described on the Open Model Zoo [dataset.md:ImageNet](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/datasets.md#imagenet) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_imagenette_dataset(output_dir):\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    img_val_path = output_dir / \"ILSVRC2012_img_val\"\n",
    "    img_val_path.mkdir(exist_ok=True, parents=True)\n",
    "    img_val_ann_path = output_dir / \"val.txt\"\n",
    "\n",
    "    data_tgzname = \"imagenette2-320.tgz\"\n",
    "    data_url = f\"https://s3.amazonaws.com/fast-ai-imageclas/{data_tgzname}\"\n",
    "    data_tgzpath = nbutils.download_file(data_url, data_tgzname, output_dir)\n",
    "\n",
    "    # uncompress files\n",
    "    tar_ref = tarfile.open(data_tgzpath, \"r:gz\")\n",
    "    tar_ref.extractall(path=output_dir)\n",
    "    tar_ref.close()\n",
    "\n",
    "    # download the class labels\n",
    "    labels_url = f\"https://github.com/openvinotoolkit/open_model_zoo/raw/master/data/dataset_classes/{LABELS_PATH.name}\"\n",
    "    nbutils.download_file(labels_url, LABELS_PATH.name, output_dir)\n",
    "\n",
    "    # load labels for lookup\n",
    "    with open(LABELS_PATH) as labels_file:\n",
    "        labels = [line.rstrip() for line in labels_file]\n",
    "\n",
    "    # move image files and generate annotation file\n",
    "    dir_dict = {}\n",
    "    winid_map = {}\n",
    "    val_path = output_dir / data_tgzpath.stem / \"val\"\n",
    "    for root, dirs, files in os.walk(val_path):\n",
    "        # match each winid directory\n",
    "        if Path(root).name != \"val\":\n",
    "            file_list = [Path(root) / fname for fname in files]\n",
    "            winid = Path(root).name\n",
    "            dir_dict[winid] = file_list\n",
    "            label_idx = [i for i, item in enumerate(labels) if item.startswith(winid)][0]\n",
    "            winid_map[winid] = label_idx\n",
    "\n",
    "    # simple shuffle and move image files prefixed with \"val_n_\" and create annotations file\n",
    "    pop_count = len(dir_dict)\n",
    "    total_files = 0\n",
    "    ann_file = open(img_val_ann_path, \"w\")\n",
    "    while pop_count > 0:\n",
    "        pop_count = 0\n",
    "        for winid in dir_dict:\n",
    "            if len(dir_dict[winid]) > 0:\n",
    "                src_path = dir_dict[winid].pop()\n",
    "                dst_path = img_val_path / f\"val_{total_files:08}_{src_path.stem}.JPEG\"\n",
    "                shutil.move(src_path, dst_path)\n",
    "                ann_file.write(f\"{dst_path.name} {winid_map[winid]}\\n\")\n",
    "                pop_count += 1\n",
    "                total_files += 1\n",
    "\n",
    "    ann_file.close()\n",
    "\n",
    "\n",
    "if not LABELS_PATH.exists():\n",
    "    set_up_imagenette_dataset(DATASET_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "The OpenVINO tool [`omz_downloader`](../104-model-tools/104-model-tools.ipynb) is used to automatically download files from the Open Model Zoo.\n",
    "\n",
    "> **NOTE**: If model IR files are available from the Open Model Zoo, then the downloaded models will appear in the `intel` subdirectory.  If no model IR files are available, then the downloaded models will appear in the `public` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_downloader --name $OMZ_MODEL_NAME --output $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to IR files\n",
    "\n",
    "The public models from the Open Model Zoo are made available in their native framework file format and must be converted to OpenVINO Intermediate Representation (IR) files before running inference.  The OpenVINO tool [`omz_convert`](../104-model-tools/104-model-tools.ipynb) is used to convert Open Model Zoo models to the IR files necessary to run inference.\n",
    "\n",
    "> **NOTE**: For models that are downloaded from the Open Model Zoo already as IR files, the converter utility will not do any conversion and will output the message \"Skipping <model_name> (no conversions defined)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_converter --name $OMZ_MODEL_NAME --precisions FP32 --download_dir $MODEL_DIR  --output $MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model to INT8\n",
    "For models downloaded from the Open Model Zoo, the [`omz_quantizer`](../104-model-tools/104-model-tools.ipynb) tool is used to quantize the model to a lower precision (e.g. quantize FP32 to INT8 precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_omz_quantizer_output(line):\n",
    "    if (line.startswith(\"Quantization command\") or line.startswith(\"Moving\") or line.startswith(\"INFO\")):\n",
    "        return line\n",
    "    return None\n",
    "\n",
    "\n",
    "cmd = f\"omz_quantizer --name {OMZ_MODEL_NAME} --model_dir {MODEL_DIR}  --output {MODEL_DIR}  --dataset_dir {DATASET_DIR} --precisions FP32-INT8\"\n",
    "run_command_line(cmd, filter_omz_quantizer_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "Now that the model has been quantized, we will run inference using both the original FP32 model and the new INT8 quantized model to see their results.  First we will run the FP32 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_base_path, image_path):\n",
    "    \"\"\"\n",
    "    runs inferrence on an image using the given model and then displays the results\n",
    "    :param model_base_path: String containing path and file name of model excluding the extension (i.e. \".xml\")\n",
    "    :param image_path: String containing full path to the input image\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    ie = IECore()\n",
    "\n",
    "    # create the network from the model\n",
    "    net = ie.read_network(\n",
    "        model=f\"{model_base_path}.xml\", weights=f\"{model_base_path}.bin\"\n",
    "    )\n",
    "    exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "    input_key = next(iter(exec_net.input_info))\n",
    "    output_key = next(iter(exec_net.outputs.keys()))\n",
    "\n",
    "    # Load image\n",
    "    image = nbutils.load_image(image_path)\n",
    "    # N,C,H,W = batch size, number of channels, height, width\n",
    "    N, C, H, W = exec_net.input_info[input_key].tensor_desc.dims\n",
    "    # The network expects images in BGR format, same as OpenCV so just resize\n",
    "    input_image = cv2.resize(src=image, dsize=(W, H))\n",
    "    # reshape image to network input shape ([W,H,C]->[B,C,H,W])\n",
    "    input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)\n",
    "    # display original image (imshow requires RGB format, so convert BGR->RGB)\n",
    "    plt.imshow(nbutils.to_rgb(image))\n",
    "\n",
    "    # Run inference, result = [1,1001] with confidence level for each of the 1000 \n",
    "    #  classes and +1 for background.  The class with the highest confidence is \n",
    "    #  used to output the final result.\n",
    "    result = exec_net.infer(inputs={input_key: input_image})[output_key][0]\n",
    "    label_id = np.argmax(result)\n",
    "    conf = round(result[label_id] * 100, 2)\n",
    "    print(f\"label_id={label_id}, conf={conf} %\")\n",
    "\n",
    "    # Convert the inference result to a class name using the labels file\n",
    "    with open(LABELS_PATH) as f:\n",
    "        labels = [line.rstrip() for line in f]\n",
    "\n",
    "    print(f\"Image contains a '{labels[label_id]}', with {conf}% confidence\")\n",
    "\n",
    "\n",
    "# find known image\n",
    "files = glob.glob(f\"{DATASET_DIR}/**/*n02102040_2051.JPEG\", recursive=True)\n",
    "test_input_image = files[0]\n",
    "\n",
    "run_inference(f\"{MODEL_FP32_DIR}/{OMZ_MODEL_NAME}\", test_input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we run the INT8 model and can compare the results to the FP32 results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(f\"{MODEL_FP32INT8_DIR}/{OMZ_MODEL_NAME}\", test_input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up to run accuracy_check\n",
    "We will check the accuracy of the two FP32 and INT8 models using  [OpenVINO's Accuracy Checker Tool](https://docs.openvino.ai/latest/omz_tools_accuracy_checker.html), [`accuracy_check`](../104-model-tools/104-model-tools.ipynb).  For each model, The Open Model Zoo includes the necessary `accuracy-check.yml` configuration and the global [`dataset_definitions.yml`](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/dataset_definitions.yml) files needed to run the `accuracy_check` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve files needed by accuracy_check\n",
    "OMZ_GITHUB_URL = \"https://github.com/openvinotoolkit/open_model_zoo/raw/master\"\n",
    "dataset_def_yml = \"dataset_definitions.yml\"\n",
    "dataset_def_yml_url = f\"{OMZ_GITHUB_URL}/data/{dataset_def_yml}\"\n",
    "model_acheck_yml = \"accuracy-check.yml\"\n",
    "model_acheck_yml_url = (\n",
    "    f\"{OMZ_GITHUB_URL}/models/public/{OMZ_MODEL_NAME}/{model_acheck_yml}\"\n",
    ")\n",
    "\n",
    "model_acheck_yml_path = nbutils.download_file(\n",
    "    model_acheck_yml_url, model_acheck_yml, OUTPUT_DIR\n",
    ")\n",
    "\n",
    "dataset_def_yml_path = nbutils.download_file(\n",
    "    dataset_def_yml_url, dataset_def_yml, OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy of the model before and after quantization\n",
    "Now we will run `accuracy_check` for both the original FP32 and the new quantized INT8 models to compare accuracies.  First we will check the accuracy of the FP32 model.\n",
    "\n",
    "> **NOTE**: In this notebook, we run accuracy_check on a subset of the images in the dataset which takes less time.  For a more accurate check, all images should be used which may be done by not specifying the \"-ss <number>\" command line argument.\n",
    "\n",
    "> **NOTE**: The higher the percentage reported by `accuracy_check` the better, however most models are not 100% accurate.  For reference on what to expect form the model, the details for [resnet-50-tf](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/resnet-50-tf) on the Open Model Zoo include the accuracy of the original trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to '-ss <number>' to use only <number> of images, or set '' to use all images\n",
    "num_subsamples = \"-ss 300\"\n",
    "\n",
    "cmd = f\"accuracy_check -tf dlsdk -td CPU -s {DATASET_DIR} -d {dataset_def_yml_path} -c {model_acheck_yml_path} -m {MODEL_FP32_DIR} {num_subsamples}\"\n",
    "run_command_line(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check the accuracy of the INT8 model and can compare the results to the FP32 results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = f\"accuracy_check -tf dlsdk -td CPU -s {DATASET_DIR} -d {dataset_def_yml_path} -c {model_acheck_yml_path} -m {MODEL_FP32INT8_DIR} {num_subsamples}\"\n",
    "run_command_line(cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark the model before and after quantization\n",
    "Finally, we will measure the inference performance of the FP32 and INT8 models using  [OpenVINO's Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html), [`benchmark_app`](../104-model-tools/104-model-tools.ipynb)\n",
    "  \n",
    "> **NOTE**: In this notebook, we run benchmark_app for 15 seconds (\"-t <time_seconds>\" argument) to give a quick indication of performance. For more accurate performance, we recommended running benchmark_app for 60 seconds in a terminal/command prompt after closing other applications.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_benchmark_output(line):\n",
    "    if not (line.startswith(r\"[\") or line.startswith(\"  \") or len(line.rstrip()) < 1):\n",
    "        return line\n",
    "    return None\n",
    "\n",
    "\n",
    "# time to run benchmark\n",
    "time_secs = 15\n",
    "\n",
    "cmd = f\"benchmark_app -m {MODEL_FP32_DIR}/{OMZ_MODEL_NAME}.xml -d CPU -api async -t {time_secs}\"\n",
    "run_command_line(cmd, filter_benchmark_output)\n",
    "print()\n",
    "cmd = f\"benchmark_app -m {MODEL_FP32INT8_DIR}/{OMZ_MODEL_NAME}.xml -d CPU -api async -t {time_secs}\"\n",
    "run_command_line(cmd, filter_benchmark_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Optionally, all the downloaded and generated files may be removed by setting `do_cleanup` to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cleanup = False\n",
    "if do_cleanup:\n",
    "    shutil.rmtree(DATASET_DIR)\n",
    "    shutil.rmtree(MODEL_DIR)\n",
    "    shutil.rmtree(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
