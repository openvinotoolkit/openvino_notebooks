{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize the Open Model Zoo resnet-50-tf model\n",
    "Quantizing a model accelerates a trained model by reducing the precision necessary for its calculations.  Acceleration comes from lower-precision calculations being faster as well as less memory needed and less data to transfer since the data type itself is smaller along with the model weights data.  Though lower-precision may reduce model accuracy, typically a model using 32-bit floating-point precision (FP32) can be quantized to use lower-precision 8-bit integers (INT8) giving good results that are worth the trade off between accuracy and speed.  To see how quantization can accelerate models, see [INT8 vs FP32 Comparison on Select Networks and Platforms](https://docs.openvino.ai/latest/openvino_docs_performance_int8_vs_fp32.html#doxid-openvino-docs-performance-int8-vs-fp32) for some benchmarking results.\n",
    "\n",
    "[Intel Distribution of OpenVINO toolkit](https://software.intel.com/openvino-toolkit) includes the [Post-Training Optimization Tool (POT)](https://docs.openvino.ai/latest/pot_README.html) to automate quantization.  For models available from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), the [`omz_quantizer`](https://pypi.org/project/openvino-dev/) tool is available to automate running POT using its [DefaultQuantization](https://docs.openvino.ai/latest/pot_compression_algorithms_quantization_default_README.html#doxid-pot-compression-algorithms-quantization-default-r-e-a-d-m-e) 8-bit quantization algorithm to quantize models down to INT8 precision.\n",
    "\n",
    "This Jupyter* Notebook will go step-by-step through the workflow of downloading the [resnet-50-tf](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/resnet-50-tf) model from the Open Model Zoo through quantization and then checking and benchmarking the results.  The workflow consists of following the steps:\n",
    "1. Download and set up the the [Imagenette](https://github.com/fastai/imagenette) (subset of [ImageNet](http://www.image-net.org/)) validation dataset to be used by omz_quantize\n",
    "2. Download model\n",
    "3. Convert model to FP32 IR files\n",
    "4. Quantize FP32 model to create INT8 IR files\n",
    "5. Run inference on original and quantized model\n",
    "6. Check accuracy before and after quantization\n",
    "7. Benchmark before and after quantization\n",
    "\n",
    "While performing the steps above, the following [OpenVINO tools](https://pypi.org/project/openvino-dev/) will be used to download, convert, quantize, check accuracy, and benchmark the model:\n",
    "- `omz_downloader` - Download model from the Open Model Zoo\n",
    "- `omz_converter` - Convert an Open Model Zoo model\n",
    "- `omz_quantizer` - Quantize an Open Model Zoo model\n",
    "- `accuracy_check` - Check the accuracy of models using a validation dataset\n",
    "- `benchmark_app` - Benchmark models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary imports\n",
    "import glob\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from subprocess import PIPE, STDOUT, Popen\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from openvino.inference_engine import IECore\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as nbutils\n",
    "\n",
    "print(\"Imports complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "By default, this notebook downloads the model, dataset, etc. to subdirectories where this notebook is located.  The following variables may be used to set file locations:\n",
    "* `OMZ_MODEL_NAME`: Model name as it appears on the Open Model Zoo\n",
    "* `DATA_DIR`: Directory where dataset will be downloaded and set up\n",
    "* `MODEL_DIR`: Models will be downloaded into the `intel` and `public` folders in this directory\n",
    "* `OUTPUT_DIR`: Directory used to store any output and other downloaded files (e.g. configuration files for running accuracy_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base settings\n",
    "OMZ_MODEL_NAME = \"resnet-50-tf\"\n",
    "DATA_DIR = Path(\"data\")\n",
    "MODEL_DIR = Path(\"model\")\n",
    "OUTPUT_DIR = Path(\"output\")\n",
    "DATASET_DIR = DATA_DIR / \"imagenette\"\n",
    "LABELS_PATH = DATASET_DIR / \"imagenet_2012.txt\"\n",
    "\n",
    "# different model precisions location\n",
    "MODEL_PUBLIC_DIR = MODEL_DIR / \"public\" / OMZ_MODEL_NAME\n",
    "MODEL_FP32_DIR = MODEL_PUBLIC_DIR / \"FP32\"\n",
    "MODEL_FP32INT8_DIR = MODEL_PUBLIC_DIR / \"FP32-INT8\"\n",
    "\n",
    "# create directories if they do not already exist\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"OMZ_MODEL_NAME={OMZ_MODEL_NAME}\")\n",
    "print(f\"MODEL_PUBLIC_DIR={MODEL_PUBLIC_DIR}\")\n",
    "print(f\"DATASET_DIR={DATASET_DIR}\")\n",
    "print(f\"OUTPUT_DIR={OUTPUT_DIR}\")\n",
    "print(f\"LABELS_PATH={LABELS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "The `run_command_line()` helper function is provided to aid filtering the output of some of the commands that will be run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_command_line(cmd: str, filter=None):\n",
    "    \"\"\"\n",
    "    runs the given command-line outputting lines as they become available to show progress in ~realtime.\n",
    "    If a filter is provided, it will be called with each line before printing the result from calling the filter\n",
    "    :param cmd: String containing complete command-line to run\n",
    "    :param filter: Optional filter called per-line before printing\n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    print(f\"running command: {cmd}\")\n",
    "    proc = Popen(cmd.split(), stdout=PIPE, stderr=STDOUT, universal_newlines=True)\n",
    "    while proc.poll() is None:\n",
    "        line = proc.stdout.readline()\n",
    "        if filter is not None:\n",
    "            line = filter(line)\n",
    "        if line is not None:\n",
    "            sys.stdout.write(\"%s\" % (line))\n",
    "\n",
    "\n",
    "print(\"helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and set up the validation dataset\n",
    "Instead of using the very large [ImageNet](http://www.image-net.org/) dataset, the smaller [Imagenette 320px](https://github.com/fastai/imagenette) dataset containing 10 classes with lower-resolution images will be used by this notebook.  The Imagenette dataset will be downloaded and arranged to look just like ImageNet so that it can be used by the `omz_quantizer` and `accuracy_check` tools.  Any ImageNet (or subset of ImageNet) dataset may be used when following the steps in the notebook, however all must be set up as described on the Open Model Zoo [dataset.md:ImageNet](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/datasets.md#imagenet) page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_up_imagenette_dataset(output_dir):\n",
    "    output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    img_val_path = output_dir / \"ILSVRC2012_img_val\"\n",
    "    img_val_path.mkdir(exist_ok=True, parents=True)\n",
    "    img_val_ann_path = output_dir / \"val.txt\"\n",
    "\n",
    "    print(\"Downloading dataset...\")\n",
    "    data_tgzname = \"imagenette2-320.tgz\"\n",
    "    data_url = f\"https://s3.amazonaws.com/fast-ai-imageclas/{data_tgzname}\"\n",
    "    data_tgzpath = nbutils.download_file(data_url, data_tgzname, output_dir)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # uncompress files\n",
    "    print(f\"Extracting dataset from {data_tgzpath.relative_to(Path.cwd())}...\")\n",
    "    tar_ref = tarfile.open(data_tgzpath, \"r:gz\")\n",
    "    tar_ref.extractall(path=output_dir)\n",
    "    tar_ref.close()\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # download the class labels\n",
    "    print(\"Downloading class labels...\")\n",
    "    labels_url = f\"https://github.com/openvinotoolkit/open_model_zoo/raw/master/data/dataset_classes/{LABELS_PATH.name}\"\n",
    "    nbutils.download_file(labels_url, LABELS_PATH.name, output_dir)\n",
    "    print(\"Done.\")\n",
    "\n",
    "    # load labels for lookup\n",
    "    with open(LABELS_PATH) as labels_file:\n",
    "        labels = [line.rstrip() for line in labels_file]\n",
    "\n",
    "    # move image files and generate annotation file\n",
    "    print(\"Moving image files and creating annotation file...\")\n",
    "    dir_dict = {}\n",
    "    winid_map = {}\n",
    "    val_path = output_dir / data_tgzpath.stem / \"val\"\n",
    "    for root, dirs, files in os.walk(val_path):\n",
    "        # match each winid directory\n",
    "        if Path(root).name != \"val\":\n",
    "            file_list = [Path(root) / fname for fname in files]\n",
    "            winid = Path(root).name\n",
    "            dir_dict[winid] = file_list\n",
    "            label_idx = [i for i, item in enumerate(labels) if item.startswith(winid)][\n",
    "                0\n",
    "            ]\n",
    "            winid_map[winid] = label_idx\n",
    "            print(\n",
    "                f\"dir {root}, # files={len(files)}, label_idx={label_idx}, {labels[label_idx]}\"\n",
    "            )\n",
    "\n",
    "    # simple shuffle and move image files prefixed with \"val_n_\" and create annotations file\n",
    "    pop_count = len(dir_dict)\n",
    "    total_files = 0\n",
    "    ann_file = open(img_val_ann_path, \"w\")\n",
    "    while pop_count > 0:\n",
    "        pop_count = 0\n",
    "        for winid in dir_dict:\n",
    "            if len(dir_dict[winid]) > 0:\n",
    "                src_path = dir_dict[winid].pop()\n",
    "                dst_path = img_val_path / f\"val_{total_files:08}_{src_path.stem}.JPEG\"\n",
    "                # print(f\"winid={winid}, moving {src_path} to {dst_path}\")\n",
    "                shutil.move(src_path, dst_path)\n",
    "                ann_file.write(f\"{dst_path.name} {winid_map[winid]}\\n\")\n",
    "                pop_count += 1\n",
    "                total_files += 1\n",
    "\n",
    "    ann_file.close()\n",
    "    print(f\"moved {total_files} files\")\n",
    "    print(\"Done.\")\n",
    "    print(\"set up of imagenette dataset is complete.\")\n",
    "\n",
    "\n",
    "if not LABELS_PATH.exists():\n",
    "    set_up_imagenette_dataset(DATASET_DIR)\n",
    "else:\n",
    "    print(f\"{LABELS_PATH} exists, skipping setting up dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download model\n",
    "The OpenVINO tool [`omz_downloader`](https://pypi.org/project/openvino-dev/) is used to automatically download files from the Open Model Zoo.  To see the complete list of available models, the following command is used:\n",
    "```bash\n",
    "omz_downloader --print_all\n",
    "```\n",
    "\n",
    "The format of the command for `omz_downloader` to download a model is:\n",
    "```bash\n",
    "omz_downloader --name <model_name> --output <path_to_downloaded_models_dir>\n",
    "```\n",
    "The input arguments are as follows:\n",
    "- **--name** : The name of the model to download. **Note**: The name must be one of the model names listed from running the command: `omz_downloader --print_all`\n",
    "- **--output** : The top directory where models will be stored after they are downloaded \n",
    "\n",
    "> **NOTE**: If model IR files are available from the Open Model Zoo, then the downloaded models will appear in the `intel` subdirectory.  If no model IR files are available, then the downloaded models will appear in the `public` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_downloader --name $OMZ_MODEL_NAME --output $MODEL_DIR\n",
    "\n",
    "print(\"All files that were downloaded:\")\n",
    "print(*glob.glob(f\"{MODEL_DIR}/**\", recursive=True), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to IR files\n",
    "\n",
    "The public models from the Open Model Zoo are made available in their native framework file format and must be converted to OpenVINO Intermediate Representation (IR) files before running inference.  The OpenVINO tool [`omz_convert`](https://pypi.org/project/openvino-dev/) is used to convert Open Model Zoo models to the IR files necessary to run inference.\n",
    "\n",
    "The format to run the `omz_converter` command is:\n",
    "```bash\n",
    "omz_converter --name <model_name> --precisions <precision1,precision2,...> \n",
    "    --download_dir <path_to_downloaded_models_dir> --output <path_to_output_models_dir>\n",
    "```\n",
    "The input arguments are as follows:\n",
    "- **--name** : The name of the model to convert. It must be one of the models listed from running the command: `omz_downloader --print_all`\n",
    "- **--precisions** : The precisions (e.g. \"FP16\", \"FP32\", etc.) of the model to create\n",
    "- **--download_dir** : The top directory where models were originally downloaded by `omz_downloader`  \n",
    "- **--output** : The top directory where models will be stored after they are converted \n",
    "\n",
    "\n",
    "> **NOTE**: For models that are downloaded from the Open Model Zoo already as IR files, the converter utility will not do any conversion and will output the message \"Skipping <model_name> (no conversions defined)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_converter --name $OMZ_MODEL_NAME --precisions FP32 --download_dir $MODEL_DIR  --output $MODEL_DIR\n",
    "\n",
    "print(\"All IR files that were created:\")\n",
    "print(*glob.glob(f\"{MODEL_FP32_DIR}/**\", recursive=True), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantize the model to INT8\n",
    "For models downloaded from the Open Model Zoo, the [`omz_quantizer`](https://pypi.org/project/openvino-dev/) tool is used to quantize the model to a lower precision (e.g. quantize FP32 to INT8 precision).\n",
    "\n",
    "The format to run the `omz_quantizer` command is:\n",
    "```bash\n",
    "omz_quantizer --name <model_name> --model_dir <path_to_models_dir> --precisions <precision> \n",
    "    --dataset_dir <path_to_dataset_dir> --output <path_to_output_models_dir>\n",
    "```\n",
    "The input arguments are as follows:\n",
    "- **--name** : The name of the model to convert. It must be one of the models listed from running the command: `omz_downloader --print_all`\n",
    "- **--model_dir** : The top directory where models were stored after conversion by `omz_converter`  \n",
    "- **--output** : The top directory where models will be stored after they are quantized \n",
    "- **--dataset_dir** : The path to the dataset top directory\n",
    "- **--precisions** : The precision (e.g. \"FP32-INT8\") of the quantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!omz_quantizer --name $OMZ_MODEL_NAME --model_dir $MODEL_DIR  --output $MODEL_DIR  --dataset_dir $DATASET_DIR --precisions FP32-INT8\n",
    "\n",
    "print(\"All FP32-INT8 IR files that were created:\")\n",
    "print(*glob.glob(f\"{MODEL_FP32INT8_DIR}/**\", recursive=True), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model\n",
    "Now that the model has been quantized, we will run inference using both the original FP32 model and the new INT8 quantized model to see their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model_base_path, image_path):\n",
    "    \"\"\"\n",
    "    runs inferrence on an image using the given model and then displays the results\n",
    "    :param model_base_path: String containing path and file name of model excluding the extension (i.e. \".xml\")\n",
    "    :param image_path: String containing full path to the input image \n",
    "    :return: none\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    ie = IECore()\n",
    "\n",
    "    # create the network from the model\n",
    "    net = ie.read_network(\n",
    "        model=f\"{model_base_path}.xml\", weights=f\"{model_base_path}.bin\"\n",
    "    )\n",
    "    exec_net = ie.load_network(network=net, device_name=\"CPU\")\n",
    "\n",
    "    input_key = next(iter(exec_net.input_info))\n",
    "    output_key = next(iter(exec_net.outputs.keys()))\n",
    "    print(f\"model loaded from: {model_base_path}/{net.name}\")\n",
    "\n",
    "    # Load image\n",
    "    image = nbutils.load_image(image_path)\n",
    "    # N,C,H,W = batch size, number of channels, height, width\n",
    "    N, C, H, W = exec_net.input_info[input_key].tensor_desc.dims\n",
    "    # The network expects images in BGR format, same as OpenCV so just resize\n",
    "    input_image = cv2.resize(src=image, dsize=(W, H))\n",
    "    # reshape image to network input shape ([W,H,C]->[B,C,H,W])\n",
    "    input_image = np.expand_dims(input_image.transpose(2, 0, 1), 0)\n",
    "    # display original image (imshow requires RGB format, so convert BGR->RGB)\n",
    "    plt.imshow(nbutils.to_rgb(image))\n",
    "\n",
    "    # Run inference, result = [1,1001] with confidence level for each of the 1000 \n",
    "    #  classes and +1 for background.  The class with the highest confidence is \n",
    "    #  used to output the final result.\n",
    "    result = exec_net.infer(inputs={input_key: input_image})[output_key][0]\n",
    "    label_id = np.argmax(result)\n",
    "    conf = round(result[label_id] * 100, 2)\n",
    "    print(f\"label_id={label_id}, conf={conf} %\")\n",
    "\n",
    "    # Convert the inference result to a class name using the labels file\n",
    "    with open(LABELS_PATH) as f:\n",
    "        labels = [line.rstrip() for line in f]\n",
    "\n",
    "    print(f\"Image contains a '{labels[label_id]}', with {conf}% confidence\")\n",
    "\n",
    "\n",
    "# find known image\n",
    "files = glob.glob(f\"{DATASET_DIR}/**/*n02102040_2051.JPEG\", recursive=True)\n",
    "test_input_image = files[0]\n",
    "\n",
    "run_inference(f\"{MODEL_FP32_DIR}/{OMZ_MODEL_NAME}\", test_input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(f\"{MODEL_FP32INT8_DIR}/{OMZ_MODEL_NAME}\", test_input_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up to run accuracy_check\n",
    "We will check the accuracy of the two FP32 and INT8 models using  [OpenVINO's Accuracy Checker Tool](https://docs.openvino.ai/latest/omz_tools_accuracy_checker.html), [`accuracy_check`](https://pypi.org/project/openvino-dev/).  For each model, The Open Model Zoo includes the necessary `accuracy-check.yml` configuration and the global [`dataset_definitions.yml`](https://github.com/openvinotoolkit/open_model_zoo/blob/master/data/dataset_definitions.yml) files needed to run the `accuracy_check` tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve files needed by accuracy_check\n",
    "OMZ_GITHUB_URL = \"https://github.com/openvinotoolkit/open_model_zoo/raw/master\"\n",
    "dataset_def_yml = \"dataset_definitions.yml\"\n",
    "dataset_def_yml_url = f\"{OMZ_GITHUB_URL}/data/{dataset_def_yml}\"\n",
    "model_acheck_yml = \"accuracy-check.yml\"\n",
    "model_acheck_yml_url = (\n",
    "    f\"{OMZ_GITHUB_URL}/models/public/{OMZ_MODEL_NAME}/{model_acheck_yml}\"\n",
    ")\n",
    "\n",
    "print(f\"Downloading {OMZ_MODEL_NAME}/accuracy-check.yml ...\")\n",
    "model_acheck_yml_path = nbutils.download_file(\n",
    "    model_acheck_yml_url, model_acheck_yml, OUTPUT_DIR\n",
    ")\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "print(f\"Downloading {dataset_def_yml} ...\")\n",
    "dataset_def_yml_path = nbutils.download_file(\n",
    "    dataset_def_yml_url, dataset_def_yml, OUTPUT_DIR\n",
    ")\n",
    "print(\"Done.\")\n",
    "print(\"set up to run accuracy_check is complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy of the model before and after quantization\n",
    "Now we will run `accuracy_check` for both the original FP32 and the new quantized INT8 models to compare accuracies.\n",
    "\n",
    "The format to run the `accuracy_check` command is:\n",
    "```bash\n",
    "accuracy_check -tf <framework> -td <device> -s <path_to_dataset> -d <path_to_dataset_definitions_yml} \n",
    "    -c {path_to_model_configuration_yml} -m <path_to_model_ir_files> -ss <number_of_subsamples>\n",
    "```\n",
    "\n",
    "The input arguments are as follows:\n",
    "- **-tf** : The name of the framework (`dlsdk` refers to use OpenVINO) in the model's configuration .yml\n",
    "- **-td** : The device (\"CPU\", \"GPU\", etc.) to use when running inference\n",
    "- **-s** : The path to the dataset files\n",
    "- **-d** : The path to the dataset definitions .yml file\n",
    "- **-c** : The path to the model's configuration .yml file\n",
    "- **-m** : The path to the model's IR files (directory holding `<model>.xml` and `<model>.bin`)\n",
    "- **-ss** : The number of images to use from the dataset.  Default is all of them.\n",
    "\n",
    "> **NOTE**: In this notebook, we run accuracy_check on a subset of the images in the dataset which takes less time.  For a more accurate check, all images should be used which may be done by not specifying the \"-ss <number>\" command line argument.\n",
    "\n",
    "> **NOTE**: The higher the percentage reported by `accuracy_check` the better, however most models are not 100% accurate.  For reference on what to expect form the model, the details for [resnet-50-tf](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/resnet-50-tf) on the Open Model Zoo include the accuracy of the original trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to '-ss <number>' to use only <number> of images, or set '' to use all images\n",
    "num_subsamples = \"-ss 300\"\n",
    "\n",
    "print(f\"Checking accuracy of FP32 model {MODEL_FP32_DIR} ...\")\n",
    "cmd = f\"accuracy_check -tf dlsdk -td CPU -s {DATASET_DIR} -d {dataset_def_yml_path} -c {model_acheck_yml_path} -m {MODEL_FP32_DIR} {num_subsamples}\"\n",
    "run_command_line(cmd)\n",
    "print(\"Done.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Checking accuracy of FP32-INT8 model {MODEL_FP32INT8_DIR} ...\")\n",
    "cmd = f\"accuracy_check -tf dlsdk -td CPU -s {DATASET_DIR} -d {dataset_def_yml_path} -c {model_acheck_yml_path} -m {MODEL_FP32INT8_DIR} {num_subsamples}\"\n",
    "run_command_line(cmd)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Benchmark the model before and after quantization\n",
    "Finally, we will measure the inference performance of the FP32 and INT8 models using  [OpenVINO's Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html), [`benchmark_app`](https://pypi.org/project/openvino-dev/)\n",
    "\n",
    "The format to run the `benchmark_app` command is:\n",
    "```bash\n",
    "benchmark_app -m <path_to_model_xml_file> -d <device> -api <api_mode> -t <time_in_seconds>\n",
    "```\n",
    "The input arguments are as follows:\n",
    "- **-m** : The path to the model's `<model>.xml` IR file (`<model>.bin` will also be read)\n",
    "- **-d** : The device (\"CPU\", \"GPU\", etc.) to run during benchmarking\n",
    "- **-api** : The API mode to use when running inference: [`async`](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html#asynchronous-api) (default) for asynchronous throughput-oriented measurement or [`sync`](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html#synchronous-api) for synchronous (latency-oriented) measurement\n",
    "- **-t** : The number of seconds to run benchmarking.  Default is 60 seconds.\n",
    "    \n",
    "> **NOTE**: In this notebook, we run benchmark_app for 15 seconds to give a quick indication of performance. For more accurate performance, we recommended running benchmark_app for 60 seconds in a terminal/command prompt after closing other applications.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_benchmark_output(line):\n",
    "    if not (line.startswith(r\"[\") or line.startswith(\"  \") or len(line.rstrip()) < 1):\n",
    "        return line\n",
    "    return None\n",
    "\n",
    "\n",
    "# time to run benchmark\n",
    "time_secs = 15\n",
    "\n",
    "print(f\"Benchmarking FP32 model {MODEL_FP32_DIR} over {time_secs} seconds ..\")\n",
    "cmd = f\"benchmark_app -m {MODEL_FP32_DIR}/{OMZ_MODEL_NAME}.xml -d CPU -api async -t {time_secs}\"\n",
    "run_command_line(cmd, filter_benchmark_output)\n",
    "print(\"Done.\\n\")\n",
    "\n",
    "print(f\"Benchmarking FP32-INT8 {MODEL_FP32INT8_DIR} over {time_secs} seconds ..\")\n",
    "cmd = f\"benchmark_app -m {MODEL_FP32INT8_DIR}/{OMZ_MODEL_NAME}.xml -d CPU -api async -t {time_secs}\"\n",
    "run_command_line(cmd, filter_benchmark_output)\n",
    "print(\"Done.\")\n",
    "print(\"benchmarking complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "Optionally, all the downloaded and generated files may be removed by setting `doCleanup` to `True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_cleanup = False\n",
    "if do_cleanup:\n",
    "    shutil.rmtree(DATASET_DIR)\n",
    "    shutil.rmtree(MODEL_DIR)\n",
    "    shutil.rmtree(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
