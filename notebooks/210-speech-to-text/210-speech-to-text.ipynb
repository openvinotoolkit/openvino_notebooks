{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports modules required to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import librosa\n",
    "except OSError:\n",
    "    import sys\n",
    "    import types\n",
    "    sys.modules['soundfile'] = types.ModuleType('fake_soundfile')\n",
    "    import librosa\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "import wave\n",
    "from openvino.inference_engine import IECore\n",
    "from os import path, makedirs, listdir\n",
    "from shutil import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "In this part you have to set up all variables further used in notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_folder = \"model\"\n",
    "download_folder = \"output\"\n",
    "data_folder = \"data\"\n",
    "\n",
    "precision = \"FP16\"\n",
    "model_name = \"quartznet-15x5-en\"\n",
    "model_extensions = (\"bin\", \"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download models and convert public model\n",
    "\n",
    "We use `omz_downloader` and `omz_converter`, which are command-line tools from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. This step is skipped if the model is already downloaded. The selected model comes from the public directory, which means it must be converted into Intermediate Representation (IR).\n",
    "\n",
    "`omz_converter` is needed to convert pre-trainded `PyTorch` model to OpenVINO IR format. \n",
    "\n",
    "If it is your first run models will download and convert here. It might take up to ten minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(download_folder, exist_ok=True)\n",
    "\n",
    "# Check if models are already downloaded in download directory\n",
    "for extension in model_extensions:\n",
    "    if not path.isfile(f'{model_folder}/{model_name}.{extension}'):\n",
    "        download_command = f\"omz_downloader --name {model_name} --output_dir {download_folder} --precision {precision} --num_attempts 3\"\n",
    "        convert_command = f\"omz_converter --name {model_name} --precisions {precision} --download_dir {download_folder} --output_dir {download_folder}\"\n",
    "        # Run commands, first download model than convert it to inferable \n",
    "        ! $download_command\n",
    "        # Models are downloaded straight to output folder, we will keep all not used files outside of models directory\n",
    "        ! $convert_command\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy models to model folder\n",
    "\n",
    "At this point both models are kept in download_folder (by default named ```output```). We need only .bin and .xml files from there that we will copy to ```model directory```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab17c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "makedirs(model_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in listdir(f\"{download_folder}/public/{model_name}/{precision}\"):\n",
    "    copy(src=f\"{download_folder}/public/{model_name}/{precision}/{file_name}\", dst=model_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load audio file\n",
    "\n",
    "Now, when model files are downloaded and converted, you need to load audio file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ad393",
   "metadata": {},
   "source": [
    "### Defining constants\n",
    "\n",
    "First step will be locating audio file and defining alphabet used by model. In this case you will use latin alphabet begining with space symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "audio_file_name = \"how_are_you_doing.wav\"\n",
    "alphabet = \" abcdefghijklmnopqrstuvwxyz'\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22895cf3",
   "metadata": {},
   "source": [
    "### Load audio file\n",
    "\n",
    "Next step is opening defined in previous cell audio file and getting params that will allow you to decide if file needs adjustments before placing into preprocessing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addacba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of usage\n",
    "wave_read = wave.open(f'{data_folder}/{audio_file_name}')\n",
    "channel_num, sample_width, sampling_rate, pcm_length, compression_type, _ = wave_read.getparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337adee3",
   "metadata": {},
   "source": [
    "### Assertions about audio file\n",
    "\n",
    "For this model we can use audio files that meets those requirements:\n",
    "* 16-bit WAV PCM\n",
    "* without any compression type (linear PCM WAV)\n",
    "* single channel (mono WAV PCM)\n",
    "* 16 KHz audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sample_width == 2, \"Only 16-bit WAV PCM supported\"\n",
    "assert compression_type == 'NONE', \"Only linear PCM WAV files supported\"\n",
    "assert channel_num == 1, \"Only mono WAV PCM supported\"\n",
    "assert sampling_rate == 16000, \"Only 16 KHz audio supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = wave_read.readframes(pcm_length * channel_num)\n",
    "audio = np.frombuffer(audio, dtype=np.int16)\n",
    "audio = audio.reshape((pcm_length, channel_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_melspectrum(audio, sampling_rate, padding=16):\n",
    "    assert sampling_rate == 16000, \"Only 16 KHz audio supported\"\n",
    "    preemph = 0.97\n",
    "    preemphased = np.concatenate([audio[:1], audio[1:] - preemph * audio[:-1].astype(np.float32)])\n",
    "\n",
    "    win_length = round(sampling_rate * 0.02)\n",
    "    spec = np.abs(librosa.core.spectrum.stft(preemphased, n_fft=512, hop_length=round(sampling_rate * 0.01),\n",
    "        win_length=win_length, center=True, window=scipy.signal.windows.hann(win_length), pad_mode='reflect'))\n",
    "    mel_basis = librosa.filters.mel(sampling_rate, 512, n_mels=64, fmin=0.0, fmax=8000.0, htk=False)\n",
    "    log_melspectrum = np.log(np.dot(mel_basis, np.power(spec, 2)) + 2 ** -24)\n",
    "\n",
    "    normalized = (log_melspectrum - log_melspectrum.mean(1)[:, None]) / (log_melspectrum.std(1)[:, None] + 1e-5)\n",
    "    remainder = normalized.shape[1] % padding\n",
    "    if remainder != 0:\n",
    "        return np.pad(normalized, ((0, 0), (0, padding - remainder)))[None]\n",
    "    return normalized[None]\n",
    "\n",
    "def ctc_greedy_decode(pred):\n",
    "    pred = np.squeeze(pred)\n",
    "    prev_id = blank_id = len(alphabet)\n",
    "    transcription = []\n",
    "    for idx in pred.argmax(axis=1):\n",
    "        if prev_id != idx != blank_id:\n",
    "            transcription.append(alphabet[idx])\n",
    "        prev_id = idx\n",
    "    return ''.join(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audio_to_melspectrum(audio.flatten(), sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = IECore()\n",
    "\n",
    "net = ie.read_network(\n",
    "    model=f\"{model_folder}/{model_name}.xml\"\n",
    ")\n",
    "net.reshape({next(iter(net.input_info)): audio.shape})\n",
    "exec_net = ie.load_network(net, \"CPU\")\n",
    "\n",
    "input_layer_ir = next(iter(exec_net.input_info))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_probs = exec_net.infer({input_layer_ir: audio}).values()\n",
    "\n",
    "character_probs = next(iter(character_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = ctc_greedy_decode(character_probs)\n",
    "print(transcription)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
