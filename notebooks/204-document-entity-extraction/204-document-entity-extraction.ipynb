{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Document Entity Extraction with OpenVINO\n",
    "\n",
    "This demo shows entity extraction from text and document inferencing using JPG and PDF files with OpenVINO. We use [small BERT-large-like model](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002) distilled and quantized to INT8 on SQuAD v1.1 training set from larger BERT-large model. The model comes from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/). At the bottom of this notebook, you will see live inference results from your inputs and templates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "import operator\n",
    "\n",
    "import pdfplumber\n",
    "import pytesseract\n",
    "\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "import numpy as np\n",
    "import tokens_bert as tokens\n",
    "from openvino.runtime import Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparations to setup tesseract OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Windows, adds directory that contains tesseract.exe to the PATH.\n",
    "# Assumes that tesseract python package is installed in the default.\n",
    "# For different installation path, set pytesseract.pytesseract.tesseract_cmd\n",
    "\n",
    "if sys.platform == \"win32\":\n",
    "    from pathlib import Path\n",
    "\n",
    "    OCR_INSTALL_DIR = r\"C:/Program Files/Tesseract-OCR\"\n",
    "    ocr_path = sorted(list(Path(OCR_INSTALL_DIR).glob(\"**/tesseract.exe\")))\n",
    "    if len(ocr_path) == 0:\n",
    "        print(\"Cannot find Tesseract OCR tool executable. Fow Windows OS, set\"\n",
    "              \" pytesseract.pytesseract.tesseract_cmd to tesseract.exe path\")\n",
    "    else:\n",
    "        pytesseract.pytesseract.tesseract_cmd = str(ocr_path[-1])\n",
    "        print(\"Successfully set pytesseract.pytesseract.tesseract_cmd \"\n",
    "              f\"to path {ocr_path[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The model\n",
    "\n",
    "### Download the model\n",
    "\n",
    "We use `omz_downloader`, which is a command-line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. If the model is already downloaded, this step is skipped.\n",
    "\n",
    "You can download and use any of the following models: `bert-large-uncased-whole-word-masking-squad-0001`, `bert-large-uncased-whole-word-masking-squad-int8-0001`, `bert-small-uncased-whole-word-masking-squad-0001`, `bert-small-uncased-whole-word-masking-squad-0002`, `bert-small-uncased-whole-word-masking-squad-int8-0002`, just change the model name below. Any of these models are already converted to OpenVINO Intermediate Representation (IR), so there is no need to use `omz_converter`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# desired precision\n",
    "precision = \"FP16-INT8\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"bert-small-uncased-whole-word-masking-squad-int8-0002\"\n",
    "\n",
    "model_path = f\"model/intel/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/intel/{model_name}/{precision}/{model_name}.bin\"\n",
    "\n",
    "download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {model_name} \" \\\n",
    "                   f\"--precision {precision} \" \\\n",
    "                   f\"--output_dir {base_model_dir} \" \\\n",
    "                   f\"--cache_dir {base_model_dir}\"\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the model\n",
    "\n",
    "Downloaded models are located in a fixed structure, which indicates vendor, model name and precision. Only a few lines of code are required to run the model. First, we initialize OpenVINO. Then we read the network architecture and model weights from the .xml and .bin files. Finally, we compile the network for the desired device. You can choose `CPU` or `GPU` in the case of this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "core = Core()\n",
    "# read the network and corresponding weights from file\n",
    "model = core.read_model(model=model_path, weights=model_weights_path)\n",
    "# load the model on the CPU (you can use GPU as well)\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_keys = list(compiled_model.inputs)\n",
    "output_keys = list(compiled_model.outputs)\n",
    "\n",
    "# get network input size\n",
    "input_size = compiled_model.input(0).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Input keys are the names of the input nodes and output keys contain names of output nodes of the network. In the case of the BERT-large-like model, we have four inputs and two outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "[i.any_name for i in input_keys], [o.any_name for o in output_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Processing\n",
    "\n",
    "NLP models usually take a list of tokens as standard input. A token is a single word converted to some integer. To provide the proper input, we need the vocabulary for such mapping. We also define some special tokens like separators or padding and a function to load the content. Content can be loaded from either simple text or perform OCR conversion from a JPG image or PDF file. Many such conversion can be supported like HTML, XML, PNG files etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# path to vocabulary file\n",
    "vocab_file_path = \"data/vocab.txt\"\n",
    "\n",
    "# create dictionary with words and their indices\n",
    "vocab = tokens.load_vocab_file(vocab_file_path)\n",
    "\n",
    "# define special tokens\n",
    "cls_token = vocab[\"[CLS]\"]\n",
    "pad_token = vocab[\"[PAD]\"]\n",
    "sep_token = vocab[\"[SEP]\"]\n",
    "\n",
    "\n",
    "# Create custom class and handle tesseract error\n",
    "class TesseractNotFoundError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "# function to load context text string or perform document OCR\n",
    "def load_context(source, source_format=\"text\"):\n",
    "    if source_format == \"document_jpg\":\n",
    "        try:\n",
    "            response = requests.get(source)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            ocr_text = pytesseract.image_to_string(img)\n",
    "        except Exception:\n",
    "            raise TesseractNotFoundError\n",
    "\n",
    "        print(\"Extracted OCR text:\\n\", ocr_text)\n",
    "        return ocr_text\n",
    "    elif source_format == \"document_pdf\":\n",
    "        with pdfplumber.open(source) as pdf:\n",
    "            first_page = pdf.pages[0]\n",
    "            ocr_text = first_page.extract_text()\n",
    "            print(\"Extracted OCR text:\\n\", ocr_text)\n",
    "            return ocr_text\n",
    "    return source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "### Preprocessing\n",
    "\n",
    "The input size in this case is 384 tokens long. The main input (`input_ids`) to used BERT model consist of two parts: entity tokens and context tokens separated by some special tokens. If entity + context are shorter than 384 tokens, padding tokens are added. If entity + context is longer than 384 tokens, the context must be split into parts and the entity with different parts of context must be fed to the network many times. We use overlapping, so neighbor parts of the context are overlapped by half size of the context part (if the context part equals 300 tokens, neighbor context parts overlap with 150 tokens). We also need to provide: `attention_mask`, which is a sequence of integer values representing the mask of valid values in the input; `token_type_ids`, which is a sequence of integer values representing the segmentation of the `input_ids` into entity and context; `position_ids`, which is a sequence of integer values from 0 to 383 representing the position index for each input token. To know more about input, please read [this](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/intel/bert-small-uncased-whole-word-masking-squad-int8-0002#input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# generator of a sequence of inputs\n",
    "def prepare_input(entity_tokens, context_tokens):\n",
    "    # length of entity in tokens\n",
    "    entity_len = len(entity_tokens)\n",
    "    # context part size\n",
    "    context_len = input_size - entity_len - 3\n",
    "\n",
    "    if context_len < 16:\n",
    "        raise RuntimeError(\"Question is too long in comparison to \"\n",
    "                           \"input size. No space for context\")\n",
    "\n",
    "    # take parts of context with overlapping by 0.5\n",
    "    for start in range(0, max(1, len(context_tokens) - context_len),\n",
    "                       context_len // 2):\n",
    "        # part of context\n",
    "        part_context_tokens = context_tokens[start:start + context_len]\n",
    "        # input: entity and context separated by special tokens\n",
    "        input_ids = [cls_token] + entity_tokens + [sep_token] + \\\n",
    "            part_context_tokens + [sep_token]\n",
    "        # 1 for any index if there is no padding token, 0 otherwise\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        # 0 for entity tokens, 1 for context part\n",
    "        token_type_ids = [0] * (entity_len + 2) + [1] * \\\n",
    "            (len(part_context_tokens) + 1)\n",
    "\n",
    "        # add padding at the end\n",
    "        (input_ids, attention_mask, token_type_ids), pad_number = pad(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids)\n",
    "\n",
    "        # create input to feed the model\n",
    "        input_dict = {\n",
    "            \"input_ids\": np.array([input_ids], dtype=np.int32),\n",
    "            \"attention_mask\": np.array([attention_mask], dtype=np.int32),\n",
    "            \"token_type_ids\": np.array([token_type_ids], dtype=np.int32),\n",
    "        }\n",
    "\n",
    "        # some models require additional position_ids\n",
    "        if \"position_ids\" in [i_key.any_name for i_key in input_keys]:\n",
    "            position_ids = np.arange(len(input_ids))\n",
    "            input_dict[\"position_ids\"] = np.array([position_ids],\n",
    "                                                  dtype=np.int32)\n",
    "\n",
    "        yield input_dict, pad_number, start\n",
    "\n",
    "\n",
    "# function to add padding\n",
    "def pad(input_ids, attention_mask, token_type_ids):\n",
    "    # how many padding tokens\n",
    "    diff_input_size = input_size - len(input_ids)\n",
    "\n",
    "    if diff_input_size > 0:\n",
    "        # add padding to all inputs\n",
    "        input_ids = input_ids + [pad_token] * diff_input_size\n",
    "        attention_mask = attention_mask + [0] * diff_input_size\n",
    "        token_type_ids = token_type_ids + [0] * diff_input_size\n",
    "\n",
    "    return (input_ids, attention_mask, token_type_ids), diff_input_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Postprocessing\n",
    "\n",
    "The results from the network are raw (logits). We need to use the softmax function to get the probability distribution. Then, we are looking for the best entity extraction in the current part of the context (the highest score) and we return the score and the context range for the extracted entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def postprocess(output_start, output_end, entity_tokens,\n",
    "                context_tokens_start_end, padding, start_idx):\n",
    "\n",
    "    def get_score(logits):\n",
    "        out = np.exp(logits)\n",
    "        return out / out.sum(axis=-1)\n",
    "\n",
    "    # get start-end scores for context\n",
    "    score_start = get_score(output_start)\n",
    "    score_end = get_score(output_end)\n",
    "\n",
    "    # index of first context token in tensor\n",
    "    context_start_idx = len(entity_tokens) + 2\n",
    "    # index of last+1 context token in tensor\n",
    "    context_end_idx = input_size - padding - 1\n",
    "\n",
    "    # find product of all start-end combinations to find the best one\n",
    "    max_score, max_start, max_end = find_best_entity_window(\n",
    "        start_score=score_start, end_score=score_end,\n",
    "        context_start_idx=context_start_idx, context_end_idx=context_end_idx\n",
    "    )\n",
    "\n",
    "    # convert to context text start-end index\n",
    "    max_start = context_tokens_start_end[max_start + start_idx][0]\n",
    "    max_end = context_tokens_start_end[max_end + start_idx][1]\n",
    "\n",
    "    return max_score, max_start, max_end\n",
    "\n",
    "\n",
    "def find_best_entity_window(start_score, end_score,\n",
    "                            context_start_idx, context_end_idx):\n",
    "    context_len = context_end_idx - context_start_idx\n",
    "    score_mat = np.matmul(\n",
    "        start_score[context_start_idx:context_end_idx].reshape(\n",
    "            (context_len, 1)),\n",
    "        end_score[context_start_idx:context_end_idx].reshape(\n",
    "            (1, context_len)),\n",
    "    )\n",
    "    # reset candidates with end before start\n",
    "    score_mat = np.triu(score_mat)\n",
    "    # reset long candidates (>16 words)\n",
    "    score_mat = np.tril(score_mat, 16)\n",
    "    # find the best start-end pair\n",
    "    max_s, max_e = divmod(score_mat.flatten().argmax(), score_mat.shape[1])\n",
    "    max_score = score_mat[max_s, max_e]\n",
    "\n",
    "    return max_score, max_s, max_e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Firstly, we need to create a list of tokens from the context and the entity. Then, we are looking for the best extracted entity by trying different parts of the context. The best extracted entity should come with the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_best_entity(entity, context):\n",
    "    # convert context string to tokens\n",
    "    context_tokens, context_tokens_start_end = tokens.text_to_tokens(\n",
    "        text=context.lower(), vocab=vocab)\n",
    "    # convert entity string to tokens\n",
    "    entity_tokens, _ = tokens.text_to_tokens(text=entity.lower(), vocab=vocab)\n",
    "\n",
    "    results = []\n",
    "    # iterate through different parts of context\n",
    "    for network_input, padding, start_idx in prepare_input(\n",
    "            entity_tokens=entity_tokens, context_tokens=context_tokens):\n",
    "        # get output layers\n",
    "        output_start_key = compiled_model.output(\"output_s\")\n",
    "        output_end_key = compiled_model.output(\"output_e\")\n",
    "\n",
    "        # openvino inference\n",
    "        result = compiled_model(network_input)\n",
    "        # postprocess result getting the score and context range for the answer\n",
    "        score_start_end = postprocess(\n",
    "            output_start=result[output_start_key][0],\n",
    "            output_end=result[output_end_key][0],\n",
    "            entity_tokens=entity_tokens,\n",
    "            context_tokens_start_end=context_tokens_start_end,\n",
    "            padding=padding, start_idx=start_idx)\n",
    "        results.append(score_start_end)\n",
    "\n",
    "    # find the highest score\n",
    "    answer = max(results, key=operator.itemgetter(0))\n",
    "    # return the part of the context, which is already an answer\n",
    "    return context[answer[1]:answer[2]], answer[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Main Processing Function\n",
    "\n",
    "Run entity extraction on specific knowledge base and iterate through the template entities. Final output is a JSON object with two fields i.e. Extraction (consists of Entity, Type and Confidence Score) and Overall Processing time. Currently application supports only few entities from healthcare domain, more templates and entities can be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "test_replace": {
     "input()": "\"What does OpenVINO mean?\"",
     "while True": "for i in range(1)"
    }
   },
   "outputs": [],
   "source": [
    "healthcare_template = [\"name\", \"age\", \"medical condition\",\n",
    "                       \"medication\", \"dosage\", \"dosage unit\"]\n",
    "\n",
    "\n",
    "def run_analyze_entities(source, source_type=\"text\"):\n",
    "    print(f\"Context: {source}\\n\", flush=True)\n",
    "\n",
    "    try:\n",
    "        context = load_context(source, source_type)\n",
    "    except TesseractNotFoundError:\n",
    "        print(\"OCR Error: To extract entities from image \"\n",
    "              \"you need to install Tesseract-OCR engine. \"\n",
    "              \"Follow this link to resolve \"\n",
    "              \"https://stackoverflow.com/questions/50655738\")\n",
    "        return\n",
    "\n",
    "    if len(context) == 0:\n",
    "        print(\"Error: Empty context or outside paragraphs\")\n",
    "        return\n",
    "\n",
    "    # measure processing time\n",
    "    start_time = time.perf_counter()\n",
    "    extract = []\n",
    "    for field in healthcare_template:\n",
    "        entity_to_find = field + \"?\"\n",
    "        entity, score = get_best_entity(entity=entity_to_find,\n",
    "                                        context=context)\n",
    "        extract.append({\"Entity\": entity, \"Type\": field,\n",
    "                        \"Score\": f\"{score:.2f}\"})\n",
    "    end_time = time.perf_counter()\n",
    "    res = {\"Extraction\": extract, \"Time\": f\"{end_time - start_time:.2f}s\"}\n",
    "    print(\"\\nJSON Output:\")\n",
    "    print(json.dumps(res, sort_keys=False, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run\n",
    "\n",
    "### Run on local paragraphs\n",
    "\n",
    "Change sources to your own medical-domain text, supported by the template, to perform entity extraction. It supports only one input text at a time. Usually, you need to wait a few seconds for the entities to be extracted, but longer the context, longer would be the waiting time. The model is very limited and sensitive for the input and predefined template. The answer can depend on whether it is supported by the template or not. The model will try to extract entities even if not supported by the template, so in that case, you can see random results.\n",
    "\n",
    "Sample source: Medical Named Entity and Relationship Extraction Paragraph (from [here](https://aws.amazon.com/comprehend/medical/features/?nc=sn&loc=2))\n",
    "\n",
    "Sample entities supported by the application healthcare-domain template:\n",
    "- Name, Age, Medical Condition, Medication, Dosage and Dosage Units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_text = \"Mr. Smith is a 63-year-old gentleman with coronary artery \"\\\n",
    "                \"disease and hypertension. CURRENT MEDICATIONS:\" \\\n",
    "                \"taking a dosage of Aspirin 20 mg once daily.\"\n",
    "run_analyze_entities(source_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on JPG Image\n",
    "\n",
    "You can also provide a sample JPG image. Note that the context (knowledge base) is built from the text available in the image. If some information is outside the paragraphs or not supported by the predefined template, the algorithm won't able to perform entity extraction correctly.\n",
    "\n",
    "Sample source: Paragraph Converted into JPG format image file (from [here](https://arxiv.org/pdf/1910.07419.pdf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id = \"164302195-74abe168-e560-4dc2-bd0e-cdb422af756a\"\n",
    "src_document = f\"https://user-images.githubusercontent.com/33627846/{id}.jpg\"\n",
    "run_analyze_entities(src_document, source_type=\"document_jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run on PDF File\n",
    "\n",
    "You can also provide a sample PDF file. Note that the context (knowledge base) is built from the text available in the PDF. If some information is outside the paragraphs or not supported by the predefined template, the algorithm won't able to perform entity extraction correctly.\n",
    "\n",
    "Sample source: Paragraph Converted into PDF format file (from [here](https://biotext.berkeley.edu/dis_treat_data.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_document = \"data/sample.pdf\"\n",
    "run_analyze_entities(src_document, source_type=\"document_pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
