{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "# Human Action Recognition with OpenVINO\n",
    "\n",
    "This notebook demonstrates live human action recognition with OpenVINO. We use the [Action Recognition Models](https://docs.openvino.ai/2020.2/usergroup13.html) from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo), specifically the [Encoder](https://docs.openvino.ai/2020.2/_models_intel_action_recognition_0001_encoder_description_action_recognition_0001_encoder.html) and [Decoder](https://docs.openvino.ai/2020.2/_models_intel_action_recognition_0001_decoder_description_action_recognition_0001_decoder.html). Both models create a sequence to sequence (`\"seq2seq\"`)<sup id=\"a1\">[1](#f1)</sup> system to identify the  human activities for [Kinetics-400 dataset](https://deepmind.com/research/open-source/kinetics). The models use the Video Transformer approach with ResNet34 encoder<sup id=\"a2\">[2](#f2)</sup>. We will see in this notebook how to create the following pipeline:\n",
    "\n",
    "<img align='center' src=\"action_recognition_pipeline.jpeg\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "At the bottom of this notebook, you will see live inference results from your webcam. You can also upload a video file.\n",
    "\n",
    "NOTE: _To use the webcam, you must run this Jupyter notebook on a computer with a webcam. If you run on a server, the webcam will not work. However, you can still do inference on a video in the final step._\n",
    "\n",
    "<b id=\"f1\">1</b> seq2seq: Deep learning models that take a sequence of items to the input and output. In this case, input: video frames, output: actions sequence. This `\"seq2seq\"` is composed of an encoder and a decoder. The encoder captures the `\"context\"` of the inputs to be analyzed by the decoder, and finally gets the human action and confidence.[$\\hookleftarrow$](#a1)\n",
    "\n",
    "<b id=\"f2\">2</b>  [Video Transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)#:~:text=A%20transformer%20is%20a%20deep,in%20computer%20vision%20(CV).), and [ResNet34](https://www.kaggle.com/pytorch/resnet34). [$\\hookleftarrow$](#a2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import collections\n",
    "import os\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython import display\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from openvino import inference_engine as ie\n",
    "\n",
    "from openvino.inference_engine import get_version\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The models\n",
    "\n",
    "### Download the models\n",
    "\n",
    "We use `omz_downloader`, which is a command-line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model.\n",
    "\n",
    "In this case you can use `\"action-recognition-0001\"` as a model name, and the system automatically downloads the two models `\"action-recognition-0001-encoder\"` and `\"action-recognition-0001-decoder\"`\n",
    "\n",
    "If you want to download another model (`\"driver-action-recognition-adas-0002-encoder\"` + `\"driver-action-recognition-adas-0002-decoder\"`, and `\"i3d-rgb-tf\"`) , please change the model name. Note: Using a model outside the list can require different pre- and post-processing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory where model will be downloaded\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# model name as named in Open Model Zoo\n",
    "model_name = \"action-recognition-0001\"\n",
    "# selected precision (FP32, FP16, FP16-INT8)\n",
    "precision = \"FP16-INT8\"\n",
    "\n",
    "model_path = f\"model/intel/{model_name}/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/intel/{model_name}/{model_name}/{precision}/{model_name}.bin\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    download_command = f\"omz_downloader \" \\\n",
    "                       f\"--name {model_name} \" \\\n",
    "                       f\"--precision {precision} \" \\\n",
    "                       f\"--output_dir {base_model_dir}\"\n",
    "    ! $download_command\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the model path of your models\n",
    "\n",
    "We will be able to work with the models locally. Please check that you can see your models in your local machine after your download process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the proper path for architecture and weights model files.\n",
    "model_path_decoder = f\"model/intel/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.xml\"\n",
    "model_weights_path_decoder = f\"model/intel/{model_name}/{model_name}-decoder/{precision}/{model_name}-decoder.bin\"\n",
    "\n",
    "model_path_encoder = f\"model/intel/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.xml\"\n",
    "model_weights_path_encoder = f\"model/intel/{model_name}/{model_name}-encoder/{precision}/{model_name}-encoder.bin\"\n",
    "\n",
    "# Testing the location of *.bin and *.xml\n",
    "files = os.listdir(f\"model/intel/{model_name}/{model_name}-encoder/{precision}\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load your labels\n",
    "\n",
    "We are using [Kinetics-400 dataset](https://deepmind.com/research/open-source/kinetics), and also we are providing you with the text file with the labels for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to use another model, you should change the labels file\n",
    "labels = \"data/kinetics.txt\"\n",
    "\n",
    "if labels:\n",
    "    with open(labels) as f:\n",
    "        labels = [line.strip() for line in f]\n",
    "else:\n",
    "    labels = None\n",
    "    \n",
    "#print(labels[0], np.shape(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the models\n",
    "\n",
    "We will load the two models for this particular architecture, Encoder and Decoder. Downloaded models are located in a fixed structure, indicating vendor, model name, and precision.\n",
    "\n",
    " 1. Initialize inference engine (IECore)\n",
    " 2. Read the network from *.bin and *.xml files (weights and architecture)\n",
    " 3. Load the model on the \"CPU.\"\n",
    " 4. Get input and output names of nodes.\n",
    "\n",
    "Only a few lines of code are required to run the model. Let's see it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Initialization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model initialization\n",
    "\n",
    "# initialize inference engine\n",
    "ie_core = ie.IECore()\n",
    "\n",
    "def model_init(model: str, weights: str) -> dict:\n",
    "    \"\"\"\n",
    "    Read the network and weights from file, load the \n",
    "    model on the CPU and get input and output names of nodes\n",
    "    \n",
    "    :param: model: model architecture path *.xml\n",
    "    :param: model: model weights path *.bin\n",
    "    :retuns: \n",
    "            exec_net: Encoder model network\n",
    "            input_key: Input node network\n",
    "            output_key: Output node network\n",
    "    \"\"\"\n",
    "    \n",
    "    # read the network and corresponding weights from file\n",
    "    net = ie_core.read_network(model, weights)\n",
    "    # load the model on the CPU (you can use GPU or MYRIAD as well)\n",
    "    exec_net = ie_core.load_network(net, \"CPU\")\n",
    "    # get input and output names of nodes\n",
    "    input_key = list(exec_net.input_info)[0]\n",
    "    output_keys = list(exec_net.outputs.keys())\n",
    "    return input_key, output_keys, exec_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization for Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder initialization \n",
    "input_key_en, output_keys_en, exec_net_en = model_init(model_path_encoder, model_weights_path_encoder)\n",
    "#Decoder initialization \n",
    "input_key_de, output_keys_de, exec_net_de = model_init(model_path_decoder, model_weights_path_decoder)\n",
    "\n",
    "# get input size - Encoder\n",
    "height_en, width_en = exec_net_en.input_info[input_key_en].tensor_desc.dims[2:]\n",
    "# get input size - Decoder\n",
    "height_de = exec_net_de.input_info[input_key_de].tensor_desc.dims[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions\n",
    "\n",
    "We will use the next AI functions to run the architecture Encoder-Decoder of ResNet34\n",
    "\n",
    "1. Preprocess the input image before running the Encoder model. (`center_crop` and `adaptative_resize`)\n",
    "2. Decode top-3 probabilities into label names. (`decode_output`)\n",
    "\n",
    "Also, we can see here the minimal configuration to display text over the live streaming or video for the main process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_crop(frame: np.array) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Center crop squared the original frame to standardize the input image to the encoder model\n",
    "    \n",
    "    :param frame: input frame\n",
    "    :returns: center-crop-squared frame\n",
    "    \"\"\"\n",
    "    img_h, img_w, _ = frame.shape\n",
    "    min_dim = min(img_h, img_w)\n",
    "    start_x = int((img_w - min_dim) /2.)\n",
    "    start_y = int((img_h - min_dim) /2.)\n",
    "    return frame[start_y:start_y+min_dim, start_x:start_x+min_dim, ...]\n",
    "\n",
    "def adaptive_resize(frame: np.array, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Adaptative resize of the cropped image to adjust to the input size model 224\n",
    "    \n",
    "    :param frame: input frame\n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized frame, np.array type \n",
    "    \"\"\"\n",
    "    h, w, _ = frame.shape\n",
    "    scale = size / min(h, w)\n",
    "    ow, oh = int(w * scale), int(h * scale)\n",
    "    if ow == w and oh == h:\n",
    "        return frame\n",
    "    return cv2.resize(frame, (ow, oh))\n",
    "\n",
    "def decode_output(probs: np.array, labels: np.array, top_k: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decodes top probabilities into corresponding label names\n",
    "    \n",
    "    :param probs: confidence vector for 400 actions\n",
    "    :param labels: list of actions\n",
    "    :param top_k: The k most probable positions in the list of labels\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions   \n",
    "    \"\"\"\n",
    "    top_k = 3\n",
    "    #top_ind = np.argsort(probs)[::-1][:top_k]\n",
    "    top_ind = np.argsort(-1*probs)[:top_k]\n",
    "    #print(top_ind)\n",
    "    #print(top_ind.shape)\n",
    "    out_label = np.array(labels)[top_ind.astype(int)]\n",
    "    decoded_labels = [out_label[0][0], out_label[0][1], out_label[0][2]]\n",
    "    top_probs = np.array(probs)[0][top_ind.astype(int)]\n",
    "    decoded_top_probs = [top_probs[0][0], top_probs[0][1], top_probs[0][2]]\n",
    "    #print(top_probs[0][0], top_probs[0][1], top_probs[0][2])\n",
    "    return decoded_labels, decoded_top_probs\n",
    "\n",
    "\"\"\"Configuration for displaying images with text\"\"\"\n",
    "FONT_COLOR = (255, 255, 255)\n",
    "FONT_STYLE = cv2.FONT_HERSHEY_DUPLEX\n",
    "FONT_SIZE = 0.7\n",
    "TEXT_VERTICAL_INTERVAL = 25\n",
    "TEXT_LEFT_MARGIN = 15    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Pipeline\n",
    "\n",
    "Following this pipeline, \n",
    "\n",
    "<img align='center' src=\"action_recognition_pipeline.jpeg\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "\n",
    "we will use the next functions to help us to:\n",
    "\n",
    "1. Preprocess frame before running the Encoder. (`preprocessing`)\n",
    "2. Encoder Inference per frame. (`encoder`)\n",
    "3. Decoder inference per set of frames. (`decoder`)\n",
    "4. Normalize the Decoder output to get confidence values per action recognition label. (`softmax`)\n",
    "\n",
    "Also, we can see here the minimal configuration to display text over the live streaming or video for the main process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(frame: np.ndarray, size: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Preparing frame before Encoder.\n",
    "    \n",
    "    The image should be scaled to its shortest dimension at \"size\" \n",
    "    and cropped, centered, and squared so that both width and \n",
    "    height have lengths \"size\". Frame must be transposed from \n",
    "    Height-Width-Channels (HWC) to Channels-Height-Width (CHW).\n",
    "    \n",
    "    :param frame: input frame \n",
    "    :param size: input size to encoder model\n",
    "    :returns: resized and cropped frame\n",
    "    \"\"\"\n",
    "    # Adaptative resize\n",
    "    preprocessed = adaptive_resize(frame, size)\n",
    "    # Center_crop                \n",
    "    preprocessed = center_crop(preprocessed)\n",
    "    # Transpose frame HWC -> CHW\n",
    "    preprocessed = preprocessed.transpose((2, 0, 1))  # HWC -> CHW\n",
    "    return preprocessed\n",
    "\n",
    "def encoder(preprocessed: np.ndarray, exec_net_en: dict, input_key_en: list, encoder_output: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Encoder Inference per frame. This function calls the network previously \n",
    "    configured for the encoder model (exec_net_en), extracts the data \n",
    "    from the output node, and appends it in an array to be used by the decoder.\n",
    "    \n",
    "    :param: preprocessed: preprocessing frame\n",
    "    :param: exec_net_en: Encoder model network\n",
    "    :param: input_key_en: Input node network\n",
    "    :param: encoder_output: embedding layer that is appended with each arriving frame\n",
    "    :returns: encoder_output: embedding layer that is appended with each arriving frame\n",
    "    \"\"\"\n",
    "    # get results on action-recognition-0001-encoder model\n",
    "    infer_result_encoder = exec_net_en.infer(inputs={input_key_en: preprocessed})\n",
    "    encoder_result = infer_result_encoder[output_keys_en[0]]\n",
    "    encoder_output.append(encoder_result)\n",
    "    return encoder_output\n",
    "\n",
    "def decoder(encoder_output: np.array, exec_net_de: dict, input_key_de: list) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Decoder inference per set of frames. This function concatenates the embedding layer\n",
    "    froms the encoder output, transpose the array to match with the decoder input size. \n",
    "    Calls the network previously configured for the decoder model (exec_net_de), extracts\n",
    "    the logits and normalize those to get confidence values along specified axis.\n",
    "    Decodes top probabilities into corresponding label names \n",
    "        \n",
    "    :param: encoder_output: embedding layer for 16 frames\n",
    "    :param: exec_net_de: Decoder model network\n",
    "    :param: input_key_de: Input node network\n",
    "    :returns: decoded_labels: The k most probable actions from the labels list\n",
    "              decoded_top_probs: confidence for the k most probable actions\n",
    "    \"\"\"\n",
    "    # Concatenate sample_duration frames in just one array\n",
    "    decoder_input = np.concatenate(encoder_output, axis=0)\n",
    "    # Organize input shape vector to the Decoder (shape: [1x16x512]]\n",
    "    decoder_input = decoder_input.transpose((2, 0, 1, 3))\n",
    "    decoder_input = np.squeeze(decoder_input, axis=3)\n",
    "\n",
    "    # get results on action-recognition-0001-decoder model\n",
    "    logits = exec_net_de.infer(inputs={input_key_de: decoder_input})\n",
    "    result_de = logits[output_keys_de[0]]\n",
    "    # Normalize logits to get confidence values along specified axis\n",
    "    probs = softmax(result_de - np.max(result_de))\n",
    "    # Decodes top probabilities into corresponding label names\n",
    "    decoded_labels, decoded_top_probs = decode_output(probs, labels, top_k=3)\n",
    "    return decoded_labels, decoded_top_probs\n",
    "\n",
    "\n",
    "def softmax(x: np.array, axis: int = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Normalizes logits to get confidence values along specified axis\n",
    "    \n",
    "    x: np.array, axis=None\n",
    "    \"\"\"\n",
    "    exp = np.exp(x)\n",
    "    return exp / np.sum(exp, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Main Processing Function\n",
    "\n",
    "Run action recognition function will run in different operations, either a webcam or a video file. See the list of procedures below:\n",
    "\n",
    "1. Create a video player to play with target fps (`utils.VideoPlayer`).\n",
    "2. Prepare a set of frames to be encoded-decoded.\n",
    "3. Run AI functions\n",
    "4. Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_action_recognition(source: str = 0, flip: bool = False, use_popup: bool = False, skip_first_frames: int = 0):\n",
    "    \"\"\"\n",
    "    Use the \"source\" webcam or video file to run the complete pipeline for action-recognition problem\n",
    "    1. Create a video player to play with target fps\n",
    "    2. Prepare a set of frames to be encoded-decoded\n",
    "    3. Preprocess frame before Encoder\n",
    "    4. Encoder Inference per frame\n",
    "    5. Decoder inference per set of frames\n",
    "    6. Visualize the results\n",
    "    \n",
    "    :param: source: webcam \"0\" or video path\n",
    "    :param: flip: to be used by VideoPlayer function for flipping capture image\n",
    "    :param: use_popup: False for showing encoded frames over this notebook, True for creating a popup window.\n",
    "    :param: skip_first_frames: to be used by VideoPlayer function for skipping first frames\n",
    "    :returns: display video over the notebook or in a popup window\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    sample_duration=16\n",
    "    size=224\n",
    "    #create a video player to play with target fps\n",
    "    player = utils.VideoPlayer(source, flip=flip, fps=30, skip_first_frames=skip_first_frames)\n",
    "    # start capturing\n",
    "    player.start()\n",
    "    try:\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(title, cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "        \n",
    "        processing_times = collections.deque()\n",
    "        frames = []\n",
    "        video_frames = []\n",
    "        encoder_output = []\n",
    "        decoded_labels = [0, 0, 0]\n",
    "        decoded_top_probs = [0, 0, 0]\n",
    "        while True:\n",
    "            \n",
    "            encoder_output = []\n",
    "            # loop over the number of required sample frames\n",
    "            # Prepare a set of frames to be enconded-decoded\n",
    "            for i in range(0, sample_duration):\n",
    "                # read a frame from the video stream\n",
    "                frame = player.next()\n",
    "                frames.append(frame)\n",
    "                \n",
    "                if frame is None:\n",
    "                    print(\"Source ended\")\n",
    "                    break\n",
    "                scale = 1280 / max(frame.shape)\n",
    "                \n",
    "                # Adaptative resize for visualization\n",
    "                if scale < 1:\n",
    "                    frame = cv2.resize(frame, None, fx=scale, fy=scale, interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                # Preprocess frame before Encoder\n",
    "                preprocessed = preprocessing(frame, size)\n",
    "                                \n",
    "                # measure processing time\n",
    "                start_time = time.time()\n",
    "                \n",
    "                #Encoder Inference per frame\n",
    "                encoder_output = encoder(preprocessed, exec_net_en, input_key_en, encoder_output)\n",
    "                \n",
    "                #Decoder inference per set of frames\n",
    "                # wait for sample duration to work with decoder model\n",
    "                if len(encoder_output) == sample_duration:\n",
    "                    decoded_labels, decoded_top_probs = decoder(encoder_output, exec_net_de, input_key_de)\n",
    "                                        \n",
    "                #Inference has finished ... Let's to display results\n",
    "                stop_time = time.time()\n",
    "                \n",
    "                # calculate processing time\n",
    "                processing_times.append(stop_time - start_time)\n",
    "                # create a text template to show inference results over video\n",
    "                text_template = '{label},{conf:.2f}%,{Time:.1f}ms,{fps:.1f}FPS'\n",
    "                # use processing times from last 200 frames\n",
    "                if len(processing_times) > 200:\n",
    "                    processing_times.popleft()\n",
    "                    \n",
    "                _, f_width = frame.shape[:2]\n",
    "                # mean processing time [ms]\n",
    "                processing_time = np.mean(processing_times) * 1000\n",
    "                fps = 1000 / processing_time\n",
    "\n",
    "                #Visualize the results\n",
    "                for i in range(0, 3):\n",
    "                    display_text = text_template.format(label=decoded_labels[i], conf=decoded_top_probs[i] * 100, Time=processing_time, fps=fps)\n",
    "                    text_loc = (TEXT_LEFT_MARGIN, TEXT_VERTICAL_INTERVAL * (i + 1))\n",
    "                    cv2.putText(frame, display_text, text_loc, FONT_STYLE, FONT_SIZE, FONT_COLOR)\n",
    "                \n",
    "                # use this workaround if there is flickering\n",
    "                if use_popup:\n",
    "                    cv2.imshow(title, frame)\n",
    "                    key = cv2.waitKey(1)\n",
    "                    # escape = 27\n",
    "                    if key == 27:\n",
    "                        break\n",
    "                else:\n",
    "                    # encode numpy array to jpg\n",
    "                    _, encoded_img = cv2.imencode(\".jpg\", frame, params=[cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "                    # create IPython image\n",
    "                    i = display.Image(data=encoded_img)\n",
    "                    # display the image in this notebook\n",
    "                    display.clear_output(wait=True)\n",
    "                    display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "\n",
    "    # stop capturing\n",
    "    player.stop()\n",
    "    if use_popup:\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Action Recognition using your webcam\n",
    "\n",
    "> NOTE: _To use the webcam, you must run this Jupyter notebook on a computer with a webcam. If you run on a server, the webcam will not work. However, you can still do inference on a video in the final step._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_action_recognition(source=0, flip=False, use_popup=False, skip_first_frames=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Action Recognition on a Video File\n",
    "\n",
    "If you don't have a webcam, you can still run this demo with a video file. Any format supported by OpenCV will work (see: https://docs.opencv.org/4.5.1/dd/d43/tutorial_py_video_display.html). You can skip first N frames to fast forward video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_file = \"C:/Intel/notebooks/openvino_notebooks/notebooks/501-action-webcam/video/paula.mp4\"\n",
    "#\"https://github.com/intel-iot-devkit/sample-videos/raw/master/head-pose-face-detection-female.mp4?raw=true\"\n",
    "#\"https://github.com/intel-iot-devkit/sample-videos/blob/master/store-aisle-detection.mp4?raw=true\"\n",
    "\n",
    "run_action_recognition(video_file, flip=False, use_popup=False, skip_first_frames=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
