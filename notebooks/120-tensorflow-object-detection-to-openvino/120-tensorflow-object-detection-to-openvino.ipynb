{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a TensorFlow Object Detection Model to OpenVINOâ„¢\n",
    "\n",
    "[TensorFlow](https://www.tensorflow.org/), or TF for short, is an open-source framework for machine learning.\n",
    "\n",
    "The [TensorFlow Object Detection API](https://github.com/tensorflow/models/tree/master/research/object_detection) is an open-source computer vision framework built on top of TensorFlow. It is used for building object detection and image segmentation models that can localize multiple objects in the same image. TensorFlow Object Detection API supports various architectures and models, which can be found and downloaded from the [TensorFlow Hub](https://tfhub.dev/tensorflow/collections/object_detection/1).\n",
    "\n",
    "This tutorial shows how to convert a TensorFlow [Faster R-CNN with Resnet-50 V1](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1) object detection model to OpenVINO [Intermediate Representation](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_IR_and_opsets.html) (OpenVINO IR) format, using [Model Optimizer](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html). After creating the OpenVINO IR, load the model in [OpenVINO Runtime](https://docs.openvino.ai/nightly/openvino_docs_OV_UG_OV_Runtime_User_Guide.html) and do inference with a sample image."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q \"openvino-dev>=2023.0.0\" \"numpy>=1.21.0\" \"opencv-python\" \"matplotlib>=3.4,<3.5.3\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notebook uses utility functions.\n",
    "The cell below will download the `notebook_utils` Python module from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the notebook utils script from the openvino_notebooks repo\n",
    "import urllib.request\n",
    "\n",
    "urllib.request.urlretrieve(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py\",\n",
    "    filename=\"notebook_utils.py\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard python modules\n",
    "from pathlib import Path\n",
    "\n",
    "# External modules and dependencies\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Notebook utils module\n",
    "from notebook_utils import download_file\n",
    "\n",
    "# OpenVINO modules\n",
    "from openvino.runtime import Core, serialize\n",
    "from openvino.tools import mo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings\n",
    "\n",
    "Define model related variables and create corresponding directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for models files\n",
    "model_dir = Path(\"model\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create directory for TensorFlow model\n",
    "tf_model_dir = model_dir / \"tf\"\n",
    "tf_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create directory for OpenVINO IR model\n",
    "ir_model_dir = model_dir / \"ir\"\n",
    "ir_model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"faster_rcnn_resnet50_v1_640x640\"\n",
    "\n",
    "openvino_ir_path = ir_model_dir / f\"{model_name}.xml\"\n",
    "\n",
    "tf_model_url = \"https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1?tf-hub-format=compressed\"\n",
    "\n",
    "tf_model_archive_filename = f\"{model_name}.tar.gz\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Model from TensorFlow Hub\n",
    "\n",
    "Download archive with TensorFlow Object Detection model ([faster_rcnn_resnet50_v1_640x640](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1)) from TensorFlow Hub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(\n",
    "    url=tf_model_url,\n",
    "    filename=tf_model_archive_filename,\n",
    "    directory=tf_model_dir\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract TensorFlow Object Detection model from the downloaded archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(tf_model_dir / tf_model_archive_filename) as file:\n",
    "    file.extractall(path=tf_model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Model to OpenVINO IR\n",
    "\n",
    "OpenVINO Model Optimizer Python API can be used to convert the TensorFlow model to OpenVINO IR. \n",
    "\n",
    "`mo.convert_model` function accept path to TensorFlow model and returns OpenVINO Model class instance which represents this model.\n",
    "Also we need to provide model input shape (`input_shape`) that is described at [model overview page on TensorFlow Hub](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1). \n",
    "Optionally, we can apply compression to FP16 model weigths using `compress_to_fp16=True` option and integrate preprocessing using this approach.\n",
    "\n",
    "The converted model is ready to load on a device using `compile_model` or saved on disk using the `serialize` function to reduce loading time when the model is run in the future. \n",
    "\n",
    "See the [Model Optimizer Developer Guide](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information about Model Optimizer and TensorFlow [models suport](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_prepare_model_convert_model_Convert_Model_From_TensorFlow.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_model = mo.convert_model(\n",
    "    saved_model_dir=tf_model_dir,\n",
    "    input_shape=[[1, 255, 255, 3]]\n",
    ")\n",
    "\n",
    "# Save converted OpenVINO IR model to the corresponding directory\n",
    "serialize(ov_model, openvino_ir_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference on the Converted Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = Core()\n",
    "openvino_ir_model = ie.read_model(openvino_ir_path)\n",
    "compiled_model = ie.compile_model(model=openvino_ir_model, device_name=\"CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Model Information\n",
    "\n",
    "Faster R-CNN with Resnet-50 V1 object detection model has one input - a three-channel image of variable size. The input tensor shape is `[1, height, width, 3]` with values in `[0, 255]`.\n",
    "\n",
    "Model output dictionary contains several tensors:\n",
    "- `num_detections` - the number of detections in `[N]` format.\n",
    "- `detection_boxes` - bounding box coordinates for all `N` detections in `[ymin, xmin, ymax, xmax]` format.\n",
    "- `detection_classes` - `N` detection class indexes size from the label file.\n",
    "- `detection_scores` - `N` detection scores (confidence) for each detected class.\n",
    "- `raw_detection_boxes` - decoded detection boxes without Non-Max suppression.\n",
    "- `raw_detection_scores` - class score logits for raw detection boxes.\n",
    "- `detection_anchor_indices` - the anchor indices of the detections after NMS.\n",
    "- `detection_multiclass_scores` - class score distribution (including background) for detection boxes in the image including background class.\n",
    "\n",
    "In this tutorial we will mostly use `detection_boxes`, `detection_classes`, `detection_scores` tensors. It is important to mention, that values of these tensors correspond to each other and are ordered by the highest detection score: the first detection box corresponds to the first detection class and to the first (and highest) detection score.\n",
    "\n",
    "See the [model overview page on TensorFlow Hub](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1) for more information about model inputs, outputs and their formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = compiled_model.inputs\n",
    "model_input = compiled_model.input(0)\n",
    "model_outputs = compiled_model.outputs\n",
    "\n",
    "print(\"Model inputs count:\", len(model_inputs))\n",
    "print(\"Model input:\", model_input)\n",
    "\n",
    "print(\"Model outputs count:\", len(model_outputs))\n",
    "print(\"Model outputs:\")\n",
    "for output in model_outputs:\n",
    "    print(\"  \", output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get an Image for Test Inference\n",
    "\n",
    "Load and save an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(\"./data/coco_bike.jpg\")\n",
    "\n",
    "download_file(\n",
    "    url=\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_bike.jpg\",\n",
    "    filename=image_path.name,\n",
    "    directory=image_path.parent,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the image, resize and convert it to the input shape of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the image\n",
    "image = cv2.imread(filename=str(image_path))\n",
    "\n",
    "# The network expects images in RGB format\n",
    "image = cv2.cvtColor(image, code=cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize the image to the network input shape\n",
    "resized_image = cv2.resize(src=image, dsize=(255, 255))\n",
    "\n",
    "# Transpose the image to the network input shape\n",
    "network_input_image = np.expand_dims(resized_image, 0)\n",
    "\n",
    "# Show the image\n",
    "plt.imshow(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_result = compiled_model(network_input_image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After model inference on the test image, object detection data can be extracted from the result.\n",
    "For further model result visualization `detection_boxes`, `detection_classes` and `detection_scores` outputs will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, detection_boxes, detection_classes, _, detection_scores, num_detections, _, _ = model_outputs\n",
    "\n",
    "image_detection_boxes = inference_result[detection_boxes]\n",
    "print(\"image_detection_boxes:\", image_detection_boxes)\n",
    "\n",
    "image_detection_classes = inference_result[detection_classes]\n",
    "print(\"image_detection_classes:\", image_detection_classes)\n",
    "\n",
    "image_detection_scores = inference_result[detection_scores]\n",
    "print(\"image_detection_scores:\", image_detection_scores)\n",
    "\n",
    "image_num_detections = inference_result[num_detections]\n",
    "print(\"image_detections_num:\", image_num_detections)\n",
    "\n",
    "# Alternatively, inference result data can be extracted by model output name with `.get()` method\n",
    "assert (inference_result[detection_boxes] == inference_result.get(\"detection_boxes\")).all(), \"extracted inference result data should be equal\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference Result Visualization\n",
    "\n",
    "Define utility functions to visualize the inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "def add_detection_box(box: np.ndarray, image: np.ndarray, label: Optional[str] = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Helper function for adding single bounding box to the image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    box : np.ndarray\n",
    "        Bounding box coordinates in format [ymin, xmin, ymax, xmax]\n",
    "    image : np.ndarray\n",
    "        The image to which detection box is added\n",
    "    label : str, optional\n",
    "        Detection box label string, if not provided will not be added to result image (default is None)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        NumPy array including both image and detection box\n",
    "\n",
    "    \"\"\"\n",
    "    ymin, xmin, ymax, xmax = box\n",
    "    point1, point2 = (int(xmin), int(ymin)), (int(xmax), int(ymax))\n",
    "    box_color = [random.randint(0, 255) for _ in range(3)]\n",
    "    line_thickness = round(0.002 * (image.shape[0] + image.shape[1]) / 2) + 1\n",
    "\n",
    "    cv2.rectangle(img=image, pt1=point1, pt2=point2, color=box_color, thickness=line_thickness, lineType=cv2.LINE_AA)\n",
    "\n",
    "    if label:\n",
    "        font_thickness = max(line_thickness - 1, 1)\n",
    "        font_face = 0\n",
    "        font_scale = line_thickness / 3\n",
    "        font_color = (255, 255, 255)\n",
    "        text_size = cv2.getTextSize(text=label, fontFace=font_face, fontScale=font_scale, thickness=font_thickness)[0]\n",
    "        # Calculate rectangle coordinates\n",
    "        rectangle_point1 = point1\n",
    "        rectangle_point2 = (point1[0] + text_size[0], point1[1] - text_size[1] - 3)\n",
    "        # Add filled rectangle\n",
    "        cv2.rectangle(img=image, pt1=rectangle_point1, pt2=rectangle_point2, color=box_color, thickness=-1, lineType=cv2.LINE_AA)\n",
    "        # Calculate text position\n",
    "        text_position = point1[0], point1[1] - 3\n",
    "        # Add text with label to filled rectangle\n",
    "        cv2.putText(img=image, text=label, org=text_position, fontFace=font_face, fontScale=font_scale, color=font_color, thickness=font_thickness, lineType=cv2.LINE_AA)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from openvino.runtime.utils.data_helpers import OVDict\n",
    "\n",
    "\n",
    "def visualize_inference_result(inference_result: OVDict, image: np.ndarray, labels_map: Dict, detections_limit: Optional[int] = None):\n",
    "    \"\"\"\n",
    "    Helper function for visualizing inference result on the image\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    inference_result : OVDict\n",
    "        Result of the compiled model inference on the test image\n",
    "    image : np.ndarray\n",
    "        Original image to use for visualization\n",
    "    labels_map : Dict\n",
    "        Dictionary with mappings of detection classes numbers and its names\n",
    "    detections_limit : int, optional\n",
    "        Number of detections to show on the image, if not provided all detections will be shown (default is None)\n",
    "    \"\"\"\n",
    "    detection_boxes: np.ndarray = inference_result.get(\"detection_boxes\")\n",
    "    detection_classes: np.ndarray = inference_result.get(\"detection_classes\")\n",
    "    detection_scores: np.ndarray = inference_result.get(\"detection_scores\")\n",
    "    num_detections: np.ndarray = inference_result.get(\"num_detections\")\n",
    "\n",
    "    detections_limit = int(\n",
    "        min(detections_limit, num_detections[0])\n",
    "        if detections_limit is not None\n",
    "        else num_detections[0]\n",
    "    )\n",
    "\n",
    "    # Normalize detection boxes coordinates to original image size\n",
    "    original_image_height, original_image_width, _ = image.shape\n",
    "    normalized_detection_boxex = detection_boxes[::] * [\n",
    "        original_image_height,\n",
    "        original_image_width,\n",
    "        original_image_height,\n",
    "        original_image_width,\n",
    "    ]\n",
    "\n",
    "    image_with_detection_boxex = np.copy(image)\n",
    "\n",
    "    for i in range(detections_limit):\n",
    "        detected_class_name = labels_map[int(detection_classes[0, i])]\n",
    "        score = detection_scores[0, i]\n",
    "        label = f\"{detected_class_name} {score:.2f}\"\n",
    "        add_detection_box(\n",
    "            box=normalized_detection_boxex[0, i],\n",
    "            image=image_with_detection_boxex,\n",
    "            label=label,\n",
    "        )\n",
    "\n",
    "    plt.imshow(image_with_detection_boxex)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow Object Detection model ([faster_rcnn_resnet50_v1_640x640](https://tfhub.dev/tensorflow/faster_rcnn/resnet50_v1_640x640/1)) used in this notebook was trained on [COCO 2017](https://cocodataset.org/) dataset with 91 classes.\n",
    "For better visualization experience we can use COCO dataset labels with human readable class names instead of class numbers or indexes. \n",
    "\n",
    "We can download COCO dataset classes labels from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_labels_file_path = Path(\"./data/coco_91cl.txt\")\n",
    "\n",
    "download_file(\n",
    "    url=\"https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/data/dataset_classes/coco_91cl.txt\",\n",
    "    filename=coco_labels_file_path.name,\n",
    "    directory=coco_labels_file_path.parent,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create dictionary `coco_labels_map` with mappings between detection classes numbers and its names from the downloaded file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(coco_labels_file_path, \"r\") as file:\n",
    "    coco_labels = file.read().strip().split(\"\\n\")\n",
    "    coco_labels_map = dict(enumerate(coco_labels, 1))\n",
    "\n",
    "print(coco_labels_map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to visualize model inference results on the original test image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_inference_result(\n",
    "    inference_result=inference_result,\n",
    "    image=image,\n",
    "    labels_map=coco_labels_map,\n",
    "    detections_limit=5,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This section contains suggestions on how to additionally improve the performance of your application using OpenVINO.\n",
    "\n",
    "### Async inference pipeline\n",
    "The key advantage of the Async API is that when a device is busy with inference, the application can perform other tasks in parallel (for example, populating inputs or scheduling other requests) rather than wait for the current inference to complete first. To understand how to perform async inference using openvino, refer to the [Async API tutorial](../115-async-api/115-async-api.ipynb).\n",
    "\n",
    "### Integration preprocessing to model\n",
    "\n",
    "Preprocessing API enables making preprocessing a part of the model reducing application code and dependency on additional image processing libraries. \n",
    "The main advantage of Preprocessing API is that preprocessing steps will be integrated into the execution graph and will be performed on a selected device (CPU/GPU etc.) rather than always being executed on CPU as part of an application. This will improve selected device utilization.\n",
    "\n",
    "For more information, refer to the [Optimize Preprocessing tutorial](../118-optimize-preprocessing/118-optimize-preprocessing.ipynb) and to the overview of [Preprocessing API](https://docs.openvino.ai/2023.0/openvino_docs_OV_Runtime_UG_Preprocessing_Overview.html).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
