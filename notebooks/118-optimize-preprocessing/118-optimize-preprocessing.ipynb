{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "rQc-wXjqrEuR"
   },
   "source": [
    "# Optimize Preprocessing\n",
    "\n",
    "When input data does not fit the model input tensor perfectly, additional operations/steps are needed to transform the data to the format expected by the model. This tutorial demonstrates how it could be performed with Preprocessing API. Preprocessing API is an easy-to-use instrument, that enables integration of preprocessing steps into an execution graph and performing it on a selected device, which can improve device utilization. For more information about Preprocessing API, see this [overview](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Preprocessing_Overview.html#) and [details](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Preprocessing_Details.html)\n",
    "\n",
    "This tutorial include following steps:\n",
    "- Downloading the model.\n",
    "- Setup preprocessing with ModelOptimizer, loading the model and inference with original image.\n",
    "- Setup preprocessing with Preprocessing API, loading the model and inference with original image.\n",
    "- Fitting image to the model input type and inference with prepared image.\n",
    "- Comparing results on one picture.\n",
    "- Comparing performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4cSNQWdbSyeo"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from openvino.tools import mo\n",
    "import matplotlib.pyplot as plt\n",
    "from openvino.runtime import Core, serialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup image and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"../data/image/coco.jpg\"\n",
    "device = \"CPU\"\n",
    "# device = \"GPU\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading the model\n",
    "\n",
    "This tutorial uses the [InceptionResNetV2](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2). The InceptionResNetV2 model is the second of the [Inception](https://github.com/tensorflow/tpu/tree/master/models/experimental/inception) family of models designed to perform image classification. Like other Inception models, InceptionResNetV2 has been pre-trained on the [ImageNet](https://image-net.org/) data set. For more details about this family of models, see the [research paper](https://arxiv.org/abs/1602.07261).\n",
    "\n",
    "\n",
    "Load the model by using [tf.keras.applications api](https://www.tensorflow.org/api_docs/python/tf/keras/applications/inception_resnet_v2) and save it to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"InceptionResNetV2\"\n",
    "\n",
    "model_dir = Path(\"model\")\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "model_path = model_dir / model_name\n",
    "\n",
    "model = tf.keras.applications.InceptionV3()\n",
    "model.save(model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core = Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the original parameters of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB));\n",
    "print(f\"The original shape of the image is {image.shape}\")\n",
    "print(f\"The original data type of the image is {image.dtype}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to OpenVINO IR and setup preprocessing steps with Model Optimizer\n",
    "\n",
    "Use Model Optimizer to convert a TensorFlow model to OpenVINO IR. `mo.convert_model` python function will be used for converting model using [OpenVINO Model Optimizer](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_Python_API.html). The function returns instance of OpenVINO Model class, which is ready to use in Python interface but can also be serialized to OpenVINO IR format for future execution using `openvino.runtime.serialize`. The models will be saved to the `./model/ir_model/` directory.\n",
    "\n",
    "In this step, some conversions can be setup, which will enable reduction of work on processing the input data before propagating it through the network. These conversions will be inserted as additional input pre-processing sub-graphs into the converted model.\n",
    "\n",
    "Setup the following conversions:\n",
    "- mean normalization with `mean_values` parameter.\n",
    "- scale with `scale_values`.\n",
    "- color conversion, the color format of example image will be `BGR`, but the model required `RGB` format, so add `reverse_input_channels=True` to process the image into the desired format.\n",
    "\n",
    "Also converting of layout could be specified with `layout` option. More information and parameters described in the [Embedding Preprocessing Computation article](https://docs.openvino.ai/2023.0/openvino_docs_MO_DG_Additional_Optimization_Use_Cases.html#embedding-preprocessing-computation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_path_mo_preprocess = model_dir / \"ir_model\" / f\"{model_name}_mo_preproc.xml\"\n",
    "\n",
    "ov_model_mo_preprocess = None\n",
    "\n",
    "if ir_path_mo_preprocess.exists():\n",
    "    ov_model_mo_preprocess = core.read_model(model=ir_path_mo_preprocess)\n",
    "    print(f\"Model in OpenVINO format already exists: {ir_path_mo_preprocess}\")\n",
    "else: \n",
    "    ov_model_mo_preprocess = mo.convert_model(saved_model_dir=model_path,\n",
    "                                              model_name=model_path.name,\n",
    "                                              mean_values=[127.5,127.5,127.5],\n",
    "                                              scale_values=[127.5,127.5,127.5],\n",
    "                                              reverse_input_channels=True,\n",
    "                                              input_shape=[1,299,299,3])\n",
    "    serialize(ov_model_mo_preprocess, str(ir_path_mo_preprocess))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_mo_preprocess(image_path, model):\n",
    "    img = cv2.imread(filename=image_path)\n",
    "\n",
    "    input_layer_ir = next(iter(model.inputs))\n",
    "\n",
    "    # N, H, W, C = batch size, height, width, number of channels\n",
    "    N, H, W, C = input_layer_ir.shape\n",
    "    # Resize image to the input size expected by the model.\n",
    "    img = cv2.resize(img, (H, W))\n",
    "\n",
    "    # Fit image data type to expected by the model value\n",
    "    img = np.float32(img)\n",
    "\n",
    "    # Reshape to match the input shape expected by the model.\n",
    "    input_tensor = np.expand_dims(img, axis=0)\n",
    "\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "mo_pp_input_tensor = prepare_image_mo_preprocess(image_path, ov_model_mo_preprocess)\n",
    "\n",
    "print(f\"The shape of the image is {mo_pp_input_tensor.shape}\")\n",
    "print(f\"The data type of the image is {mo_pp_input_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile model and perform inerence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model_mo_pp = core.compile_model(model=ov_model_mo_preprocess, device_name=device)\n",
    "\n",
    "output_layer = compiled_model_mo_pp.output(0)\n",
    "\n",
    "result = compiled_model_mo_pp(mo_pp_input_tensor)[output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup preprocessing steps with Preprocessing API and perform inference\n",
    "\n",
    "Intuitively, preprocessing API consists of the following parts:\n",
    "- Tensor - declares user data format, like shape, layout, precision, color format from actual user’s data.\n",
    "- Steps - describes sequence of preprocessing steps which need to be applied to user data.\n",
    "- Model - specifies model data format. Usually, precision and shape are already known for model, only additional information, like layout can be specified.\n",
    "\n",
    "Graph modifications of a model shall be performed after the model is read from a drive and before it is loaded on the actual device.\n",
    "\n",
    "Pre-processing support following operations (please, see more details [here](https://docs.openvino.ai/2023.0/classov_1_1preprocess_1_1PreProcessSteps.html#doxid-classov-1-1preprocess-1-1-pre-process-steps-1aeacaf406d72a238e31a359798ebdb3b7))\n",
    "- Mean/Scale Normalization\n",
    "- Converting Precision\n",
    "- Converting layout (transposing)\n",
    "- Resizing Image\n",
    "- Color Conversion\n",
    "- Custom Operations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to OpenVINO IR with Model Optimizer\n",
    "\n",
    "The options for preprocessing are not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir_path = model_dir / \"ir_model\" / f\"{model_name}.xml\"\n",
    "\n",
    "ppp_model = None\n",
    "\n",
    "if ir_path.exists():\n",
    "    ppp_model = core.read_model(model=ir_path)\n",
    "    print(f\"Model in OpenVINO format already exists: {ir_path}\")\n",
    "else: \n",
    "    ppp_model = mo.convert_model(saved_model_dir=model_path,\n",
    "                                 input_shape=[1,299,299,3])\n",
    "    serialize(ppp_model, str(ir_path))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create PrePostProcessor Object\n",
    "\n",
    "The [PrePostProcessor()](https://docs.openvino.ai/2023.0/classov_1_1preprocess_1_1PrePostProcessor.html#doxid-classov-1-1preprocess-1-1-pre-post-processor) class enables specifying the preprocessing and postprocessing steps for a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.preprocess import PrePostProcessor\n",
    "\n",
    "ppp = PrePostProcessor(ppp_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare User’s Data Format\n",
    "\n",
    "To address particular input of a model/preprocessor, use the `PrePostProcessor.input(input_name)` method. If the model has only one input, then simple `PrePostProcessor.input()` will get a reference to pre-processing builder for this input (a tensor, the steps, a model). In general, when a model has multiple inputs/outputs, each one can be addressed by a tensor name or by its index.\n",
    "By default, information about user’s input tensor will be initialized to same data (type/shape/etc) as model’s input parameter. User application can override particular parameters according to application’s data. Refer to the following [page](https://docs.openvino.ai/2023.0/classov_1_1preprocess_1_1InputTensorInfo.html#doxid-classov-1-1preprocess-1-1-input-tensor-info-1a98fb73ff9178c8c71d809ddf8927faf5) for more information about parameters for overriding.\n",
    "\n",
    "Below is all the specified input information:\n",
    "- Precision is `U8` (unsigned 8-bit integer).\n",
    "- Size is non-fixed, setup of one determined shape size can be done with `.set_shape([1, 577, 800, 3])`\n",
    "- Layout is `“NHWC”`. It means, for example: height=577, width=800, channels=3.\n",
    "\n",
    "The height and width are necessary for resizing, and channels are needed for mean/scale normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Type, Layout\n",
    "\n",
    "# setup formant of data\n",
    "ppp.input().tensor().set_element_type(Type.u8)\\\n",
    "                    .set_spatial_dynamic_shape()\\\n",
    "                    .set_layout(Layout('NHWC'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declaring Model Layout\n",
    "\n",
    "Model input already has information about precision and shape. Preprocessing API is not intended to modify this. The only thing that may be specified is input data [layout](https://docs.openvino.ai/2023.0/openvino_docs_OV_UG_Layout_Overview.html#doxid-openvino-docs-o-v-u-g-layout-overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer_ir = next(iter(ppp_model.inputs))\n",
    "print(f\"The input shape of the model is {input_layer_ir.shape}\")\n",
    "\n",
    "ppp.input().model().set_layout(Layout('NHWC'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Steps\n",
    "\n",
    "Now, the sequence of preprocessing steps can be defined. For more information about preprocessing steps, see [here](https://docs.openvino.ai/2023.0/api/ie_python_api/_autosummary/openvino.preprocess.PreProcessSteps.html).\n",
    "\n",
    "Perform the following:\n",
    "- Convert `U8` to `FP32` precision.\n",
    "- Resize to height/width of a model. Be aware that if a model accepts dynamic size, for example, `{?, 3, ?, ?}` resize will not know how to resize the picture. Therefore, in this case, target height/ width should be specified. For more details, see also the [PreProcessSteps.resize()](https://docs.openvino.ai/2023.0/classov_1_1preprocess_1_1PreProcessSteps.html#doxid-classov-1-1preprocess-1-1-pre-process-steps-1a40dab78be1222fee505ed6a13400efe6).\n",
    "- Subtract mean from each channel.\n",
    "- Divide each pixel data to appropriate scale value.\n",
    "\n",
    "There is no need to specify conversion layout. If layouts are different, then such conversion will be added explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.preprocess import ResizeAlgorithm\n",
    "\n",
    "ppp.input().preprocess().convert_element_type(Type.f32) \\\n",
    "                        .resize(ResizeAlgorithm.RESIZE_LINEAR)\\\n",
    "                        .mean([127.5,127.5,127.5])\\\n",
    "                        .scale([127.5,127.5,127.5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrating Steps into a Model\n",
    "\n",
    "Once the preprocessing steps have been finished, the model can be finally built. It is possible to display PrePostProcessor configuration for debugging purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dump preprocessor: {ppp}')\n",
    "model_with_preprocess = ppp.build()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_image_api_preprocess(image_path, model=None):\n",
    "    image = cv2.imread(image_path)\n",
    "    input_tensor = np.expand_dims(image, 0)\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "compiled_model_with_preprocess_api = core.compile_model(model=ppp_model, device_name=device)\n",
    "\n",
    "ppp_output_layer = compiled_model_with_preprocess_api.output(0)\n",
    "\n",
    "ppp_input_tensor = prepare_image_api_preprocess(image_path)\n",
    "results = compiled_model_with_preprocess_api(ppp_input_tensor)[ppp_output_layer][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit image manually and perform inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = core.read_model(model=ir_path)\n",
    "compiled_model = core.compile_model(model=model, device_name=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load image and fit it to model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_image_preprocessing(path_to_image, compiled_model):\n",
    "    input_layer_ir = next(iter(compiled_model.inputs))\n",
    "\n",
    "    # N, H, W, C = batch size, height, width, number of channels\n",
    "    N, H, W, C = input_layer_ir.shape\n",
    "    \n",
    "    # load  image, image will be resized to model input size and converted to RGB\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(H, W), color_mode='rgb')\n",
    "\n",
    "    x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "\n",
    "    # will scale input pixels between -1 and 1\n",
    "    input_tensor = tf.keras.applications.inception_resnet_v2.preprocess_input(x)\n",
    "\n",
    "    return input_tensor\n",
    "\n",
    "\n",
    "input_tensor = manual_image_preprocessing(image_path, compiled_model)\n",
    "print(f\"The shape of the image is {input_tensor.shape}\")\n",
    "print(f\"The data type of the image is {input_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "result = compiled_model(input_tensor)[output_layer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare results on one image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_results(input_tensor, compiled_model, imagenet_classes):\n",
    "    output_layer = compiled_model.output(0)\n",
    "\n",
    "    results = compiled_model(input_tensor)[output_layer][0]\n",
    "\n",
    "    top_indices = np.argsort(results)[-5:][::-1]\n",
    "    top_softmax = results[top_indices]\n",
    "\n",
    "    for index, softmax_probability in zip(top_indices, top_softmax):\n",
    "        print(f\"{imagenet_classes[index]}, {softmax_probability:.5f}\")\n",
    "\n",
    "    return top_indices, top_softmax\n",
    "\n",
    "\n",
    "# Convert the inference result to a class name.\n",
    "imagenet_classes = open(\"../data/datasets/imagenet/imagenet_2012.txt\").read().splitlines()\n",
    "imagenet_classes = ['background'] + imagenet_classes\n",
    "\n",
    "# get result for inference with preprocessing api\n",
    "print(\"Result of inference for preprocessing with ModelOptimizer:\")\n",
    "res = check_results(mo_pp_input_tensor, compiled_model_mo_pp, imagenet_classes)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# get result for inference with preprocessing api\n",
    "print(\"Result of inference with Preprocessing API:\")\n",
    "res = check_results(ppp_input_tensor, compiled_model_with_preprocess_api, imagenet_classes)\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# get result for inference with the manual preparing of the image\n",
    "print(\"Result of inference with manual image setup:\")\n",
    "res = check_results(input_tensor, compiled_model, imagenet_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_performance(compiled_model, preprocessing_function=None):\n",
    "    num_images = 1000\n",
    "\n",
    "    start = time.perf_counter()\n",
    "\n",
    "    for _ in range(num_images):\n",
    "        input_tensor = preprocessing_function(image_path, compiled_model)\n",
    "        compiled_model(input_tensor)\n",
    "\n",
    "    end = time.perf_counter()\n",
    "    time_ir = end - start\n",
    "\n",
    "    return time_ir, num_images\n",
    "\n",
    "\n",
    "time_ir, num_images = check_performance(compiled_model_mo_pp, prepare_image_mo_preprocess)\n",
    "print(\n",
    "    f\"IR model in OpenVINO Runtime/CPU with preprocessing API: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "time_ir, num_images = check_performance(compiled_model, manual_image_preprocessing)\n",
    "print(\n",
    "    f\"IR model in OpenVINO Runtime/CPU with preprocessing API: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "time_ir, num_images = check_performance(compiled_model_with_preprocess_api, prepare_image_api_preprocess)\n",
    "print(\n",
    "    f\"IR model in OpenVINO Runtime/CPU with preprocessing API: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "tutorial_tiny.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('notebooks_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cec18e25feb9469b5ff1085a8097bdcd86db6a4ac301d6aeff87d0f3e7ce4ca5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
