{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc41ac0",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe80a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import openvino.runtime as ov\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dde5b43",
   "metadata": {},
   "source": [
    "# Transformers Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1742fd4b",
   "metadata": {},
   "source": [
    "[ONNX](https://onnx.ai/) is a representation format for deep learning models that allows AI developers to easily transfer models between different frameworks. It is hugely popular among deep learning tools, like PyTorch, Caffe2, Apache MXNet, Microsoft Cognitive Toolkit, and many others. The transformers model from Huggingface needs to be converted into ONNX format first, and then converted to OpenVINO IR format. This process to export a model to ONNX format is known as [Serialization](https://huggingface.co/docs/transformers/serialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db803ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m transformers.onnx -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "serialize_command = f\"python -m transformers.onnx \\\n",
    "    -m {checkpoint} \\\n",
    "    --feature sequence-classification model/\"\n",
    "! $serialize_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae70bbf5",
   "metadata": {},
   "source": [
    "## Initializing The Tokenizer\n",
    "\n",
    "Text preprocessing is the method of cleaning the input text to make it fit for being fed into the model. [Tokenization](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4) is used in natural language processing to split paragraphs and sentences into smaller units that can be more easily assigned meaning. It involves cleaning of the data and assigning tokens or ids to the words where words are represented in a vector space where similar words have similar vectors which helps to understand the contexts of the sentence. We're making use of a [AutoTokenizer](https://huggingface.co/docs/transformers/main_classes/tokenizer) from Huggingface, which is basically a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782bbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path=checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fff79fc",
   "metadata": {},
   "source": [
    "# Model Optimizer\n",
    "\n",
    "[Model Optimizer](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) is a cross-platform command-line tool that facilitates the transition between training and deployment environments, performs static model analysis, and adjusts deep learning models for optimal execution on end-point target devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f48c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"model.onnx\"\n",
    "MODEL_DIR = \"model/\"\n",
    "MODEL_DIR = f\"{MODEL_DIR}\"\n",
    "onnx_model_path = Path(MODEL_DIR) / onnx_model_path\n",
    "\n",
    "optimizer_command = f\"mo \\\n",
    "    --input_model {onnx_model_path} \\\n",
    "    --output_dir {MODEL_DIR} \\\n",
    "    --model_name {checkpoint} \\\n",
    "    --input input_ids,attention_mask \\\n",
    "    --input_shape [1,128],[1,128]\"\n",
    "! $optimizer_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cc074e",
   "metadata": {},
   "source": [
    "OpenVINOâ„¢ Runtime uses [Infer Request](https://docs.openvino.ai/latest/openvino_docs_OV_UG_Infer_request.html) mechanism which allows running models on different devices in asynchronous or synchronous manners. The model graph is sent as an argument to the OpenVINO API and an inference request is created. The default inference mode is AUTO but it can be changed according to requirement and hardwares available. You can explore the different inference modes and their usage [here.](https://docs.openvino.ai/latest/openvino_docs_Runtime_Inference_Modes_Overview.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31a2644",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "core = ov.Core()\n",
    "ir_model_xml = str((Path(MODEL_DIR) / checkpoint).with_suffix(\".xml\"))\n",
    "compiled_model = core.compile_model(ir_model_xml)\n",
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb25e2",
   "metadata": {},
   "source": [
    "Defining a softmax function to extract the prediction from the output of the IR format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de01fccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e778507",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd429d55",
   "metadata": {},
   "source": [
    "Creating a generic inference function to read the input and infer the result into 2 classes: Positive or Negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c91a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(input_text):\n",
    "    input_text = tokenizer(\n",
    "        input_text,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\",\n",
    "    )\n",
    "    inputs = dict(input_text)\n",
    "    label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "    result = infer_request.infer(inputs=inputs)\n",
    "    for i in result.values():\n",
    "        probability = np.argmax(softmax(i))\n",
    "    return label[probability]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60e79fd",
   "metadata": {},
   "source": [
    "For a single input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf976f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I had a wonderful day\"\n",
    "start_time = time.perf_counter()\n",
    "result = infer(input_text)\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Label: \", result)\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4d013",
   "metadata": {},
   "source": [
    "Read from a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f57d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.perf_counter()\n",
    "with open(\"data/sample.txt\", \"r\") as f:\n",
    "    input_text = f.readlines()\n",
    "    for lines in input_text:\n",
    "        print(\"User Input: \", lines)\n",
    "        result = infer(lines)\n",
    "        print(\"Label: \", result, \"\\n\")\n",
    "end_time = time.perf_counter()\n",
    "total_time = end_time - start_time\n",
    "print(\"Total Time: \", \"%.2f\" % total_time, \" seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
