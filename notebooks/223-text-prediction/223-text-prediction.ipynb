{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Interactive Text Prediction with OpenVINO\n",
    "\n",
    "This notebook shows interactive text prediction with OpenVINO. We use the [GPT-2](https://github.com/openvinotoolkit/open_model_zoo/tree/master/models/public/gpt-2) model, which is a part of the Generative Pre-trained Transformer (GPT) family. GPT-2 is pre-trained on a large corpus of English text using unsupervised training. The model is available from [Open Model Zoo](https://github.com/openvinotoolkit/open_model_zoo/), which we will use to download and convert the model to OpenVINO IR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "from openvino.runtime import Core\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "sys.path.append(\"../utils\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# directory where the model will be downloaded.\n",
    "base_model_dir = \"model\"\n",
    "\n",
    "# name of the model\n",
    "model_name = 'gpt-2'\n",
    "\n",
    "# desired precision\n",
    "precision = \"FP16\"\n",
    "\n",
    "model_path = f\"model/public/{model_name}/{precision}/{model_name}.xml\"\n",
    "model_weights_path = f\"model/public/{model_name}/{precision}/{model_name}.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download GPT-2 from Open Model Zoo\n",
    "\n",
    "We use `omz_downloader`, which is a command-line tool from the `openvino-dev` package. `omz_downloader` automatically creates a directory structure and downloads the selected model. Skip this step if the model is already downloaded. For this demo, we have to download and use `gpt-2` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_command = f\"omz_downloader \" \\\n",
    "                   f\"--name {model_name} \" \\\n",
    "                   f\"--output_dir {base_model_dir} \" \\\n",
    "                   f\"--cache_dir {base_model_dir}\"\n",
    "\n",
    "display(Markdown(f\"Download command: `{download_command}`\"))\n",
    "display(Markdown(f\"Downloading {model_name}... (This may take a few minutes depending on your connection.)\"))\n",
    "\n",
    "! $download_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert GPT-2 to OpenVINO IR\n",
    "Since the downloaded GPT-2 model is not yet in OpenVINO IR format, we to perform an additional step to convert it. Use following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path(model_path).exists():\n",
    "    convert_command = (\n",
    "        f\"omz_converter --name {model_name} --precisions {precision}\"\n",
    "        f\" --download_dir {base_model_dir} --output_dir {base_model_dir}\"\n",
    "    )\n",
    "    display(Markdown(f\"Convert command: `{convert_command}`\"))\n",
    "    display(Markdown(f\"Converting {model_name}\"))\n",
    "\n",
    "    ! $convert_command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Load the model\n",
    "\n",
    "Converted models are located in a fixed directory structure, which indicates source, model name and precision. We start by building an Inference Engine object. Then we read the network architecture and model weights from the .xml and .bin files, respectively. Finally, we compile the model for the desired device. Because we use the dynamic shapes feature, which is only available on CPU, we must use `CPU` for the device. Dynamic shapes support on GPU is coming soon.\n",
    "\n",
    "Since the text recognition model is with dynamic input shape and current release of OpenVINO 2022.1 does not support dynamic shape on `iGPU`, you cannot directly switch device to `iGPU` for inference in this case. Otherwise, you may need to resize the input images to this model into a fixed size and then try running the inference on `iGPU`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# initialize inference engine\n",
    "ie_core = Core()\n",
    "\n",
    "# read the model and corresponding weights from file\n",
    "model = ie_core.read_model(model=model_path, weights=model_weights_path)\n",
    "\n",
    "# assigning dynamic shapes to every input layer\n",
    "for input_layer in model.inputs:\n",
    "    input_shape = input_layer.partial_shape\n",
    "    input_shape[0] = -1\n",
    "    input_shape[1] = -1\n",
    "    model.reshape({input_layer: input_shape})\n",
    "\n",
    "# compile the model for the CPU\n",
    "compiled_model = ie_core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "# get input and output names of nodes\n",
    "input_keys = next(iter(compiled_model.inputs))\n",
    "output_keys = next(iter(compiled_model.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Input keys are the names of the input nodes and output keys contain names of output nodes of the network. In the case of the gpt-2 model, we have `batch size` and `sequence length` as inputs and `batch size`, `sequence length` and `vocab size` as outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Processing\n",
    "\n",
    "NLP models usually take a list of tokens as standard input. A token is a single word converted to some integer. To provide the proper input, we need the vocabulary for such mapping. So first let's input the vocabulary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_vocab_file(vocab_file_path):\n",
    "    with open(vocab_file_path, \"r\", encoding=\"utf-8\") as content:\n",
    "        return json.load(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocal_file_path = f\"model/public/{model_name}/gpt2/vocab.json\"\n",
    "vocab = load_vocab_file(vocal_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following function converts text to tokens\n",
    "def tokenize(text):\n",
    "    input_ids = tokenizer(text)['input_ids']\n",
    "    input_ids = np.array(input_ids).reshape(1, -1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last token in the vocabulary is `endoftext` token. We shall store the index of this token so that we can use this index for padding at later stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token_id = len(vocab) - 1\n",
    "tokenizer._convert_id_to_token(len(vocab) - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Softmax layer\n",
    "We shall need softmax function to convert top-k logits into the probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    summation = e_x.sum(axis=-1, keepdims=True)\n",
    "    return e_x / summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the minimum length of the sequence  \n",
    "If the minimum length of the sequence is not reached, the following code will diminish the probability of occurrence of the `eos` token. Thereby continuing the process of generation of the next words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_logits(input_ids, scores, eos_token_id, min_length=0):\n",
    "    cur_length = input_ids.shape[-1]\n",
    "    if cur_length < min_length:\n",
    "        scores[:, eos_token_id] = -float(\"inf\")\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K sampling\n",
    "In Top-K sampling, we filter the K most likely next words and redistribute the probability mass among only those K next words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_logits(scores, top_k):\n",
    "    filter_value = -float(\"inf\")\n",
    "    top_k = min(max(top_k, 1), scores.shape[-1])\n",
    "    top_k_scores = -np.sort(-scores)[:, :top_k]\n",
    "    indices_to_remove = scores < np.min(top_k_scores)\n",
    "    filtred_scores = np.ma.array(scores, mask=indices_to_remove,\n",
    "                                 fill_value=filter_value).filled()\n",
    "    return filtred_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Function\n",
    "Generating the predicted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequence(input_ids, max_sequence_length=128,\n",
    "                      eos_token_id=eos_token_id):\n",
    "    while True:\n",
    "        cur_input_len = len(input_ids[0])\n",
    "        pad_len = max_sequence_length - cur_input_len\n",
    "        model_input = np.concatenate((input_ids,\n",
    "                                      [[eos_token_id] * pad_len]), axis=-1)\n",
    "        # passing the padded sequnce into the model\n",
    "        outputs = compiled_model(inputs=[model_input])[output_keys]\n",
    "        next_token_logits = outputs[:, cur_input_len - 1, :]\n",
    "        # pre-process distribution\n",
    "        next_token_scores = process_logits(input_ids,\n",
    "                                           next_token_logits, eos_token_id)\n",
    "        top_k = 20\n",
    "        next_token_scores = get_top_k_logits(next_token_scores, top_k)\n",
    "        # get next token id\n",
    "        probs = softmax(next_token_scores)\n",
    "        next_tokens = np.random.choice(probs.shape[-1], 1,\n",
    "                                       p=probs[0], replace=True)\n",
    "        # break the loop if max length or end of text token is reached\n",
    "        if cur_input_len == max_sequence_length or next_tokens == eos_token_id:\n",
    "            break\n",
    "        else:\n",
    "            input_ids = np.concatenate((input_ids, [next_tokens]), axis=-1)\n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "Input the text in the input bar to get the predicted sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Deep learning is a type of machine learning that uses neural networks\"\n",
    "input_ids = tokenize(text)\n",
    "output_ids = generate_sequence(input_ids)\n",
    "S = \" \"\n",
    "# Convert IDs to words and make the sentence from it\n",
    "for i in output_ids[0]:\n",
    "    S += tokenizer.convert_tokens_to_string(tokenizer._convert_id_to_token(i))\n",
    "print(\"Input Text: \", text)\n",
    "print()\n",
    "print(f\"Predicted Sequence:{S}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "033257a69bf603b2de0dc0c42b5465d421ac707c57e304e82520be1d43cc042f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
