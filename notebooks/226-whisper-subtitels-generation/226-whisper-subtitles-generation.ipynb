{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video subtitles generation using Whisper and OpenVINO\n",
    "\n",
    "Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web.  It is a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
    "\n",
    "\n",
    "![asr-training-data-desktop.svg](asr-training-data-desktop.svg)\n",
    "\n",
    "You can find more information about this model in [paper](https://cdn.openai.com/papers/whisper.pdf), [OpenAI blogpost](https://openai.com/blog/whisper/), [model card](https://github.com/openai/whisper/blob/main/model-card.md) and [repository](https://github.com/openai/whisper).\n",
    "\n",
    "In this notebook we will use its capabilities for generation subtitles to video.\n",
    "Notebook contains following steps:\n",
    "1. Convert model to IR using OpenVINO Model Optimizer tool.\n",
    "3. Run Whisper pipeline with OpenVINO models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clone and install model repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'whisper'...\n",
      "remote: Enumerating objects: 228, done.\u001b[K\n",
      "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
      "remote: Compressing objects: 100% (10/10), done.\u001b[K\n",
      "remote: Total 228 (delta 4), reused 4 (delta 3), pack-reused 215\u001b[K\n",
      "Receiving objects: 100% (228/228), 3.12 MiB | 277.00 KiB/s, done.\n",
      "Resolving deltas: 100% (122/122), done.\n",
      "/home/ea/work/openvino_notebooks/notebooks/226-whisper-subtitels-generation/whisper\n",
      "/usr/lib/python3.8/distutils/dist.py:274: UserWarning: Unknown distribution option: 'readme'\n",
      "  warnings.warn(msg)\n",
      "running develop\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/setuptools/command/easy_install.py:156: EasyInstallDeprecationWarning: easy_install command is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "/home/ea/work/notebooks_env/lib/python3.8/site-packages/setuptools/command/install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "  warnings.warn(\n",
      "running egg_info\n",
      "creating whisper.egg-info\n",
      "writing whisper.egg-info/PKG-INFO\n",
      "writing dependency_links to whisper.egg-info/dependency_links.txt\n",
      "writing entry points to whisper.egg-info/entry_points.txt\n",
      "writing requirements to whisper.egg-info/requires.txt\n",
      "writing top-level names to whisper.egg-info/top_level.txt\n",
      "writing manifest file 'whisper.egg-info/SOURCES.txt'\n",
      "file whisper.py (for module whisper) not found\n",
      "reading manifest file 'whisper.egg-info/SOURCES.txt'\n",
      "reading manifest template 'MANIFEST.in'\n",
      "adding license file 'LICENSE'\n",
      "writing manifest file 'whisper.egg-info/SOURCES.txt'\n",
      "running build_ext\n",
      "Creating /home/ea/work/notebooks_env/lib/python3.8/site-packages/whisper.egg-link (link to .)\n",
      "whisper 1.0 is already the active version in easy-install.pth\n",
      "Installing whisper script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Installed /home/ea/work/openvino_notebooks/notebooks/226-whisper-subtitels-generation/whisper\n",
      "Processing dependencies for whisper==1.0\n",
      "Searching for ffmpeg-python==0.2.0\n",
      "Best match: ffmpeg-python 0.2.0\n",
      "Processing ffmpeg_python-0.2.0-py3.8.egg\n",
      "ffmpeg-python 0.2.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages/ffmpeg_python-0.2.0-py3.8.egg\n",
      "Searching for transformers==4.24.0\n",
      "Best match: transformers 4.24.0\n",
      "Adding transformers 4.24.0 to easy-install.pth file\n",
      "Installing transformers-cli script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for more-itertools==9.0.0\n",
      "Best match: more-itertools 9.0.0\n",
      "Processing more_itertools-9.0.0-py3.8.egg\n",
      "more-itertools 9.0.0 is already the active version in easy-install.pth\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages/more_itertools-9.0.0-py3.8.egg\n",
      "Searching for tqdm==4.64.1\n",
      "Best match: tqdm 4.64.1\n",
      "Adding tqdm 4.64.1 to easy-install.pth file\n",
      "Installing tqdm script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for torch==1.13.0\n",
      "Best match: torch 1.13.0\n",
      "Adding torch 1.13.0 to easy-install.pth file\n",
      "Installing convert-caffe2-to-onnx script to /home/ea/work/notebooks_env/bin\n",
      "Installing convert-onnx-to-caffe2 script to /home/ea/work/notebooks_env/bin\n",
      "Installing torchrun script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for numpy==1.21.6\n",
      "Best match: numpy 1.21.6\n",
      "Adding numpy 1.21.6 to easy-install.pth file\n",
      "Installing f2py script to /home/ea/work/notebooks_env/bin\n",
      "Installing f2py3 script to /home/ea/work/notebooks_env/bin\n",
      "Installing f2py3.8 script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for future==0.18.2\n",
      "Best match: future 0.18.2\n",
      "Adding future 0.18.2 to easy-install.pth file\n",
      "Installing futurize script to /home/ea/work/notebooks_env/bin\n",
      "Installing pasteurize script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for packaging==21.3\n",
      "Best match: packaging 21.3\n",
      "Adding packaging 21.3 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for PyYAML==6.0\n",
      "Best match: PyYAML 6.0\n",
      "Adding PyYAML 6.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for requests==2.27.1\n",
      "Best match: requests 2.27.1\n",
      "Adding requests 2.27.1 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for tokenizers==0.13.1\n",
      "Best match: tokenizers 0.13.1\n",
      "Adding tokenizers 0.13.1 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for filelock==3.8.0\n",
      "Best match: filelock 3.8.0\n",
      "Adding filelock 3.8.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for huggingface-hub==0.10.1\n",
      "Best match: huggingface-hub 0.10.1\n",
      "Adding huggingface-hub 0.10.1 to easy-install.pth file\n",
      "Installing huggingface-cli script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for regex==2022.3.2\n",
      "Best match: regex 2022.3.2\n",
      "Adding regex 2022.3.2 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for nvidia-cuda-runtime-cu11==11.7.99\n",
      "Best match: nvidia-cuda-runtime-cu11 11.7.99\n",
      "Adding nvidia-cuda-runtime-cu11 11.7.99 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for nvidia-cudnn-cu11==8.5.0.96\n",
      "Best match: nvidia-cudnn-cu11 8.5.0.96\n",
      "Adding nvidia-cudnn-cu11 8.5.0.96 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for nvidia-cublas-cu11==11.10.3.66\n",
      "Best match: nvidia-cublas-cu11 11.10.3.66\n",
      "Adding nvidia-cublas-cu11 11.10.3.66 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for typing-extensions==4.4.0\n",
      "Best match: typing-extensions 4.4.0\n",
      "Adding typing-extensions 4.4.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "Best match: nvidia-cuda-nvrtc-cu11 11.7.99\n",
      "Adding nvidia-cuda-nvrtc-cu11 11.7.99 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for pyparsing==2.4.7\n",
      "Best match: pyparsing 2.4.7\n",
      "Adding pyparsing 2.4.7 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for charset-normalizer==2.0.12\n",
      "Best match: charset-normalizer 2.0.12\n",
      "Adding charset-normalizer 2.0.12 to easy-install.pth file\n",
      "Installing normalizer script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for idna==3.4\n",
      "Best match: idna 3.4\n",
      "Adding idna 3.4 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for urllib3==1.26.12\n",
      "Best match: urllib3 1.26.12\n",
      "Adding urllib3 1.26.12 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for certifi==2022.9.24\n",
      "Best match: certifi 2022.9.24\n",
      "Adding certifi 2022.9.24 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for wheel==0.37.1\n",
      "Best match: wheel 0.37.1\n",
      "Adding wheel 0.37.1 to easy-install.pth file\n",
      "Installing wheel script to /home/ea/work/notebooks_env/bin\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Searching for setuptools==59.5.0\n",
      "Best match: setuptools 59.5.0\n",
      "Adding setuptools 59.5.0 to easy-install.pth file\n",
      "\n",
      "Using /home/ea/work/notebooks_env/lib/python3.8/site-packages\n",
      "Finished processing dependencies for whisper==1.0\n",
      "Collecting git+https://github.com/pytube/pytube\n",
      "  Cloning https://github.com/pytube/pytube to /tmp/pip-req-build-zb8gno7o\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/pytube/pytube /tmp/pip-req-build-zb8gno7o\n",
      "  Resolved https://github.com/pytube/pytube to commit 84faec34c8a66f502ac635a5610445dbff160654\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: moviepy in /home/ea/work/notebooks_env/lib/python3.8/site-packages (1.0.3)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from moviepy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from moviepy) (1.21.6)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from moviepy) (2.9.0)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from moviepy) (0.1.10)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from moviepy) (4.4.2)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from moviepy) (2.27.1)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from moviepy) (0.4.7)\n",
      "Requirement already satisfied: pillow in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from imageio<3.0,>=2.5->moviepy) (9.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (1.26.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ea/work/notebooks_env/lib/python3.8/site-packages (from requests<3.0,>=2.8.1->moviepy) (2022.9.24)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/whisper.git\n",
    "%cd whisper\n",
    "!python setup.py develop\n",
    "!pip install git+https://github.com/pytube/pytube\n",
    "!pip install moviepy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate model\n",
    "Whisper is a Transformer based encoder-decoder model, also referred to as a sequence-to-sequence model. It maps a sequence of audio spectrogram features to a sequence of text tokens. First, the raw audio inputs are converted to a log-Mel spectrogram by action of the feature extractor. The Transformer encoder then encodes the spectrogram to form a sequence of encoder hidden states. Finally, the decoder autoregressively predicts text tokens, conditional on both the previous tokens and the encoder hidden states.\n",
    "\n",
    "You can see model architecture on diagram below:\n",
    "\n",
    "![whisper_architecture.svg](whisper_architecture.svg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several models of different sizes and capabilities trained by model authors. In this tutorial we will use `base` model, but the same actions are also applicable to other models from Whisper family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"base\")\n",
    "model.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert model to OpenVINO Intermediate Representation (IR) format.\n",
    "\n",
    "For starting work with OpenVINO we should convert model to OpenVINO format.\n",
    "OpenVINO supports Pytorch via ONNX conversion.  We will use `torch.onnx.export` for exportingWe need to provide initialized model object and example of inputs for shape inference.\n",
    "We will use `mo.convert_model` functionality for conversion ONNX models. \n",
    "The `mo.convert_model` function returns OpenVINO model ready to use for model object for loading on device and making prediction.\n",
    "We can save it on drive for next usage with `openvino.runtime.serialize`.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper Encoder to IR\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "mel = torch.zeros((1, 80, 3000))\n",
    "audio_features = model.encoder(mel)\n",
    "torch.onnx.export(model.encoder, mel, 'whisper_encoder.onnx', input_names=['mel'], output_names=['output_features'])\n",
    "encoder_model = mo.convert_model('whisper_encoder.onnx', compress_to_fp16=True, input='mel[1 80 -1]')\n",
    "serialize(encoder_model, 'whisper_encoder.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whisper decoder to IR\n",
    "\n",
    "For reducing computational complexity, decoder uses cached key/value projections in attention modules from previous steps. We need to modify this process for correct tracing to ONNX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Optional, Union, List\n",
    "from functools import partial\n",
    "\n",
    "positional_embeddings_size = model.decoder.positional_embedding.shape[0]\n",
    "\n",
    "def save_to_cache(cache, module, output):\n",
    "    if module not in cache or output.shape[1] > positional_embeddings_size:\n",
    "        cache[module] = output  # save as-is, for the first token or cross attention\n",
    "    else:\n",
    "        cache[module] = torch.cat([cache[module], output], dim=1).detach()\n",
    "    return cache[module]\n",
    "  \n",
    "def attention_forward(\n",
    "        attention_module,\n",
    "        x: torch.Tensor,\n",
    "        xa: Optional[torch.Tensor] = None,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "        idx: int = 0\n",
    "):\n",
    "    q = attention_module.query(x)\n",
    "\n",
    "    if kv_cache is None or xa is None:\n",
    "        # hooks, if installed (i.e. kv_cache is not None), will prepend the cached kv tensors;\n",
    "        # otherwise, perform key/value projections for self- or cross-attention as usual.\n",
    "        k = attention_module.key(x if xa is None else xa)\n",
    "        v = attention_module.value(x if xa is None else xa)\n",
    "        if kv_cache is not None:\n",
    "            k = save_to_cache(kv_cache, f'k_{idx}', k)\n",
    "            v = save_to_cache(kv_cache, f'v_{idx}', v)\n",
    "    else:\n",
    "        # for cross-attention, calculate keys and values once and reuse in subsequent calls.\n",
    "        k = kv_cache.get(f'k_{idx}', save_to_cache(kv_cache, f'k_{idx}', attention_module.key(xa)))\n",
    "        v = kv_cache.get(f'v_{idx}', save_to_cache(kv_cache, f'v_{idx}', attention_module.value(xa)))\n",
    "\n",
    "    wv = attention_module.qkv_attention(q, k, v, mask)\n",
    "    return attention_module.out(wv), kv_cache\n",
    "\n",
    "\n",
    "def block_forward(\n",
    "        residual_block,\n",
    "        x: torch.Tensor,\n",
    "        xa: Optional[torch.Tensor] = None,\n",
    "        mask: Optional[torch.Tensor] = None,\n",
    "        kv_cache: Optional[dict] = None,\n",
    "        idx:int = 0\n",
    "    ):\n",
    "        x0, kv_cache = residual_block.attn(residual_block.attn_ln(x), mask=mask, kv_cache=kv_cache, idx=f'{idx}a')\n",
    "        x = x + x0\n",
    "        if residual_block.cross_attn:\n",
    "            x1, kv_cache = residual_block.cross_attn(residual_block.cross_attn_ln(x), xa, kv_cache=kv_cache, idx=f'{idx}c')\n",
    "            x = x + x1\n",
    "        x = x + residual_block.mlp(residual_block.mlp_ln(x))\n",
    "        return x, kv_cache\n",
    "\n",
    "for idx, block in enumerate(model.decoder.blocks):\n",
    "    block.forward = partial(block_forward, block, idx=idx)\n",
    "    block.attn.forward = partial(attention_forward, block.attn)\n",
    "    if block.cross_attn:\n",
    "        block.cross_attn.forward = partial(attention_forward, block.cross_attn)\n",
    "\n",
    "\n",
    "def decoder_forward(decoder, x: torch.Tensor, xa: torch.Tensor, kv_cache: Optional[dict] = None):\n",
    "    \"\"\"\n",
    "    x : torch.LongTensor, shape = (batch_size, <= n_ctx) the text tokens\n",
    "    xa : torch.Tensor, shape = (batch_size, n_mels, n_audio_ctx)\n",
    "        the encoded audio features to be attended on\n",
    "    \"\"\"\n",
    "    offset = next(iter(kv_cache.values())).shape[1] if kv_cache else 0\n",
    "    x = decoder.token_embedding(x) + decoder.positional_embedding[offset : offset + x.shape[-1]]\n",
    "    x = x.to(xa.dtype)\n",
    "\n",
    "    for block in decoder.blocks:\n",
    "        x, kv_cache = block(x, xa, mask=decoder.mask, kv_cache=kv_cache)\n",
    "\n",
    "    x = decoder.ln(x)\n",
    "    logits = (x @ torch.transpose(decoder.token_embedding.weight.to(x.dtype), 1, 0)).float()\n",
    "\n",
    "    return logits, kv_cache\n",
    "\n",
    "model.decoder.forward = partial(decoder_forward, model.decoder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = torch.ones((5, 3), dtype=torch.int64)\n",
    "\n",
    "logits, kv_cache = model.decoder(tokens, audio_features, kv_cache={})\n",
    "kv_cache = {k: v for k, v in kv_cache.items()}\n",
    "tokens = torch.ones((5, 1), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [f'out_{k}' for k in kv_cache.keys()]\n",
    "inputs = [f'in_{k}' for k in kv_cache.keys()]\n",
    "dynamic_axes = {'tokens': {0: 'beam_size', 1: 'seq_len'}, 'audio_features': {0: 'beam_size'}, 'logits': {0: 'beam_size', 1: 'seq_len'}}\n",
    "dynamic_outs = {o: {0: 'beam_size', 1: 'prev_seq_len'} for o in outputs}\n",
    "dynamic_inp = {i: {0: 'beam_size', 1: 'prev_seq_len'}  for i in inputs}\n",
    "dynamic_axes.update(dynamic_outs)\n",
    "dynamic_axes.update(dynamic_inp)\n",
    "torch.onnx.export(\n",
    "    model.decoder, {'x': tokens, 'xa': audio_features, 'kv_cache': kv_cache},\n",
    "'whisper_decoder.onnx',\n",
    "input_names=['tokens', 'audio_features'] + inputs,\n",
    "output_names=['logits'] + outputs,\n",
    "dynamic_axes=dynamic_axes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapes = 'tokens[1..5 1..224],audio_features[1..5 1500 512]'\n",
    "for k, v in kv_cache.items():\n",
    "    if k.endswith('a'):\n",
    "        input_shapes += f',in_{k}[1..5 0..224 512]' \n",
    "decoder_model = mo.convert_model(input_model='whisper_decoder.onnx', compress_to_fp16=True, input=input_shapes)\n",
    "serialize(decoder_model, 'whisper_decoder.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare inference pipeline\n",
    "\n",
    "For running Pytorch Whisper model, you need just call `transcribe(autio)` function.  We will try to reuse original model pipeline for autio transcribing. In order to run model using OpenVINO, we need just update model parts and decoding functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenVINOAudioEncoder(torch.nn.Module):\n",
    "    def __init__(self, core, model_path, device='CPU'):\n",
    "        super().__init__()\n",
    "        self.model = core.read_model(model_path)\n",
    "        self.compiled_model = core.compile_model(self.model, device)\n",
    "        self.output_blob = self.compiled_model.output(0)\n",
    "\n",
    "    def forward(self, mel:torch.Tensor):\n",
    "        return torch.from_numpy(self.compiled_model(mel)[self.output_blob])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenVINOTextDecoder(torch.nn.Module):\n",
    "    def __init__(self, core, model_path, device='CPU'):\n",
    "        super().__init__()\n",
    "        self._core = core\n",
    "        self.model = core.read_model(model_path)\n",
    "        self._input_names = [inp.any_name for inp in self.model.inputs]\n",
    "        self.compiled_model = core.compile_model(self.model, device)\n",
    "        self.device = device\n",
    "    \n",
    "    def init_past_inputs(self, feed_dict):\n",
    "        beam_size = feed_dict['tokens'].shape[0]\n",
    "        audio_len = feed_dict['audio_features'].shape[-1]\n",
    "        previous_seq_len = 0\n",
    "        for name in self._input_names:\n",
    "            if name in ['tokens', 'audio_features']:\n",
    "                continue\n",
    "            feed_dict[name] = np.zeros((beam_size, previous_seq_len, audio_len), dtype=np.float32)\n",
    "        return feed_dict\n",
    "\n",
    "    def preprocess_kv_cache_inputs(self, feed_dict, kv_cache):\n",
    "        if not kv_cache:\n",
    "            return self.init_past_inputs(feed_dict)\n",
    "        for k, v in kv_cache.items():\n",
    "            new_k = f'in_{k}'\n",
    "            if new_k in self._input_names:\n",
    "                feed_dict[new_k] = v\n",
    "        return feed_dict\n",
    "\n",
    "    def postprocess_outputs(self, outputs):\n",
    "        logits = None\n",
    "        kv_cache = {}\n",
    "        for output_t, out in outputs.items():\n",
    "            if 'logits' in output_t.get_names():\n",
    "                logits = torch.from_numpy(out)\n",
    "            else:\n",
    "                tensor_name = output_t.any_name\n",
    "                kv_cache[tensor_name.replace('out_', '')] = torch.from_numpy(out)\n",
    "        return logits, kv_cache\n",
    "\n",
    "    def forward(self, x:torch.Tensor, xa:torch.Tensor, kv_cache: Optional[dict]=None):\n",
    "        feed_dict = {'tokens': x, 'audio_features': xa}\n",
    "        feed_dict = (self.preprocess_kv_cache_inputs(feed_dict, kv_cache))\n",
    "        res = self.compiled_model(feed_dict)\n",
    "        return self.postprocess_outputs(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from whisper.decoding import DecodingTask, Inference, DecodingOptions, DecodingResult\n",
    "\n",
    "\n",
    "class OpenVINOInference(Inference):\n",
    "    def __init__(self, model: \"Whisper\", initial_token_length: int):\n",
    "        self.model: \"Whisper\" = model\n",
    "        self.initial_token_length = initial_token_length\n",
    "        self.kv_cache = {}\n",
    "    \n",
    "    def logits(self, tokens: torch.Tensor, audio_features: torch.Tensor) -> torch.Tensor:\n",
    "        if tokens.shape[-1] > self.initial_token_length:\n",
    "            # only need to use the last token except in the first forward pass\n",
    "            tokens = tokens[:, -1:]\n",
    "        logits, self.kv_cache = self.model.decoder(tokens, audio_features, kv_cache=self.kv_cache)\n",
    "        return logits\n",
    "\n",
    "    def cleanup_caching(self):\n",
    "        self.kv_cache = {}\n",
    "\n",
    "    def rearrange_kv_cache(self, source_indices):\n",
    "        for module, tensor in self.kv_cache.items():\n",
    "            # update the key/value cache to contain the selected sequences\n",
    "            self.kv_cache[module] = tensor[source_indices]\n",
    "\n",
    "\n",
    "class OpenVINODecodingTask(DecodingTask):\n",
    "    def __init__(self, model: \"Whisper\", options: DecodingOptions):\n",
    "        super().__init__(model, options)\n",
    "        self.inference = OpenVINOInference(model, len(self.initial_tokens))\n",
    "\n",
    "@torch.no_grad()\n",
    "def decode(model: \"Whisper\", mel: torch.Tensor, options: DecodingOptions = DecodingOptions()) -> Union[DecodingResult, List[DecodingResult]]:\n",
    "    \"\"\"\n",
    "    Performs decoding of 30-second audio segment(s), provided as Mel spectrogram(s).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: Whisper\n",
    "        the Whisper model instance\n",
    "\n",
    "    mel: torch.Tensor, shape = (80, 3000) or (*, 80, 3000)\n",
    "        A tensor containing the Mel spectrogram(s)\n",
    "\n",
    "    options: DecodingOptions\n",
    "        A dataclass that contains all necessary options for decoding 30-second segments\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: Union[DecodingResult, List[DecodingResult]]\n",
    "        The result(s) of decoding contained in `DecodingResult` dataclass instance(s)\n",
    "    \"\"\"\n",
    "    single = mel.ndim == 2\n",
    "    if single:\n",
    "        mel = mel.unsqueeze(0)\n",
    "\n",
    "    result = OpenVINODecodingTask(model, options).run(mel)\n",
    "    \n",
    "    if single:\n",
    "        result = result[0]\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model.decoder\n",
    "del model.encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "from collections import namedtuple\n",
    "\n",
    "Parameter = namedtuple('Parameter', ['device'])\n",
    "\n",
    "core = Core()\n",
    "\n",
    "model.encoder = OpenVINOAudioEncoder(core, 'whisper_encoder.xml')\n",
    "model.decoder = OpenVINOTextDecoder(core, 'whisper_decoder.xml')\n",
    "model.decode = partial(decode, model)\n",
    "\n",
    "def parameters():\n",
    "    return iter([Parameter(torch.device('cpu'))])\n",
    "\n",
    "model.parameters = parameters\n",
    "\n",
    "def logits(model, tokens: torch.Tensor, audio_features: torch.Tensor):\n",
    "    return model.decoder(tokens, audio_features, None)[0]\n",
    "\n",
    "model.logits = partial(logits, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from pytube import YouTube\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "def resample(audio, src_sample_rate, dst_sample_rate):\n",
    "    if src_sample_rate == dst_sample_rate:\n",
    "        return audio\n",
    "    duration = audio.shape[0] / src_sample_rate\n",
    "    resampled_data = np.zeros(shape=(int(duration * dst_sample_rate)), dtype=np.float32)\n",
    "    x_old = np.linspace(0, duration, audio.shape[0], dtype=np.float32)\n",
    "    x_new = np.linspace(0, duration, resampled_data.shape[0], dtype=np.float32)\n",
    "    resampled_audio = np.interp(x_new, x_old, audio)\n",
    "    return resampled_audio.astype(np.float32)\n",
    "\n",
    "def audio_to_float(audio):\n",
    "    return audio.astype(np.float32) / np.iinfo(audio.dtype).max\n",
    "\n",
    "\n",
    "def get_audio(video_file):\n",
    "    input_video = VideoFileClip(str(video_file))\n",
    "    input_video.audio.write_audiofile(video_file.stem + '.wav')\n",
    "    input_audio_file = video_file.stem + '.wav'\n",
    "    sample_rate, audio = wavfile.read(io.BytesIO(open(input_audio_file, 'rb').read()))\n",
    "    audio = audio_to_float(audio)\n",
    "    if audio.ndim == 2:\n",
    "        audio = audio.mean(axis=1)\n",
    "    resampled_audio = resample(audio, sample_rate, 16000)\n",
    "    return resampled_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ea/work/openvino_notebooks/notebooks/226-whisper-subtitels-generation/whisper/downloaded_video.mp4'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIDEO_LINK = 'https://www.youtube.com/watch?v=kgL5LBM-hFI'\n",
    "\n",
    "yt = YouTube(VIDEO_LINK)  \n",
    "yt.streams.get_highest_resolution().download(filename = Path(\"downloaded_video.mp4\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in downloaded_video.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "audio = get_audio(Path('downloaded_video.mp4'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = model.transcribe(audio, beam_size=5, best_of=5, task='translate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_timestamp(seconds: float):\n",
    "    assert seconds >= 0, \"non-negative timestamp expected\"\n",
    "    milliseconds = round(seconds * 1000.0)\n",
    "\n",
    "    hours = milliseconds // 3_600_000\n",
    "    milliseconds -= hours * 3_600_000\n",
    "\n",
    "    minutes = milliseconds // 60_000\n",
    "    milliseconds -= minutes * 60_000\n",
    "\n",
    "    seconds = milliseconds // 1_000\n",
    "    milliseconds -= seconds * 1_000\n",
    "\n",
    "    return (f\"{hours}:\" if hours > 0 else \"00:\") + f\"{minutes:02d}:{seconds:02d},{milliseconds:03d}\"\n",
    "\n",
    "def prepare_srt_bilingual(transcription, translation):\n",
    "    segment_lines = []\n",
    "    for segment1, segment2 in zip(transcription['segments'], translation['segments']):\n",
    "        segment_lines.append(str(segment1['id'] + 1) +'\\n')\n",
    "        time_start = format_timestamp(segment1['start'])\n",
    "        time_end = format_timestamp(segment1['end'])\n",
    "        time_str = f'{time_start} --> {time_end}\\n'\n",
    "        segment_lines.append(time_str)\n",
    "        segment_lines.append(segment1['text'] + '\\n' + segment2['text'] + '\\n\\n')\n",
    "    return segment_lines\n",
    "\n",
    "\n",
    "def prepare_srt(transcription):\n",
    "    segment_lines = []\n",
    "    for segment in transcription['segments']:\n",
    "        segment_lines.append(str(segment['id'] + 1) +'\\n')\n",
    "        time_start = format_timestamp(segment['start'])\n",
    "        time_end = format_timestamp(segment['end'])\n",
    "        time_str = f'{time_start} --> {time_end}\\n'\n",
    "        segment_lines.append(time_str)\n",
    "        segment_lines.append(segment['text'] + '\\n\\n')\n",
    "    return segment_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "srt_lines = prepare_srt(transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc45532f108f4c029bca9fbd6e5d7c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Video(value=b'\\x00\\x00\\x00\\x18ftypmp42\\x00\\x00\\x00\\x00isommp42\\x00\\x00Aimoov\\x00\\x00\\x00lmvhd...', height='320…"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipywidgets import Video\n",
    "Video.from_file(\"downloaded_video.mp4\", width=320, height=320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "00:00:00,000 --> 00:00:05,000\n",
      " Oh, what's that?\n",
      "\n",
      "2\n",
      "00:00:05,000 --> 00:00:09,000\n",
      " Oh, wow.\n",
      "\n",
      "3\n",
      "00:00:09,000 --> 00:00:10,000\n",
      " Hello, humans.\n",
      "\n",
      "4\n",
      "00:00:13,000 --> 00:00:15,000\n",
      " Focus on me.\n",
      "\n",
      "5\n",
      "00:00:15,000 --> 00:00:18,000\n",
      " Focus on the guard.\n",
      "\n",
      "6\n",
      "00:00:18,000 --> 00:00:22,000\n",
      " Don't tell anyone what you've seen in here.\n",
      "\n",
      "7\n",
      "00:00:22,000 --> 00:00:30,000\n",
      " Have you seen what's in there?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(''.join(srt_lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0404472fd7b5b63117a9fa5c50283296e2708c2449c6090d2cdf8903f95897f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
