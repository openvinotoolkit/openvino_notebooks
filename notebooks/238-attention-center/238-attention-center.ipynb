{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The attention center model with OpenVINO™\n",
    "\n",
    "This notebook demonstrates how to use the [attention center model](https://github.com/google/attention-center/tree/main) with OpenVINO. This model is in the [TensorFlow Lite format](https://www.tensorflow.org/lite), which is supported in OpenVINO now by TFlite frontend.\n",
    "\n",
    "The attention center model takes an RGB image as input and return a 2D point as output. This 2D point is the predicted center of human attention on the image i.e. the most salient part of images, on which people pay attention fist to.\n",
    "This allows find the most visually salient regions and handle it as early as possible. For example it could be used for the latest generatipon image format(such as JPEG XL), which supports encoding the parts that you pay attention to fist. It can help to improve user experience, image will appear to load faster.\n",
    "\n",
    "Attention center model architecture is:\n",
    "> The attention center model is a deep neural net, which takes an image as input, and uses a pre-trained classification network, e.g, ResNet, MobileNet, etc., as the backbone. Several intermediate layers that output from the backbone network are used as input for the attention center prediction module. These different intermediate layers contain different information e.g., shallow layers often contain low level information like intensity/color/texture, while deeper layers usually contain higher and more semantic information like shape/object. All are useful for the attention prediction. The attention center prediction applies convolution, deconvolution and/or resizing operator together with aggregation and sigmoid function to generate a weighting map for the attention center. And then an operator (the Einstein summation operator in our case) can be applied to compute the (gravity) center from the weighting map. An L2 norm between the predicted attention center and the ground-truth attention center can be computed as the training loss. [[Open sourcing the attention center model]](https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html)\n",
    "\n",
    "The attention center model has been trained with images from the [Common objects](https://cocodataset.org/#home) in context annotated with saliency from the [salicon dataset](http://salicon.net/).\n",
    "\n",
    "Check out [this article](https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html) to find more information about this model.\n",
    "\n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "* Downloading the model\n",
    "* Loading the model and make inference with TensowFlow Lite API\n",
    "* Loading the model and make inference with OpenVINO API\n",
    "* Comparing performance\n",
    "* Run Live Attention Center Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import sys\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize, Core\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the attention-center model\n",
    "\n",
    "Download the model as part of repo https://github.com/google/attention-center/tree/main. The repo include model in folder `./model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not Path('./attention-center').exists():\n",
    "    ! git clone https://github.com/google/attention-center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tensorflow Lite model to OpenVINO IR format\n",
    "\n",
    "The attention-center model is pre-trained model in TensorFlow Lite format. In this Notebook the model will be converted to \n",
    "OpenVINO IR format with Model Optimizer. This step will be skipped if the model have already been converted. For more information about Model Optimizer, please, see the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html). \n",
    "\n",
    "Also TFLite models format is supported in OpenVINO by TFlite frontend, so the model can be passed directly to `core.read_model()`. You can find example in [002-openvino-api](https://github.com/openvinotoolkit/openvino_notebooks/tree/main/notebooks/002-openvino-api)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = Path(\"./attention-center/model/center.tflite\")\n",
    "\n",
    "ir_model_path = Path(\"./model/ir_center_model.xml\")\n",
    "\n",
    "core = Core()\n",
    "\n",
    "if not ir_model_path.exists():\n",
    "    model = mo.convert_model(tflite_model_path)\n",
    "    serialize(model, ir_model_path.as_posix())\n",
    "    print(\"IR model saved to {}\".format(ir_model_path))\n",
    "else:\n",
    "    print(\"Read IR model from {}\".format(ir_model_path))\n",
    "    model = core.read_model(ir_model_path)\n",
    "\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image to use with attention-center model\n",
    "\n",
    "The attention-center model takes an RGB image with shape (480, 640) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image():\n",
    "    def __init__(self, model_input_image_shape, image_path=None, image=None):\n",
    "        self.model_input_image_shape = model_input_image_shape\n",
    "        self.image = None\n",
    "        self.real_input_image_shape = None\n",
    "\n",
    "        if image_path is not None:\n",
    "            self.image = cv2.imread(image_path)\n",
    "            self.real_input_image_shape = self.image.shape\n",
    "        elif image is not None:\n",
    "            self.image = image\n",
    "            self.real_input_image_shape = self.image.shape\n",
    "        else:\n",
    "            raise Exception(\"Sorry, image can't be found, please, specify image_path or image\")\n",
    "\n",
    "    def prepare_image_tensor(self):\n",
    "        rgb_image = cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB)\n",
    "        resized_image = cv2.resize(rgb_image, (self.model_input_image_shape[1], self.model_input_image_shape[0]))\n",
    "\n",
    "        image_tensor = tf.constant(np.expand_dims(resized_image, axis=0),\n",
    "                                   dtype=tf.float32)\n",
    "        return image_tensor\n",
    "\n",
    "    def scalt_center_to_real_image_shape(self, predicted_center):\n",
    "        new_center_y = round(predicted_center[0] * self.real_input_image_shape[1] / self.model_input_image_shape[1])\n",
    "        new_center_x = round(predicted_center[1] * self.real_input_image_shape[0] / self.model_input_image_shape[0])\n",
    "        return (new_center_y, new_center_x)\n",
    "\n",
    "    def draw_attention_center_point(self, predicted_center):\n",
    "        image_with_circle = cv2.circle(self.image,\n",
    "                                       predicted_center,\n",
    "                                       radius=10,\n",
    "                                       color=(3, 3, 255),\n",
    "                                       thickness=-1)\n",
    "        return image_with_circle\n",
    "\n",
    "    def print_image(self, predicted_center=None):\n",
    "        image_to_print = self.image\n",
    "        if predicted_center is not None:\n",
    "            image_to_print = self.draw_attention_center_point(predicted_center)\n",
    "\n",
    "        plt.imshow(cv2.cvtColor(image_to_print, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "image_file_name = Path(\"../data/image/coco.jpg\")\n",
    "input_image = Image((480, 640), image_file_name.as_posix())\n",
    "image_tensor = input_image.prepare_image_tensor()\n",
    "input_image.print_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result with TensorFlow Lite API\n",
    "\n",
    "The attention-center model return a 2D point as outputs, which is the predicted center of human attention on the image. Output points scaled on model input and that we need to postprocess them to scale in original image resolution instead of printing raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a tflite model\n",
    "with open(tflite_model_path, 'rb') as f:\n",
    "    tflite_model_content = f.read()\n",
    "# init Interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_content)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# create the interpreter's input index-by-name map\n",
    "index_map = {m['name']: m['index'] for m in interpreter.get_input_details()}\n",
    "\n",
    "# make inference\n",
    "interpreter.set_tensor(index_map['image:0'], image_tensor)\n",
    "interpreter.invoke()\n",
    "output_details = interpreter.get_output_details()\n",
    "# make inference, get result in input image resolution\n",
    "tf_res = interpreter.get_tensor(output_details[0]['index'])\n",
    "# scale point to original image resulution\n",
    "tf_predicted_center = input_image.scalt_center_to_real_image_shape(tf_res[0])\n",
    "print(f'Predicted attention center point {tf_predicted_center}')\n",
    "input_image.print_image(tf_predicted_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result with OpenVINO IR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# make inference, get result in input image resolution\n",
    "res = compiled_model([image_tensor])[output_layer]\n",
    "# scale point to original image resulution\n",
    "predicted_center = input_image.scalt_center_to_real_image_shape(res[0])\n",
    "print(f'Prediction attention center point {predicted_center}')\n",
    "input_image.print_image(predicted_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 100\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    res = compiled_model([image_tensor])[output_layer]\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "\n",
    "print(\n",
    "    f\"IR model in OpenVINO Runtime/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    interpreter.set_tensor(index_map['image:0'], image_tensor)\n",
    "    interpreter.invoke()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    pred_from_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "\n",
    "print(\n",
    "    f\"TensorFlow API: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live attention center detection\n",
    "\n",
    "Use a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`.\n",
    "\n",
    ">**NOTE**: To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server (for example, Binder), the webcam will not work. Popup mode may not work if you run this notebook on a remote computer (for example, Binder).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_live_attention_center_detection(source=0,\n",
    "                                        flip=False,\n",
    "                                        use_popup=False,\n",
    "                                        skip_first_frames=0,\n",
    "                                        model=model,\n",
    "                                        device='CPU'):\n",
    "    player = None\n",
    "    compiled_model = core.compile_model(model, device)\n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = utils.VideoPlayer(\n",
    "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
    "        )\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "\n",
    "            # prepare the image, reshape it and change color format\n",
    "            image = Image((480, 640), image=frame)\n",
    "            image_tensor = image.prepare_image_tensor()\n",
    "\n",
    "            output_layer = compiled_model.output(0)\n",
    "\n",
    "            # make inference\n",
    "            start_time = time.time()\n",
    "            res = compiled_model([image_tensor])[output_layer]\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            # draw the attention center point on image\n",
    "            predicted_center = image.scalt_center_to_real_image_shape(res[0])\n",
    "            frame = image.draw_attention_center_point(predicted_center)\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    ext=\".jpg\", img=frame, params=[cv2.IMWRITE_JPEG_QUALITY, 100]\n",
    "                )\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=True)\n",
    "                display.display(i)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run live attention center detection\n",
    "\n",
    "Note that in some images may be several part be visually important, so the attention center point will be placed in the middle.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_live_attention_center_detection(source=0, flip=True, use_popup=False, model=model, device=\"AUTO\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c707170576399eaaed0c4f2e01a2d1b61ba791ba1842c47e5b3e4f6f79b82ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
