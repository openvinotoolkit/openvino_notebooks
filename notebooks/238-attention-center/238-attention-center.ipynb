{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The attention center model with OpenVINOâ„¢\n",
    "\n",
    "This notebook demonstrates how the use [attention center model](https://github.com/google/attention-center/tree/main) with OpenVINO. This model is in the [TensorFlow Lite format](https://www.tensorflow.org/lite). Check out [this article](https://opensource.googleblog.com/2022/12/open-sourcing-attention-center-model.html) to find more information about this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize, Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the attention-center model\n",
    "\n",
    "Download the model as part of repo https://github.com/google/attention-center/tree/main. The repo include model in folder `./model`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not Path('./attention-center').exists():\n",
    "    ! git clone https://github.com/google/attention-center"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Tensorflow Lite model to OpenVINO IR format\n",
    "\n",
    "The attention-center model is pre-trained model is in TensorFlow Lite format. To use it with OpenVINO, convert it to OpenVINO IR format with Model Optimizer. For more information about Model Optimizer, please, see the [Model Optimizer Developer Guide](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html). This step is also skipped if the model is already converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = Path(\"./attention-center/model/center.tflite\")\n",
    "\n",
    "ir_model_path = Path(\"./model/ir_center_model.xml\")\n",
    "\n",
    "core = Core()\n",
    "\n",
    "if not ir_model_path.exists():\n",
    "    model = mo.convert_model(tflite_model_path)\n",
    "    serialize(model, ir_model_path.as_posix())\n",
    "    print(\"IR model saved to {}\".format(ir_model_path))\n",
    "else:\n",
    "    print(\"Read IR model from {}\".format(ir_model_path))\n",
    "    model = core.read_model(ir_model_path)\n",
    "\n",
    "compiled_model = core.compile_model(model=model, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "print(input_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare image to use with attention-center model\n",
    "\n",
    "The attention-center model takes an RGB image with shape (480, 640) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image():\n",
    "    def __init__(self, model_input_image_shape, filename):\n",
    "        self.model_input_image_shape = model_input_image_shape\n",
    "\n",
    "        self.image = cv2.imread(filename)\n",
    "        self.real_input_image_shape = self.image.shape\n",
    "\n",
    "    def prepare_image_tensor(self):\n",
    "        rgb_image = cv2.cvtColor(self.image, cv2.COLOR_BGR2RGB)\n",
    "        resized_image = cv2.resize(rgb_image, (self.model_input_image_shape[1], self.model_input_image_shape[0]))\n",
    "\n",
    "        image_tensor = tf.constant(np.expand_dims(resized_image, axis=0),\n",
    "                                   dtype=tf.float32)\n",
    "        return image_tensor\n",
    "\n",
    "    def scalt_center_to_real_image_shape(self, predicted_center):\n",
    "        new_center_y = round(predicted_center[0] * self.real_input_image_shape[1] / self.model_input_image_shape[1])\n",
    "        new_center_x = round(predicted_center[1] * self.real_input_image_shape[0] / self.model_input_image_shape[0])\n",
    "        return (new_center_y, new_center_x)\n",
    "\n",
    "    def print_image(self, predicted_center=None):\n",
    "        image_to_print = self.image\n",
    "        if predicted_center is not None:\n",
    "            image_to_print = cv2.circle(image_to_print,\n",
    "                                        predicted_center,\n",
    "                                        radius=10,\n",
    "                                        color=(3, 3, 255),\n",
    "                                        thickness=-1)\n",
    "\n",
    "        plt.imshow(cv2.cvtColor(image_to_print, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "\n",
    "image_file_name = Path(\"../data/image/coco.jpg\")\n",
    "input_image = Image((480, 640), image_file_name.as_posix())\n",
    "image_tensor = input_image.prepare_image_tensor()\n",
    "input_image.print_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result with OpenVINO IR model\n",
    "\n",
    "The attention-center model return a 2D point as outputs, which is the predicted center of human attention on the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer = compiled_model.output(0)\n",
    "\n",
    "# makes inference\n",
    "res = compiled_model([image_tensor])[output_layer]\n",
    "print(f'Prediction for image reshaped to model input picture {res[0][0], res[0][1]}')\n",
    "predicted_center = input_image.scalt_center_to_real_image_shape(res[0])\n",
    "print(f'Prediction for real image resolution {predicted_center}')\n",
    "input_image.print_image(predicted_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get result with TensorFlow Lite API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a tflite model\n",
    "with open(tflite_model_path, 'rb') as f:\n",
    "    tflite_model_content = f.read()\n",
    "# init Interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model_content)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# create the interpreter's input index-by-name map\n",
    "index_map = {m['name']: m['index'] for m in interpreter.get_input_details()}\n",
    "\n",
    "# make inference\n",
    "interpreter.set_tensor(index_map['image:0'], image_tensor)\n",
    "interpreter.invoke()\n",
    "output_details = interpreter.get_output_details()\n",
    "tf_res = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(f'Prediction for image reshaped to model input picture ({tf_res[0][0], tf_res[0][1]})')\n",
    "\n",
    "tf_predicted_center = input_image.scalt_center_to_real_image_shape(tf_res[0])\n",
    "print(f'Prediction for real image resolution {tf_predicted_center}')\n",
    "input_image.print_image(tf_predicted_center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 100\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    res = compiled_model([image_tensor])[output_layer]\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "\n",
    "print(\n",
    "    f\"IR model in OpenVINO Runtime/CPU: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")\n",
    "\n",
    "\n",
    "start = time.perf_counter()\n",
    "for _ in range(num_images):\n",
    "    interpreter.set_tensor(index_map['image:0'], image_tensor)\n",
    "    interpreter.invoke()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    pred_from_tflite = interpreter.get_tensor(output_details[0]['index'])\n",
    "end = time.perf_counter()\n",
    "time_ir = end - start\n",
    "\n",
    "print(\n",
    "    f\"TensorFlow API: {time_ir/num_images:.4f} \"\n",
    "    f\"seconds per image, FPS: {num_images/time_ir:.2f}\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c707170576399eaaed0c4f2e01a2d1b61ba791ba1842c47e5b3e4f6f79b82ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
