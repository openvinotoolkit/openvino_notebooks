{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3c04f1a",
   "metadata": {},
   "source": [
    "# Create LLM Agent using OpenVINO and Qwen\n",
    "\n",
    "LLM are limited to the knowledge on which they have been trained and the additional knowledge provided as context, as a result, if a useful piece of information is missing the provided knowledge, the model cannot “go around” and try to find it in other sources. This is the reason why we need to introduce the concept of Agents.\n",
    "\n",
    "The core idea of agents is to use a language model to choose a sequence of actions to take. In agents, a language model is used as a reasoning engine to determine which actions to take and in which order. Agents can be seen as applications powered by LLMs and integrated with a set of tools like search engines, databases, websites, and so on. Within an agent, the LLM is the reasoning engine that, based on the user input, is able to plan and execute a set of actions that are needed to fulfill the request.\n",
    "\n",
    "![agent](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/22fa5396-8381-400f-a78f-97e25d57d807)\n",
    "\n",
    "[Qwen-Agent](https://github.com/QwenLM/Qwen-Agent) is a framework for developing LLM applications based on the instruction following, tool usage, planning, and memory capabilities of Qwen. It also comes with example applications such as Browser Assistant, Code Interpreter, and Custom Assistant.\n",
    "\n",
    "This notebook explores how to create a Function calling Agent step by step using OpenVINO and Qwen-Agent.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Create a Function calling agent](#Create-a-Function-calling-agent)\n",
    "  - [Create functions](#Create-functions)\n",
    "  - [Download model](#Download-model)\n",
    "  - [Select inference device for LLM](#Select-inference-device-for-LLM)\n",
    "  - [Create LLM for Qwen-Agent](#Create-LLM-for-Qwen-Agent)\n",
    "  - [Create Function-calling pipeline](#Create-Function-calling-pipeline)\n",
    "- [Interactive Demo](#Interactive-Demo)\n",
    "  - [Create tools](#Create-tools)\n",
    "  - [Create AI agent demo with Qwen-Agent and Gradio UI](#Create-AI-agent-demo-with-Qwen-Agent-and-Gradio-UI)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7bb0a67",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "47d43de7-9946-482d-84cb-222294c1cda8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "\"torch>=2.1\"\\\n",
    "\"datasets\"\\\n",
    "\"accelerate\"\\\n",
    "\"qwen-agent>=0.06\" \"transformers>=4.38.1\" \"modelscope_studio\" \"langchain>=0.2.3\" \"langchain-community>=0.2.4\" \"wikipedia\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a2e19-56be-478c-87cf-4d906b08a20f",
   "metadata": {},
   "source": [
    "## Create a Function calling agent\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Function calling allows a model to detect when one or more tools should be called and respond with the inputs that should be passed to those tools. In an API call, you can describe tools and have the model intelligently choose to output a structured object like JSON containing arguments to call these tools. The goal of tools APIs is to more reliably return valid and useful tool calls than what can be done using a generic text completion or chat API.\n",
    "\n",
    "We can take advantage of this structured output, combined with the fact that you can bind multiple tools to a tool calling chat model and allow the model to choose which one to call, to create an agent that repeatedly calls tools and receives results until a query is resolved."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c722d565",
   "metadata": {},
   "source": [
    "### Create a function\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "First, we need to create a example function/tool for getting the informantion of current weahter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e8bfe609-1823-4df7-9a68-f210a58a0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_current_weather(city_name):\n",
    "    \"\"\"Get the current weather in a given city name\"\"\"\n",
    "    if not isinstance(city_name, str):\n",
    "        raise TypeError(\"City name must be a string\")\n",
    "    key_selection = {\n",
    "        \"current_condition\": [\n",
    "            \"temp_C\",\n",
    "            \"FeelsLikeC\",\n",
    "            \"humidity\",\n",
    "            \"weatherDesc\",\n",
    "            \"observation_time\",\n",
    "        ],\n",
    "    }\n",
    "    resp = requests.get(f\"https://wttr.in/{city_name}?format=j1\")\n",
    "    resp.raise_for_status()\n",
    "    resp = resp.json()\n",
    "    ret = {k: {_v: resp[k][0][_v] for _v in v} for k, v in key_selection.items()}\n",
    "\n",
    "    return str(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31041310-cc81-4989-b8c1-c83face90b2c",
   "metadata": {},
   "source": [
    "Wrap the function's name and decription into a json list, and it will help LLM to find out which function should be called for current task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ea4ce13",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [{\n",
    "    'name': 'get_current_weather',\n",
    "    'description': 'Get the current weather in a given city name',\n",
    "    'parameters': {\n",
    "        'type': 'object',\n",
    "        'properties': {\n",
    "            'city_name': {\n",
    "                'type': 'string',\n",
    "                'description': 'The city and state, e.g. San Francisco, CA',\n",
    "            },\n",
    "        },\n",
    "        'required': ['city_name'],\n",
    "    },\n",
    "}]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "259e1f0d",
   "metadata": {},
   "source": [
    "### Download model\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Large Language Models (LLMs) are a core component of Agent. In this example, we will demostrate how to create a OpenVINO LLM model in Qwen-Agent framework. Since Qwen2 can support function calling during text generation, we select `Qwen/Qwen2-7B-Instruct` as LLM in agent pipeline.\n",
    "\n",
    "* **Qwen/Qwen2-7B-Instruct** - Qwen2 is the new series of Qwen large language models. Compared with the state-of-the-art opensource language models, including the previous released Qwen1.5, Qwen2 has generally surpassed most opensource models and demonstrated competitiveness against proprietary models across a series of benchmarks targeting for language understanding, language generation, multilingual capability, coding, mathematics, reasoning, etc. [Model Card](https://huggingface.co/Qwen/Qwen2-7B-Instruct)\n",
    "\n",
    "To run LLM locally, we have to download the model in the first step. It is possible to [export your model](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#export) to the OpenVINO IR format with the CLI, and load the model from local folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "981df8fe-cfcf-455a-919e-dda36f3b5dfb",
   "metadata": {
    "test_replace": {
     "mistralai/Mistral-7B-Instruct-v0.3": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "model_id = \"Qwen/Qwen2-7B-Instruct\"\n",
    "model_path = \"Qwen2-7B-Instruct-ov\"\n",
    "\n",
    "if not Path(model_path).exists():\n",
    "    !optimum-cli export openvino --model {model_id} --task text-generation-with-past --trust-remote-code --weight-format int4 {model_path}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6cfdbbae",
   "metadata": {},
   "source": [
    "### Select inference device for LLM\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1ea3bdb-f97c-4374-880a-2b62abb5baaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e216e8f3a08746f08ec4960fc5e541f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'GPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openvino as ov\n",
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "support_devices = core.available_devices\n",
    "if \"NPU\" in support_devices:\n",
    "    support_devices.remove(\"NPU\")\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "77244c52",
   "metadata": {},
   "source": [
    "### Create LLM for Qwen-Agent\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "OpenVINO has been integrated into the `Qwen-Agent` framework. You can use following method to create a OpenVINO based LLM for a `Qwen-Agent` pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abfaab28-fd5b-46cd-88ad-b60ea5a3cdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from qwen_agent.llm import get_chat_model\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "llm_cfg = {\n",
    "    'ov_model_dir': model_path,\n",
    "    'model_type': 'openvino',\n",
    "    'device': device.value,\n",
    "    'ov_config': ov_config\n",
    "}\n",
    "llm = get_chat_model(llm_cfg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d70905e2",
   "metadata": {},
   "source": [
    "You can get additional inference speed improvement with [Dynamic Quantization of activations and KV-cache quantization] on CPU(https://docs.openvino.ai/2024/learn-openvino/llm_inference_guide/llm-inference-hf.html#enabling-openvino-runtime-optimizations). These options can be enabled with `ov_config` as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "52a9a190",
   "metadata": {},
   "source": [
    "## Create Function-calling pipeline\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "After defining the functions and LLM, we can build the agent pipeline with capability of function calling.\n",
    "\n",
    "The workflow of Qwen2 function calling consists of several steps:\n",
    "\n",
    "1. Role `user` sending the request.\n",
    "2. Check if the model wanted to call a function, and call the function if needed\n",
    "3. Get the observation from `function`'s results.\n",
    "4. Consolidate the observation into final respone of `assistant`.\n",
    "\n",
    "A typical multi-turn dialogue structure is as follows:\n",
    "\n",
    "```\n",
    "{'role': 'user', 'content': 'create a picture of cute cat'},\n",
    "{'role': 'assistant', 'content': '', 'function_call': {'name': 'my_image_gen', 'arguments': '{\"prompt\": \"a cute cat\"}'}},\n",
    "{'role': 'function', 'content': '{\"image_url\": \"https://image.pollinations.ai/prompt/a%20cute%20cat\"}', 'name': 'my_image_gen'},\n",
    "{'role': 'assistant', 'content': \"Here is the image of a cute cat based on your description:\\n\\n![](https://image.pollinations.ai/prompt/a%20cute%20cat).\"},\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4799540b-eee0-491f-a5b6-5bae68c22af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# User question:\n",
      "[{'role': 'user', 'content': \"What's the weather like in San Francisco?\"}]\n",
      "# Assistant Response 1:\n",
      "{'role': 'assistant', 'content': '', 'function_call': {'name': 'get_current_weather', 'arguments': '{\"city_name\": \"San Francisco, CA\"}'}}\n",
      "# Function Response:\n",
      "{'current_condition': {'temp_C': '19', 'FeelsLikeC': '19', 'humidity': '63', 'weatherDesc': [{'value': 'Clear'}], 'observation_time': '03:09 AM'}}\n",
      "# Assistant Response 2:\n",
      "{'role': 'assistant', 'content': 'The current weather in San Francisco is Clear. The temperature is 19°C and it feels like the same temperature due to humidity which is at 63%. The observation time was recorded at 03:09 AM.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "print('# User question:')\n",
    "messages = [{'role': 'user', 'content': \"What's the weather like in San Francisco?\"}]\n",
    "print(messages)\n",
    "\n",
    "print('# Assistant Response 1:')\n",
    "responses = []\n",
    "\n",
    "# Step 1: Role `user` sending the request\n",
    "for responses in llm.chat(\n",
    "        messages=messages,\n",
    "        functions=functions,\n",
    "        stream=False,\n",
    "):\n",
    "    print(responses)\n",
    "\n",
    "messages.append(responses)\n",
    "\n",
    "# Step 2: check if the model wanted to call a function, and call the function if needed\n",
    "last_response = messages[-1]\n",
    "if last_response.get('function_call', None):\n",
    "    available_functions = {\n",
    "        'get_current_weather': get_current_weather,\n",
    "    }  # only one function in this example, but you can have multiple\n",
    "    function_name = last_response['function_call']['name']\n",
    "    function_to_call = available_functions[function_name]\n",
    "    function_args = json.loads(last_response['function_call']['arguments'])\n",
    "    function_response = function_to_call(\n",
    "        city_name=function_args.get('city_name'),\n",
    "    )\n",
    "    print('# Function Response:')\n",
    "    print(function_response)\n",
    "\n",
    "    # Step 3: Get the observation from `function`'s results\n",
    "    messages.append({\n",
    "        'role': 'function',\n",
    "        'name': function_name,\n",
    "        'content': function_response,\n",
    "    })\n",
    "\n",
    "    print('# Assistant Response 2:')\n",
    "    # Step 4: Consolidate the observation from function into final respone\n",
    "    for responses in llm.chat(\n",
    "            messages=messages,\n",
    "            functions=functions,\n",
    "            stream=False,\n",
    "    ):  # get a new response from the model where it can see the function response\n",
    "        print(responses)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "688ced57",
   "metadata": {},
   "source": [
    "## Interactive Demo\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Let's create a interactive agent using [Gradio](https://www.gradio.app/).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce623516",
   "metadata": {},
   "source": [
    "### Create tools\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Qwen-Agent provides a mechanism for [registering tools](https://github.com/QwenLM/Qwen-Agent/blob/main/docs/tool.md). For example, to register your own image generation tool:\n",
    "\n",
    "- Specify the tool’s name, description, and parameters. Note that the string passed to `@register_tool('my_image_gen')` is automatically added as the `.name` attribute of the class and will serve as the unique identifier for the tool.\n",
    "- Implement the `call(...)` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f64b67c-1259-4fe6-bfc3-af317bfe04f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Tool `image_generation` already exists! Please ensure that the tool name is unique.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson5\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mqwen_agent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseTool, register_tool\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129;43m@register_tool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mImageGeneration\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mBaseTool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAI painting (image generation) service, input text description, and return the image URL drawn based on text information.\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mname\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdescription\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDetailed description of the desired image content, in English\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrequired\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/qwen_agent/tools/base.py:18\u001b[0m, in \u001b[0;36mregister_tool.<locals>.decorator\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m     16\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTool `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` already exists! Overwriting with class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTool `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` already exists! Please ensure that the tool name is unique.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m!=\u001b[39m name):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m conflicts with @register_tool(name=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Tool `image_generation` already exists! Please ensure that the tool name is unique."
     ]
    }
   ],
   "source": [
    "import urllib.parse\n",
    "import json5\n",
    "from qwen_agent.tools.base import BaseTool, register_tool\n",
    "\n",
    "@register_tool('image_generation')\n",
    "class ImageGeneration(BaseTool):\n",
    "    description = 'AI painting (image generation) service, input text description, and return the image URL drawn based on text information.'\n",
    "    parameters = [{\n",
    "        'name': 'prompt',\n",
    "        'type': 'string',\n",
    "        'description': 'Detailed description of the desired image content, in English',\n",
    "        'required': True\n",
    "    }]\n",
    "\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        prompt = json5.loads(params)['prompt']\n",
    "        prompt = urllib.parse.quote(prompt)\n",
    "        return json5.dumps(\n",
    "            {'image_url': f'https://image.pollinations.ai/prompt/{prompt}'},\n",
    "            ensure_ascii=False)\n",
    "\n",
    "\n",
    "@register_tool('get_current_weather')\n",
    "class GetCurrentWeather(BaseTool):\n",
    "    description = 'Get the current weather in a given city name.'\n",
    "    parameters = [{\n",
    "        'name': 'city_name',\n",
    "        'type': 'string',\n",
    "        'description': 'The city and state, e.g. San Francisco, CA',\n",
    "        'required': True\n",
    "    }]\n",
    "\n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        # `params` are the arguments generated by the LLM agent.\n",
    "        city_name = json5.loads(params)['city_name']\n",
    "        key_selection = {\n",
    "            \"current_condition\": [\n",
    "                \"temp_C\",\n",
    "                \"FeelsLikeC\",\n",
    "                \"humidity\",\n",
    "                \"weatherDesc\",\n",
    "                \"observation_time\",\n",
    "            ],\n",
    "        }\n",
    "        resp = requests.get(f\"https://wttr.in/{city_name}?format=j1\")\n",
    "        resp.raise_for_status()\n",
    "        resp = resp.json()\n",
    "        ret = {k: {_v: resp[k][0][_v] for _v in v} for k, v in key_selection.items()}\n",
    "        return str(ret)\n",
    "\n",
    "@register_tool('wikipedia')\n",
    "class Wikipedia(BaseTool):\n",
    "    description = 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects.'\n",
    "    parameters = [{\n",
    "        'name': 'query',\n",
    "        'type': 'string',\n",
    "        'description': 'Query to look up on wikipedia',\n",
    "        'required': True\n",
    "    }]\n",
    "    \n",
    "    def call(self, params: str, **kwargs) -> str:\n",
    "        # `params` are the arguments generated by the LLM agent.\n",
    "        from langchain.tools import WikipediaQueryRun\n",
    "        from langchain_community.utilities import WikipediaAPIWrapper\n",
    "        query = json5.loads(params)['query']\n",
    "        wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(top_k_results=2, doc_content_chars_max=1000))\n",
    "        resutlt = wikipedia.run(query)\n",
    "        return str(resutlt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea649d2f-ab4b-4228-8f32-75b01a98d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = ['image_generation', 'get_current_weather', 'wikipedia']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148b7c9d-2b8b-45fa-9fc1-c37ebdc1e5fb",
   "metadata": {},
   "source": [
    "### Create AI agent demo with Qwen-Agent and Gradio UI\n",
    "\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The Agent class serves as a higher-level interface for Qwen-Agent, where an Agent object integrates the interfaces for tool calls and LLM (Large Language Model). The Agent receives a list of messages as input and produces a generator that yields a list of messages, effectively providing a stream of output messages.\n",
    "\n",
    "Qwen-Agent offer a generic Agent class: the `Assistant` class, which, when directly instantiated, can handle the majority of Single-Agent tasks. Features:\n",
    "\n",
    "- It supports role-playing;\n",
    "- It provides automatic planning and tool calls abilities;\n",
    "- RAG (Retrieval-Augmented Generation): It accepts documents input, and can use an integrated RAG strategy to parse the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "52aed08c-b515-40b5-b371-16e27fb89e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from qwen_agent.agents import Assistant\n",
    "from qwen_agent.gui import WebUI\n",
    "\n",
    "bot = Assistant(llm=llm_cfg,\n",
    "                function_list=tools,\n",
    "               name=\"OpenVINO Agent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "752e052d-74d7-4df7-81eb-bbf317196ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://10.3.233.99:4543\n",
      "IMPORTANT: You are using gradio version 4.21.0, however version 4.29.0 is available, please upgrade.\n",
      "--------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 94\u001b[0m\n\u001b[1;32m     85\u001b[0m chatbot_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt.suggestions\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBased on current weather in London, show me a picture of Big Ben\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is OpenVINO ?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreate an image of pink cat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather like in New York now ?\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent.avatar\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./openvino-logo.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_placeholder\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease input your request here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     89\u001b[0m }\n\u001b[1;32m     90\u001b[0m demo \u001b[38;5;241m=\u001b[39m OpenVINOUI(\n\u001b[1;32m     91\u001b[0m     bot,\n\u001b[1;32m     92\u001b[0m     chatbot_config\u001b[38;5;241m=\u001b[39mchatbot_config,\n\u001b[1;32m     93\u001b[0m )\n\u001b[0;32m---> 94\u001b[0m \u001b[43mdemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m10.3.233.99\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver_port\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4543\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 82\u001b[0m, in \u001b[0;36mOpenVINOUI.run\u001b[0;34m(self, messages, share, server_name, server_port, concurrency_limit, enable_mention, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     stop\u001b[38;5;241m.\u001b[39mclick(\n\u001b[1;32m     74\u001b[0m         fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_cancel,\n\u001b[1;32m     75\u001b[0m         inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m         queue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     80\u001b[0m     clear\u001b[38;5;241m.\u001b[39mclick(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, chatbot, queue\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshare\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshare\u001b[49m\u001b[43m,\u001b[49m\u001b[43mserver_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mserver_port\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserver_port\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/gradio/blocks.py:2165\u001b[0m, in \u001b[0;36mBlocks.launch\u001b[0;34m(self, inline, inbrowser, share, debug, max_threads, auth, auth_message, prevent_thread_lock, show_error, server_name, server_port, height, width, favicon_path, ssl_keyfile, ssl_certfile, ssl_keyfile_password, ssl_verify, quiet, show_api, allowed_paths, blocked_paths, root_path, app_kwargs, state_session_capacity, share_server_address, share_server_protocol, auth_dependency, _frontend)\u001b[0m\n\u001b[1;32m   2157\u001b[0m \u001b[38;5;66;03m# If running in a colab or not able to access localhost,\u001b[39;00m\n\u001b[1;32m   2158\u001b[0m \u001b[38;5;66;03m# a shareable link must be created.\u001b[39;00m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2160\u001b[0m     _frontend\n\u001b[1;32m   2161\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wasm_utils\u001b[38;5;241m.\u001b[39mIS_WASM\n\u001b[1;32m   2162\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m networking\u001b[38;5;241m.\u001b[39murl_ok(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_url)\n\u001b[1;32m   2163\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshare\n\u001b[1;32m   2164\u001b[0m ):\n\u001b[0;32m-> 2165\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2167\u001b[0m     )\n\u001b[1;32m   2169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_colab \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m quiet:\n\u001b[1;32m   2170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "\u001b[0;31mValueError\u001b[0m: When localhost is not accessible, a shareable link must be created. Please set share=True or check your proxy settings to allow access to localhost."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-36 (generate_and_signal_complete):\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 761, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/qwen_agent/llm/openvino.py\", line 115, in generate_and_signal_complete\n",
      "    self.ov_model.generate(**generate_cfg)\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py\", line 659, in generate\n",
      "    result = super().generate(\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/transformers/generation/utils.py\", line 1622, in generate\n",
      "    result = self._sample(\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/transformers/generation/utils.py\", line 2791, in _sample\n",
      "    outputs = self(\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/optimum/modeling_base.py\", line 92, in __call__\n",
      "    return self.forward(*args, **kwargs)\n",
      "  File \"/home/ethan/intel/openvino_notebooks/openvino_env/lib/python3.10/site-packages/optimum/intel/openvino/modeling_decoder.py\", line 498, in forward\n",
      "    self.request.wait()\n",
      "RuntimeError: Exception from src/inference/src/cpp/infer_request.cpp:245:\n",
      "Exception from src/bindings/python/src/pyopenvino/core/infer_request.hpp:54:\n",
      "Caught exception: Infer Request was canceled\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Optional, Union\n",
    "from qwen_agent.llm.schema import CONTENT, FILE, IMAGE, NAME, ROLE, USER, Message\n",
    "from qwen_agent.gui.utils import convert_fncall_to_text, convert_history_to_chatbot, get_avatar_image\n",
    "\n",
    "class OpenVINOUI(WebUI):\n",
    "    \n",
    "    def request_cancel(self):\n",
    "        self.agent_list[0].llm.ov_model.request.cancel()\n",
    "\n",
    "    def run(self,\n",
    "        messages: List[Message] = None,\n",
    "        share: bool = False,\n",
    "        server_name: str = None,\n",
    "        server_port: int = None,\n",
    "        concurrency_limit: int = 10,\n",
    "        enable_mention: bool = False,\n",
    "        **kwargs):\n",
    "        self.run_kwargs = kwargs\n",
    "    \n",
    "        from qwen_agent.gui.gradio import gr, mgr\n",
    "    \n",
    "        with gr.Blocks(\n",
    "            theme=gr.themes.Soft(),\n",
    "            css=\".disclaimer {font-variant-caps: all-small-caps;}\",\n",
    "        ) as self.demo:\n",
    "            gr.Markdown(f\"\"\"<h1><center>OpenVINO Qwen Agent </center></h1>\"\"\")\n",
    "            history = gr.State([])\n",
    "    \n",
    "            with gr.Row():\n",
    "                with gr.Column(scale=4):\n",
    "                    chatbot = mgr.Chatbot(\n",
    "                        value=convert_history_to_chatbot(messages=messages),\n",
    "                        avatar_images=[\n",
    "                            self.user_config,\n",
    "                            self.agent_config_list,\n",
    "                        ],\n",
    "                        height=900,\n",
    "                        avatar_image_width=80,\n",
    "                        flushing=False,\n",
    "                        show_copy_button=True,\n",
    "                    )\n",
    "                    input = mgr.MultimodalInput(placeholder=self.input_placeholder, submit_button_props={\"size\": \"lg\", \"variant\": \"primary\"})\n",
    "                    with gr.Row():\n",
    "                        stop = gr.Button(\"Stop\", variant=\"stop\")\n",
    "                        clear = gr.Button(\"Clear\", variant=\"stop\")    \n",
    "                with gr.Column(scale=1):\n",
    "                    agent_interactive = self.agent_list[0]\n",
    "                    capabilities = [key for key in agent_interactive.function_map.keys()]\n",
    "                    gr.CheckboxGroup(\n",
    "                        label='Tools',\n",
    "                        value=capabilities,\n",
    "                        choices=capabilities,\n",
    "                        interactive=False,\n",
    "                    )\n",
    "            with gr.Row():\n",
    "                gr.Examples(self.prompt_suggestions, inputs=[input], label=\"Click on any example and press the 'Submit' button\")\n",
    "            \n",
    "            input_promise = input.submit(\n",
    "                fn=self.add_text,\n",
    "                inputs=[input, chatbot, history],\n",
    "                outputs=[input, chatbot, history],\n",
    "                queue=False,\n",
    "            )\n",
    "            input_promise.then(self.flushed, None, [input])\n",
    "    \n",
    "            self.demo.load(None)\n",
    "            \n",
    "            input_promise = input_promise.then(\n",
    "                self.agent_run,\n",
    "                [chatbot, history],\n",
    "                [chatbot, history],\n",
    "            )\n",
    "            stop.click(\n",
    "                fn=self.request_cancel,\n",
    "                inputs=None,\n",
    "                outputs=None,\n",
    "                cancels=[input_promise],\n",
    "                queue=False,\n",
    "            )\n",
    "            clear.click(lambda: None, None, chatbot, queue=False)\n",
    "    \n",
    "        self.demo.launch(share=share,server_name=server_name,server_port=server_port)\n",
    "    \n",
    "    \n",
    "chatbot_config = {\n",
    "    'prompt.suggestions': [\"Based on current weather in London, show me a picture of Big Ben\", \"What is OpenVINO ?\", \"Create an image of pink cat\", \"What is the weather like in New York now ?\"],\n",
    "    'agent.avatar': \"./openvino-logo.png\",\n",
    "    'input_placeholder': \"Please input your request here\"\n",
    "}\n",
    "demo = OpenVINOUI(\n",
    "    bot,\n",
    "    chatbot_config=chatbot_config,\n",
    ")\n",
    "demo.run(server_name = '10.3.233.99', server_port = 4543)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9f9bbfb-a2e9-469a-892b-fc6617d08eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 4543\n"
     ]
    }
   ],
   "source": [
    "demo.demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4c5a4c-5229-48d6-be83-d58c41562f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/2abb2389-e612-4599-82c6-64cdac259120",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
