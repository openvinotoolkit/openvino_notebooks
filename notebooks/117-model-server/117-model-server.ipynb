{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "01412caf",
   "metadata": {},
   "source": [
    "# Hello Model Server\n",
    "\n",
    "A basic introduction to using OpenVINO™ Model Server (OVMS).\n",
    "\n",
    "## What is Model Server ?\n",
    "Model Server hosts models and makes them accessible to software components over standard network protocols: a client sends a request to the model server, which performs model inference and sends a response back to the client. Model Server offers many advantages for efficient model deployment:\n",
    "\n",
    "- Remote inference enables using lightweight clients with only the necessary functions to perform API calls to edge or cloud deployments.\n",
    "- Applications are independent of the model framework, hardware device, and infrastructure.\n",
    "- Client applications in any programming language that supports REST or gRPC calls can be used to run inference remotely on the model server.\n",
    "- Clients require fewer updates since client libraries change very rarely.\n",
    "- Model topology and weights are not exposed directly to client applications, making it easier to control access to the model.\n",
    "- Ideal architecture for microservices-based applications and deployments in cloud environments – including Kubernetes and OpenShift clusters.\n",
    "- Efficient resource utilization with horizontal and vertical inference scaling.\n",
    "  \n",
    "![ovms_diagram](https://user-images.githubusercontent.com/91237924/215658773-4720df00-3b95-4a84-85a2-40f06138e914.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "efce7a1c",
   "metadata": {},
   "source": [
    "## Serving with OpenVINO Model Server\n",
    "OpenVINO Model Server (OVMS) is a high-performance system for serving models. Implemented in C++ for scalability and optimized for deployment on Intel architectures, the model server uses the same architecture and API as TensorFlow Serving and KServe while applying OpenVINO for inference execution. Inference service is provided via gRPC or REST API, making deploying new algorithms and AI experiments easy.\n",
    "\n",
    "![ovms_high_level](https://user-images.githubusercontent.com/91237924/215658767-0e0fc221-aed0-4db1-9a82-6be55f244dba.png)\n",
    "\n",
    "To quickly start using OpenVINO™ Model Server follow these steps:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "740bfdd8",
   "metadata": {},
   "source": [
    "## Step 1: Prepare Docker\n",
    "Install [Docker Engine](https://docs.docker.com/engine/install/), including its [post-installation](https://docs.docker.com/engine/install/linux-postinstall/) steps, on your development system. To verify installation, test it using the following command. If it displays a test image and a message, it is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d7aedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run hello-world"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85b48949",
   "metadata": {},
   "source": [
    "## Step 2: Download the Model Server\n",
    "Download the Docker image that contains OpenVINO Model Server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99737c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker pull openvino/model_server:2022.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c8052a30",
   "metadata": {},
   "source": [
    "## Step 3: Preparing a Model Repository\n",
    "The models need to be placed and mounted in a particular directory structure and according to the following rules:\n",
    "```\n",
    "tree models/\n",
    "models/\n",
    "├── model1\n",
    "│   ├── 1\n",
    "│   │   ├── ir_model.bin\n",
    "│   │   └── ir_model.xml\n",
    "│   └── 2\n",
    "│       ├── ir_model.bin\n",
    "│       └── ir_model.xml\n",
    "├── model2\n",
    "│   └── 1\n",
    "│       ├── ir_model.bin\n",
    "│       ├── ir_model.xml\n",
    "│       └── mapping_config.json\n",
    "├── model3\n",
    "│    └── 1\n",
    "│        └── model.onnx\n",
    "├── model4\n",
    "│      └── 1\n",
    "│        ├── model.pdiparams\n",
    "│        └── model.pdmodel\n",
    "└── model5\n",
    "       └── 1\n",
    "         └── TF_fronzen_model.pb\n",
    "```\n",
    "\n",
    "\n",
    "* Each model should be stored in a dedicated directory, e.g. model1 and model2.\n",
    "\n",
    "* Each model directory should include a sub-folder for each of its versions (1,2, etc). The versions and their folder names should be positive integer values.\n",
    "\n",
    "* Note: In execution, the versions are enabled according to a pre-defined version policy. If the client does not specify the version number in parameters, by default, the latest version is served.\n",
    "\n",
    "* Every version folder must include model files, that is, .bin and .xml for IR, .onnx for ONNX, .pdiparams and .pdmodel for Paddlepaddle. The file name can be arbitrary.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2a7a9a3",
   "metadata": {},
   "source": [
    "### For Linux and MacOS user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230a63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp ../004-hello-detection/model/* models/detection/1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "382f1855",
   "metadata": {},
   "source": [
    "### For Windows user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32681eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!copy ..\\004-hello-detection\\model\\* models\\detection\\1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b66674df",
   "metadata": {},
   "source": [
    "## Step 4: Start the Model Server Container\n",
    "Start the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d --rm  --name=\"ovms\" -v $(pwd)/models:/models -p 9000:9000 openvino/model_server:2022.3 \\\n",
    "--model_path /models/detection/ --model_name detection --port 9000 --shape auto"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8ab7f4c",
   "metadata": {},
   "source": [
    "The required Model Server parameters are listed below. For additional configuration options, see the Model Server Parameters section.\n",
    "\n",
    "<table class=\"table\">\n",
    "<colgroup>\n",
    "<col style=\"width: 20%\" />\n",
    "<col style=\"width: 80%\" />\n",
    "</colgroup>\n",
    "<tbody>\n",
    "<tr class=\"row-odd\"><td><p><cite>–rm</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">remove the container when exiting the Docker container</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><cite>-d</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">runs the container in the background</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><cite>-v</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">defines how to mount the model folder in the Docker container</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><cite>-p</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">exposes the model serving port outside the Docker container</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><cite>openvino/model_server:latest</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">represents the image name; the ovms binary is the Docker entry point</div>\n",
    "<div class=\"line\">varies by tag and build process - see tags: <a class=\"reference external\" href=\"https://hub.docker.com/r/openvino/model_server/tags/\">https://hub.docker.com/r/openvino/model_server/tags/</a> for a full tag list.</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><cite>–model_path</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">model location, which can be:</div>\n",
    "<div class=\"line\">a Docker container path that is mounted during start-up</div>\n",
    "<div class=\"line\">a Google Cloud Storage path <cite>gs://&lt;bucket&gt;/&lt;model_path&gt;</cite></div>\n",
    "<div class=\"line\">an AWS S3 path <cite>s3://&lt;bucket&gt;/&lt;model_path&gt;</cite></div>\n",
    "<div class=\"line\">an Azure blob path <cite>az://&lt;container&gt;/&lt;model_path&gt;</cite></div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><cite>–model_name</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">the name of the model in the model_path</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td><p><cite>–port</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">the gRPC server port</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td><p><cite>–rest_port</cite></p></td>\n",
    "<td><div class=\"line-block\">\n",
    "<div class=\"line\">the REST server port</div>\n",
    "</div>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "If the serving port ```9000``` is already in use, please switch it to another avaiable port on your system."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c39fb8d",
   "metadata": {},
   "source": [
    "## Step 5: Prepare the Example Client Components\n",
    "OpenVINO Model Server exposes two sets of APIs: one compatible with ```TensorFlow Serving``` and another one, with ```KServe API```, for inference. Both APIs work on ```gRPC``` and ```REST```interfaces. Supporting two sets of APIs makes OpenVINO Model Server easier to plug into existing systems the already leverage one of these APIs for inference. This example will demostrate how to write a TensorFlow Serving API client for object detection.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Install necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b230f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ovmsclient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a4e10e1",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9de88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ovmsclient import make_grpc_client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b29be4",
   "metadata": {},
   "source": [
    "### Request Model Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297f02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "address = \"localhost:9000\"\n",
    "\n",
    "# Bind the grpc address to the client object\n",
    "client = make_grpc_client(address)\n",
    "model_status = client.get_model_status(model_name=\"detection\")\n",
    "print(model_status)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9b2f712a",
   "metadata": {},
   "source": [
    "### Request Model Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf507d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metadata = client.get_model_metadata(model_name=\"detection\")\n",
    "print(model_metadata)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eed3ebb2",
   "metadata": {},
   "source": [
    "### Load input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9786dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text detection models expect an image in BGR format.\n",
    "image = cv2.imread(\"../data/image/intel_rnb.jpg\")\n",
    "fp_image = image.astype(\"float32\")\n",
    "\n",
    "# Resize the image to meet network expected input sizes.\n",
    "input_shape = model_metadata['inputs']['image']['shape']\n",
    "height, width = input_shape[2], input_shape[3]\n",
    "resized_image = cv2.resize(fp_image, (height, width))\n",
    "\n",
    "# Reshape to the network input shape.\n",
    "input_image = np.expand_dims(resized_image.transpose(2, 0, 1), 0)\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "49985095",
   "metadata": {},
   "source": [
    "### Request Prediction on a Numpy Array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05d8f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = {\"image\": input_image}\n",
    "\n",
    "# Run inference on model server and receive the result data\n",
    "boxes = client.predict(inputs=inputs, model_name=\"detection\")['boxes']\n",
    "\n",
    "# Remove zero only boxes.\n",
    "boxes = boxes[~np.all(boxes == 0, axis=1)]\n",
    "print(boxes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a27258cf",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6693c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each detection, the description is in the [x_min, y_min, x_max, y_max, conf] format:\n",
    "# The image passed here is in BGR format with changed width and height. To display it in colors expected by matplotlib, use cvtColor function\n",
    "def convert_result_to_image(bgr_image, resized_image, boxes, threshold=0.3, conf_labels=True):\n",
    "    # Define colors for boxes and descriptions.\n",
    "    colors = {\"red\": (255, 0, 0), \"green\": (0, 255, 0)}\n",
    "\n",
    "    # Fetch the image shapes to calculate a ratio.\n",
    "    (real_y, real_x), (resized_y, resized_x) = bgr_image.shape[:2], resized_image.shape[:2]\n",
    "    ratio_x, ratio_y = real_x / resized_x, real_y / resized_y\n",
    "\n",
    "    # Convert the base image from BGR to RGB format.\n",
    "    rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Iterate through non-zero boxes.\n",
    "    for box in boxes:\n",
    "        # Pick a confidence factor from the last place in an array.\n",
    "        conf = box[-1]\n",
    "        if conf > threshold:\n",
    "            # Convert float to int and multiply corner position of each box by x and y ratio.\n",
    "            # If the bounding box is found at the top of the image, \n",
    "            # position the upper box bar little lower to make it visible on the image. \n",
    "            (x_min, y_min, x_max, y_max) = [\n",
    "                int(max(corner_position * ratio_y, 10)) if idx % 2 \n",
    "                else int(corner_position * ratio_x)\n",
    "                for idx, corner_position in enumerate(box[:-1])\n",
    "            ]\n",
    "\n",
    "            # Draw a box based on the position, parameters in rectangle function are: image, start_point, end_point, color, thickness.\n",
    "            rgb_image = cv2.rectangle(rgb_image, (x_min, y_min), (x_max, y_max), colors[\"green\"], 3)\n",
    "\n",
    "            # Add text to the image based on position and confidence.\n",
    "            # Parameters in text function are: image, text, bottom-left_corner_textfield, font, font_scale, color, thickness, line_type.\n",
    "            if conf_labels:\n",
    "                rgb_image = cv2.putText(\n",
    "                    rgb_image,\n",
    "                    f\"{conf:.2f}\",\n",
    "                    (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8,\n",
    "                    colors[\"red\"],\n",
    "                    1,\n",
    "                    cv2.LINE_AA,\n",
    "                )\n",
    "\n",
    "    return rgb_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14476f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(convert_result_to_image(image, resized_image, boxes, conf_labels=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b8dd7a",
   "metadata": {},
   "source": [
    "To stop the model server container, you can use the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942cf5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker stop ovms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4e6dc0",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. [OpenVINO™ Model Server](https://docs.openvino.ai/latest/ovms_what_is_openvino_model_server.html)\n",
    "2. [openvinotoolkit/model_server](https://github.com/openvinotoolkit/model_server/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
