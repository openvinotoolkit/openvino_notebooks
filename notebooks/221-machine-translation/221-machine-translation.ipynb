{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine translation demo\n",
    "This demo utilizes Intel's pre-trained model that translates from English to German. More information about the model can be found [here](https://github.com/openvinotoolkit/open_model_zoo/blob/master/models/intel/machine-translation-nar-en-de-0002/README.md).\n",
    "\n",
    "This model encodes sentences using the `SentecePieceBPETokenizer` from HuggingFace. The tokenizer vocabulary is downloaded automatically with the OMZ tool.\n",
    "\n",
    "**Inputs**\n",
    "The model's input is a sequence of up to 150 tokens with the following structure: `<s>` + _tokenized sentence_ + `<s>` + `<pad>` (`<pad>` tokens pad the remaining blank spaces).\n",
    "\n",
    "**Output**\n",
    "After the inference, we have a sequence of up to 200 tokens. The structure is the same as the one for the input.\n",
    "\n",
    "#### Table of contents:\n",
    "- [Downloading model](#Downloading-model)\n",
    "- [Load and configure the model](#Load-and-configure-the-model)\n",
    "- [Select inference device](#Select-inference-device)\n",
    "- [Load tokenizers](#Load-tokenizers)\n",
    "- [Perform translation](#Perform-translation)\n",
    "- [Translate the sentence](#Translate-the-sentence)\n",
    "    - [Test your translation](#Test-your-translation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Install requirements\n",
    "%pip install -q \"openvino>=2023.1.0\"\n",
    "%pip install -q tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import openvino as ov\n",
    "import numpy as np\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "from tokenizers import SentencePieceBPETokenizer\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Downloading model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The following command will download the model to the current directory. Make sure you have run `pip install openvino-dev` beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'model/machine-translation-nar-en-de-0002.xml' already exists.\n",
      "'model/machine-translation-nar-en-de-0002.bin' already exists.\n",
      "'model/tokenizer_src/merges.txt' already exists.\n",
      "'model/tokenizer_tgt/merges.txt' already exists.\n",
      "'model/tokenizer_src/vocab.json' already exists.\n",
      "'model/tokenizer_tgt/vocab.json' already exists.\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://storage.openvinotoolkit.org/repositories/open_model_zoo/2023.0/models_bin/1\"\n",
    "model_name = \"machine-translation-nar-en-de-0002\"\n",
    "precision = \"FP32\"\n",
    "model_base_dir = Path(\"model\")\n",
    "model_base_dir.mkdir(exist_ok=True)\n",
    "model_path = model_base_dir / f\"{model_name}.xml\"\n",
    "src_tok_dir = model_base_dir / \"tokenizer_src\"\n",
    "target_tok_dir = model_base_dir / \"tokenizer_tgt\"\n",
    "src_tok_dir.mkdir(exist_ok=True)\n",
    "target_tok_dir.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(base_url + f'/{model_name}/{precision}/{model_name}.xml', f\"{model_name}.xml\", model_base_dir)\n",
    "download_file(base_url + f'/{model_name}/{precision}/{model_name}.bin', f\"{model_name}.bin\", model_base_dir)\n",
    "download_file(f\"{base_url}/{model_name}/tokenizer_src/merges.txt\", \"merges.txt\", src_tok_dir)\n",
    "download_file(f\"{base_url}/{model_name}/tokenizer_tgt/merges.txt\", \"merges.txt\", target_tok_dir)\n",
    "download_file(f\"{base_url}/{model_name}/tokenizer_src/vocab.json\", \"vocab.json\", src_tok_dir)\n",
    "download_file(f\"{base_url}/{model_name}/tokenizer_tgt/vocab.json\", \"vocab.json\", target_tok_dir);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load and configure the model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The model is now available in the `model/` folder. Below, we load and configure its inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "core = ov.Core()\n",
    "model = core.read_model(model_path)\n",
    "input_name = \"tokens\"\n",
    "output_name = \"pred\"\n",
    "model.output(output_name)\n",
    "max_tokens = model.input(input_name).shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f00f770589424e890e8f564f21f981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=2, options=('CPU', 'GPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "compiled_model = core.compile_model(model, device.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Load tokenizers\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "NLP models usually take a list of tokens as standard input. A token is a single word converted to some integer. To provide the proper input, we need the vocabulary for such mapping. We use `merges.txt` to find out what sequences of letters form a token. `vocab.json` specifies the mapping between tokens and integers.\n",
    "\n",
    "The input needs to be transformed into a token sequence the model understands, and the output must be transformed into a sentence that is human readable.\n",
    "\n",
    "Initialize the tokenizer for the input `src_tokenizer` and the output `tgt_tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "src_tokenizer = SentencePieceBPETokenizer.from_file(\n",
    "    str(src_tok_dir / 'vocab.json'),\n",
    "    str(src_tok_dir / 'merges.txt')\n",
    ")\n",
    "tgt_tokenizer = SentencePieceBPETokenizer.from_file(\n",
    "    str(target_tok_dir / 'vocab.json'),\n",
    "    str(target_tok_dir / 'merges.txt')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Perform translation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The following function translates a sentence in English to German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def translate(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenize the sentence using the downloaded tokenizer and run the model,\n",
    "    whose output is decoded into a human readable string.\n",
    "\n",
    "    :param sentence: a string containing the phrase to be translated\n",
    "    :return: the translated string\n",
    "    \"\"\"\n",
    "    # Remove leading and trailing white spaces\n",
    "    sentence = sentence.strip()\n",
    "    assert len(sentence) > 0\n",
    "    tokens = src_tokenizer.encode(sentence).ids\n",
    "    # Transform the tokenized sentence into the model's input format\n",
    "    tokens = [src_tokenizer.token_to_id('<s>')] + \\\n",
    "        tokens + [src_tokenizer.token_to_id('</s>')]\n",
    "    pad_length = max_tokens - len(tokens)\n",
    "\n",
    "    # If the sentence size is less than the maximum allowed tokens,\n",
    "    # fill the remaining tokens with '<pad>'.\n",
    "    if pad_length > 0:\n",
    "        tokens = tokens + [src_tokenizer.token_to_id('<pad>')] * pad_length\n",
    "    assert len(tokens) == max_tokens, \"input sentence is too long\"\n",
    "    encoded_sentence = np.array(tokens).reshape(1, -1)\n",
    "\n",
    "    # Perform inference\n",
    "    enc_translated = compiled_model({input_name: encoded_sentence})\n",
    "    output_key = compiled_model.output(output_name)\n",
    "    enc_translated = enc_translated[output_key][0]\n",
    "\n",
    "    # Decode the sentence\n",
    "    sentence = tgt_tokenizer.decode(enc_translated)\n",
    "\n",
    "    # Remove <pad> tokens, as well as '<s>' and '</s>' tokens which mark the\n",
    "    # beginning and ending of the sentence.\n",
    "    for s in ['</s>', '<s>', '<pad>']:\n",
    "        sentence = sentence.replace(s, '')\n",
    "\n",
    "    # Transform sentence into lower case and join words by a white space\n",
    "    sentence = sentence.lower().split()\n",
    "    sentence = \" \".join(key for key, _ in itertools.groupby(sentence))\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translate the sentence\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The following function is a basic loop that translates sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def run_translator():\n",
    "    \"\"\"\n",
    "    Run the translation in real time, reading the input from the user.\n",
    "    This function prints the translated sentence and the time\n",
    "    spent during inference.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        input_sentence = input()\n",
    "        if input_sentence == \"\":\n",
    "            break\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        translated = translate(input_sentence)\n",
    "        end_time = time.perf_counter()\n",
    "        print(f'Translated: {translated}')\n",
    "        print(f'Time: {end_time - start_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# uncomment the following line for a real time translation of your input\n",
    "# run_translator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your translation\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Run the following cell with an English sentence to have it translated to German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translated: mein name ist openvino.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"My name is openvino\"\n",
    "print(f'Translated: {translate(sentence)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "01b019391e6a49e8b7efcad791f50d3a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "2d40087b6ce7442fbb8839663147f588": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "54f00f770589424e890e8f564f21f981": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "DropdownModel",
      "state": {
       "_options_labels": [
        "CPU",
        "GPU",
        "AUTO"
       ],
       "description": "Device:",
       "index": 2,
       "layout": "IPY_MODEL_2d40087b6ce7442fbb8839663147f588",
       "style": "IPY_MODEL_01b019391e6a49e8b7efcad791f50d3a"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}