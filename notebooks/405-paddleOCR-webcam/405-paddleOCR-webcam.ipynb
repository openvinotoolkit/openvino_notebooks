{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0d9296-7fa6-4025-aedf-d2a19b05ff0d",
   "metadata": {},
   "source": [
    "# PaddleOCR with OpenVINO\n",
    "\n",
    "This demo shows how to run PPOCR model on OpenVINO natively. Instead of exporting the PaddlePaddle model to ONNX and then convert to the Intermediate Representation (IR) format through OpenVINO model optimizer, we can now read directly from the Paddlepaddle Model without any conversions. [PaddleOCR](https://github.com/PaddlePaddle/PaddleOCR) is an ultra-light OCR model trained with Paddlepaddle deep learning frame work, aims to create multilingual and practical OCR tools. \n",
    "\n",
    "The paddleOCR pre-trained model used in the demo refer to the \"Chinese and English ultra-lightweight PP-OCR model (9.4M)\". More open-sourced pre-trained models could be downloaded at [PaddleOCR Github](https://github.com/PaddlePaddle/PaddleOCR)  or [PaddleOCR Gitee](https://gitee.com/paddlepaddle/PaddleOCR). Working pipeline of the paddleOCR is as follows:\n",
    "\n",
    "<img align='center' src= \"https://raw.githubusercontent.com/yoyowz/classification/master/images/paddleOCR_pipeline.png\" alt=\"drawing\" width=\"1000\"/>\n",
    "\n",
    "> Note: _To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server, the webcam will not work. You can still do inference on a video._\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5a53f7-e1c5-4aca-879f-da2dd081b989",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Run Paddle Detection with OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9486a04-b8bb-4bf5-9e13-845f2143a71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import paddle\n",
    "import math\n",
    "import time\n",
    "import collections\n",
    "\n",
    "from openvino.runtime import Core,PartialShape,Dimension\n",
    "from IPython import display\n",
    "import copy\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "from pre_post_processing import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4ea41d-18a8-4914-b367-d5717111d8e8",
   "metadata": {},
   "source": [
    "### Load the models for PaddleOCR\n",
    "\n",
    "PaddleOCR includes two parts of deep learning models, text detection and text recognition. Pre-trained models used in the demo are stored in the \"model\" folder. Other pre-trained models for PaddleOCR could be download at [PaddleOCR Github](https://github.com/PaddlePaddle/PaddleOCR)  or [PaddleOCR Gitee](https://gitee.com/paddlepaddle/PaddleOCR).\n",
    "\n",
    "Only a few lines of code are required to run the model. First, we create an Inference Engine. Then we read the network architecture and model weights from the .pdmoel and .pdiparams files to load onto the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541150c-0f98-41c6-a97c-97acb26efd2f",
   "metadata": {},
   "source": [
    "#### Load the Network for Text Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5c83a-961c-4d98-8b20-5e96c8ef71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "det_model_dir = \"./model/ch_ppocr_mobile_v2.0_det_infer\"\n",
    "det_model_file_path = det_model_dir + \"/inference.pdmodel\"\n",
    "det_params_file_path = det_model_dir + \"/inference.pdiparams\"\n",
    "\n",
    "# initialize inference engine for text detection\n",
    "det_ie = Core()\n",
    "det_net = det_ie.read_model(model=det_model_file_path, weights=det_params_file_path)\n",
    "det_compiled_model = det_ie.compile_model(model=det_net, device_name=\"CPU\")\n",
    "\n",
    "# get input and output nodes for text detection\n",
    "det_input_layer = next(iter(det_compiled_model.inputs))\n",
    "det_output_layer = next(iter(det_compiled_model.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec5c940-626c-4cf7-a90f-833200969846",
   "metadata": {},
   "source": [
    "#### Load the Network for Text Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c0a07a-8186-47b5-ad95-f104a84d13d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_model_dir = \"./model/ch_ppocr_mobile_v2.0_rec_infer\"\n",
    "rec_model_file_path = rec_model_dir + \"/inference.pdmodel\"\n",
    "rec_params_file_path = rec_model_dir + \"/inference.pdiparams\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d196913-6542-4177-87ab-c5aa1994f8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Initialize the Paddle recognition inference on CPU\n",
    "rec_ie = Core()\n",
    "# read the model and corresponding weights from file\n",
    "rec_net = rec_ie.read_model(model=rec_model_file_path, weights=rec_params_file_path)\n",
    "\n",
    "# assign dynamic shapes to every input layer on the last dimension\n",
    "for input_layer in rec_net.inputs:\n",
    "    input_shape = input_layer.partial_shape\n",
    "    input_shape[3] = Dimension(-1)\n",
    "    rec_net.reshape({input_layer: input_shape})\n",
    "\n",
    "rec_compiled_model = rec_ie.compile_model(model=rec_net, device_name=\"CPU\")\n",
    "\n",
    "# get input and output nodes\n",
    "rec_input_layer = next(iter(rec_compiled_model.inputs))\n",
    "rec_output_layer = next(iter(rec_compiled_model.outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a1a11-faec-41af-bf43-08b90d28cec3",
   "metadata": {},
   "source": [
    "### Preprocessing image functions for text detection and recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bc8364-109b-4a32-b12b-bcb85f23b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_preprocess(input_image, size):\n",
    "    img = cv2.resize(input_image, (size,size))\n",
    "    img = np.transpose(img, [2,0,1]) / 255\n",
    "    img = np.expand_dims(img, 0)\n",
    "    ##NormalizeImage: {mean: [0.485, 0.456, 0.406], std: [0.229, 0.224, 0.225], is_scale: True}\n",
    "    img_mean = np.array([0.485, 0.456,0.406]).reshape((3,1,1))\n",
    "    img_std = np.array([0.229, 0.224, 0.225]).reshape((3,1,1))\n",
    "    img -= img_mean\n",
    "    img /= img_std\n",
    "    return img.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9329d709-14bc-45aa-a1d7-d0d6d608933b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess for Paddle Recognition\n",
    "def resize_norm_img(img, max_wh_ratio):\n",
    "        rec_image_shape = [3, 32, 320]\n",
    "        imgC, imgH, imgW = rec_image_shape\n",
    "        assert imgC == img.shape[2]\n",
    "        character_type = \"ch\"\n",
    "        if character_type == \"ch\":\n",
    "            imgW = int((32 * max_wh_ratio))\n",
    "        h, w = img.shape[:2]\n",
    "        ratio = w / float(h)\n",
    "        if math.ceil(imgH * ratio) > imgW:\n",
    "            resized_w = imgW\n",
    "        else:\n",
    "            resized_w = int(math.ceil(imgH * ratio))\n",
    "        resized_image = cv2.resize(img, (resized_w, imgH))\n",
    "        resized_image = resized_image.astype('float32')\n",
    "        resized_image = resized_image.transpose((2, 0, 1)) / 255\n",
    "        resized_image -= 0.5\n",
    "        resized_image /= 0.5\n",
    "        padding_im = np.zeros((imgC, imgH, imgW), dtype=np.float32)\n",
    "        padding_im[:, :, 0:resized_w] = resized_image\n",
    "        return padding_im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d3695c-42c3-43d3-8472-9f16913182bf",
   "metadata": {},
   "source": [
    "### Main processing function for PaddleOCR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ce8c76-3ea5-402c-b820-a403bf12cc05",
   "metadata": {},
   "source": [
    "###Run paddleOCR function in different operations, either a webcam or a video file. See the list of procedures below:\n",
    "\n",
    "1. Create a video player to play with target fps (`utils.VideoPlayer`).\n",
    "2. Prepare a set of frames for text detection and recognition.\n",
    "3. Run AI inference for both text detection and recognition.\n",
    "4. Visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5b68ee-bd25-4dd8-9e87-3fe6971c6e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_paddle_ocr(source=0, flip=False, use_popup=False, skip_first_frames=0):\n",
    "    # create video player to play with target fps\n",
    "    player = None\n",
    "    try:\n",
    "        player = utils.VideoPlayer(source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames)\n",
    "        #Start video capturing\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "        processing_times = collections.deque()\n",
    "        det_request = det_compiled_model.create_infer_request()\n",
    "        while True:\n",
    "            # grab the frame\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            # if frame larger than full HD, reduce size to improve the performance\n",
    "            scale = 1280 / max(frame.shape)\n",
    "            if scale < 1:\n",
    "                frame = cv2.resize(src=frame, dsize=None, fx=scale, fy=scale,\n",
    "                                   interpolation=cv2.INTER_AREA)\n",
    "            # preprocess image for text detection\n",
    "            test_image = image_preprocess(frame,640)\n",
    "                \n",
    "            # measure processing time for text detection\n",
    "            start_time = time.time()\n",
    "            #perform the inference step\n",
    "            det_request.infer(inputs={det_input_layer.any_name: test_image})\n",
    "            det_results = det_request.get_tensor(det_output_layer).data\n",
    "            stop_time = time.time()\n",
    "\n",
    "            # Postprocessing for Paddle Detection\n",
    "            ori_im = frame.copy()\n",
    "            data = {'image': frame}\n",
    "            data_resize = DetResizeForTest(data)\n",
    "            data_norm = NormalizeImage(data_resize)\n",
    "            data_list = []\n",
    "            keep_keys =  ['image', 'shape']\n",
    "            for key in keep_keys:\n",
    "                data_list.append(data[key])\n",
    "            img, shape_list = data_list\n",
    "\n",
    "            shape_list = np.expand_dims(shape_list, axis=0) \n",
    "            pred = det_results[0]    \n",
    "            if isinstance(pred, paddle.Tensor):\n",
    "                pred = pred.numpy()\n",
    "            segmentation = pred > 0.3\n",
    "\n",
    "            boxes_batch = []\n",
    "            for batch_index in range(pred.shape[0]):\n",
    "                src_h, src_w, ratio_h, ratio_w = shape_list[batch_index]\n",
    "                mask = segmentation[batch_index]\n",
    "                boxes, scores = boxes_from_bitmap(pred[batch_index], mask,src_w, src_h)\n",
    "                boxes_batch.append({'points': boxes})\n",
    "            post_result = boxes_batch\n",
    "            dt_boxes = post_result[0]['points']\n",
    "            dt_boxes = filter_tag_det_res(dt_boxes, ori_im.shape)\n",
    "            #Draw boxes on detected text locations\n",
    "            src_im = draw_text_det_res(dt_boxes, frame)\n",
    "\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # use processing times from last 200 frames\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "            processing_time_det = np.mean(processing_times) * 1000\n",
    "\n",
    "            #Preprocess detection results for recognition\n",
    "            dt_boxes = sorted_boxes(dt_boxes)\n",
    "            img_crop_list = []   \n",
    "            if dt_boxes != []:\n",
    "                for bno in range(len(dt_boxes)):\n",
    "                    tmp_box = copy.deepcopy(dt_boxes[bno])\n",
    "                    img_crop = get_rotate_crop_image(ori_im, tmp_box)\n",
    "                    img_crop_list.append(img_crop)\n",
    "\n",
    "                #Recognition starts from here\n",
    "                img_num = len(img_crop_list)\n",
    "                # Calculate the aspect ratio of all text bars\n",
    "                width_list = []\n",
    "                for img in img_crop_list:\n",
    "                    width_list.append(img.shape[1] / float(img.shape[0]))\n",
    "                # Sorting can speed up the recognition process\n",
    "                indices = np.argsort(np.array(width_list))\n",
    "                rec_res = [['', 0.0]] * img_num\n",
    "                batch_num = 6\n",
    "                rec_processing_times = 0\n",
    "\n",
    "                #For each detected text box, run inference for text recognition\n",
    "                for beg_img_no in range(0, img_num, batch_num):\n",
    "                    end_img_no = min(img_num, beg_img_no + batch_num)\n",
    "\n",
    "                    norm_img_batch = []\n",
    "                    max_wh_ratio = 0\n",
    "                    for ino in range(beg_img_no, end_img_no):\n",
    "                        h, w = img_crop_list[indices[ino]].shape[0:2]\n",
    "                        wh_ratio = w * 1.0 / h\n",
    "                        max_wh_ratio = max(max_wh_ratio, wh_ratio)\n",
    "                    for ino in range(beg_img_no, end_img_no):\n",
    "                        norm_img = resize_norm_img(img_crop_list[indices[ino]],max_wh_ratio)\n",
    "                        norm_img = norm_img[np.newaxis, :]\n",
    "                        norm_img_batch.append(norm_img)\n",
    "\n",
    "                    norm_img_batch = np.concatenate(norm_img_batch)\n",
    "                    norm_img_batch = norm_img_batch.copy()\n",
    "\n",
    "                    #Run inference for text recognition \n",
    "                    for index in range(len(norm_img_batch)):\n",
    "                        rec_request = rec_compiled_model.create_infer_request()\n",
    "                        rec_request.infer(inputs={rec_input_layer.any_name: norm_img_batch})\n",
    "                        rec_results = rec_request.get_tensor(rec_output_layer).data\n",
    "                    preds = rec_results\n",
    "\n",
    "                    #Postprocessing recognition results\n",
    "                    postprocess_op = build_post_process(postprocess_params)\n",
    "                    rec_result = postprocess_op(preds)\n",
    "                    for rno in range(len(rec_result)):\n",
    "                        rec_res[indices[beg_img_no + rno]] = rec_result[rno]\n",
    "\n",
    "                #Text recognition results, rec_res, include two parts:\n",
    "                #txts are the recognized text results, scores are the recognition confidence level                   \n",
    "                if rec_res != []:\n",
    "                    image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "                    boxes = dt_boxes\n",
    "                    txts = [rec_res[i][0] for i in range(len(rec_res))] \n",
    "                    scores = [rec_res[i][1] for i in range(len(rec_res))] \n",
    "\n",
    "                    #draw text recognition results beside the image\n",
    "                    draw_img = draw_ocr_box_txt(\n",
    "                                image,\n",
    "                                boxes,\n",
    "                                txts,\n",
    "                                scores,\n",
    "                                drop_score=0.5)\n",
    "\n",
    "                    #Visualize PPOCR results\n",
    "                    _, f_width = draw_img.shape[:2]\n",
    "                    fps = 1000 / processing_time_det\n",
    "                    cv2.putText(img=draw_img, text=f\"OpenVINO Inference time: {processing_time_det:.1f}ms ({fps:.1f} FPS)\", org=(20, 40),\n",
    "                            fontFace=cv2.FONT_HERSHEY_COMPLEX, fontScale=f_width / 1000,\n",
    "                            color=(0, 0, 255), thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "                    # use this workaround if there is flickering\n",
    "                    if use_popup: \n",
    "                        draw_img = cv2.cvtColor(draw_img, cv2.COLOR_RGB2BGR)\n",
    "                        cv2.imshow(winname=title, mat=draw_img)\n",
    "                        key = cv2.waitKey(1)\n",
    "                        # escape = 27\n",
    "                        if key == 27:\n",
    "                            break\n",
    "                    else:\n",
    "                        # encode numpy array to jpg\n",
    "                        draw_img = cv2.cvtColor(draw_img, cv2.COLOR_RGB2BGR)\n",
    "                        _, encoded_img = cv2.imencode(ext=\".jpg\", img=draw_img,\n",
    "                                                            params=[cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "                        # create IPython image\n",
    "                        i = display.Image(data=encoded_img)\n",
    "                        # display the image in this notebook\n",
    "                        display.clear_output(wait=True)\n",
    "                        display.display(i)\n",
    "            \n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # stop capturing\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f8855f-418a-4bda-8799-0953dda895c5",
   "metadata": {},
   "source": [
    "## Run Live PaddleOCR with OpenVINO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642697d-d000-4a10-8e7b-2a519cf9e687",
   "metadata": {},
   "source": [
    "Run using a webcam as the video input. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`. *Note popup mode may not work if you run this notebook on a remote computer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc274952-19aa-480d-ba50-a1146a89771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_paddle_ocr(source=2, flip=False, use_popup=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9c2356-16e2-4ec9-a41e-ae7d03a25b5e",
   "metadata": {},
   "source": [
    "If you don't have a webcam, you can still run this demo with a video file. Any [format supported by OpenCV](https://docs.opencv.org/4.5.1/dd/d43/tutorial_py_video_display.html) will work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c9d077-70df-4a28-a372-7ee5168f6720",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test OCR results on video file\n",
    "\n",
    "video_file = \"./data/test_video4.mp4\"\n",
    "run_paddle_ocr(source= video_file, flip=False, use_popup=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
