{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fbbf681",
   "metadata": {},
   "source": [
    "# Image generation with DeepFloyd IF and OpenVINOâ„¢\n",
    "\n",
    "DeepFloyd IF is an advanced open-source text-to-image model that delivers remarkable photorealism and language comprehension. DeepFloyd IF consists of a frozen text encoder and three cascaded pixel diffusion modules: a base model that creates 64x64 pixel images based on text prompts and two super-resolution models, each designed to generate images with increasing resolution: 256x256 pixel and 1024x1024 pixel. All stages of the model employ a frozen text encoder, built on the T5 transformer, to derive text embeddings, which are then passed to a UNet architecture enhanced with cross-attention and attention pooling.\n",
    "\n",
    "### Text encoder impact\n",
    "\n",
    " - **Profound text prompt comprehension.**\n",
    "The generation pipeline leverages the T5-XXL-1.1 Large Language Model (LLM) as a text encoder. Its intelligence is backed by a substantial number of text-image cross-attention layers, this ensures superior alignment between the prompt and the generated image.\n",
    "\n",
    " - **Realistic text in generated images.**\n",
    "Capitalizing on the capabilities of the T5 model, DeepFloyd IF produces readable text depictions alongside objects with distinct attributes, which have typically been a challenge for most existing text-to-image models.\n",
    "\n",
    "\n",
    "### DeepFloyd IF Distinctive Features\n",
    "\n",
    "First of all, it is **Modular**.\n",
    "DeepFloyd IF pipeline is a consecutive inference of several neural networks.\n",
    "\n",
    "Which makes it **Cascaded**.\n",
    "The base model generates low-resolution samples, then super-resolution models upsample the images to produce high-resolution results.\n",
    "The models were individually trained at different resolutions. \n",
    "\n",
    "DeepFloyd IF employs **Diffusion** models. Diffusion models are machine learning systems that are trained to denoise random Gaussian noise step by step, to get to a sample of interest, such as an image.\n",
    "Diffusion models have been shown to achieve state-of-the-art results for generating image data.\n",
    "\n",
    "And finally, DeepFloyd IF operates in **Pixel** space. Unlike latent diffusion models (Stable Diffusion for instance), the diffusion is implemented on a pixel level.\n",
    "\n",
    "![deepfloyd_if_scheme](https://github.com/deep-floyd/IF/raw/develop/pics/deepfloyd_if_scheme.jpg)\n",
    "\n",
    "The graph above depicts the three-stage generation pipeline:\n",
    "A text prompt is passed through the frozen T5-XXL LLM to convert it into a vector in embedded space. \n",
    "\n",
    "1. Stage 1: The first diffusion model in the cascade transforms the embedding vector into a 64x64 image. The DeepFloyd team has trained **three versions** of the base model, each with different parameters: IF-I 400M, IF-I 900M, and IF-I 4.3B. The smallest one is used by default, but users are free to change the checkpoint name to [\"DeepFloyd/IF-I-L-v1.0\"](https://huggingface.co/DeepFloyd/IF-I-L-v1.0) or [\"DeepFloyd/IF-I-XL-v1.0\"](https://huggingface.co/DeepFloyd/IF-I-XL-v1.0)\n",
    "\n",
    "2. Stage 2: To upscale the image, two text-conditional super-resolution models (Efficient U-Net) are applied to the output of the first diffusion model. The first of these upscales the sample from 64x64 pixel to 256x256 pixel resolution. Again, several versions of this model are available: IF-II 400M (default) and IF-II 1.2B (checkpoint name \"DeepFloyd/IF-II-L-v1.0\").\n",
    "\n",
    "3. Stage 3: Follows the same path as Stage 2 and upscales the image to 1024x1024 pixel resolution. It is not released yet, so we will use a conventional Super Resolution network to get hi-res results.\n",
    "\n",
    "#### Table of content:\n",
    "- [Prerequisites](#Prerequisites-Uparrow)\n",
    "    - [Authentication](#Authentication-Uparrow)\n",
    "- [DeepFloyd IF in Diffusers library](#DeepFloyd-IF-in-Diffusers-library-Uparrow)\n",
    "- [Convert models to OpenVINO Intermediate representation (IR) format](#Convert-models-to-OpenVINO-Intermediate-representation-(IR)-format-Uparrow)\n",
    "- [1. Convert Text Encoder](#1.-Convert-Text-Encoder-Uparrow)\n",
    "- [Convert the first Pixel Diffusion module's UNet](#Convert-the-first-Pixel-Diffusion-module's-UNet-Uparrow)\n",
    "- [Convert the second pixel diffusion module](#Convert-the-second-pixel-diffusion-module-Uparrow)\n",
    "- [Prepare Inference pipeline](#Prepare-Inference-pipeline-Uparrow)\n",
    "- [Run Text-to-Image generation](#Run-Text-to-Image-generation-Uparrow)\n",
    "    - [Text Encoder inference](#Text-Encoder-inference-Uparrow)\n",
    "    - [First Stage diffusion block inference](#First-Stage-diffusion-block-inference-Uparrow)\n",
    "    - [Second Stage diffusion block inference](#Second-Stage-diffusion-block-inference-Uparrow)\n",
    "    - [Third Stage diffusion block](#Third-Stage-diffusion-block-Uparrow)\n",
    "    - [Upscale the generated image using a Super Resolution network](#Upscale-the-generated-image-using-a-Super-Resolution-network-Uparrow)\n",
    "        - [Download the Super Resolution model weights](#Download-the-Super-Resolution-model-weights-Uparrow)\n",
    "        - [Reshape the model's inputs](#Reshape-the-model's-inputs-Uparrow)\n",
    "        - [Prepare the input images and run the model](#Prepare-the-input-images-and-run-the-model-Uparrow)\n",
    "        - [Display the result](#Display-the-result-Uparrow)\n",
    "- [Run Text-to-Image generation](#Run-Text-to-Image-generation-Uparrow)\n",
    "- [Next steps](#Next-steps-Uparrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0c79e7",
   "metadata": {},
   "source": [
    "> **NOTE**:\n",
    "\n",
    "> - _This example requires the download of roughly 27 GB of model checkpoints, which could take some time depending on your internet connection speed. Additionally, the converted models will consume another 27 GB of disk space._\n",
    "> - _Please be aware that a minimum of 32 GB of RAM is necessary to convert and run inference on the models. There may be instances where the notebook appears to freeze or stop responding._\n",
    "> - _To access the model checkpoints, you'll need a Hugging Face account. You'll also be prompted to explicitly accept the [model license](https://huggingface.co/DeepFloyd/IF-I-M-v1.0)._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca7602",
   "metadata": {},
   "source": [
    "## Prerequisites [$\\Uparrow$](#Table-of-content:)\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "699e9c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Set up requirements\n",
    "\n",
    "# %pip install -q --upgrade pip\n",
    "# %pip install -q \"transformers<4.30\" \"diffusers>=0.16.1\" accelerate safetensors sentencepiece huggingface_hub\n",
    "# %pip install -q \"openvino>=2023.1.0\" opencv-python\n",
    "# %pip install -q gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7701f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from pathlib import Path\n",
    "\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers.utils.pil_utils import pt_to_pil\n",
    "import openvino as ov\n",
    "import torch\n",
    "from utils import TextEncoder, UnetFirstStage, UnetSecondStage, convert_result_to_image, download_omz_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e1c7f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_variant = 'fp16'\n",
    "model_dtype = torch.float32\n",
    "ir_input_type = ov.Type.f32\n",
    "compress_to_fp16 = False\n",
    "\n",
    "models_dir = Path('./models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "encoder_ir_path = models_dir / 'encoder_ir.xml'\n",
    "first_stage_unet_ir_path = models_dir / 'unet_ir_I.xml'\n",
    "second_stage_unet_ir_path = models_dir / 'unet_ir_II.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f032a",
   "metadata": {},
   "source": [
    "### Authentication [$\\Uparrow$](#Table-of-content:)\n",
    "In order to access IF checkpoints, users need to provide an authentication token.\n",
    "\n",
    "If you already have a token, you can input it into the provided form in the next cell. If not, please proceed according to the following instructions:\n",
    "\n",
    "1. Make sure to have a [Hugging Face](https://huggingface.co/) account and be logged in\n",
    "2. Accept the license on the model card of [DeepFloyd/IF-I-M-v1.0](https://huggingface.co/DeepFloyd/IF-I-M-v1.0)\n",
    "3. To generate a token, proceed to [this page](https://huggingface.co/settings/tokens)\n",
    "\n",
    "Uncheck the `Add token as git credential?` box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e82d3344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/epavel/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Execute this cell to access the authentication form\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a93f32",
   "metadata": {},
   "source": [
    "## DeepFloyd IF in Diffusers library [$\\Uparrow$](#Table-of-content:)\n",
    "To work with IF by DeepFloyd Lab, we will use [Hugging Face Diffusers package](https://github.com/huggingface/diffusers). Diffusers package exposes the `DiffusionPipeline` class, simplifying experiments with diffusion models. The code below demonstrates how to create a `DiffusionPipeline` using IF configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5001b4dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "safety_checker/model.safetensors not found\n",
      "\n",
      "A mixture of fp16 and non-fp16 filenames will be loaded.\n",
      "Loaded fp16 filenames:\n",
      "[unet/diffusion_pytorch_model.fp16.bin, text_encoder/pytorch_model.fp16-00002-of-00002.bin, text_encoder/pytorch_model.fp16-00001-of-00002.bin]\n",
      "Loaded non-fp16 filenames:\n",
      "[watermarker/diffusion_pytorch_model.bin, safety_checker/pytorch_model.bin\n",
      "If this behavior is not expected, please check your folder structure.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b66505f9f14f89b923fc9d676048c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5412be27ba4c818c497fc2e1e603f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A mixture of fp16 and non-fp16 filenames will be loaded.\n",
      "Loaded fp16 filenames:\n",
      "[unet/diffusion_pytorch_model.fp16.safetensors, safety_checker/model.fp16.safetensors, text_encoder/model.fp16-00001-of-00002.safetensors, text_encoder/model.fp16-00002-of-00002.safetensors]\n",
      "Loaded non-fp16 filenames:\n",
      "[watermarker/diffusion_pytorch_model.safetensors\n",
      "If this behavior is not expected, please check your folder structure.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55881bbebf0c445888dcb2b423cdb656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.31 s, sys: 10.7 s, total: 17 s\n",
      "Wall time: 5.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Downloading the model weights may take some time. The approximate total checkpoints size is 27GB.\n",
    "stage_1 = DiffusionPipeline.from_pretrained(\n",
    "    \"DeepFloyd/IF-I-M-v1.0\",\n",
    "    variant=checkpoint_variant,\n",
    "    torch_dtype=model_dtype\n",
    ")\n",
    "\n",
    "stage_2 = DiffusionPipeline.from_pretrained(\n",
    "    \"DeepFloyd/IF-II-M-v1.0\",\n",
    "    text_encoder=None,\n",
    "    variant=checkpoint_variant,\n",
    "    torch_dtype=model_dtype\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b8cd8f",
   "metadata": {},
   "source": [
    "## Convert models to OpenVINO Intermediate representation (IR) format [$\\Uparrow$](#Table-of-content:)\n",
    "Model conversion API enables direct conversion of PyTorch models. We will utilize the `ov.convert_model` method to acquire OpenVINO IR versions of the models. This requires providing a model object, input data for model tracing, and other relevant parameters.\n",
    "\n",
    "The pipeline consists of three important parts:\n",
    "\n",
    " - A Text Encoder that translates user prompts to vectors in the latent space that the Diffusion model can understand.\n",
    " - A Stage 1 U-Net for step-by-step denoising latent image representation.\n",
    " - A Stage 2 U-Net that takes low resolution output from the previous step and the latent representations to upscale the resulting image.\n",
    " \n",
    "Let us convert each part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d2557",
   "metadata": {},
   "source": [
    "## 1. Convert Text Encoder [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "The text encoder is responsible for converting the input prompt, such as \"ultra close-up color photo portrait of rainbow owl with deer horns in the woods\" into an embedding space that can be fed to the next stage's U-Net. Typically, it is a transformer-based encoder that maps a sequence of input tokens to a sequence of text embeddings.\n",
    "\n",
    "The input for the text encoder consists of a tensor `input_ids`, which contains token indices from the text processed by the tokenizer and padded to the maximum length accepted by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d03da4",
   "metadata": {},
   "source": [
    "_Note_ the `input` argument passed to the `convert_model` method. The `convert_model` can be called with the `input shape` argument and/or the PyTorch-specific `example_input` argument. However, in this case, the tuple was utilized to describe the model input and provide it as the `input` argument. This solution offers a framework-agnostic solution and enables the definition of complex inputs. It allows specifying the input name, shape, type, and value within a single tuple, providing greater flexibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a617d3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n",
      "WARNING:root:Failed to send event with the following error: <urlopen error [SSL: UNEXPECTED_EOF_WHILE_READING] EOF occurred in violation of protocol (_ssl.c:1007)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 26s, sys: 14.1 s, total: 2min 40s\n",
      "Wall time: 33.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not encoder_ir_path.exists():\n",
    "    encoder_ir = ov.convert_model(\n",
    "        stage_1.text_encoder,\n",
    "        example_input=torch.ones(1, 77).long(),\n",
    "        input=((1, 77), ov.Type.i64),\n",
    "    )\n",
    "\n",
    "    # Serialize the IR model to disk, we will load it at inference time\n",
    "    ov.save_model(encoder_ir, encoder_ir_path, compress_to_fp16=compress_to_fp16)\n",
    "    del encoder_ir\n",
    "\n",
    "del stage_1.text_encoder\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5214461",
   "metadata": {},
   "source": [
    "## Convert the first Pixel Diffusion module's UNet [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "U-Net model gradually denoises latent image representation guided by text encoder hidden state.\n",
    "\n",
    "U-Net model has three inputs:\n",
    "\n",
    "`sample` - latent image sample from previous step. Generation process has not been started yet, so you will use random noise.\n",
    "`timestep` - current scheduler step.\n",
    "`encoder_hidden_state` - hidden state of text encoder.\n",
    "Model predicts the sample state for the next step.\n",
    "\n",
    "The first Diffusion module in the cascade generates 64x64 pixel low resolution images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8adf1419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 206 ms, sys: 74.5 ms, total: 280 ms\n",
      "Wall time: 78.5 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not first_stage_unet_ir_path.exists():\n",
    "    unet_1_ir = ov.convert_model(\n",
    "        stage_1.unet,\n",
    "        example_input=(torch.ones(2, 3, 64, 64), torch.tensor(1).int(), torch.ones(2, 77, 4096)),\n",
    "        input=[((2, 3, 64, 64), ir_input_type), ((), ov.Type.i32), ((2, 77, 4096), ir_input_type)],\n",
    "    )\n",
    "\n",
    "    ov.save_model(unet_1_ir, first_stage_unet_ir_path, compress_to_fp16=compress_to_fp16)\n",
    "\n",
    "    del unet_1_ir\n",
    "\n",
    "stage_1_config = stage_1.unet.config\n",
    "del stage_1.unet\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69845c5f",
   "metadata": {},
   "source": [
    "## Convert the second pixel diffusion module [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62def079",
   "metadata": {},
   "source": [
    "The second Diffusion module in the cascade generates 256x256 pixel images.\n",
    "\n",
    "The second stage pipeline will use bilinear interpolation to upscale the 64x64 image that was generated in the previous stage to a higher 256x256 resolution. Then it will denoise the image taking into account the encoded user prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6ec825e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 110 ms, sys: 67.8 ms, total: 178 ms\n",
      "Wall time: 90.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not second_stage_unet_ir_path.exists():\n",
    "    unet_2_ir = ov.convert_model(\n",
    "        stage_2.unet,\n",
    "        example_input=(\n",
    "            torch.ones(2, 6, 256, 256),\n",
    "            torch.tensor(1).int(),\n",
    "            torch.ones(2, 77, 4096),\n",
    "            torch.ones(2).int(),\n",
    "        ),\n",
    "        input=[\n",
    "            ((2, 6, 256, 256), ir_input_type),\n",
    "            ((), ov.Type.i32),\n",
    "            ((2, 77, 4096), ir_input_type),\n",
    "            ((2,), ov.Type.i32),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    ov.save_model(unet_2_ir, second_stage_unet_ir_path, compress_to_fp16=compress_to_fp16)\n",
    "\n",
    "    del unet_2_ir\n",
    "\n",
    "stage_2_config = stage_2.unet.config\n",
    "del stage_2.unet\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e578523",
   "metadata": {},
   "source": [
    "## Prepare Inference pipeline [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "The original pipeline from the source repository will be reused in this example. In order to achieve this, adapter classes were created to enable OpenVINO models to replace Pytorch models and integrate seamlessly into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f86c94e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "core = ov.Core()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f843da5-9210-4f95-8c05-65b67c96b60e",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0630c71b-8dab-42ca-b6d8-dbcd127da805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd546e703c9e4b19a55bffc586b7d87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'GPU', 'AUTO'), value='GPU')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='GPU',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cd5641",
   "metadata": {},
   "source": [
    "## Run Text-to-Image generation [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "Now, we can set a text prompt for image generation and execute the inference pipeline. Optionally, you can also modify the random generator seed for latent state initialization and adjust the number of images to be generated for the given prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8503b91",
   "metadata": {},
   "source": [
    "### Text Encoder inference [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f45f70b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_upcast_utils import is_model_partially_upcasted, partially_upcast_nodes_to_fp32\n",
    "encoder_ov_model = core.read_model(encoder_ir_path)\n",
    "if device.value in ['GPU'] and not is_model_partially_upcasted(encoder_ov_model):\n",
    "    example_input_prompt = 'ultra close color photo portrait of rainbow owl with deer horns in the woods'\n",
    "    text_inputs = stage_1.tokenizer(example_input_prompt, max_length=77, padding=\"max_length\", return_tensors=\"np\")\n",
    "    upcasted_ov_model = partially_upcast_nodes_to_fp32(encoder_ov_model, text_inputs.input_ids)\n",
    "    del encoder_ov_model\n",
    "    gc.collect();\n",
    "    import os\n",
    "    os.remove(encoder_ir_path)\n",
    "    ov.save_model(upcasted_ov_model, encoder_ir_path, compress_to_fp16=compress_to_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4ce5b37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been loaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "prompt = 'ultra close color photo portrait of rainbow owl with deer horns in the woods'\n",
    "negative_prompt = 'blurred unreal uncentered occluded'\n",
    "\n",
    "# Initialize TextEncoder wrapper class\n",
    "stage_1.text_encoder = TextEncoder(str(encoder_ir_path).replace('.xml', '_calibrated.xml'), dtype=model_dtype, device=device.value)\n",
    "# stage_1.text_encoder = TextEncoder(encoder_ir_path, dtype=model_dtype, device=device.value)\n",
    "\n",
    "print('The model has been loaded')\n",
    "\n",
    "# Generate text embeddings\n",
    "prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt, negative_prompt=negative_prompt)\n",
    "\n",
    "# Delete the encoder to free up memory\n",
    "del stage_1.text_encoder.encoder_openvino\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89caf0",
   "metadata": {},
   "source": [
    "### First Stage diffusion block inference [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "910ca973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has been loaded\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d7f6d4000034ff99ac814ecdc0390e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epavel/opt/envs/py310-dev/lib/python3.10/site-packages/diffusers/utils/pil_utils.py:43: RuntimeWarning: invalid value encountered in cast\n",
      "  images = (images * 255).round().astype(\"uint8\")\n"
     ]
    },
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD5/ooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooAKKKKACiiigAooooA//2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAATUlEQVR4Ae3QAQ0AAADCoPdPbQ8HESgMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwIABAwYMGDBgwICB94EBMEAAAVx9hPsAAAAASUVORK5CYII=",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "# Changing the following parameters will affect the model output\n",
    "# Note that increasing the number of diffusion steps will increase the inference time linearly.\n",
    "RANDOM_SEED = 42\n",
    "N_DIFFUSION_STEPS = 50\n",
    "\n",
    "# Initialize the First Stage UNet wrapper class\n",
    "stage_1.unet = UnetFirstStage(\n",
    "    first_stage_unet_ir_path,\n",
    "    stage_1_config,\n",
    "    dtype=model_dtype,\n",
    "    device=device.value\n",
    ")\n",
    "print('The model has been loaded')\n",
    "\n",
    "# Fix PRNG seed\n",
    "generator = torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Inference\n",
    "image = stage_1(prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds,\n",
    "                generator=generator, output_type=\"pt\", num_inference_steps=N_DIFFUSION_STEPS).images\n",
    "\n",
    "# Delete the model to free up memory\n",
    "del stage_1.unet.unet_openvino\n",
    "gc.collect()\n",
    "\n",
    "# Show the image\n",
    "pt_to_pil(image)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23824abc",
   "metadata": {},
   "source": [
    "### Second Stage diffusion block inference [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Initialize the Second Stage UNet wrapper class\n",
    "stage_2.unet = UnetSecondStage(\n",
    "    second_stage_unet_ir_path,\n",
    "    stage_2_config,\n",
    "    dtype=model_dtype,\n",
    "    device=device.value\n",
    ")\n",
    "print('The model has been loaded')\n",
    "\n",
    "image = stage_2(\n",
    "    image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds,\n",
    "    generator=generator, output_type=\"pt\", num_inference_steps=20).images\n",
    "\n",
    "# Delete the model to free up memory\n",
    "del stage_2.unet.unet_openvino\n",
    "gc.collect()\n",
    "\n",
    "# Show the image\n",
    "pil_image = pt_to_pil(image)[0]\n",
    "pil_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7287779",
   "metadata": {},
   "source": [
    "### Third Stage diffusion block [$\\Uparrow$](#Table-of-content:)\n",
    "The final block, which upscales images to a higher resolution (1024x1024 px), has not been released by DeepFloyd yet. Stay tuned!\n",
    "\n",
    "### Upscale the generated image using a Super Resolution network [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "Though the third stage has not been officially released, we'll employ the Super Resolution network from [Example #202](https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/202-vision-superresolution/202-vision-superresolution-image.ipynb) to enhance our low-resolution result!\n",
    "\n",
    "Note, this step will be substituted with the Third IF stage upon its release!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2a2e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary requirement\n",
    "!pip install -q matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a25fb",
   "metadata": {},
   "source": [
    "#### Download the Super Resolution model weights [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265a0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# 1032: 4x superresolution, 1033: 3x superresolution\n",
    "model_name = 'single-image-super-resolution-1032'\n",
    "download_omz_model(model_name, models_dir)\n",
    "\n",
    "sr_model_xml_path = models_dir / f'{model_name}.xml'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c226ee8",
   "metadata": {},
   "source": [
    "#### Reshape the model's inputs [$\\Uparrow$](#Table-of-content:)\n",
    "We need to reshape the inputs for the model. This is necessary because the IR model was converted with a different target input resolution.\n",
    "The Second IF stage returns 256x256 pixel images. Using the 4x Super Resolution model makes our target image size 1024x1024 pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bb4608",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = core.read_model(model=sr_model_xml_path)\n",
    "model.reshape({\n",
    "    0: [1, 3, 256, 256],\n",
    "    1: [1, 3, 1024, 1024]\n",
    "})\n",
    "compiled_sr_model = core.compile_model(model=model, device_name=device.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105ba49",
   "metadata": {},
   "source": [
    "#### Prepare the input images and run the model [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2becf3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = np.array(pil_image)\n",
    "bicubic_image = cv2.resize(\n",
    "    src=original_image, dsize=(1024, 1024), interpolation=cv2.INTER_CUBIC\n",
    ")\n",
    "\n",
    "# Reshape the images from (H,W,C) to (N,C,H,W) as expected by the model.\n",
    "input_image_original = np.expand_dims(original_image.transpose(2, 0, 1), axis=0)\n",
    "input_image_bicubic = np.expand_dims(bicubic_image.transpose(2, 0, 1), axis=0)\n",
    "\n",
    "# Model Inference\n",
    "result = compiled_sr_model(\n",
    "    [input_image_original, input_image_bicubic]\n",
    ")[compiled_sr_model.output(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1850b8",
   "metadata": {},
   "source": [
    "#### Display the result [$\\Uparrow$](#Table-of-content:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = convert_result_to_image(result)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9596a70a-9012-49a8-987e-f039b2fc5193",
   "metadata": {},
   "source": [
    "## Try out the converted pipeline with Gradio [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "The demo app below is created using [Gradio package](https://www.gradio.app/docs/interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb7da90-1737-4c89-9e04-5af73252f2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build up the pipeline\n",
    "stage_1.text_encoder = TextEncoder(encoder_ir_path, dtype=model_dtype, device=device.value)\n",
    "print('The model has been loaded')\n",
    "\n",
    "stage_1.unet = UnetFirstStage(\n",
    "    first_stage_unet_ir_path,\n",
    "    stage_1_config,\n",
    "    dtype=model_dtype,\n",
    "    device=device.value\n",
    ")\n",
    "print('The Stage-1 UNet has been loaded')\n",
    "\n",
    "stage_2.unet = UnetSecondStage(\n",
    "    second_stage_unet_ir_path,\n",
    "    stage_2_config,\n",
    "    dtype=model_dtype,\n",
    "    device=device.value\n",
    ")\n",
    "print('The Stage-2 UNet has been loaded')\n",
    "\n",
    "generator = torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# Combine the models' calls into a single `_generate` function\n",
    "def _generate(prompt, negative_prompt):\n",
    "    # Text encoder inference\n",
    "    prompt_embeds, negative_embeds = stage_1.encode_prompt(prompt, negative_prompt=negative_prompt)\n",
    "    # Stage-1 UNet Inference\n",
    "    image = stage_1(\n",
    "        prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds,\n",
    "        generator=generator, output_type=\"pt\", num_inference_steps=N_DIFFUSION_STEPS\n",
    "    ).images\n",
    "    # Stage-2 UNet Inference\n",
    "    image = stage_2(\n",
    "        image=image, prompt_embeds=prompt_embeds, negative_prompt_embeds=negative_embeds,\n",
    "        generator=generator, output_type=\"pt\", num_inference_steps=20\n",
    "    ).images\n",
    "    # Infer Super Resolution model\n",
    "    original_image = np.array(pt_to_pil(image)[0])\n",
    "    bicubic_image = cv2.resize(\n",
    "        src=original_image, dsize=(1024, 1024), interpolation=cv2.INTER_CUBIC\n",
    "    )\n",
    "    # Reshape the images from (H,W,C) to (N,C,H,W) as expected by the model.\n",
    "    input_image_original = np.expand_dims(original_image.transpose(2, 0, 1), axis=0)\n",
    "    input_image_bicubic = np.expand_dims(bicubic_image.transpose(2, 0, 1), axis=0)\n",
    "    # Model Inference\n",
    "    result = compiled_sr_model(\n",
    "        [input_image_original, input_image_bicubic]\n",
    "    )[compiled_sr_model.output(0)]\n",
    "    return convert_result_to_image(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e83a6-4f89-4bc6-98fc-54cab0d8de7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "demo = gr.Interface(\n",
    "    _generate,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Text Prompt\"),\n",
    "        gr.Textbox(label=\"Negative Text Prompt\"),\n",
    "    ],\n",
    "    outputs=[\n",
    "        \"image\"\n",
    "    ],\n",
    "    examples=[\n",
    "        [\n",
    "            \"ultra close color photo portrait of rainbow owl with deer horns in the woods\",\n",
    "            \"blurred unreal uncentered occluded\"\n",
    "        ],\n",
    "        [\n",
    "            \"A scaly mischievous dragon is driving the car in a street art style\",\n",
    "            \"blurred uncentered occluded\"\n",
    "        ],\n",
    "    ],\n",
    ")\n",
    "try:\n",
    "    demo.queue().launch(debug=True)\n",
    "except Exception:\n",
    "    demo.queue().launch(share=True, debug=True)\n",
    "\n",
    "# If you are launching remotely, specify server_name and server_port\n",
    "# EXAMPLE: `demo.launch(server_name='your server name', server_port='server port in int')`\n",
    "# To learn more please refer to the Gradio docs: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fb018c",
   "metadata": {},
   "source": [
    "## Next steps [$\\Uparrow$](#Table-of-content:)\n",
    "\n",
    "Open the [238-deep-floyd-if-optimize](238-deep-floyd-if-optimize.ipynb) notebook to quantize stage 1 and stage 2 U-Net models with the Post-training Quantization API of NNCF and compress weights of the text encoder. Then compare the converted and optimized OpenVINO models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
