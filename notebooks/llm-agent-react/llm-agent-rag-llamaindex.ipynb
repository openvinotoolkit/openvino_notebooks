{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c006eb1c-3cbd-42ff-9198-537898f496a5",
   "metadata": {},
   "source": [
    "# Create an Agentic RAG using OpenVINO and LlamaIndex\n",
    "\n",
    "An **agent** is an automated reasoning and decision engine. It takes in a user input/query and can make internal decisions for executing that query in order to return the correct result. The key agent components can include, but are not limited to:\n",
    "\n",
    "- Breaking down a complex question into smaller ones\n",
    "- Choosing an external Tool to use + coming up with parameters for calling the Tool\n",
    "- Planning out a set of tasks\n",
    "- Storing previously completed tasks in a memory module\n",
    "\n",
    "[LlamaIndex](https://docs.llamaindex.ai/en/stable/) is a framework for building context-augmented generative AI applications with LLMs.LlamaIndex imposes no restriction on how you use LLMs. You can use LLMs as auto-complete, chatbots, semi-autonomous agents, and more. It just makes using them easier. You can build agents on top of your existing LlamaIndex RAG pipeline to empower it with automated decision capabilities. A lot of modules (routing, query transformations, and more) are already agentic in nature in that they use LLMs for decision making.\n",
    "\n",
    "**Agentic RAG = Agent-based RAG implementation**\n",
    "\n",
    "While standard RAG excels at simple queries across a few documents, agentic RAG takes it a step further and emerges as a potent solution for question answering. It introduces a layer of intelligence by employing AI agents. These agents act as autonomous decision-makers, analyzing initial findings and strategically selecting the most effective tools for further data retrieval. This multi-step reasoning capability empowers agentic RAG to tackle intricate research tasks, like summarizing, comparing information across multiple documents and even formulating follow-up questions -all in an orchestrated and efficient manner.\n",
    "\n",
    "![agentic-rag](https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/871cb90d-27fd-4a87-aa3c-f4cdb199a148)\n",
    "\n",
    "This example will demonstrate using RAG engines as a tool in an agent with OpenVINO and LlamaIndex.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Download models](#Download-models)\n",
    "  - [Download LLM](#Download-LLM)\n",
    "  - [Download Embedding model](#Download-Embedding-model)\n",
    "- [Create models](#Create-models)\n",
    "  - [Create OpenVINO LLM](#Create-OpenVINO-LLM)\n",
    "  - [Create OpenVINO Embedding](#Create-OpenVINO-Embedding)\n",
    "- [Create tools](#Create-tools)\n",
    "- [Run Agentic RAG](#Run-Agentic-RAG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377b6144-99c7-4af8-938b-b6fdbca652d7",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa478c3-8110-4801-b249-fd3b651dc60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GIT_CLONE_PROTECTION_ACTIVE\"] = \"false\"\n",
    "\n",
    "%pip install -Uq pip\n",
    "%pip uninstall -q -y optimum optimum-intel\n",
    "%pip install -q --extra-index-url https://download.pytorch.org/whl/cpu\\\n",
    "\"llama-index\" \"pymupdf\" \"llama-index-readers-file\" \"llama-index-llms-openvino\" \"llama-index-embeddings-openvino\" \"transformers>=4.40\" \\\n",
    "\"git+https://github.com/huggingface/optimum-intel.git\"\\\n",
    "\"git+https://github.com/openvinotoolkit/nncf.git\"\\\n",
    "\"datasets\"\\\n",
    "\"accelerate\"\n",
    "%pip install --pre -Uq openvino openvino-tokenizers[transformers] --extra-index-url https://storage.openvinotoolkit.org/simple/wheels/nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b19fb6e-e3f4-43e6-b7f1-5bbf2719fa85",
   "metadata": {
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import io\n",
    "\n",
    "text_example_en_path = Path(\"text_example_en.pdf\")\n",
    "text_example_en = \"https://github.com/user-attachments/files/16171326/xeon6-e-cores-network-and-edge-brief.pdf\"\n",
    "\n",
    "if not text_example_en_path.exists():\n",
    "    r = requests.get(url=text_example_en)\n",
    "    content = io.BytesIO(r.content)\n",
    "    with open(\"text_example_en.pdf\", \"wb\") as f:\n",
    "        f.write(content.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd371ab3",
   "metadata": {},
   "source": [
    "## Download models\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "### Download LLM\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "To run LLM locally, we have to download the model in the first step. It is possible to [export your model](https://github.com/huggingface/optimum-intel?tab=readme-ov-file#export) to the OpenVINO IR format with the CLI, and load the model from local folder.\n",
    "\n",
    "Large Language Models (LLMs) are a core component of agent. LlamaIndex does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. In this example, we select `Meta-Llama-3-8B-Instruct` as LLM in agent pipeline.\n",
    "\n",
    "* **llama-3-8b-instruct** - Llama 3 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align with human preferences for helpfulness and safety. The Llama 3 instruction tuned models are optimized for dialogue use cases and outperform many of the available open source chat models on common industry benchmarks. More details about model can be found in [Meta blog post](https://ai.meta.com/blog/meta-llama-3/), [model website](https://llama.meta.com/llama3) and [model card](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n",
    ">**Note**: run model with demo, you will need to accept license agreement. \n",
    ">You must be a registered user in ü§ó Hugging Face Hub. Please visit [HuggingFace model card](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct), carefully read terms of usage and click accept button.  You will need to use an access token for the code below to run. For more information on access tokens, refer to [this section of the documentation](https://huggingface.co/docs/hub/security-tokens).\n",
    ">You can login on Hugging Face Hub in notebook environment, using following code:\n",
    " \n",
    "```python\n",
    "    ## login to huggingfacehub to get access to pretrained model \n",
    "\n",
    "    from huggingface_hub import notebook_login, whoami\n",
    "\n",
    "    try:\n",
    "        whoami()\n",
    "        print('Authorization token already provided')\n",
    "    except OSError:\n",
    "        notebook_login()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fdc4ba-74c4-4869-898e-131f47827e8f",
   "metadata": {
"test_replace": {
     "meta-llama/Meta-Llama-3-8B-Instruct": "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "llm_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm_model_path = \"Meta-Llama-3-8B-Instruct-ov\"\n",
    "\n",
    "if not Path(llm_model_path).exists():\n",
    "    !optimum-cli export openvino --model {llm_model_id} --task text-generation-with-past --trust-remote-code --weight-format int4 {llm_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7b7a18",
   "metadata": {},
   "source": [
    "### Download Embedding model\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Embedding model is another key component in RAG pipeline. It takes text as input, and return a long list of numbers used to capture the semantics of the text. An OpenVINO embedding model and tokenizer can be exported by `feature-extraction` task with `optimum-cli`. In this tutorial, we use [bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5) as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5ba0440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library name is not specified. There are multiple possible variants: `sentence_transformers`, `transformers`.`transformers` will be selected. If you want to load your model with the `sentence-transformers` library instead, please set --library sentence_transformers\n",
      "Framework not specified. Using pt to export the model.\n",
      "Using framework PyTorch: 2.3.1+cpu\n",
      "Overriding 1 configuration item(s)\n",
      "\t- use_cache -> False\n",
      "['input_ids', 'attention_mask', 'token_type_ids']\n",
      "Detokenizer is not supported, convert tokenizer only.\n"
     ]
    }
   ],
   "source": [
    "embedding_model_id = \"BAAI/bge-small-en-v1.5\"\n",
    "embedding_model_path = \"bge-small-en-v1.5-ov\"\n",
    "\n",
    "if not Path(embedding_model_path).exists():\n",
    "    !optimum-cli export openvino --model {embedding_model_id} --task feature-extraction {embedding_model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157eca4c",
   "metadata": {},
   "source": [
    "## Create models\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "### Create OpenVINO LLM\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Select device for LLM model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bca3764d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87d85d17bd4f49228648d945ac9db175",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import openvino as ov\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "support_devices = core.available_devices\n",
    "\n",
    "llm_device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "llm_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3f28b",
   "metadata": {},
   "source": [
    "OpenVINO models can be run locally through the `OpenVINOLLM` class in [LlamaIndex](https://docs.llamaindex.ai/en/stable/examples/llm/openvino/). If you have an Intel GPU, you can specify `device_map=\"gpu\"` to run inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c259c61-5eef-41a8-a9f7-462f27d0c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.openvino import OpenVINOLLM\n",
    "\n",
    "ov_config = {\"PERFORMANCE_HINT\": \"LATENCY\", \"NUM_STREAMS\": \"1\", \"CACHE_DIR\": \"\"}\n",
    "\n",
    "\n",
    "def completion_to_prompt(completion):\n",
    "    return f\"<|begin_of_text|><|start_header_id|>system<|end_header_id|><|eot_id|><|start_header_id|>user<|end_header_id|>{completion}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "\n",
    "\n",
    "llm = OpenVINOLLM(\n",
    "    model_name=str(llm_model_path),\n",
    "    tokenizer_name=str(llm_model_path),\n",
    "    context_window=3900,\n",
    "    max_new_tokens=1000,\n",
    "    model_kwargs={\"ov_config\": ov_config},\n",
    "    device_map=llm_device.value,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41820eb6",
   "metadata": {},
   "source": [
    "### Create OpenVINO Embedding\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "Select device for embedding model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e41705e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d3127cedece41f79a2fcdcdae21eb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', options=('CPU', 'AUTO'), value='CPU')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "support_devices = core.available_devices\n",
    "\n",
    "embedding_device = widgets.Dropdown(\n",
    "    options=support_devices + [\"AUTO\"],\n",
    "    value=\"CPU\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "embedding_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd61822",
   "metadata": {},
   "source": [
    "A Hugging Face embedding model can be supported by OpenVINO through [`OpenVINOEmbeddings`](https://docs.llamaindex.ai/en/stable/examples/embeddings/openvino/) class of LlamaIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3448c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling the model to CPU ...\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface_openvino import OpenVINOEmbedding\n",
    "\n",
    "embedding = OpenVINOEmbedding(folder_name=embedding_model_path, device=embedding_device.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7d4971",
   "metadata": {},
   "source": [
    "## Create tools\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "In this examples, we will create 2 customized tools for `multiply` and `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f594cf18-8100-4207-9ec0-7ded996e85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers and returns the product\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
    "\n",
    "\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers and returns the sum\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "add_tool = FunctionTool.from_defaults(fn=add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f05a4ab",
   "metadata": {},
   "source": [
    "To demonstrate using RAG engines as a tool in an agent, we're going to create a very simple RAG query engine as one of the tools. \n",
    "\n",
    ">**Note**: For a full RAG pipeline with OpenVINO, you can check the [RAG notebooks](../llm-rag-llamaindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eea245b9-73c5-431e-af47-3e676888bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.readers.file import PyMuPDFReader\n",
    "from llama_index.core import VectorStoreIndex, Settings\n",
    "\n",
    "Settings.embed_model = embedding\n",
    "Settings.llm = llm\n",
    "loader = PyMuPDFReader()\n",
    "documents = loader.load(file_path=text_example_en_path)\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d807600",
   "metadata": {},
   "source": [
    "Now we turn our query engine into a tool by supplying the appropriate metadata (for the python functions, this was being automatically extracted so we didn't need to add it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b8cd9c9-a595-4baf-9adc-77f740f19f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "budget_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine,\n",
    "    name=\"Xeon6\",\n",
    "    description=\"A RAG engine with some basic facts about Intel Xeon 6 processors with E-cores\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cdfcbc",
   "metadata": {},
   "source": [
    "## Run Agentic RAG\n",
    "\n",
    "[back to top ‚¨ÜÔ∏è](#Table-of-contents:)\n",
    "\n",
    "We modify our agent by adding this engine to our array of tools (we also remove the llm parameter, since it's now provided by settings):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8aefd1d-be3c-46f9-bd67-5c8557f9b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools([multiply_tool, add_tool, budget_tool], llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0033ed",
   "metadata": {},
   "source": [
    "Ask a question using multiple tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbf386c9-f74e-4948-9ea0-94b69b7b2e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: Xeon6\n",
      "Action Input: {'input': 'maximum cores in a single socket'}\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;34mObservation: \n",
      "\n",
      "According to the provided context information, the maximum cores in a single socket is 144.\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;3;38;5;200mThought: The current language of the user is English. I need to use a tool to help me answer the question.\n",
      "Action: multiply\n",
      "Action Input: {'a': 144, 'b': 4}\n",
      "\u001b[0m\u001b[1;3;34mObservation: 576\n",
      "\u001b[0m\u001b[1;3;38;5;200mThought: The current language of the user is English. I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The maximum number of cores in an Intel Xeon 6 processor server with 4 sockets is 576.\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"What's the maximum number of cores in an Intel Xeon 6 processor server with 4 sockets ? Go step by step, using a tool to do any math.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/91237924/871cb90d-27fd-4a87-aa3c-f4cdb199a148",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [
     "LLM"
    ],
    "tasks": [
     "Text Generation"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
