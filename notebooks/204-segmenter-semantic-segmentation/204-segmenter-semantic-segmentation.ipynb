{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmentation with OpenVINO™ using Segmenter\n",
    "\n",
    "Semantic segmentation is a difficult computer vision problem with many applications such as autonomous driving, robotics, augmented reality, and many others.\n",
    "Its goal is to assign labels to each pixel according to the object it belongs to, creating so-called segmentation masks.\n",
    "To properly assign this label, the model needs to consider the local as well as global context of the image.\n",
    "This is where transformers offer their advantage as they work well in capturing global context.\n",
    "\n",
    "Segmenter is based on Vision Transformer working as an encoder, and Mask Transformer working as a decoder.\n",
    "With this configuration, it achieves good results on different datasets such as ADE20K, Pascal Context, and Cityscapes.\n",
    "It works as shown in the diagram below, by taking the image, splitting it into patches, and then encoding these patches.\n",
    "Mask transformer combines encoded patches with class masks and decodes them into a segmentation map as the output, where each pixel has a label assigned to it.\n",
    "\n",
    "![Segmenteer diagram](https://user-images.githubusercontent.com/24582831/148507554-87eb80bd-02c7-4c31-b102-c6141e231ec8.png)\n",
    "> Credits for this image go to [original authors of Segmenter](https://github.com/rstrudel/segmenter).\n",
    "\n",
    "More about the model and its details can be found in the following paper:\n",
    "[Segmenter: Transformer for Semantic Segmentation](https://arxiv.org/abs/2105.05633) or in the [repository](https://github.com/rstrudel/segmenter).\n",
    "#### Table of contents:\n",
    "- [Get and prepare PyTorch model](#Get-and-prepare-PyTorch-model)\n",
    "    - [Prerequisites](#Prerequisites)\n",
    "    - [Loading PyTorch model](#Loading-PyTorch-model)\n",
    "- [Preparing preprocessing and visualization functions](#Preparing-preprocessing-and-visualization-functions)\n",
    "    - [Preprocessing](#Preprocessing)\n",
    "    - [Visualization](#Visualization)\n",
    "- [Validation of inference of original model](#Validation-of-inference-of-original-model)\n",
    "- [Convert PyTorch model to OpenVINO Intermediate Representation (IR)](#Convert-PyTorch-model-to-OpenVINO-Intermediate-Representation-(IR))\n",
    "- [Verify converted model inference](#Verify-converted-model-inference)\n",
    "    - [Select inference device](#Select-inference-device)\n",
    "- [Benchmarking performance of converted model](#Benchmarking-performance-of-converted-model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate how to convert and use Segmenter in OpenVINO, this notebook consists of the following steps:\n",
    "\n",
    "* Preparing PyTorch Segmenter model\n",
    "* Preparing preprocessing and visualization functions\n",
    "* Validating inference of original model\n",
    "* Converting PyTorch model to OpenVINO IR\n",
    "* Validating inference of the converted model\n",
    "* Benchmark performance of the converted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get and prepare PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The first thing we'll need to do is clone [repository](https://github.com/rstrudel/segmenter) containing model and helper functions. We will use Tiny model with mask transformer, that is `Seg-T-Mask/16`. There are also better, but much larger models available in the linked repo. This model is pre-trained on [ADE20K](https://groups.csail.mit.edu/vision/datasets/ADE20K/) dataset used for segmentation.\n",
    "\n",
    "The code from the repository already contains functions that create model and load weights, but we will need to download config and trained weights (checkpoint) file and add some additional helper functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# clone Segmenter repo\n",
    "if not Path(\"segmenter\").exists():\n",
    "    !git clone https://github.com/rstrudel/segmenter\n",
    "else:\n",
    "    print(\"Segmenter repo already cloned\")\n",
    "\n",
    "# include path to Segmenter repo to use its functions\n",
    "sys.path.append(\"./segmenter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing requirements\n",
    "%pip install -q \"openvino>=2023.1.0\"\n",
    "%pip install -r segmenter/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "# Fetch the notebook utils script from the openvino_notebooks repo\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(\n",
    "    url='https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/main/notebooks/utils/notebook_utils.py',\n",
    "    filename='notebook_utils.py'\n",
    ")\n",
    "from notebook_utils import download_file, load_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need `timm`, `mmsegmentation`, `einops` and `mmcv`, to use functions from segmenter repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will clone the Segmenter repo and then download weights and config for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download config and pretrained model weights\n",
    "# here we use tiny model, there are also better but larger models available in repository\n",
    "WEIGHTS_LINK = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/segmenter/checkpoints/ade20k/seg_tiny_mask/checkpoint.pth\"\n",
    "CONFIG_LINK = \"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/models/segmenter/checkpoints/ade20k/seg_tiny_mask/variant.yml\"\n",
    "\n",
    "MODEL_DIR = Path(\"model/\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(WEIGHTS_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "download_file(CONFIG_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "\n",
    "WEIGHT_PATH = MODEL_DIR / \"checkpoint.pth\"\n",
    "CONFIG_PATH = MODEL_DIR / \"variant.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading PyTorch model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "PyTorch models are usually an instance of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary containing model weights.\n",
    "Typical steps to get the model are therefore:\n",
    "\n",
    "1. Create an instance of the model class\n",
    "2. Load checkpoint state dict, which contains pre-trained model weights\n",
    "3. Turn the model to evaluation mode, to switch some operations to inference mode\n",
    "\n",
    "We will now use already provided helper functions from repository to initialize the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from segmenter.segm.model.factory import load_model\n",
    "\n",
    "pytorch_model, config = load_model(WEIGHT_PATH)\n",
    "# put model into eval mode, to set it for inference\n",
    "pytorch_model.eval()\n",
    "print(\"PyTorch model loaded and ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load normalization settings from config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from segmenter.segm.data.utils import STATS\n",
    "# load normalization name, in our case \"vit\" since we are using transformer\n",
    "normalization_name = config[\"dataset_kwargs\"][\"normalization\"]\n",
    "# load normalization params, mean and std from STATS\n",
    "normalization = STATS[normalization_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing preprocessing and visualization functions\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now we will define utility functions for preprocessing and visualizing the results.\n",
    "\n",
    "### Preprocessing\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Inference input is tensor with shape `[1, 3, H, W]` in `B, C, H, W` format, where:\n",
    "\n",
    "* `B` - batch size (in our case 1, as we are just adding 1 with unsqueeze)\n",
    "* `C` - image channels (in our case RGB - 3)\n",
    "* `H` - image height\n",
    "* `W` - image width\n",
    "\n",
    "Resizing to the correct scale and splitting to batches is done inside inference, so we don't need to resize or split the image in preprocessing.\n",
    "\n",
    "Model expects images in RGB channels format, scaled to [0, 1] range and normalized with given mean and standard deviation provided in `config.yml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "\n",
    "def preprocess(im: Image, normalization: dict) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Preprocess image: scale, normalize and unsqueeze\n",
    "\n",
    "    :param im: input image\n",
    "    :param normalization: dictionary containing normalization data from config file\n",
    "    :return:\n",
    "            im: processed (scaled and normalized) image\n",
    "    \"\"\"\n",
    "    # change PIL image to tensor and scale to [0, 1]\n",
    "    im = F.pil_to_tensor(im).float() / 255\n",
    "    # normalize by given mean and standard deviation\n",
    "    im = F.normalize(im, normalization[\"mean\"], normalization[\"std\"])\n",
    "    # change dim from [C, H, W] to [1, C, H, W]\n",
    "    im = im.unsqueeze(0)\n",
    "\n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Inference output contains labels assigned to each pixel, so the output in our case is `[150, H, W]` in `CL, H, W` format where:\n",
    "\n",
    "* `CL` - number of classes for labels (in our case 150)\n",
    "* `H` - image height\n",
    "* `W` - image width\n",
    "\n",
    "Since we want to visualize this output, we reduce dimensions to `[1, H, W]` where we keep only class with the highest value as that is the predicted label.\n",
    "We then combine original image with colors corresponding to the inferred labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from segmenter.segm.data.utils import dataset_cat_description, seg_to_rgb\n",
    "from segmenter.segm.data.ade20k import ADE20K_CATS_PATH\n",
    "\n",
    "\n",
    "def apply_segmentation_mask(pil_im: Image, results: torch.Tensor) -> Image:\n",
    "    \"\"\"\n",
    "    Combine segmentation masks with the image\n",
    "\n",
    "    :param pil_im: original input image\n",
    "    :param results: tensor containing segmentation masks for each pixel\n",
    "    :return:\n",
    "            pil_blend: image with colored segmentation masks overlay\n",
    "    \"\"\"\n",
    "    cat_names, cat_colors = dataset_cat_description(ADE20K_CATS_PATH)\n",
    "\n",
    "    # 3D array, where each pixel has values for all classes, take index of max as label\n",
    "    seg_map = results.argmax(0, keepdim=True)\n",
    "    # transform label id to colors\n",
    "    seg_rgb = seg_to_rgb(seg_map, cat_colors)\n",
    "    seg_rgb = (255 * seg_rgb.cpu().numpy()).astype(np.uint8)\n",
    "    pil_seg = Image.fromarray(seg_rgb[0])\n",
    "\n",
    "    # overlay segmentation mask over original image\n",
    "    pil_blend = Image.blend(pil_im, pil_seg, 0.5).convert(\"RGB\")\n",
    "\n",
    "    return pil_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation of inference of original model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now that we have everything ready, we can perform segmentation on example image `coco_hollywood.jpg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from segmenter.segm.model.utils import inference\n",
    "\n",
    "# load image with PIL\n",
    "image = load_image(\"https://storage.openvinotoolkit.org/repositories/openvino_notebooks/data/data/image/coco_hollywood.jpg\")\n",
    "# load_image reads the image in BGR format, [:,:,::-1] reshape transfroms it to RGB\n",
    "pil_image = Image.fromarray(image[:,:,::-1])\n",
    "\n",
    "# preprocess image with normalization params loaded in previous steps\n",
    "image = preprocess(pil_image, normalization)\n",
    "\n",
    "# inference function needs some meta parameters, where we specify that we don't flip images in inference mode\n",
    "im_meta = dict(flip=False)\n",
    "# perform inference with function from repository\n",
    "original_results = inference(model=pytorch_model,\n",
    "                             ims=[image],\n",
    "                             ims_metas=[im_meta],\n",
    "                             ori_shape=image.shape[2:4],\n",
    "                             window_size=config[\"inference_kwargs\"][\"window_size\"],\n",
    "                             window_stride=config[\"inference_kwargs\"][\"window_stride\"],\n",
    "                             batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After inference is complete, we need to transform output to segmentation mask where each class has specified color, using helper functions from previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine segmentation mask with image\n",
    "blended_image = apply_segmentation_mask(pil_image, original_results)\n",
    "\n",
    "# show image with segmentation mask overlay\n",
    "blended_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that model segments the image into meaningful parts.\n",
    "Since we are using tiny variant of model, the result is not as good as it is with larger models, but it already shows nice segmentation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PyTorch model to OpenVINO Intermediate Representation (IR)\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Now that we've verified that the inference of PyTorch model works, we will convert it to OpenVINO IR format.\n",
    "\n",
    "To do this, we first get input dimensions from the model configuration file and create torch dummy input.\n",
    "Input dimensions are in our case `[2, 3, 512, 512]` in `B, C, H, W]` format, where:\n",
    "\n",
    "* `B` - batch size\n",
    "* `C` - image channels (in our case RGB - 3)\n",
    "* `H` - model input image height\n",
    "* `W` - model input image width\n",
    "\n",
    "> Note that H and W are here fixed to 512, as this is required by the model. Resizing is done inside the inference function from the original repository.\n",
    "\n",
    "After that, we use `ov.convert_model` function from PyTorch to convert the model to OpenVINO model,  which is ready to use in Python interface but can also be serialized to OpenVINO IR format for future execution using `ov.save_model`.\n",
    "The process can generate some warnings, but they are not a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "\n",
    "# get input sizes from config file\n",
    "batch_size = 2\n",
    "channels = 3\n",
    "image_size = config[\"dataset_kwargs\"][\"image_size\"]\n",
    "\n",
    "# make dummy input with correct shapes obtained from config file\n",
    "dummy_input = torch.randn(batch_size, channels, image_size, image_size)\n",
    "\n",
    "model = ov.convert_model(pytorch_model, example_input=dummy_input, input=([batch_size, channels, image_size, image_size], ))\n",
    "# serialize model for saving IR\n",
    "ov.save_model(model, MODEL_DIR / \"segmenter.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify converted model inference\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "To test that model was successfully converted, we can use same inference function from original repository, but we need to make custom class.\n",
    "\n",
    "`SegmenterOV` class contains OpenVINO model, with all attributes and methods required by inference function.\n",
    "This way we don't need to write any additional custom code required to process input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class SegmenterOV:\n",
    "    \"\"\"\n",
    "    Class containing OpenVINO model with all attributes required to work with inference function.\n",
    "\n",
    "    :param model: compiled OpenVINO model\n",
    "    :type model: CompiledModel\n",
    "    :param output_blob: output blob used in inference\n",
    "    :type output_blob: ConstOutput\n",
    "    :param config: config file containing data about model and its requirements\n",
    "    :type config: dict\n",
    "    :param n_cls: number of classes to be predicted\n",
    "    :type n_cls: int\n",
    "    :param normalization:\n",
    "    :type normalization: dict\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: Path, device:str = \"CPU\"):\n",
    "        \"\"\"\n",
    "        Constructor method.\n",
    "        Initializes OpenVINO model and sets all required attributes\n",
    "\n",
    "        :param model_path: path to model's .xml file, also containing variant.yml\n",
    "        :param device: device string for selecting inference device\n",
    "        \"\"\"\n",
    "        # init OpenVino core\n",
    "        core = ov.Core()\n",
    "        # read model\n",
    "        model_xml = core.read_model(model_path)\n",
    "        self.model = core.compile_model(model_xml, device)\n",
    "        self.output_blob = self.model.output(0)\n",
    "\n",
    "        # load model configs\n",
    "        variant_path = Path(model_path).parent / \"variant.yml\"\n",
    "        with open(variant_path, \"r\") as f:\n",
    "            self.config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        # load normalization specs from config\n",
    "        normalization_name = self.config[\"dataset_kwargs\"][\"normalization\"]\n",
    "        self.normalization = STATS[normalization_name]\n",
    "\n",
    "        # load number of classes from config\n",
    "        self.n_cls = self.config[\"net_kwargs\"][\"n_cls\"]\n",
    "\n",
    "    def forward(self, data: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform inference on data and return the result in Tensor format\n",
    "\n",
    "        :param data: input data to model\n",
    "        :return: data inferred by model\n",
    "        \"\"\"\n",
    "        return torch.from_numpy(self.model(data)[self.output_blob])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created `SegmenterOV` helper class, we can use it in inference function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select inference device\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "select device from dropdown list for running inference using OpenVINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value='AUTO',\n",
    "    description='Device:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# load model into SegmenterOV class\n",
    "model = SegmenterOV(MODEL_DIR / \"segmenter.xml\", device.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# perform inference with same function as in case of PyTorch model from repository\n",
    "results = inference(model=model,\n",
    "                    ims=[image],\n",
    "                    ims_metas=[im_meta],\n",
    "                    ori_shape=image.shape[2:4],\n",
    "                    window_size=model.config[\"inference_kwargs\"][\"window_size\"],\n",
    "                    window_stride=model.config[\"inference_kwargs\"][\"window_stride\"],\n",
    "                    batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# combine segmentation mask with image\n",
    "converted_blend = apply_segmentation_mask(pil_image, results)\n",
    "\n",
    "# show image with segmentation mask overlay\n",
    "converted_blend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we get the same results as with original model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking performance of converted model\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Finally, use the OpenVINO [Benchmark Tool](https://docs.openvino.ai/2023.0/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance of the model.\n",
    "\n",
    "> NOTE: For more accurate performance, it is recommended to run `benchmark_app` in a terminal/command prompt after closing other applications. Run `benchmark_app -m model.xml -d CPU` to benchmark async inference on CPU for one minute. Change `CPU` to `GPU` to benchmark on GPU. Run `benchmark_app --help` to see an overview of all command-line options.\n",
    "\n",
    "> Keep in mind that the authors of original paper used V100 GPU, which is significantly more powerful than the CPU used to obtain the following throughput. Therefore, FPS can't be compared directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inference FP32 model (OpenVINO IR)\n",
    "!benchmark_app -m ./model/segmenter.xml -d $device.value -api async"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
