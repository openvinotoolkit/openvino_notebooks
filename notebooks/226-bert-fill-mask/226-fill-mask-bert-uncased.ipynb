{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1007aeb7",
   "metadata": {},
   "source": [
    "### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5168e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import tokenizer as tk\n",
    "import openvino.runtime as ov\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import time\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e2746c",
   "metadata": {},
   "source": [
    "### Defining checkpoint "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84839d2d",
   "metadata": {},
   "source": [
    "We're defining the predefined huggingface model which we'll be using for the fill mask task. I'm using the bert-large-uncased-whole-word-masking model for this notebook. You can alternatively make a selection from any other model from the [list](https://huggingface.co/models?pipeline_tag=fill-mask). \n",
    "\n",
    "*P.S. There might be slight changes in the preprocessing and postprocessing steps of a few models.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cba2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"bert-large-uncased-whole-word-masking\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913c8096",
   "metadata": {},
   "source": [
    "### Serialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0e383",
   "metadata": {},
   "source": [
    "Transformers provides a **transformers.onnx package** that enables you to convert model checkpoints to an ONNX graph by leveraging configuration objects. These configuration objects come ready made for a number of model architectures, and are designed to be easily extendable to other architectures.\n",
    "More details about serialization and supported models can be found [here](https://huggingface.co/docs/transformers/serialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268bc058",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m transformers.onnx -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eda682",
   "metadata": {},
   "outputs": [],
   "source": [
    "serialize_command = f\"python -m transformers.onnx \\\n",
    "    -m {checkpoint} \\\n",
    "    --feature masked-lm model/\"\n",
    "! $serialize_command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81275bfd",
   "metadata": {},
   "source": [
    "### Model Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc99df",
   "metadata": {},
   "source": [
    "Model Optimizer is a cross-platform command-line tool that facilitates the transition between training and deployment environments, performs static model analysis, and adjusts deep learning models for optimal execution on end-point target devices. [Click here](https://docs.openvino.ai/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) to find details and features of model optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def1ed1",
   "metadata": {},
   "source": [
    "For my model, I'm using the onnx model and truncating the input size to 128, and using the input features input_ids, attention_mask, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce93341",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_model_path = \"model.onnx\"\n",
    "MODEL_DIR = \"model/\"\n",
    "MODEL_DIR = f\"{MODEL_DIR}\"\n",
    "onnx_model_path = Path(MODEL_DIR) / onnx_model_path\n",
    "\n",
    "optimizer_command = f\"mo \\\n",
    "    --input_model {onnx_model_path} \\\n",
    "    --output_dir {MODEL_DIR} \\\n",
    "    --model_name {checkpoint} \\\n",
    "    --input input_ids,attention_mask,token_type_ids \\\n",
    "    --input_shape [1,128],[1,128],[1,128]\"\n",
    "! $optimizer_command"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e40bd07",
   "metadata": {},
   "source": [
    "### Inference Request\n",
    "\n",
    "I'm setting up the [inference request](https://docs.openvino.ai/latest/openvino_docs_OV_UG_Infer_request.html) for my model using the model graph file (.xml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16782026",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "core = ov.Core()\n",
    "ir_model_xml = str((Path(MODEL_DIR) / checkpoint).with_suffix(\".xml\"))\n",
    "compiled_model = core.compile_model(ir_model_xml)\n",
    "infer_request = compiled_model.create_infer_request()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179c126",
   "metadata": {},
   "source": [
    "### Softmax Function\n",
    "\n",
    "Creating a softmax function to postprocess the outputs we get from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399a845e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aecef48",
   "metadata": {},
   "source": [
    "### Postprocessing\n",
    "Here we create the output function, i.e. \n",
    "1. Preprocess the inputs passed to the model through the custom tokenizer built on the bert-base-uncased vocab file.\n",
    "2. Check for any irregularities:\n",
    "        i. If there is more than one [MASK] token, the sentence is not accepted because Fill Mask task supports exactly one                [MASK] token per sentence.\n",
    "        ii. If there is no [MASK] token in the sentence, the [MASK] token is appended at the end of the sentence.\n",
    "3. After we've got the outputs (in the form of logits) from the model, the softmax function is performed to predict the masked token.\n",
    "4. We're displaying the top 10 results and rewriting the input sentence with the masked token replaced by the prediction\n",
    "\n",
    "To check out the individual functions of the tokenizer, check the [tokenizer.py file](../utils/tokenizer.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b87916",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(text):\n",
    "    output = []\n",
    "    special_tokens = tk.special_tokens_list()\n",
    "    multiple_mask = tk.check_mask_token(text)\n",
    "    err_text = f\"ERROR:\\\n",
    "        Too many\\\n",
    "        {special_tokens['mask_token']}\\\n",
    "        tokens in sentence.\\\n",
    "        Sentence should\\\n",
    "        have exactly one\\\n",
    "        {special_tokens['mask_token']}\\\n",
    "        token for\\\n",
    "        Fill Mask task\"\n",
    "    err_text = err_text.strip()\n",
    "    if multiple_mask > 1:\n",
    "        output = err_text\n",
    "    else:\n",
    "        inputs = tk.preprocess_text(text, 128)\n",
    "        if inputs == -2:\n",
    "            output = err_text\n",
    "        else:\n",
    "            result = infer_request.infer(inputs)\n",
    "            input_ids = inputs[\"input_ids\"][0]\n",
    "            mask_token_index = tk.word_to_token(special_tokens[\"mask_token\"])\n",
    "            masked_index = [\n",
    "                i for i in range(0, len(input_ids))\n",
    "                if input_ids[i] == mask_token_index\n",
    "            ][0]\n",
    "            for i in result.values():\n",
    "                outputs = i\n",
    "            logits = outputs[0, masked_index, :]\n",
    "            prob = softmax(logits)\n",
    "            prob = torch.from_numpy(prob)\n",
    "            value, prediction = prob.topk(10)\n",
    "            isMaskExists = False\n",
    "            text = text.strip()\n",
    "            if special_tokens[\"mask_token\"] in text:\n",
    "                part1 = text.split(\"[MASK]\")[0]\n",
    "                part2 = text.split(\"[MASK]\")[1]\n",
    "                isMaskExists = True\n",
    "\n",
    "            for v, p in zip(value.tolist(), prediction.tolist()):\n",
    "                word = tk.tokens_to_ids(p)\n",
    "                if isMaskExists:\n",
    "                    output.append(\n",
    "                        {\n",
    "                            \"Sequence\": part1 + word + part2,\n",
    "                            \"token\": word,\n",
    "                            \"score\": \"%.5f\" % v,\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    output.append(\n",
    "                        {\"Sequence\": text + \" \" + word,\n",
    "                         \"token\": word,\n",
    "                         \"score\": \"%.5f\" % v}\n",
    "                    )\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e89d9e",
   "metadata": {},
   "source": [
    "Calling the postprocess function and recording the total time for the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6f02ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getResults(text):\n",
    "    print(\"Original Text: \", text)\n",
    "    start_time = time.perf_counter()\n",
    "    result = postprocess(text)\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    total_time = end_time - start_time\n",
    "    if type(result) != str:\n",
    "        result.append({\"total_time\": str(\"%.2f\" % total_time) + \" seconds\"})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7aff0a",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "Time for results now! Let's go. Replace the sentence with a sentence of your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf5ec5e",
   "metadata": {},
   "source": [
    "#### 1. Exactly one [MASK] token inside the sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11960662",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How are you? I haven't [MASK] you in a while.\"\n",
    "result = getResults(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3734f40b",
   "metadata": {},
   "source": [
    "#### 2. No [MASK] token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5414ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Have you seen my            \"\n",
    "result = getResults(text)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b86d5ab",
   "metadata": {},
   "source": [
    "#### 3. Multiple [MASK] tokens in a single sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c257b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"How are [MASK] ? I haven't [MASK] you in a [MASK] .\"\n",
    "result = getResults(text)\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
