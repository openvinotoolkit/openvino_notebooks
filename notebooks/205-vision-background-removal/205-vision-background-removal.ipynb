{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93aa099e",
   "metadata": {},
   "source": [
    "# Image Background Removal with U^2-Net and OpenVINO\n",
    "\n",
    "This notebook demostrates background removal in images using U$^2$-Net and OpenVINO.\n",
    "\n",
    "For more information about U$^2$-Net, including source code and test data, see their Github page at https://github.com/xuebinqin/U-2-Net and their research paper: [U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection](https://arxiv.org/pdf/2005.09007.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5492ca1",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "The PyTorch U$^2$-Net model is converted to ONNX and loaded with OpenVINO. The model source is https://github.com/xuebinqin/U-2-Net. For a more detailed overview of loading PyTorch models in OpenVINO, including how to load an ONNX model in OpenVINO directly, without converting to IR format, check out the [PyTorch/ONNX](../102-pytorch-onnx-to-openvino/102-pytorch-onnx-to-openvino.ipynb) notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57b48f0",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286df544",
   "metadata": {
    "id": "QB4Yo-rGGLmV",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Import the PyTorch Library and U$^2$-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124fd11",
   "metadata": {
    "id": "2ynWRum4iiTz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from collections import namedtuple\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import HTML, FileLink, display\n",
    "from model.u2net import U2NET, U2NETP\n",
    "from openvino.inference_engine import IECore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bae3e45",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Settings\n",
    "\n",
    "This tutorial supports using the original U$^2$-Net salient object detection model, as well as the smaller U2NETP version. Two sets of weights are supported for the original model: salient object detection and human segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c87b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIR = \"data\"\n",
    "model_config = namedtuple(\"ModelConfig\", [\"name\", \"url\", \"model\", \"model_args\"])\n",
    "\n",
    "u2net_lite = model_config(\n",
    "    name=\"u2net_lite\",\n",
    "    url=\"https://drive.google.com/uc?id=1rbSTGKAE-MTxBYHd-51l2hMOQPT_7EPy\",\n",
    "    model=U2NETP,\n",
    "    model_args=(),\n",
    ")\n",
    "u2net = model_config(\n",
    "    name=\"u2net\",\n",
    "    url=\"https://drive.google.com/uc?id=1ao1ovG1Qtx4b7EoskHXmi2E9rp5CHLcZ\",\n",
    "    model=U2NET,\n",
    "    model_args=(3, 1),\n",
    ")\n",
    "u2net_human_seg = model_config(\n",
    "    name=\"u2net_human_seg\",\n",
    "    url=\"https://drive.google.com/uc?id=1-Yg0cxgrNhHP-016FPdp902BR-kSsA4P\",\n",
    "    model=U2NET,\n",
    "    model_args=(3, 1),\n",
    ")\n",
    "\n",
    "# Set u2net_model to one of the three configurations listed above\n",
    "u2net_model = u2net_lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799f33c",
   "metadata": {
    "tags": [
     "hide_output",
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# The filenames of the downloaded and converted models\n",
    "MODEL_DIR = \"model\"\n",
    "model_path = Path(MODEL_DIR) / u2net_model.name / Path(u2net_model.name).with_suffix(\".pth\")\n",
    "onnx_path = model_path.with_suffix(\".onnx\")\n",
    "ir_path = model_path.with_suffix(\".xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8522caea",
   "metadata": {
    "id": "u5xKw0hR0jq6",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Load the U$^2$-Net Model\n",
    "\n",
    "The U$^2$-Net human segmentation model weights are stored on Google Drive. They will be downloaded if they have not been downloaded yet. The next cell loads the model and the pretrained weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eae9467",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "if not model_path.exists():\n",
    "    import gdown\n",
    "\n",
    "    os.makedirs(name=model_path.parent, exist_ok=True)\n",
    "    print(\"Start downloading model weights file... \")\n",
    "    with open(model_path, \"wb\") as model_file:\n",
    "        gdown.download(url=u2net_model.url, output=model_file)\n",
    "        print(f\"Model weights have been downloaded to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fb802",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the model\n",
    "net = u2net_model.model(*u2net_model.model_args)\n",
    "net.eval()\n",
    "\n",
    "# Load the weights\n",
    "print(f\"Loading model weights from: '{model_path}'\")\n",
    "net.load_state_dict(state_dict=torch.load(model_path, map_location=\"cpu\"))\n",
    "\n",
    "# Save the model if it doesn't exist yet\n",
    "if not model_path.exists():\n",
    "    print(\"\\nSaving the model\")\n",
    "    torch.save(obj=net.state_dict(), f=str(model_path))\n",
    "    print(f\"Model saved at {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d55f18",
   "metadata": {
    "id": "Rhc_7EObUypw",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Convert PyTorch U$^2$-Net model to ONNX and IR\n",
    "\n",
    "### Convert PyTorch model to ONNX\n",
    "\n",
    "The output for this cell will show some warnings. These are most likely harmless. Conversion succeeded if the last line of the output says `ONNX model exported to [filename].onnx.` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef50a4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ipQWpbgQUxoo",
    "outputId": "bbc1734a-c2a2-4261-ed45-264b9e3edd00",
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "if not onnx_path.exists():\n",
    "    dummy_input = torch.randn(1, 3, 512, 512)\n",
    "    torch.onnx.export(model=net, args=dummy_input, f=onnx_path, opset_version=11)\n",
    "    print(f\"ONNX model exported to {onnx_path}.\")\n",
    "else:\n",
    "    print(f\"ONNX model {onnx_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960b0c2f",
   "metadata": {
    "id": "6JSoEIk60uxV",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Convert ONNX model to OpenVINO IR Format\n",
    "\n",
    "Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR format, with FP16 precision. The models are saved to the current directory. We add the mean values to the model and scale the output with the standard deviation with `--scale_values`. With these options, it is not necessary to normalize input data before propagating it through the network. The mean and standard deviation values can be found in the [dataloader](https://github.com/xuebinqin/U-2-Net/blob/master/data_loader.py) file in the [U^2-Net repository](https://github.com/xuebinqin/U-2-Net/) and multiplied by 255 to support images with pixel values from 0-255.\n",
    "\n",
    "See the [Model Optimizer Developer Guide](https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) for more information about Model Optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae354c",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "Call the OpenVINO Model Optimizer tool to convert the ONNX model to OpenVINO IR, with FP16 precision. Executing this command may take a while. There may be some errors or warnings in the output. Model Optimization was successful if the last lines of the output include `[ SUCCESS ] Generated IR version 10 model.\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340fb5b5",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Construct the command for Model Optimizer\n",
    "# Set log_level to CRITICAL to suppress warnings that can be ignored for this demo\n",
    "mo_command = f\"\"\"mo\n",
    "                 --input_model \"{onnx_path}\"\n",
    "                 --input_shape \"[1,3, 512, 512]\"\n",
    "                 --mean_values=\"[123.675, 116.28 , 103.53]\"\n",
    "                 --scale_values=\"[58.395, 57.12 , 57.375]\"\n",
    "                 --data_type FP16\n",
    "                 --output_dir \"{model_path.parent}\"\n",
    "                 --log_level \"CRITICAL\"\n",
    "                 \"\"\"\n",
    "mo_command = \" \".join(mo_command.split())\n",
    "print(\"Model Optimizer command to convert the ONNX model to OpenVINO:\")\n",
    "print(mo_command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b28b55",
   "metadata": {
    "id": "6YUwrq7QWSzw",
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "if not ir_path.exists():\n",
    "    print(\"Exporting ONNX model to IR... This may take a few minutes.\")\n",
    "    mo_result = %sx $mo_command\n",
    "    print(\"\\n\".join(mo_result))\n",
    "else:\n",
    "    print(f\"IR model {ir_path} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7603",
   "metadata": {
    "id": "JyD5EKka34Wd",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Load and Pre-Process Input Image\n",
    "\n",
    "The IR model expects images in RGB format. OpenCV reads images in BGR. We convert the images to RGB, resize them to `512 x 512` and transpose the dimensions to the format that is expected by the IR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a86e7fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DGFW5VXL3x9G",
    "outputId": "300eacff-c6de-4eb5-e99a-8def5260da1a",
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "IMAGE_PATH = Path(IMAGE_DIR) / \"coco_hollywood.jpg\"\n",
    "image = cv2.cvtColor(\n",
    "    src=cv2.imread(filename=str(IMAGE_PATH)),\n",
    "    code=cv2.COLOR_BGR2RGB,\n",
    ")\n",
    "\n",
    "resized_image = cv2.resize(src=image, dsize=(512, 512))\n",
    "# Convert the image shape to shape and data type expected by network\n",
    "# for OpenVINO IR model: (1, 3, 512, 512)\n",
    "input_image = np.expand_dims(np.transpose(resized_image, (2, 0, 1)), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10f73a",
   "metadata": {
    "id": "FnEiEbNq4Csh",
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Do Inference on IR Model\n",
    "\n",
    "Load the IR model to Inference Engine and do inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6cff378",
   "metadata": {
    "id": "otfT6EDk03KV",
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Load network to Inference Engine\n",
    "ie = IECore()\n",
    "net_ir = ie.read_network(model=ir_path)\n",
    "exec_net_ir = ie.load_network(network=net_ir, device_name=\"CPU\")\n",
    "# Get names of input and output layers\n",
    "input_layer_ir = next(iter(exec_net_ir.input_info))\n",
    "output_layer_ir = next(iter(exec_net_ir.outputs))\n",
    "\n",
    "# Run the Inference on the Input image...\n",
    "start_time = time.perf_counter()\n",
    "res_ir = exec_net_ir.infer(inputs={input_layer_ir: input_image})\n",
    "res_ir = res_ir[output_layer_ir]\n",
    "end_time = time.perf_counter()\n",
    "print(\n",
    "    f\"Inference finished. Inference time: {end_time-start_time:.3f} seconds, \"\n",
    "    f\"FPS: {1/(end_time-start_time):.2f}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b7f850",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## Visualize Results\n",
    "\n",
    "Show the original image, the segmentation result, and the original image with the background removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f22737",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "# Resize the network result to the image shape and round the values\n",
    "# to 0 (background) and 1 (foreground)\n",
    "# Network result has shape (1,1,512,512), np.squeeze converts this to (512, 512)\n",
    "resized_result = np.rint(\n",
    "    cv2.resize(src=np.squeeze(res_ir), dsize=(image.shape[1], image.shape[0]))\n",
    ").astype(np.uint8)\n",
    "\n",
    "# Create a copy of the image and set all background values to 255 (white)\n",
    "bg_removed_result = image.copy()\n",
    "bg_removed_result[resized_result == 0] = 255\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 7))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(resized_result, cmap=\"gray\")\n",
    "ax[2].imshow(bg_removed_result)\n",
    "for a in ax:\n",
    "    a.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29ee84",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "### Add a Background Image\n",
    "\n",
    "In the segmentation result, all foreground pixels have a value of 1, all background pixels a value of 0. Replace the background image as follows:\n",
    "\n",
    "- Load a new image `background_image`\n",
    "- Resize this image to the same size as the original image\n",
    "- In the `background_image` set all the pixels where the resized segmentation result has a value of 1 - the foreground pixels in the original image - to 0.\n",
    "- Add the `bg_removed_result` from the previous step - the part of the original image that only contains foreground pixels - to the `background_image`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5628a0c",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "BACKGROUND_FILE = \"data/wall.jpg\"\n",
    "OUTPUT_DIR = \"output\"\n",
    "\n",
    "os.makedirs(name=OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "background_image = cv2.cvtColor(src=cv2.imread(filename=BACKGROUND_FILE), code=cv2.COLOR_BGR2RGB)\n",
    "background_image = cv2.resize(src=background_image, dsize=(image.shape[1], image.shape[0]))\n",
    "\n",
    "# Set all the foreground pixels from the result to 0\n",
    "# in the background image and add the background-removed image\n",
    "background_image[resized_result == 1] = 0\n",
    "new_image = background_image + bg_removed_result\n",
    "\n",
    "# Save the generated image\n",
    "new_image_path = Path(f\"{OUTPUT_DIR}/{IMAGE_PATH.stem}-{Path(BACKGROUND_FILE).stem}.jpg\")\n",
    "cv2.imwrite(filename=str(new_image_path), img=cv2.cvtColor(new_image, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "# Display the original image and the image with the new background side by side\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 7))\n",
    "ax[0].imshow(image)\n",
    "ax[1].imshow(new_image)\n",
    "for a in ax:\n",
    "    a.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Create a link to download the image\n",
    "image_link = FileLink(new_image_path)\n",
    "image_link.html_link_str = \"<a href='%s' download>%s</a>\"\n",
    "display(\n",
    "    HTML(\n",
    "        f\"The generated image <code>{new_image_path.name}</code> is saved in \"\n",
    "        f\"the directory <code>{new_image_path.parent}</code>. You can also \"\n",
    "        \"download the image by clicking on this link: \"\n",
    "        f\"{image_link._repr_html_()}\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11acf85",
   "metadata": {
    "tags": [
     "hide"
    ]
   },
   "source": [
    "## References\n",
    "\n",
    "* PIP install openvino-dev: https://github.com/openvinotoolkit/openvino/blob/releases/2021/3/docs/install_guides/pypi-openvino-dev.md\n",
    "* OpenVINO ONNX support: https://docs.openvinotoolkit.org/latest/openvino_docs_IE_DG_ONNX_Support.html\n",
    "* Model Optimizer Documentation: https://docs.openvinotoolkit.org/latest/openvino_docs_MO_DG_prepare_model_convert_model_Converting_Model_General.html\n",
    "* U^2-Net: https://github.com/xuebinqin/U-2-Net \n",
    "* U^2-Net research paper: [U^2-Net: Going Deeper with Nested U-Structure for Salient Object Detection](https://arxiv.org/pdf/2005.09007.pdf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "openvino_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
