{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a465fbf",
   "metadata": {},
   "source": [
    "# Latent consistency model using Optimum-Intel OpenVINO on AI PC\n",
    "This notebook provides instructions how to run Latent consistency model (LCM) on AI PC. It allows to setup standard Hugging Face diffuser pipeline and Optimum Intel pipeline optimized for Intel hardware including CPU and integrated GPU (iGPU). Running inference on CPU and iGPU it is easy to compare performance and time required to generate an image for provided prompt.\n",
    "\n",
    "![](https://github.com/openvinotoolkit/openvino_notebooks/assets/10940214/1858dae4-72fd-401e-b055-66d503d82446)\n",
    "\n",
    "Optimum Intel is a interface from Hugging Face between both diffusers and transformers libraries and various tools provided by Intel to accelerate pipelines on Intel hardware. It allows  to perform quantization of the models hosted on Hugging Face.\n",
    "In this notebook OpenVINO and NNCF are used for AI-inference acceleration and quantization tools as a backend for Optimum Intel! \n",
    "\n",
    "For more details, please refer to Optimum Intel repository on github\n",
    "https://github.com/huggingface/optimum-intel\n",
    "\n",
    "<img src=\"lcm.png\"/>\n",
    "\n",
    "LCMs are the next generation of generative models after Latent Diffusion Models (LDMs). They are proposed to overcome the slow iterative sampling process of Latent Diffusion Models (LDMs), enabling fast inference with minimal steps (from 2 to 4) on any pre-trained LDMs (e.g. Stable Diffusion). To read more about LCM please refer to https://latent-consistency-models.github.io/\n",
    "\n",
    "#### Table of contents:\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Full precision model on the CPU](#Full-precision-model-on-the-CPU)\n",
    "- [Full precision model on the CPU with OpenVINO acceleration](#Full-precision-model-on-the-CPU-OV)\n",
    "- [Runnning AI-inference on the GPU with OpenVINO acceleration](#Full-precision-model-on-the-GPU-OV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523a3f91",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2a1a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"optimum-intel[openvino,diffusers]@git+https://github.com/huggingface/optimum-intel.git\" \"ipywidgets\" \"transformers>=4.33.0\" --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6960adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e8c87b",
   "metadata": {},
   "source": [
    "### Showing Info Available Devices\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The `available_devices` property shows the available devices in your system. The \"FULL_DEVICE_NAME\" option to `ie.get_property()` shows the name of the device. Check what is the ID name for the discrete GPU, if you have integrated GPU (iGPU) and discrete GPU (dGPU), it will show `device_name=\"GPU.0\"` for iGPU and `device_name=\"GPU.1\"` for dGPU. If you just have either an iGPU or dGPU that will be assigned to `\"GPU\"`\n",
    "\n",
    "Note: For more details about GPU with OpenVINO visit this [link](https://docs.openvino.ai/nightly/openvino_docs_install_guides_configurations_for_intel_gpu.html). If you have been facing any issue in Ubuntu 20.04 or Windows 11 read this [blog](https://blog.openvino.ai/blog-posts/install-gpu-drivers-windows-ubuntu)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71b081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "ie = Core()\n",
    "devices = ie.available_devices\n",
    "\n",
    "for device in devices:\n",
    "    device_name = ie.get_property(device, \"FULL_DEVICE_NAME\")\n",
    "    print(f\"{device}: {device_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857b2f8",
   "metadata": {},
   "source": [
    "### Using full precision model in CPU with `LatentConsistencyModelPipeline`\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c65ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import LatentConsistencyModelPipeline\n",
    "import gc\n",
    "\n",
    "pipeline = LatentConsistencyModelPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n",
    "pipeline.save_pretrained(\"./lcm_cpu\")\n",
    "\n",
    "prompt = \"green wood dragon in the sky 8k\"\n",
    "\n",
    "image = pipeline(\n",
    "    prompt=prompt, num_inference_steps=4, guidance_scale=8.0\n",
    ").images[0]\n",
    "image.save(\"image_cpu.png\")\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fcee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipeline\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7860bb7f",
   "metadata": {},
   "source": [
    "### Using full precision model in CPU with `OVLatentConsistencyModelPipeline`\n",
    "[back to top ⬆️](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8578ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.intel import OVLatentConsistencyModelPipeline\n",
    "\n",
    "ov_pipeline = OVLatentConsistencyModelPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\", export=True, compile=False)\n",
    "ov_pipeline.reshape(batch_size=1, height=512, width=512, num_images_per_prompt=1)\n",
    "ov_pipeline.save_pretrained(\"./openvino_ir\")\n",
    "ov_pipeline.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225db137",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"green wood dragon in the sky 8k\"\n",
    "image_ov_cpu = ov_pipeline(prompt=prompt, num_inference_steps=4, guidance_scale=8.0).images[0]\n",
    "image_ov_cpu.save(\"image_opt_cpu.png\")\n",
    "image_ov_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4444a06",
   "metadata": {},
   "source": [
    "### Running inference on iGPU with `OVLatentConsistencyModelPipeline`\n",
    "[back to top ⬆️](#Table-of-contents:)\n",
    "\n",
    "The model in this notebook is FP32 precision, but accelerated AI-inference using XMX is supported for FP16 data type and FP32 precision for GPU may produce high memory footprint and latency. Therefore, default precision for GPU in OpenVINO is FP16. OpenVINO GPU Plug-In converts FP32 to FP16 on the fly and there is no need to do it manually  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a93b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_pipeline.to(\"GPU\")\n",
    "ov_pipeline.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d175c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_ov_gpu = ov_pipeline(prompt=prompt, num_inference_steps=4, guidance_scale=8.0).images[0]\n",
    "image_ov_gpu.save(\"image_opt_cpu.png\")\n",
    "image_ov_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7601e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "del ov_pipeline\n",
    "gc.collect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/blob/main/notebooks/236-stable-diffusion-v2/236-stable-diffusion-v2-optimum-demo.png?raw=true",
   "tags": {
    "categories": [
     "Model Demos",
     "AI Trends"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Text-to-Image"
    ]
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
