{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert and Optimize RVM(Robust Video Matting) with OpenVINOâ„¢\n",
    "\n",
    "The RVM algorithm is specifically designed for robust human video matting. Unlike existing neural models that process frames as independent images, RVM uses a recurrent neural network to process videos with temporal memory. RVM can perform matting in real-time on any videos without additional inputs.  \n",
    "More details about its realization can be found in original model [paper](https://arxiv.org/abs/2108.11515) and [repository](https://github.com/PeterL1n/RobustVideoMatting).\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize PyTorch/* RVM with OpenVINO. The tutorial consists of the following steps:\n",
    "- Prepare PyTorch model and videos\n",
    "- Validate original model\n",
    "- Convert PyTorch model to ONNX\n",
    "- Convert ONNX model to OpenVINO IR\n",
    "- Validate converted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Pytorch model\n",
    "\n",
    "Generally, PyTorch models represent an instance of the [torch.nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) class, initialized by a state dictionary with model weights.\n",
    "\n",
    "We will use the RVM MobileNetv3 model, which available in this [repo](https://github.com/PeterL1n/RobustVideoMatting).\n",
    "\n",
    "In this case, the model creators provide a tool that enables converting the RVM model to ONNX, so we don't need to do these steps manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../utils\")\n",
    "from notebook_utils import download_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone RVM repo\n",
    "if not Path('RobustVideoMatting').exists():\n",
    "    !git clone https://github.com/PeterL1n/RobustVideoMatting.git\n",
    "%cd RobustVideoMatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained model weights\n",
    "MODEL_LINK = \"https://github.com/PeterL1n/RobustVideoMatting/releases/download/v1.0.0/rvm_mobilenetv3.pth\"\n",
    "VIDEO_LINK = \"https://drive.google.com/uc?id=1I0v72-hNlK1hm9q1OwyaATUYApXpotS6\"\n",
    "MODEL_DIR = Path(\"../model/\")\n",
    "VIDEO_DIR = Path(\"../video/\")\n",
    "MODEL_DIR.mkdir(exist_ok=True)\n",
    "VIDEO_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "download_file(MODEL_LINK, directory=MODEL_DIR, show_progress=True)\n",
    "download_file(VIDEO_LINK, directory=VIDEO_DIR, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this file, we need to install the relevant dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install av==8.0.3 pims==0.5 torchvision==0.10.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check model inference\n",
    "\n",
    "`inference.py` script run pytorch model inference and save video as result. This will takes a few time which depends on your device performance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the original video\n",
    "\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "import io\n",
    "video = io.open('../video/asianboss2.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important parameters:\n",
    "* `--variant` - types of backbone networks\n",
    "* `--checkpoint` - path to model weigths checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform inference\n",
    "!python inference.py --variant mobilenetv3 --checkpoint ../model/rvm_mobilenetv3.pth --device cpu --input-source \"../video/asianboss2.mp4\" --output-type video --output-composition \"../video/rvm_pth.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize inference result\n",
    "\n",
    "video = io.open('../video/rvm_pth.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "To export and ONNX format of the model, we will use the [onnx branch](https://github.com/PeterL1n/RobustVideoMatting/tree/onnx) of RVM repo script. Let's clone it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not Path('onnx').exists():\n",
    "    !git clone https://github.com/PeterL1n/RobustVideoMatting.git  -b onnx onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can execute `onnx/epxort_onnx.py` to convert the pytorch file into an onnx file. After testing, the current file only supports float32 precision, which you can only set `--precision` to `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./onnx/export_onnx.py --model-variant mobilenetv3 --checkpoint ../model/rvm_mobilenetv3.pth --precision float32 --opset 12 --device cpu --output ../model/rvm_mobilenetv3.onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert ONNX Model to OpenVINO Intermediate Representation (IR)\n",
    "While ONNX models are directly supported by OpenVINO runtime, it can be useful to convert them to IR format to take advantage of OpenVINO optimization tools and features.\n",
    "\n",
    "`mo.convert_model` python function can be used for converting model using OpenVINO Model Optimizer.  \n",
    "The ONNX model can be exported to OpenVINO IR with `serialize()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.tools import mo\n",
    "from openvino.runtime import serialize\n",
    "\n",
    "model = mo.convert_model('../model/rvm_mobilenetv3.onnx')\n",
    "# serialize model for saving IR\n",
    "serialize(model, '../model/rvm_mobilenetv3.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify model inference\n",
    "\n",
    "To test model work, we create inference pipeline similar to `inference.py`. As the expected type of the original pytorch model is torch.tensor, which can't be used as the input of the openvino model. On the other hand, for the ov model, the input dimension requires explicit initialization.\n",
    "\n",
    "Our pipeline consists from preprocessing step, inference of OpenVINO model and results post-processing to get matting video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "# read converted model\n",
    "model = core.read_model('../model/rvm_mobilenetv3.xml')\n",
    "# load model on CPU device\n",
    "compiled_model = core.compile_model(model, 'CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import av\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from typing import Optional, Tuple\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from inference_utils import VideoReader, VideoWriter\n",
    "from openvino.runtime import Core\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def write_numpy(writer, frames):\n",
    "    writer.stream.width = frames.shape[3]\n",
    "    writer.stream.height = frames.shape[2]\n",
    "    frames *= 255\n",
    "    frames = frames.transpose(0, 2, 3, 1).astype(np.uint8)\n",
    "    for t in range(frames.shape[0]):\n",
    "        frame = frames[t]\n",
    "        frame = av.VideoFrame.from_ndarray(frame, format='rgb24')\n",
    "        writer.container.mux(writer.stream.encode(frame))\n",
    "\n",
    "\n",
    "def convert_video(model,\n",
    "                  input_source: str,\n",
    "                  input_resize: Optional[Tuple[int, int]] = None,\n",
    "                  downsample_ratio: Optional[float] = None,\n",
    "                  output_type: str = 'video',\n",
    "                  output_composition: Optional[str] = None,\n",
    "                  output_alpha: Optional[str] = None,\n",
    "                  output_foreground: Optional[str] = None,\n",
    "                  output_video_mbps: Optional[float] = None,\n",
    "                  seq_chunk: int = 1,\n",
    "                  num_workers: int = 0,\n",
    "                  progress: bool = True,\n",
    "                  device: Optional[str] = None,\n",
    "                  dtype: Optional[torch.dtype] = None):\n",
    "\n",
    "    # Initialize transform\n",
    "    if input_resize is not None:\n",
    "        transform = transforms.Compose(\n",
    "            [transforms.Resize(input_resize[::-1]),\n",
    "             transforms.ToTensor()])\n",
    "    else:\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "    # Initialize reader\n",
    "    if os.path.isfile(input_source):\n",
    "        source = VideoReader(input_source, transform)\n",
    "    reader = DataLoader(source,\n",
    "                        batch_size=seq_chunk,\n",
    "                        pin_memory=True,\n",
    "                        num_workers=num_workers)\n",
    "\n",
    "    # Initialize writers\n",
    "    if output_type == 'video':\n",
    "        frame_rate = source.frame_rate if isinstance(source,\n",
    "                                                     VideoReader) else 30\n",
    "        output_video_mbps = 1 if output_video_mbps is None else output_video_mbps\n",
    "        if output_composition is not None:\n",
    "            writer_com = VideoWriter(path=output_composition,\n",
    "                                     frame_rate=frame_rate,\n",
    "                                     bit_rate=int(output_video_mbps * 1000000))\n",
    "\n",
    "    # Inference\n",
    "    # refer here /https://github.com/PeterL1n/RobustVideoMatting/blob/master/documentation/inference.md\n",
    "    if (output_composition is not None) and (output_type == 'video'):\n",
    "        bgr = np.reshape(\n",
    "            np.array([120, 255, 155], dtype=np.float32) / 255, [1, 3, 1, 1])\n",
    "    try:\n",
    "        bar = tqdm(total=len(source), disable=not progress, dynamic_ncols=True)\n",
    "        rec = [np.zeros([1, 1, 1, 1], dtype=np.float16)] * 4\n",
    "        for src in reader:\n",
    "\n",
    "            if downsample_ratio is None:\n",
    "                downsample_ratio = np.asarray(\n",
    "                    [min(512 / max(*src.shape[2:]), 1)], dtype=np.float32)\n",
    "            src = np.array(src, dtype=np.float32)\n",
    "\n",
    "            inputs = {\n",
    "                \"src\": src,\n",
    "                \"downsample_ratio\": downsample_ratio,\n",
    "                \"r1i\": rec[0],\n",
    "                \"r2i\": rec[1],\n",
    "                \"r3i\": rec[2],\n",
    "                \"r4i\": rec[3]\n",
    "            }\n",
    "\n",
    "            request = model.create_infer_request()\n",
    "            request.infer(inputs=inputs)\n",
    "            fgr = request.get_output_tensor(0).data  # 1,3,1080,1920\n",
    "            pha = request.get_output_tensor(1).data  # 1,1,1080,1920\n",
    "            rec[0] = request.get_output_tensor(2).data  #\n",
    "            rec[1] = request.get_output_tensor(3).data\n",
    "            rec[2] = request.get_output_tensor(4).data\n",
    "            rec[3] = request.get_output_tensor(5).data\n",
    "            if output_composition is not None:\n",
    "                if output_type == 'video':\n",
    "                    com = fgr * pha + bgr * (1 - pha)\n",
    "                write_numpy(writer_com, com)\n",
    "            bar.update(1)\n",
    "\n",
    "    finally:\n",
    "        # Clean up\n",
    "        if output_composition is not None:\n",
    "            writer_com.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_video(compiled_model,\n",
    "              input_source='../video/asianboss2.mp4',\n",
    "              output_type='video',\n",
    "              output_composition='../video/res_ov.mp4',\n",
    "              device='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's look at the inference results of the openvino models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize results\n",
    "ov_res_video = io.open('../video/res_ov.mp4', 'r+b').read()\n",
    "encoded = base64.b64encode(video)\n",
    "HTML(data='''<video alt=\"test\" controls>\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "             </video>'''.format(encoded.decode('ascii')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "8099d661a5c21dff0f5d0ff7c3e601a2ff58b1dd63158c59104b47cb214a8cb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
