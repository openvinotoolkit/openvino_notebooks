{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpeechBrain Emotion Recognition with OpenVINO\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\"> <b>Important note:</b> This notebook requires python >= 3.9. Please make sure that your environment fulfill to this requirement  before running it </div>\n",
    "\n",
    "[SpeechBrain](https://github.com/speechbrain/speechbrain) is an open-source PyTorch toolkit that accelerates Conversational AI development, i.e., the technology behind speech assistants, chatbots, and large language models. \n",
    "\n",
    "Lear more in [GitHub repo](https://github.com/speechbrain/speechbrain) and [paper](https://arxiv.org/pdf/2106.04624)\n",
    "\n",
    "This notebook tutorial demonstrates optimization and inference of speechbrain emotion recognition model with OpenVINO.\n",
    "\n",
    "#### Table of contents:\n",
    "\n",
    "- [Installations](#Installations)\n",
    "- [Imports](#Imports)\n",
    "- [Prepare base model](#Prepare-base-model)\n",
    "- [Initialize model](#Initialize-model)\n",
    "- [PyTorch inference](#PyTorch-inference)\n",
    "- [SpeechBrain model optimization with Intel OpenVINO](#SpeechBrain-model-optimization-with-Intel-OpenVINO)\n",
    "    - [Step 1: Prepare input tensor](#Step-1:-Prepare-input-tensor)\n",
    "    - [Step 2: Convert model to OpenVINO IR](#Step-2:-Convert-model-to-OpenVINO-IR)\n",
    "    - [Step 3: OpenVINO model inference](#Step-3:-OpenVINO-model-inference)\n",
    "\n",
    "\n",
    "### Installation Instructions\n",
    "\n",
    "This is a self-contained example that relies solely on its own code.\n",
    "\n",
    "We recommend  running the notebook in a virtual environment. You only need a Jupyter server to start.\n",
    "For details, please refer to [Installation Guide](https://github.com/openvinotoolkit/openvino_notebooks/blob/latest/README.md#-installation-guide).\n",
    "\n",
    "<img referrerpolicy=\"no-referrer-when-downgrade\" src=\"https://static.scarf.sh/a.png?x-pxid=5b5a4db0-7875-4bfb-bdbd-01698b5b1a77&file=notebooks/speechbrain-emotion-recognition/speechbrain-emotion-recognition.ipynb\" />\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"speechbrain>=1.0.0\" --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q --upgrade --force-reinstall torch torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"transformers>=4.30.0\" \"huggingface_hub>=0.8.0\" \"SoundFile\"\n",
    "%pip install -q \"openvino>=2024.1.0\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from speechbrain.inference.interfaces import foreign_class\n",
    "\n",
    "import openvino as ov"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare base model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The foreign_class function in SpeechBrain is a utility that allows you to load and use custom PyTorch models within the SpeechBrain ecosystem. It provides a convenient way to integrate external or custom-built models into SpeechBrain's inference pipeline without modifying the core SpeechBrain codebase.\n",
    "\n",
    "1. source: This argument specifies the source or location of the pre-trained model checkpoint. In this case, \"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\" refers to a pre-trained model checkpoint available on the Hugging Face Hub.\n",
    "2. pymodule_file: This argument is the path to a Python file containing the definition of your custom PyTorch model class. In this example, \"custom_interface.py\" is the name of the Python file that defines the CustomEncoderWav2vec2Classifier class.\n",
    "3. classname: This argument specifies the name of the custom PyTorch model class defined in the pymodule_file. In this case, \"CustomEncoderWav2vec2Classifier\" is the name of the class that extends SpeechBrain's Pretrained class and implements the necessary methods for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = foreign_class(\n",
    "    source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", pymodule_file=\"custom_interface.py\", classname=\"CustomEncoderWav2vec2Classifier\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wav2vec2 torch model\n",
    "torch_model = classifier.mods[\"wav2vec2\"].model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch inference \n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform emotion recognition on the sample audio file.\n",
    "\n",
    "1. out_prob: Tensor or list containing the predicted probabilities or log probabilities for each emotion class.\n",
    "2. score: Scalar value representing the predicted probability or log probability of the most likely emotion class.\n",
    "3. index: Integer value representing the index of the most likely emotion class in the out_prob tensor or list.\n",
    "4. text_lab: String or list of strings containing the textual labels corresponding to the predicted emotion classes ([\"anger\", \"happiness\", \"sadness\", \"neutrality\"]). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Recognition with SpeechBrain PyTorch model: ['ang']\n"
     ]
    }
   ],
   "source": [
    "out_prob, score, index, text_lab = classifier.classify_file(\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP/anger.wav\")\n",
    "print(f\"Emotion Recognition with SpeechBrain PyTorch model: {text_lab}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpeechBrain model optimization with Intel OpenVINO\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Prepare input tensor\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using sample audio file\n",
    "signals = []\n",
    "batch_size = 1\n",
    "signal, sr = torchaudio.load(str(\"./anger.wav\"), channels_first=False)\n",
    "norm_audio = classifier.audio_normalizer(signal, sr)\n",
    "signals.append(norm_audio)\n",
    "\n",
    "sequence_length = norm_audio.shape[-1]\n",
    "\n",
    "wavs = torch.stack(signals, dim=0)\n",
    "wav_len = torch.tensor([sequence_length] * batch_size).unsqueeze(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Convert model to OpenVINO IR\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model optimization process\n",
    "input_tensor = wavs.float()\n",
    "ov_model = ov.convert_model(torch_model, example_input=input_tensor)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: OpenVINO model inference\n",
    "[back to top ⬆️](#Table-of-contents:)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e5c6191b9e24e17a44eece6a41cccdc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Device:', index=1, options=('CPU', 'AUTO'), value='AUTO')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "core = ov.Core()\n",
    "\n",
    "# Device selection\n",
    "device = widgets.Dropdown(\n",
    "    options=core.available_devices + [\"AUTO\"],\n",
    "    value=\"AUTO\",\n",
    "    description=\"Device:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion Recognition with OpenVINO Model: ['ang']\n"
     ]
    }
   ],
   "source": [
    "# OpenVINO Compiled model\n",
    "compiled_model = core.compile_model(ov_model, device.value)\n",
    "\n",
    "# Perform model inference\n",
    "output_tensor = compiled_model(wavs)[0]\n",
    "output_tensor = torch.from_numpy(output_tensor)\n",
    "\n",
    "# output post-processing\n",
    "outputs = classifier.mods.avg_pool(output_tensor, wav_len)\n",
    "outputs = outputs.view(outputs.shape[0], -1)\n",
    "outputs = classifier.mods.output_mlp(outputs).squeeze(1)\n",
    "ov_out_prob = classifier.hparams.softmax(outputs)\n",
    "score, index = torch.max(ov_out_prob, dim=-1)\n",
    "text_lab = classifier.hparams.label_encoder.decode_torch(index)\n",
    "\n",
    "print(f\"Emotion Recognition with OpenVINO Model: {text_lab}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "openvino_notebooks": {
   "imageUrl": "https://github.com/openvinotoolkit/openvino_notebooks/assets/29454499/727848d6-02a9-4259-ad48-8492879867a1",
   "tags": {
    "categories": [
     "Optimize"
    ],
    "libraries": [],
    "other": [],
    "tasks": [
     "Audio Classification"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
