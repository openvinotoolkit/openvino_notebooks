{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAFT Optical Flow Estimation with OpenVINO™\n",
    "\n",
    "This tutorial explains converting the RAFT ONNX\\* model with OpenVNO and provides the image and video inference demo.\n",
    "\n",
    "This tutorial demonstrates step-by-step instructions on how to run and optimize RAFT with OpenVINO based on the [ONNX-RAFT-Optical-Flow-Estimation](https://github.com/ibaiGorordo/ONNX-RAFT-Optical-Flow-Estimation/tree/main). \n",
    "\n",
    "The tutorial consists of the following steps:\n",
    "- Download the ONNX model\n",
    "- Convert ONNX model to OpenVINO IR\n",
    "- Evaluate the performance\n",
    "- Test on images and video"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get ONNX model\n",
    "Here we choose the model for the SINTEL dataset with input size 360x480 and iteration times 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "\n",
    "base_model_dir = Path(\"models\")\n",
    "\n",
    "model_name = \"iter20\"\n",
    "\n",
    "archive_name = \"resources.tar.gz\"\n",
    "\n",
    "model_url = f\"https://s3.ap-northeast-2.wasabisys.com/pinto-model-zoo/252_RAFT/{model_name}/{archive_name}\"\n",
    "\n",
    "downloaded_model_path = base_model_dir / Path(archive_name)\n",
    "\n",
    "if not downloaded_model_path.exists():\n",
    "    utils.download_file(model_url, downloaded_model_path.name, downloaded_model_path.parent)\n",
    "\n",
    "model_onnx_path= base_model_dir / Path('raft_sintel_iter20_360x480.onnx')\n",
    "\n",
    "if not model_onnx_path.exists():\n",
    "    with tarfile.open(downloaded_model_path) as file:\n",
    "        file.extractall(base_model_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert model to OpenVINO IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import serialize\n",
    "from openvino.tools.mo import convert_model\n",
    "\n",
    "model_ov_path = model_onnx_path.with_suffix(\".xml\")\n",
    "\n",
    "if not model_ov_path.exists():\n",
    "    ov_model = convert_model(model_onnx_path, compress_to_fp16=True)\n",
    "    serialize(ov_model, xml_path=str(model_ov_path))\n",
    "else: print('OpenVINO IR already converted')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance\n",
    "Use the OpenVINO [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance on CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m {model_ov_path} -d CPU -niter 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on images\n",
    "Load the model with model caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.runtime import Core\n",
    "\n",
    "core = Core()\n",
    "core.set_property({'CACHE_DIR': '.cache'})\n",
    "\n",
    "model_ov = core.read_model(model=model_ov_path)\n",
    "compiled_model = core.compile_model(model=model_ov, device_name=\"CPU\")\n",
    "\n",
    "input_layer = compiled_model.input(0)\n",
    "output_layer = compiled_model.output(1)\n",
    "\n",
    "N, C, H, W = input_layer.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download two KITTI frames and do pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def prepare_input(img:np.ndarray):\n",
    "    \"\"\"\n",
    "    pre-processing: resize, transpose and set data type\n",
    "    :param img (np.ndarray): input image\n",
    "    :return img_input.astype(np.float32) (np.ndarray)\n",
    "    \"\"\"\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    img_input = cv2.resize(img, (W, H))\n",
    "    img_input = img_input.transpose(2, 0, 1)\n",
    "    img_input = img_input[np.newaxis,:,:,:]      \n",
    "\n",
    "    return img_input.astype(np.float32)\n",
    "\n",
    "\n",
    "IMAGE_PATH_1 = \"../data/image/kitti_1.png\"\n",
    "IMAGE_PATH_2 = \"../data/image/kitti_2.png\"\n",
    "\n",
    "\n",
    "if not Path(IMAGE_PATH_1).exists():\n",
    "    image_url_1 = \"https://raw.githubusercontent.com/liruoteng/OpticalFlowToolkit/master/data/example/KITTI/frame1.png\"\n",
    "    image_url_2 = \"https://raw.githubusercontent.com/liruoteng/OpticalFlowToolkit/master/data/example/KITTI/frame2.png\"\n",
    "\n",
    "    downloaded_path_1 = Path(IMAGE_PATH_1)\n",
    "    downloaded_path_2 = Path(IMAGE_PATH_2)\n",
    "    utils.download_file(image_url_1, downloaded_path_1.name, downloaded_path_1.parent)\n",
    "    utils.download_file(image_url_2, downloaded_path_2.name, downloaded_path_2.parent)\n",
    "\n",
    "img1 = np.array(Image.open(IMAGE_PATH_1), dtype=np.uint8)\n",
    "img2 = np.array(Image.open(IMAGE_PATH_2), dtype=np.uint8)\n",
    "input_tensor1 = prepare_input(img1)\n",
    "input_tensor2 = prepare_input(img2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do Inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = compiled_model([input_tensor1, input_tensor2])[compiled_model.output(1)]\n",
    "flow_map_ov = result[0].transpose(1, 2, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-processing:\n",
    "\n",
    "Utils function for transferring flow to Middlebury color code image from [OpticalFlowToolkit](https://github.com/liruoteng/OpticalFlowToolkit/blob/5cf87b947a0032f58c922bbc22c0afb30b90c418/lib/flowlib.py#L249). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_FLOW_THRESH = 1e7\n",
    "\n",
    "def make_color_wheel():\n",
    "    \"\"\"\n",
    "    Generate color wheel according Middlebury color code\n",
    "    :return: Color wheel\n",
    "    \"\"\"\n",
    "    RY = 15\n",
    "    YG = 6\n",
    "    GC = 4\n",
    "    CB = 11\n",
    "    BM = 13\n",
    "    MR = 6\n",
    "\n",
    "    ncols = RY + YG + GC + CB + BM + MR\n",
    "\n",
    "    colorwheel = np.zeros([ncols, 3])\n",
    "\n",
    "    col = 0\n",
    "\n",
    "    # RY\n",
    "    colorwheel[0:RY, 0] = 255\n",
    "    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n",
    "    col += RY\n",
    "\n",
    "    # YG\n",
    "    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n",
    "    colorwheel[col:col+YG, 1] = 255\n",
    "    col += YG\n",
    "\n",
    "    # GC\n",
    "    colorwheel[col:col+GC, 1] = 255\n",
    "    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n",
    "    col += GC\n",
    "\n",
    "    # CB\n",
    "    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n",
    "    colorwheel[col:col+CB, 2] = 255\n",
    "    col += CB\n",
    "\n",
    "    # BM\n",
    "    colorwheel[col:col+BM, 2] = 255\n",
    "    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n",
    "    col += + BM\n",
    "\n",
    "    # MR\n",
    "    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n",
    "    colorwheel[col:col+MR, 0] = 255\n",
    "\n",
    "    return colorwheel\n",
    "\n",
    "colorwheel = make_color_wheel()\n",
    "\n",
    "def compute_color(u, v):\n",
    "    \"\"\"\n",
    "    compute optical flow color map\n",
    "    :param u: optical flow horizontal map\n",
    "    :param v: optical flow vertical map\n",
    "    :return: optical flow in color code\n",
    "    \"\"\"\n",
    "    [h, w] = u.shape\n",
    "    img = np.zeros([h, w, 3])\n",
    "    nanIdx = np.isnan(u) | np.isnan(v)\n",
    "    u[nanIdx] = 0\n",
    "    v[nanIdx] = 0\n",
    "\n",
    "    ncols = np.size(colorwheel, 0)\n",
    "\n",
    "    rad = np.sqrt(u**2+v**2)\n",
    "\n",
    "    a = np.arctan2(-v, -u) / np.pi\n",
    "\n",
    "    fk = (a+1) / 2 * (ncols - 1) + 1\n",
    "\n",
    "    k0 = np.floor(fk).astype(int)\n",
    "\n",
    "    k1 = k0 + 1\n",
    "    k1[k1 == ncols+1] = 1\n",
    "    f = fk - k0\n",
    "\n",
    "    for i in range(0, np.size(colorwheel,1)):\n",
    "        tmp = colorwheel[:, i]\n",
    "        col0 = tmp[k0-1] / 255\n",
    "        col1 = tmp[k1-1] / 255\n",
    "        col = (1-f) * col0 + f * col1\n",
    "\n",
    "        idx = rad <= 1\n",
    "        col[idx] = 1-rad[idx]*(1-col[idx])\n",
    "        notidx = np.logical_not(idx)\n",
    "\n",
    "        col[notidx] *= 0.75\n",
    "        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n",
    "\n",
    "    return img\n",
    "\n",
    "def flow_to_image(flow):\n",
    "    \"\"\"\n",
    "    Convert flow into middlebury color code image\n",
    "    :param flow: optical flow map\n",
    "    :return: optical flow image in middlebury color\n",
    "    \"\"\"\n",
    "    u = flow[:, :, 0]\n",
    "    v = flow[:, :, 1]\n",
    "\n",
    "    maxu = -999.\n",
    "    maxv = -999.\n",
    "    minu = 999.\n",
    "    minv = 999.\n",
    "\n",
    "    idxUnknow = (abs(u) > UNKNOWN_FLOW_THRESH) | (abs(v) > UNKNOWN_FLOW_THRESH)\n",
    "    u[idxUnknow] = 0\n",
    "    v[idxUnknow] = 0\n",
    "\n",
    "    maxu = max(maxu, np.max(u))\n",
    "    minu = min(minu, np.min(u))\n",
    "\n",
    "    maxv = max(maxv, np.max(v))\n",
    "    minv = min(minv, np.min(v))\n",
    "\n",
    "    rad = np.sqrt(u ** 2 + v ** 2)\n",
    "    maxrad = max(-1, np.max(rad))\n",
    "\n",
    "    u = u/(maxrad + np.finfo(float).eps)\n",
    "    v = v/(maxrad + np.finfo(float).eps)\n",
    "\n",
    "    img = compute_color(u, v)\n",
    "\n",
    "    idx = np.repeat(idxUnknow[:, :, np.newaxis], 3, axis=2)\n",
    "    img[idx] = 0\n",
    "\n",
    "    return np.uint8(img)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for visualizing the flow image and arrow image with matplotlib quiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_flow(flow_map: np.ndarray, ori_width: int, ori_height: int):\n",
    "    \"\"\"\n",
    "    Convert flow into middlebury color code image and resize\n",
    "    :param flow_map: the model inference result\n",
    "    :param ori_width: the original image width\n",
    "    :param ori_height: the original height\n",
    "    :return resized_img: resized flow image\n",
    "    \"\"\"\n",
    "    flow_img = flow_to_image(flow_map)\n",
    "    flow_img = cv2.cvtColor(flow_img, cv2.COLOR_RGB2BGR)\n",
    "    resized_img = cv2.resize(flow_img, (ori_width, ori_height))\n",
    "    return resized_img\n",
    "\n",
    "def visualize_arrow(origin_img: np.ndarray, flow_map: np.ndarray):\n",
    "    \"\"\"\n",
    "    visualize the flow map with 2D field of arrows via matplotlib.pyplot.quiver\n",
    "    :param origin_img: original KITTI frame\n",
    "    :param flow_map: the inference result\n",
    "    \"\"\"\n",
    "    img_height = origin_img.shape[:2][0]\n",
    "    img_width = origin_img.shape[:2][1]\n",
    "    flow_img = cv2.resize(flow_map, (img_width, img_height)) \n",
    "    num = 30 \n",
    "    x, y = np.meshgrid(np.arange(0, img_height, num, dtype=int), \n",
    "                       np.arange(0, img_width, num, dtype=int))\n",
    "    u = flow_img[:,:,0]\n",
    "    v = flow_img[:,:,1]\n",
    "    U = u[x,y]\n",
    "    V = -v[x,y]\n",
    "\n",
    "    plt.imshow(origin_img)\n",
    "    plt.quiver(y,x,U,V,np.hypot(U, V))\n",
    "    plt.title('arrow_img')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def visualize_flow_img(origin_img, flow_img):\n",
    "    \"\"\"\n",
    "    visualize the resized flow image with mask\n",
    "    :param origin_img: original KITTI frame\n",
    "    :param flow_map: the resized flow image\n",
    "    \"\"\"\n",
    "    alpha = 0.5\n",
    "    masked_img = cv2.addWeighted(origin_img, alpha, flow_img, (1-alpha),0)\n",
    "    return masked_img"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run image inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_arrow(img1, flow_map_ov)\n",
    "\n",
    "img1_height = img1.shape[:2][0]\n",
    "img1_width = img1.shape[:2][1]\n",
    "flow_img_ov = draw_flow(flow_map_ov,img1_width, img1_height)\n",
    "mask_img = visualize_flow_img(img1, flow_img_ov)\n",
    "plt.imshow(mask_img)\n",
    "plt.title('mask_img')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the OpenVINO [Benchmark Tool](https://docs.openvino.ai/latest/openvino_inference_engine_tools_benchmark_tool_README.html) to measure the inference performance on CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!benchmark_app -m {model_ov_path} -d CPU -niter 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on video\n",
    "\n",
    "### Prerequisites\n",
    "Install imageio for generating GIF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import sys\n",
    "import collections\n",
    "import time\n",
    "from IPython import display\n",
    "sys.path.append(\"../utils\")\n",
    "import notebook_utils as utils\n",
    "\n",
    "# Main processing function to run object detection.\n",
    "def video_inference(source=0, flip=False, use_popup=False, skip_first_frames=10, model=model_ov, device=\"CPU\"):\n",
    "    player = None\n",
    "\n",
    "    if device != \"CPU\":\n",
    "        model.reshape({0: [1,3,480,640]})\n",
    "    compiled_model = core.compile_model(model, device)\n",
    "    \n",
    "    try:\n",
    "        # Create a video player to play with target fps.\n",
    "        player = utils.VideoPlayer(\n",
    "            source=source, flip=flip, fps=30, skip_first_frames=skip_first_frames\n",
    "        )\n",
    "        # Start capturing.\n",
    "        player.start()\n",
    "        if use_popup:\n",
    "            title = \"Press ESC to Exit\"\n",
    "            cv2.namedWindow(\n",
    "                winname=title, flags=cv2.WINDOW_GUI_NORMAL | cv2.WINDOW_AUTOSIZE\n",
    "            )\n",
    "        frames = []\n",
    "        processing_times = collections.deque()\n",
    "        while True:\n",
    "            # Grab the frame.\n",
    "            frame = player.next()\n",
    "\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "            input_img_1 = np.array(frame)\n",
    "            plt.imshow(input_img_1)\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            frame = player.next()\n",
    "            if frame is None:\n",
    "                print(\"Source ended\")\n",
    "                break\n",
    "\n",
    "            input_img_2 = np.array(frame)\n",
    "\n",
    "            input_tensor1 = prepare_input(input_img_1)\n",
    "            input_tensor2 = prepare_input(input_img_2)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            result = compiled_model([input_tensor1, input_tensor2])[compiled_model.output(1)]\n",
    "\n",
    "            flow_map_ov = result[0].transpose(1, 2, 0)\n",
    "\n",
    "            stop_time = time.time()\n",
    "            \n",
    "            input_img_1_height = input_img_1.shape[:2][0]\n",
    "            input_img_1_width = input_img_1.shape[:2][1]\n",
    "            flow_img_ov = draw_flow(flow_map_ov, input_img_1_width, input_img_1_height)\n",
    "            output_frame = visualize_flow_img(input_img_1, flow_img_ov)\n",
    "            frames.append(output_frame)\n",
    "            # frame = input_img_1\n",
    "            processing_times.append(stop_time - start_time)\n",
    "            # Use processing times from last 200 frames.\n",
    "            if len(processing_times) > 200:\n",
    "                processing_times.popleft()\n",
    "\n",
    "            _, f_width = output_frame.shape[:2]\n",
    "            # Mean processing time [ms].\n",
    "            processing_time = np.mean(processing_times) * 1000\n",
    "            fps = 1000 / processing_time\n",
    "            cv2.putText(\n",
    "                img=output_frame,\n",
    "                text=f\"Inference time: {processing_time:.1f}ms ({fps:.1f} FPS)\",\n",
    "                org=(20, 40),\n",
    "                fontFace=cv2.FONT_HERSHEY_COMPLEX,\n",
    "                fontScale=f_width / 1000,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=1,\n",
    "                lineType=cv2.LINE_AA,\n",
    "            )\n",
    "            # Use this workaround if there is flickering.\n",
    "            if use_popup:\n",
    "                cv2.imshow(winname=title, mat=output_frame)\n",
    "                key = cv2.waitKey(1)\n",
    "                # escape = 27\n",
    "                if key == 27:\n",
    "                    break\n",
    "            else:\n",
    "                # Encode numpy array to jpg.\n",
    "                _, encoded_img = cv2.imencode(\n",
    "                    ext=\".jpg\", img=output_frame, params=[cv2.IMWRITE_JPEG_QUALITY, 1000]\n",
    "                )\n",
    "                # Create an IPython image.\n",
    "                i = display.Image(data=encoded_img)\n",
    "                # Display the image in this notebook.\n",
    "                display.clear_output(wait=False)\n",
    "                display.display(i)\n",
    "        print(\"Saving GIF file\")\n",
    "        with imageio.get_writer(\"test.gif\", mode=\"I\") as writer:\n",
    "            for idx, frame in enumerate(frames):\n",
    "                # print(\"Adding frame to GIF file: \", idx + 1)\n",
    "                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                writer.append_data(rgb_frame)\n",
    "    # ctrl-c\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n",
    "    # any different error\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if player is not None:\n",
    "            # Stop capturing.\n",
    "            player.stop()\n",
    "        if use_popup:\n",
    "            cv2.destroyAllWindows()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run video inference\n",
    "\n",
    "Use the video as the video input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_inference(source=\"../data/video/Coco Walking in Berkeley.mp4\", flip=False, use_popup=False, model=model_ov, device=\"CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a webcam as the video input. \n",
    "\n",
    ">**NOTE**: Run live inference. By default, the primary webcam is set with `source=0`. If you have multiple webcams, each one will be assigned a consecutive number starting at 0. Set `flip=True` when using a front-facing camera. Some web browsers, especially Mozilla Firefox, may cause flickering. If you experience flickering, set `use_popup=True`. To use this notebook with a webcam, you need to run the notebook on a computer with a webcam. If you run the notebook on a server (for example, Binder), the webcam will not work. Popup mode may not work if you run this notebook on a remote computer (for example, Binder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_inference(source=0, flip=False, use_popup=False, model=model_ov, device=\"CPU\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. RAFT: Recurrent All Pairs Field Transforms for Optical Flow (https://arxiv.org/pdf/2003.12039.pdf)\n",
    "2. ONNX-RAFT-Optical-Flow-Estimation: https://github.com/ibaiGorordo/ONNX-RAFT-Optical-Flow-Estimation/tree/main \n",
    "\n",
    "ONNX-RAFT-Optical-Flow-Estimation used in this notebook licensed under the MIT License. The license is displayed below: <br>\n",
    "MIT License\n",
    "\n",
    "Copyright (c) 2022 Ibai Gorordo\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy<br>\n",
    "of this software and associated documentation files (the \"Software\"), to deal<br>\n",
    "in the Software without restriction, including without limitation the rights<br>\n",
    "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell<br>\n",
    "copies of the Software, and to permit persons to whom the Software is<br>\n",
    "furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all<br>\n",
    "copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR<br>\n",
    "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,<br>\n",
    "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE<br>\n",
    "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER<br>\n",
    "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,<br>\n",
    "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE<br>\n",
    "SOFTWARE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38ov22.3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
