{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37b660e4-3cba-412b-ab1a-b4acb88ff329",
   "metadata": {},
   "source": [
    "# Image generation with Latent Consistency Model and OpenVINO\n",
    "\n",
    "LCMs: The next generation of generative models after Latent Diffusion Models (LDMs). \n",
    "Latent Diffusion models (LDMs) have achieved remarkable results in synthesizing high-resolution images. However, the iterative sampling is computationally intensive and leads to slow generation.\n",
    "\n",
    "Inspired by [Consistency Models](https://arxiv.org/abs/2303.01469), [Latent Consistency Models](https://arxiv.org/pdf/2310.04378.pdf) (LCMs) were proposed, enabling swift inference with minimal steps on any pre-trained LDMs, including Stable Diffusion. \n",
    "The [Consistency Model (CM) (Song et al., 2023)](https://arxiv.org/abs/2303.01469) is a new family of generative models that enables one-step or few-step generation. The core idea of the CM is to learn the function that maps any points on a trajectory of the PF-ODE (probability flow of [ordinary differential equation](https://en.wikipedia.org/wiki/Ordinary_differential_equation)) to that trajectoryâ€™s origin (i.e., the solution of the PF-ODE). By learning consistency mappings that maintain point consistency on ODE-trajectory, these models allow for single-step generation, eliminating the need for computation-intensive iterations. However, CM is constrained to pixel space image generation tasks, making it unsuitable for synthesizing high-resolution images. LCMs adopt a consistency model in the image latent space for generation high-resolution images.  Viewing the guided reverse diffusion process as solving an augmented probability flow ODE (PF-ODE), LCMs are designed to directly predict the solution of such ODE in latent space, mitigating the need for numerous iterations and allowing rapid, high-fidelity sampling. Utilizing image latent space in large-scale diffusion models like Stable Diffusion (SD) has effectively enhanced image generation quality and reduced computational load. The authors of LCMs provide a simple and efficient one-stage guided consistency distillation method named Latent Consistency Distillation (LCD) to distill SD for few-step (2âˆ¼4) or even 1-step sampling and propose the SKIPPING-STEP technique to further accelerate the convergence. More details about proposed approach and models can be found in [project page](https://latent-consistency-models.github.io/), [paper](https://arxiv.org/abs/2310.04378) and [original repository](https://github.com/luosiallen/latent-consistency-model).\n",
    "\n",
    "In this tutorial, we consider how to convert and run LCM using OpenVINO. An additional part demonstrates how to run quantization with [NNCF](https://github.com/openvinotoolkit/nncf/) to speed up pipeline.\n",
    "\n",
    "#### Table of contents:\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Prepare models for OpenVINO format conversion](#Prepare-models-for-OpenVINO-format-conversion)\n",
    "- [Convert models to OpenVINO format](#Convert-models-to-OpenVINO-format)\n",
    "    - [Text Encoder](#Text-Encoder)\n",
    "    - [U-Net](#U-Net)\n",
    "    - [VAE](#VAE)\n",
    "- [Prepare inference pipeline](#Prepare-inference-pipeline)\n",
    "    - [Configure Inference Pipeline](#Configure-Inference-Pipeline)\n",
    "- [Text-to-image generation](#Text-to-image-generation)\n",
    "- [Quantization](#Quantization)\n",
    "    - [Prepare calibration dataset](#Prepare-calibration-dataset)\n",
    "    - [Run quantization](#Run-quantization)\n",
    "    - [Compare inference time of the FP16 and INT8 models](#Compare-inference-time-of-the-FP16-and-INT8-models)\n",
    "    - [Compare UNet file size](#Compare-UNet-file-size)\n",
    "- [Interactive demo](#Interactive-demo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee66539d-99cc-45a3-80e4-4fbd6b520650",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df6bc0c-fa5b-478b-ad44-b2f711497754",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q \"torch\" --index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q \"openvino>=2023.1.0\" transformers \"diffusers>=0.23.1\" opencv-python pillow gradio \"nncf>=2.6.0\" datasets --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "%pip install -q accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9523be62-2c38-480d-ae53-13910ae6d049",
   "metadata": {},
   "source": [
    "## Prepare models for OpenVINO format conversion\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "In this tutorial we will use [LCM_Dreamshaper_v7](https://huggingface.co/SimianLuo/LCM_Dreamshaper_v7) from [HuggingFace hub](https://huggingface.co/). This model distilled from [Dreamshaper v7](https://huggingface.co/Lykon/dreamshaper-7) fine-tune of [Stable-Diffusion v1-5](https://huggingface.co/runwayml/stable-diffusion-v1-5) using Latent Consistency Distillation (LCD) approach discussed above. This model is also integrated into [Diffusers](https://huggingface.co/docs/diffusers/index) library. ðŸ¤— Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules. This allows us to compare running original Stable Diffusion (from this [notebook](../225-stable-diffusion-text-to-image)) and distilled using LCD. The distillation approach efficiently converts a pre-trained guided diffusion model into a latent consistency model by solving an augmented PF-ODE.\n",
    "\n",
    "For starting work with LCM, we should instantiate generation pipeline first. `DiffusionPipeline.from_pretrained` method download all pipeline components for LCM and configure them. This model uses custom inference pipeline stored as part of model repository, we also should provide which module should be loaded for initialization using `custom_pipeline` argument and revision for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d143e91b-81b1-4ed0-96dd-63e2f639cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from diffusers import DiffusionPipeline\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "TEXT_ENCODER_OV_PATH = Path(\"model/text_encoder.xml\")\n",
    "UNET_OV_PATH = Path(\"model/unet.xml\")\n",
    "VAE_DECODER_OV_PATH = Path(\"model/vae_decoder.xml\")\n",
    "\n",
    "\n",
    "def load_orginal_pytorch_pipeline_componets(skip_models=False, skip_safety_checker=False):\n",
    "    pipe = DiffusionPipeline.from_pretrained(\"SimianLuo/LCM_Dreamshaper_v7\")\n",
    "    scheduler = pipe.scheduler\n",
    "    tokenizer = pipe.tokenizer\n",
    "    feature_extractor = pipe.feature_extractor if not skip_safety_checker else None\n",
    "    safety_checker = pipe.safety_checker if not skip_safety_checker else None\n",
    "    text_encoder, unet, vae = None, None, None\n",
    "    if not skip_models:\n",
    "        text_encoder = pipe.text_encoder\n",
    "        text_encoder.eval()\n",
    "        unet = pipe.unet\n",
    "        unet.eval()\n",
    "        vae = pipe.vae\n",
    "        vae.eval()\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    return (\n",
    "        scheduler,\n",
    "        tokenizer,\n",
    "        feature_extractor,\n",
    "        safety_checker,\n",
    "        text_encoder,\n",
    "        unet,\n",
    "        vae,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bc3fa8-fb23-4a89-94de-02d1ea7f40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_conversion = (\n",
    "    TEXT_ENCODER_OV_PATH.exists()\n",
    "    and UNET_OV_PATH.exists()\n",
    "    and VAE_DECODER_OV_PATH.exists()\n",
    ")\n",
    "\n",
    "(\n",
    "    scheduler,\n",
    "    tokenizer,\n",
    "    feature_extractor,\n",
    "    safety_checker,\n",
    "    text_encoder,\n",
    "    unet,\n",
    "    vae,\n",
    ") = load_orginal_pytorch_pipeline_componets(skip_conversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8972c4f5-4dbb-4650-a97a-8debbfab1d58",
   "metadata": {},
   "source": [
    "## Convert models to OpenVINO format\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "Starting from 2023.0 release, OpenVINO supports PyTorch models directly via Model Conversion API. `ov.convert_model` function accepts instance of PyTorch model and example inputs for tracing and returns object of `ov.Model` class, ready to use or save on disk using `ov.save_model` function. \n",
    "\n",
    "\n",
    "Like original Stable Diffusion pipeline, the LCM pipeline consists of three important parts:\n",
    "\n",
    "* Text Encoder to create condition to generate an image from a text prompt.\n",
    "* U-Net for step-by-step denoising latent image representation.\n",
    "* Autoencoder (VAE) for decoding latent space to image.\n",
    "\n",
    "Let us convert each part:\n",
    "\n",
    "\n",
    "### Text Encoder\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "The text-encoder is responsible for transforming the input prompt, for example, \"a photo of an astronaut riding a horse\" into an embedding space that can be understood by the U-Net. It is usually a simple transformer-based encoder that maps a sequence of input tokens to a sequence of latent text embeddings.\n",
    "\n",
    "Input of the text encoder is the tensor `input_ids` which contains indexes of tokens from text processed by tokenizer and padded to maximum length accepted by model. Model outputs are two tensors: `last_hidden_state` - hidden state from the last MultiHeadAttention layer in the model and `pooler_out` - Pooled output for whole model hidden states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ce345b-8204-4dfb-be46-61261e5572fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import openvino as ov\n",
    "\n",
    "\n",
    "def cleanup_torchscript_cache():\n",
    "    \"\"\"\n",
    "    Helper for removing cached model representation\n",
    "    \"\"\"\n",
    "    torch._C._jit_clear_class_registry()\n",
    "    torch.jit._recursive.concrete_type_store = torch.jit._recursive.ConcreteTypeStore()\n",
    "    torch.jit._state._clear_class_state()\n",
    "\n",
    "\n",
    "def convert_encoder(text_encoder: torch.nn.Module, ir_path: Path):\n",
    "    \"\"\"\n",
    "    Convert Text Encoder mode.\n",
    "    Function accepts text encoder model, and prepares example inputs for conversion,\n",
    "    Parameters:\n",
    "        text_encoder (torch.nn.Module): text_encoder model from Stable Diffusion pipeline\n",
    "        ir_path (Path): File for storing model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    input_ids = torch.ones((1, 77), dtype=torch.long)\n",
    "    # switch model to inference mode\n",
    "    text_encoder.eval()\n",
    "\n",
    "    # disable gradients calculation for reducing memory consumption\n",
    "    with torch.no_grad():\n",
    "        # Export model to IR format\n",
    "        ov_model = ov.convert_model(\n",
    "            text_encoder,\n",
    "            example_input=input_ids,\n",
    "            input=[\n",
    "                (-1, 77),\n",
    "            ],\n",
    "        )\n",
    "    ov.save_model(ov_model, ir_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()\n",
    "    print(f\"Text Encoder successfully converted to IR and saved to {ir_path}\")\n",
    "\n",
    "\n",
    "if not TEXT_ENCODER_OV_PATH.exists():\n",
    "    convert_encoder(text_encoder, TEXT_ENCODER_OV_PATH)\n",
    "else:\n",
    "    print(f\"Text encoder will be loaded from {TEXT_ENCODER_OV_PATH}\")\n",
    "\n",
    "del text_encoder\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea926988-97d0-4722-9c5d-72a16635c86e",
   "metadata": {},
   "source": [
    "### U-Net\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "U-Net model, similar to Stable Diffusion UNet model, has four inputs:\n",
    "\n",
    "* `sample` - latent image sample from previous step. Generation process has not been started yet, so you will use random noise.\n",
    "* `timestep` - current scheduler step.\n",
    "* `encoder_hidden_state` - hidden state of text encoder.\n",
    "* `timestep_cond` - timestep condition for generation. This input is not present in original Stable Diffusion U-Net model and introduced by LCM for improving generation quality using Classifier-Free Guidance. [Classifier-free guidance (CFG)](https://arxiv.org/abs/2207.12598) is crucial for synthesizing high-quality text-aligned images in Stable Diffusion, because it controls how similar the generated image will be to the prompt. In Latent Consistency Models, CFG serves as augmentation parameter for PF-ODE.\n",
    "\n",
    "Model predicts the `sample` state for the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c814d43-62c1-42d6-bb2c-29b6cbdaa74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_unet(unet: torch.nn.Module, ir_path: Path):\n",
    "    \"\"\"\n",
    "    Convert U-net model to IR format.\n",
    "    Function accepts unet model, prepares example inputs for conversion,\n",
    "    Parameters:\n",
    "        unet (StableDiffusionPipeline): unet from Stable Diffusion pipeline\n",
    "        ir_path (Path): File for storing model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # prepare inputs\n",
    "    dummy_inputs = {\n",
    "        \"sample\": torch.randn((1, 4, 64, 64)),\n",
    "        \"timestep\": torch.ones([1]).to(torch.float32),\n",
    "        \"encoder_hidden_states\": torch.randn((1, 77, 768)),\n",
    "        \"timestep_cond\": torch.randn((1, 256)),\n",
    "    }\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(unet, example_input=dummy_inputs)\n",
    "    ov.save_model(ov_model, ir_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    gc.collect()\n",
    "    print(f\"Unet successfully converted to IR and saved to {ir_path}\")\n",
    "\n",
    "\n",
    "if not UNET_OV_PATH.exists():\n",
    "    convert_unet(unet, UNET_OV_PATH)\n",
    "else:\n",
    "    print(f\"Unet will be loaded from {UNET_OV_PATH}\")\n",
    "del unet\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecdac70-4194-4345-b816-26454e5941d4",
   "metadata": {},
   "source": [
    "### VAE\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the U-Net model. The decoder, conversely, transforms the latent representation back into an image.\n",
    "\n",
    "During latent diffusion training, the encoder is used to get the latent representations (latents) of the images for the forward diffusion process, which applies more and more noise at each step. During inference, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. When you run inference for text-to-image, there is no initial image as a starting point. You can skip this step and directly generate initial random noise.\n",
    "\n",
    "In our inference pipeline, we will not use VAE encoder part and skip its conversion for reducing memory consumption. The process of conversion VAE encoder, can be found in Stable Diffusion notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2852e77-0d0a-4fe8-8199-e85dc43fc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_vae_decoder(vae: torch.nn.Module, ir_path: Path):\n",
    "    \"\"\"\n",
    "    Convert VAE model for decoding to IR format.\n",
    "    Function accepts vae model, creates wrapper class for export only necessary for inference part,\n",
    "    prepares example inputs for conversion,\n",
    "    Parameters:\n",
    "        vae (torch.nn.Module): VAE model frm StableDiffusion pipeline\n",
    "        ir_path (Path): File for storing model\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    class VAEDecoderWrapper(torch.nn.Module):\n",
    "        def __init__(self, vae):\n",
    "            super().__init__()\n",
    "            self.vae = vae\n",
    "\n",
    "        def forward(self, latents):\n",
    "            return self.vae.decode(latents)\n",
    "\n",
    "    vae_decoder = VAEDecoderWrapper(vae)\n",
    "    latents = torch.zeros((1, 4, 64, 64))\n",
    "\n",
    "    vae_decoder.eval()\n",
    "    with torch.no_grad():\n",
    "        ov_model = ov.convert_model(vae_decoder, example_input=latents)\n",
    "    ov.save_model(ov_model, ir_path)\n",
    "    del ov_model\n",
    "    cleanup_torchscript_cache()\n",
    "    print(f\"VAE decoder successfully converted to IR and saved to {ir_path}\")\n",
    "\n",
    "\n",
    "if not VAE_DECODER_OV_PATH.exists():\n",
    "    convert_vae_decoder(vae, VAE_DECODER_OV_PATH)\n",
    "else:\n",
    "    print(f\"VAE decoder will be loaded from {VAE_DECODER_OV_PATH}\")\n",
    "\n",
    "del vae\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38976ba-29bf-49f2-8fc8-02d66f342d07",
   "metadata": {},
   "source": [
    "## Prepare inference pipeline\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "Putting it all together, let us now take a closer look at how the model works in inference by illustrating the logical flow.\n",
    "\n",
    "![lcm-pipeline](https://user-images.githubusercontent.com/29454499/277402235-079bacfb-3b6d-424b-8d47-5ddf601e1639.png)\n",
    "\n",
    "The pipeline takes a latent image representation and a text prompt is transformed to text embedding via CLIP's text encoder as an input. The initial latent image representation generated using random noise generator. In difference, with original Stable Diffusion pipeline, LCM also uses guidance scale for getting timestep conditional embeddings as input for diffusion process, while in Stable Diffusion, it used for scaling output latents.\n",
    "\n",
    "Next, the U-Net iteratively *denoises* the random latent image representations while being conditioned on the text embeddings. The output of the U-Net, being the noise residual, is used to compute a denoised latent image representation via a scheduler algorithm. LCM introduces own scheduling algorithm that extends the denoising procedure introduced in denoising diffusion probabilistic models (DDPMs) with non-Markovian guidance.\n",
    "The *denoising* process is repeated given number of times (by default 50 in original SD pipeline, but for LCM small number of steps required ~2-8) to step-by-step retrieve better latent image representations.\n",
    "When complete, the latent image representation is decoded by the decoder part of the variational auto encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ff1b96-5f3c-4bab-bfe8-fbe52dc5bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional, Any, List, Dict\n",
    "from transformers import CLIPTokenizer, CLIPImageProcessor\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import (\n",
    "    StableDiffusionSafetyChecker,\n",
    ")\n",
    "from diffusers.pipelines.stable_diffusion import StableDiffusionPipelineOutput\n",
    "from diffusers.image_processor import VaeImageProcessor\n",
    "\n",
    "\n",
    "class OVLatentConsistencyModelPipeline(DiffusionPipeline):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vae_decoder: ov.Model,\n",
    "        text_encoder: ov.Model,\n",
    "        tokenizer: CLIPTokenizer,\n",
    "        unet: ov.Model,\n",
    "        scheduler: None,\n",
    "        safety_checker: StableDiffusionSafetyChecker,\n",
    "        feature_extractor: CLIPImageProcessor,\n",
    "        requires_safety_checker: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vae_decoder = vae_decoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.tokenizer = tokenizer\n",
    "        self.register_to_config(unet=unet)\n",
    "        self.scheduler = scheduler\n",
    "        self.safety_checker = safety_checker\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.vae_scale_factor = 2**3\n",
    "        self.image_processor = VaeImageProcessor(vae_scale_factor=self.vae_scale_factor)\n",
    "\n",
    "    def _encode_prompt(\n",
    "        self,\n",
    "        prompt,\n",
    "        num_images_per_prompt,\n",
    "        prompt_embeds: None,\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        Encodes the prompt into text encoder hidden states.\n",
    "        Args:\n",
    "            prompt (`str` or `List[str]`, *optional*):\n",
    "                prompt to be encoded\n",
    "            num_images_per_prompt (`int`):\n",
    "                number of images that should be generated per prompt\n",
    "            prompt_embeds (`torch.FloatTensor`, *optional*):\n",
    "                Pre-generated text embeddings. Can be used to easily tweak text inputs, *e.g.* prompt weighting. If not\n",
    "                provided, text embeddings will be generated from `prompt` input argument.\n",
    "        \"\"\"\n",
    "\n",
    "        if prompt_embeds is None:\n",
    "\n",
    "            text_inputs = self.tokenizer(\n",
    "                prompt,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            text_input_ids = text_inputs.input_ids\n",
    "            untruncated_ids = self.tokenizer(\n",
    "                prompt, padding=\"longest\", return_tensors=\"pt\"\n",
    "            ).input_ids\n",
    "\n",
    "            if untruncated_ids.shape[-1] >= text_input_ids.shape[\n",
    "                -1\n",
    "            ] and not torch.equal(text_input_ids, untruncated_ids):\n",
    "                removed_text = self.tokenizer.batch_decode(\n",
    "                    untruncated_ids[:, self.tokenizer.model_max_length - 1 : -1]\n",
    "                )\n",
    "                logger.warning(\n",
    "                    \"The following part of your input was truncated because CLIP can only handle sequences up to\"\n",
    "                    f\" {self.tokenizer.model_max_length} tokens: {removed_text}\"\n",
    "                )\n",
    "\n",
    "            prompt_embeds = self.text_encoder(text_input_ids, share_inputs=True, share_outputs=True)\n",
    "            prompt_embeds = torch.from_numpy(prompt_embeds[0])\n",
    "\n",
    "        bs_embed, seq_len, _ = prompt_embeds.shape\n",
    "        # duplicate text embeddings for each generation per prompt\n",
    "        prompt_embeds = prompt_embeds.repeat(1, num_images_per_prompt, 1)\n",
    "        prompt_embeds = prompt_embeds.view(\n",
    "            bs_embed * num_images_per_prompt, seq_len, -1\n",
    "        )\n",
    "\n",
    "        # Don't need to get uncond prompt embedding because of LCM Guided Distillation\n",
    "        return prompt_embeds\n",
    "\n",
    "    def run_safety_checker(self, image, dtype):\n",
    "        if self.safety_checker is None:\n",
    "            has_nsfw_concept = None\n",
    "        else:\n",
    "            if torch.is_tensor(image):\n",
    "                feature_extractor_input = self.image_processor.postprocess(\n",
    "                    image, output_type=\"pil\"\n",
    "                )\n",
    "            else:\n",
    "                feature_extractor_input = self.image_processor.numpy_to_pil(image)\n",
    "            safety_checker_input = self.feature_extractor(\n",
    "                feature_extractor_input, return_tensors=\"pt\"\n",
    "            )\n",
    "            image, has_nsfw_concept = self.safety_checker(\n",
    "                images=image, clip_input=safety_checker_input.pixel_values.to(dtype)\n",
    "            )\n",
    "        return image, has_nsfw_concept\n",
    "\n",
    "    def prepare_latents(\n",
    "        self, batch_size, num_channels_latents, height, width, dtype, latents=None\n",
    "    ):\n",
    "        shape = (\n",
    "            batch_size,\n",
    "            num_channels_latents,\n",
    "            height // self.vae_scale_factor,\n",
    "            width // self.vae_scale_factor,\n",
    "        )\n",
    "        if latents is None:\n",
    "            latents = torch.randn(shape, dtype=dtype)\n",
    "        # scale the initial noise by the standard deviation required by the scheduler\n",
    "        latents = latents * self.scheduler.init_noise_sigma\n",
    "        return latents\n",
    "\n",
    "    def get_w_embedding(self, w, embedding_dim=512, dtype=torch.float32):\n",
    "        \"\"\"\n",
    "        see https://github.com/google-research/vdm/blob/dc27b98a554f65cdc654b800da5aa1846545d41b/model_vdm.py#L298\n",
    "        Args:\n",
    "        timesteps: torch.Tensor: generate embedding vectors at these timesteps\n",
    "        embedding_dim: int: dimension of the embeddings to generate\n",
    "        dtype: data type of the generated embeddings\n",
    "        Returns:\n",
    "        embedding vectors with shape `(len(timesteps), embedding_dim)`\n",
    "        \"\"\"\n",
    "        assert len(w.shape) == 1\n",
    "        w = w * 1000.0\n",
    "\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = torch.log(torch.tensor(10000.0)) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=dtype) * -emb)\n",
    "        emb = w.to(dtype)[:, None] * emb[None, :]\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1)\n",
    "        if embedding_dim % 2 == 1:  # zero pad\n",
    "            emb = torch.nn.functional.pad(emb, (0, 1))\n",
    "        assert emb.shape == (w.shape[0], embedding_dim)\n",
    "        return emb\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(\n",
    "        self,\n",
    "        prompt: Union[str, List[str]] = None,\n",
    "        height: Optional[int] = 512,\n",
    "        width: Optional[int] = 512,\n",
    "        guidance_scale: float = 7.5,\n",
    "        num_images_per_prompt: Optional[int] = 1,\n",
    "        latents: Optional[torch.FloatTensor] = None,\n",
    "        num_inference_steps: int = 4,\n",
    "        lcm_origin_steps: int = 50,\n",
    "        prompt_embeds: Optional[torch.FloatTensor] = None,\n",
    "        output_type: Optional[str] = \"pil\",\n",
    "        return_dict: bool = True,\n",
    "        cross_attention_kwargs: Optional[Dict[str, Any]] = None,\n",
    "    ):\n",
    "\n",
    "        # 1. Define call parameters\n",
    "        if prompt is not None and isinstance(prompt, str):\n",
    "            batch_size = 1\n",
    "        elif prompt is not None and isinstance(prompt, list):\n",
    "            batch_size = len(prompt)\n",
    "        else:\n",
    "            batch_size = prompt_embeds.shape[0]\n",
    "\n",
    "        # do_classifier_free_guidance = guidance_scale > 0.0\n",
    "        # In LCM Implementation:  cfg_noise = noise_cond + cfg_scale * (noise_cond - noise_uncond) , (cfg_scale > 0.0 using CFG)\n",
    "\n",
    "        # 2. Encode input prompt\n",
    "        prompt_embeds = self._encode_prompt(\n",
    "            prompt,\n",
    "            num_images_per_prompt,\n",
    "            prompt_embeds=prompt_embeds,\n",
    "        )\n",
    "\n",
    "        # 3. Prepare timesteps\n",
    "        self.scheduler.set_timesteps(num_inference_steps, original_inference_steps=lcm_origin_steps)\n",
    "        timesteps = self.scheduler.timesteps\n",
    "\n",
    "        # 4. Prepare latent variable\n",
    "        num_channels_latents = 4\n",
    "        latents = self.prepare_latents(\n",
    "            batch_size * num_images_per_prompt,\n",
    "            num_channels_latents,\n",
    "            height,\n",
    "            width,\n",
    "            prompt_embeds.dtype,\n",
    "            latents,\n",
    "        )\n",
    "\n",
    "        bs = batch_size * num_images_per_prompt\n",
    "\n",
    "        # 5. Get Guidance Scale Embedding\n",
    "        w = torch.tensor(guidance_scale).repeat(bs)\n",
    "        w_embedding = self.get_w_embedding(w, embedding_dim=256)\n",
    "\n",
    "        # 6. LCM MultiStep Sampling Loop:\n",
    "        with self.progress_bar(total=num_inference_steps) as progress_bar:\n",
    "            for i, t in enumerate(timesteps):\n",
    "\n",
    "                ts = torch.full((bs,), t, dtype=torch.long)\n",
    "\n",
    "                # model prediction (v-prediction, eps, x)\n",
    "                model_pred = self.unet([latents, ts, prompt_embeds, w_embedding], share_inputs=True, share_outputs=True)[0]\n",
    "\n",
    "                # compute the previous noisy sample x_t -> x_t-1\n",
    "                latents, denoised = self.scheduler.step(\n",
    "                    torch.from_numpy(model_pred), t, latents, return_dict=False\n",
    "                )\n",
    "                progress_bar.update()\n",
    "\n",
    "        if not output_type == \"latent\":\n",
    "            image = torch.from_numpy(self.vae_decoder(denoised / 0.18215, share_inputs=True, share_outputs=True)[0])\n",
    "            image, has_nsfw_concept = self.run_safety_checker(\n",
    "                image, prompt_embeds.dtype\n",
    "            )\n",
    "        else:\n",
    "            image = denoised\n",
    "            has_nsfw_concept = None\n",
    "\n",
    "        if has_nsfw_concept is None:\n",
    "            do_denormalize = [True] * image.shape[0]\n",
    "        else:\n",
    "            do_denormalize = [not has_nsfw for has_nsfw in has_nsfw_concept]\n",
    "\n",
    "        image = self.image_processor.postprocess(\n",
    "            image, output_type=output_type, do_denormalize=do_denormalize\n",
    "        )\n",
    "\n",
    "        if not return_dict:\n",
    "            return (image, has_nsfw_concept)\n",
    "\n",
    "        return StableDiffusionPipelineOutput(\n",
    "            images=image, nsfw_content_detected=has_nsfw_concept\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de4233d-f6df-44a5-a319-af24fd2d6ee1",
   "metadata": {},
   "source": [
    "### Configure Inference Pipeline\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "First, you should create instances of OpenVINO Model and compile it using selected device. Select device from dropdown list for running inference using OpenVINO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5c1b46-d95e-4251-8435-9c7bc83264c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openvino as ov\n",
    "core = ov.Core()\n",
    "\n",
    "core.available_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0c5b5-835b-4e96-bb74-86243f531648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#default gpu is integrated one.\n",
    "selected_gpu = 'GPU.0' if 'GPU.0' in core.available_devices else 'GPU'\n",
    "\n",
    "cache_path = Path(\"model/model_cache\")\n",
    "cache_path.mkdir(exist_ok=True)\n",
    "# Enable caching for OpenVINO Runtime\n",
    "config_dict = {\"CACHE_DIR\": str(cache_path)}\n",
    "\n",
    "text_enc = core.compile_model(TEXT_ENCODER_OV_PATH, \"CPU\")\n",
    "text_gpu_enc = core.compile_model(TEXT_ENCODER_OV_PATH, selected_gpu, config=config_dict)\n",
    "\n",
    "unet_model = core.compile_model(UNET_OV_PATH, \"CPU\")\n",
    "unet_gpu_model = core.compile_model(UNET_OV_PATH, selected_gpu, config=config_dict)\n",
    "\n",
    "vae_decoder = core.compile_model(VAE_DECODER_OV_PATH, \"CPU\")\n",
    "vae_gpu_decoder = core.compile_model(VAE_DECODER_OV_PATH, selected_gpu, config=config_dict)\n",
    "\n",
    "num_inference_steps = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca4d4ac-66e0-4636-bb26-d3e66eb90639",
   "metadata": {},
   "source": [
    "Model tokenizer and scheduler are also important parts of the pipeline. This pipeline is also can use Safety Checker, the filter for detecting that corresponding generated image contains \"not-safe-for-work\" (nsfw) content. The process of nsfw content detection requires to obtain image embeddings using CLIP model, so additionally feature extractor component should be added in the pipeline. We reuse tokenizer, feature extractor, scheduler and safety checker from original LCM pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce8a15-163b-4b13-a575-fcb44fd038bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ov_pipe = OVLatentConsistencyModelPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_enc,\n",
    "    unet=unet_model,\n",
    "    vae_decoder=vae_decoder,\n",
    "    scheduler=scheduler,\n",
    "    feature_extractor=feature_extractor,\n",
    "    safety_checker=safety_checker,\n",
    ")\n",
    "ov_gpu_pipe = OVLatentConsistencyModelPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_gpu_enc,\n",
    "    unet=unet_gpu_model,\n",
    "    vae_decoder=vae_gpu_decoder,\n",
    "    scheduler=scheduler,\n",
    "    feature_extractor=feature_extractor,\n",
    "    safety_checker=safety_checker,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919bf570",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "[NNCF](https://github.com/openvinotoolkit/nncf/) enables post-training quantization by adding quantization layers into model graph and then using a subset of the training dataset to initialize the parameters of these additional quantization layers. Quantized operations are executed in `INT8` instead of `FP32`/`FP16` making model inference faster.\n",
    "\n",
    "According to `LatentConsistencyModelPipeline` structure, UNet used for iterative denoising of input. It means that model runs in the cycle repeating inference on each diffusion step, while other parts of pipeline take part only once. That is why computation cost and speed of UNet denoising becomes the critical path in the pipeline. Quantizing the rest of the SD pipeline does not significantly improve inference performance but can lead to a substantial degradation of accuracy.\n",
    "\n",
    "The optimization process contains the following steps:\n",
    "\n",
    "1. Create a calibration dataset for quantization.\n",
    "2. Run `nncf.quantize()` to obtain quantized model.\n",
    "3. Save the `INT8` model using `openvino.save_model()` function.\n",
    "\n",
    "Please select below whether you would like to run quantization to improve model inference speed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7383b987",
   "metadata": {},
   "source": [
    "Let's load `skip magic` extension to skip quantization if `to_quantize` is not selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "to_quantize = True\n",
    "int8_pipe = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2931a018",
   "metadata": {},
   "source": [
    "### Prepare calibration dataset\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "We use a portion of [`laion/laion2B-en`](https://huggingface.co/datasets/laion/laion2B-en) dataset from Hugging Face as calibration data.\n",
    "To collect intermediate model inputs for calibration we should customize `CompiledModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8951ec45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import set_seed\n",
    "from typing import Any, Dict, List\n",
    "\n",
    "set_seed(1)\n",
    "\n",
    "class CompiledModelDecorator(ov.CompiledModel):\n",
    "    def __init__(self, compiled_model, prob: float, data_cache: List[Any] = None):\n",
    "        super().__init__(compiled_model)\n",
    "        self.data_cache = data_cache if data_cache else []\n",
    "        self.prob = np.clip(prob, 0, 1)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        if np.random.rand() >= self.prob:\n",
    "            self.data_cache.append(*args)\n",
    "        return super().__call__(*args, **kwargs)\n",
    "\n",
    "def collect_calibration_data(lcm_pipeline: OVLatentConsistencyModelPipeline, subset_size: int) -> List[Dict]:\n",
    "    original_unet = lcm_pipeline.unet\n",
    "    lcm_pipeline.unet = CompiledModelDecorator(original_unet, prob=0.3)\n",
    "\n",
    "    dataset = datasets.load_dataset(\"conceptual_captions\", split=\"train\").shuffle(seed=42)\n",
    "    lcm_pipeline.set_progress_bar_config(disable=True)\n",
    "    safety_checker = lcm_pipeline.safety_checker\n",
    "    lcm_pipeline.safety_checker = None\n",
    "\n",
    "    # Run inference for data collection\n",
    "    pbar = tqdm(total=subset_size)\n",
    "    diff = 0\n",
    "    for batch in dataset:\n",
    "        prompt = batch[\"caption\"]\n",
    "        if len(prompt) > tokenizer.model_max_length:\n",
    "            continue\n",
    "        _ = lcm_pipeline(\n",
    "            prompt,\n",
    "            num_inference_steps=num_inference_steps,\n",
    "            guidance_scale=8.0,\n",
    "            lcm_origin_steps=50,\n",
    "            output_type=\"pil\",\n",
    "            height=512,\n",
    "            width=512,\n",
    "        )\n",
    "        collected_subset_size = len(lcm_pipeline.unet.data_cache)\n",
    "        if collected_subset_size >= subset_size:\n",
    "            pbar.update(subset_size - pbar.n)\n",
    "            break\n",
    "        pbar.update(collected_subset_size - diff)\n",
    "        diff = collected_subset_size\n",
    "\n",
    "    calibration_dataset = lcm_pipeline.unet.data_cache\n",
    "    lcm_pipeline.set_progress_bar_config(disable=False)\n",
    "    lcm_pipeline.unet = original_unet\n",
    "    lcm_pipeline.safety_checker = safety_checker\n",
    "    return calibration_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "UNET_INT8_OV_PATH = Path(\"model/unet_int8.xml\")\n",
    "if not UNET_INT8_OV_PATH.exists():\n",
    "    subset_size = 200\n",
    "    unet_calibration_data = collect_calibration_data(ov_pipe, subset_size=subset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0421a05a",
   "metadata": {},
   "source": [
    "### Run quantization\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n",
    "\n",
    "Create a quantized model from the pre-trained converted OpenVINO model.\n",
    "\n",
    "> **NOTE**: Quantization is time and memory consuming operation. Running quantization code below may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37de4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nncf\n",
    "from nncf.scopes import IgnoredScope\n",
    "\n",
    "if UNET_INT8_OV_PATH.exists():\n",
    "    print(\"Loading quantized model\")\n",
    "    quantized_unet = core.read_model(UNET_INT8_OV_PATH)\n",
    "else:\n",
    "    unet = core.read_model(UNET_OV_PATH)\n",
    "    quantized_unet = nncf.quantize(\n",
    "        model=unet,\n",
    "        subset_size=subset_size,\n",
    "        preset=nncf.QuantizationPreset.MIXED,\n",
    "        calibration_dataset=nncf.Dataset(unet_calibration_data),\n",
    "        model_type=nncf.ModelType.TRANSFORMER,\n",
    "        advanced_parameters=nncf.AdvancedQuantizationParameters(\n",
    "            disable_bias_correction=True\n",
    "        )\n",
    "    )\n",
    "    ov.save_model(quantized_unet, UNET_INT8_OV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e10903",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_optimized = core.compile_model(UNET_INT8_OV_PATH, \"CPU\")\n",
    "\n",
    "int8_pipe = OVLatentConsistencyModelPipeline(\n",
    "    tokenizer=tokenizer,\n",
    "    text_encoder=text_enc,\n",
    "    unet=unet_optimized,\n",
    "    vae_decoder=vae_decoder,\n",
    "    scheduler=scheduler,\n",
    "    feature_extractor=feature_extractor,\n",
    "    safety_checker=safety_checker,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90608e5",
   "metadata": {},
   "source": [
    "Let us check predictions with the quantized UNet using the same input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035dffa6-0ce9-4d22-8638-a2217d06e828",
   "metadata": {},
   "source": [
    "## Interactive demo\n",
    "[back to top â¬†ï¸](#Table-of-contents:)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3696ec23-8959-4087-a4a7-48f2b7fb0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gradio as gr\n",
    "from functools import partial\n",
    "import time\n",
    "import sys\n",
    "sys.path.append(\"../utils\")\n",
    "import demo_utils as utils\n",
    "import cv2\n",
    "import numpy\n",
    "from PIL import Image\n",
    "\n",
    "MAX_SEED = np.iinfo(np.int32).max\n",
    "\n",
    "examples = [\n",
    "    \"portrait photo of a girl, photograph, highly detailed face, depth of field, moody light, golden hour,\"\n",
    "    \"style by Dan Winters, Russell James, Steve McCurry, centered, extremely detailed, Nikon D850, award winning photography\",\n",
    "    \"Self-portrait oil painting, a beautiful cyborg with golden hair, 8k\",\n",
    "    \"Astronaut in a jungle, cold color palette, muted colors, detailed, 8k\",\n",
    "    \"A photo of beautiful mountain with realistic sunset and blue lake, highly detailed, masterpiece\",\n",
    "]\n",
    "\n",
    "def randomize_seed_fn(seed: int, randomize_seed: bool = True) -> int:\n",
    "    if randomize_seed:\n",
    "        seed = random.randint(0, MAX_SEED)\n",
    "    return seed\n",
    "\n",
    "MAX_IMAGE_SIZE = 1024\n",
    "\n",
    "def generate(\n",
    "    pipeline: OVLatentConsistencyModelPipeline,\n",
    "    prompt: str,\n",
    "    seed: int = 0,\n",
    "    size: int = 512,\n",
    "    guidance_scale: float = 8.0,\n",
    "    num_inference_steps: int = 8,\n",
    "    randomize_seed: bool = False,\n",
    "    num_images: int = 1,\n",
    "    progress=gr.Progress(track_tqdm=True),\n",
    "):\n",
    "    seed = randomize_seed_fn(seed, randomize_seed)\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    result = pipeline(\n",
    "        prompt=prompt,\n",
    "        width=size,\n",
    "        height=size,\n",
    "        guidance_scale=guidance_scale,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        num_images_per_prompt=num_images,\n",
    "        lcm_origin_steps=50,\n",
    "        output_type=\"pil\",\n",
    "    ).images[0]\n",
    "\n",
    "    opencv_image=numpy.array(result)\n",
    "    utils.draw_ov_watermark(opencv_image, size=0.60)\n",
    "    result = Image.fromarray(opencv_image)\n",
    "\n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate processing time\n",
    "    processing_time = end_time - start_time\n",
    "\n",
    "    return result, seed, round(processing_time, 5), prompt\n",
    "\n",
    "generate_original = partial(generate, ov_pipe)\n",
    "generate_optimized = partial(generate, int8_pipe)\n",
    "generate_gpu = partial(generate, ov_gpu_pipe)\n",
    "quantized_model_present = int8_pipe is not None\n",
    "gpu_model_present = \"GPU\" in ov.Core().available_devices or \"GPU.0\" in ov.Core().available_devices\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Group():\n",
    "        with gr.Row():\n",
    "            prompt = gr.Text(\n",
    "                label=\"Prompt\",\n",
    "                max_lines=1,\n",
    "                placeholder=\"Enter your prompt here\",\n",
    "            )\n",
    "        with gr.Row():\n",
    "            with gr.Column() as cpu_column:\n",
    "                result = gr.Image(label=\"Result (Original)\" if quantized_model_present else \"Image\", type=\"pil\")\n",
    "                run_button = gr.Button(\"Run on CPU\")\n",
    "                # Add processing time label for CPU\n",
    "                result_cpu_time_label = gr.Text(\"\", label=\"Processing Time CPU\", type=\"text\")\n",
    "                # Add prompt description\n",
    "                result_original_prompt_label = gr.Text(\"\", label=\"Prompt:\", type=\"text\")\n",
    "                \n",
    "            with gr.Column(visible=quantized_model_present) as cpu_quantized_column:\n",
    "                result_optimized = gr.Image(label=\"Result (Quantized)\", type=\"pil\", visible=quantized_model_present)\n",
    "                run_quantized_button = gr.Button(value=\"Run quantized on CPU\", visible=quantized_model_present)\n",
    "                # Add processing time label for optimized CPU\n",
    "                result_optimized_time_label = gr.Text(\"\", label=\"Processing Time CPU (Quantized)\", type=\"text\")\n",
    "                # Add prompt description\n",
    "                result_quantized_prompt_label = gr.Text(\"\", label=\"Prompt:\", type=\"text\")\n",
    "                \n",
    "            with gr.Column(visible=gpu_model_present) as gpu_column:\n",
    "                result_gpu = gr.Image(label=\"Result (GPU)\", type=\"pil\", visible=gpu_model_present)\n",
    "                run_gpu_button = gr.Button(value=\"Run on GPU\", visible=gpu_model_present)\n",
    "                # Add processing time label for GPU\n",
    "                result_gpu_time_label = gr.Text(\"\", label=\"Processing Time GPU\", type=\"text\")\n",
    "                # Add prompt description\n",
    "                result_gpu_prompt_label = gr.Text(\"\", label=\"Prompt:\", type=\"text\")\n",
    "\n",
    "    with gr.Accordion(\"Advanced options\", open=False):\n",
    "        with gr.Row():\n",
    "            seed = gr.Slider(label=\"Seed\", minimum=0, maximum=MAX_SEED, step=1, value=0, randomize=True, scale=1)\n",
    "            randomize_seed = gr.Checkbox(label=\"Randomize seed across runs\", value=False, scale=0)\n",
    "            randomize_seed_button = gr.Button(\"Randomize seed\", scale=0)\n",
    "\n",
    "        with gr.Row():\n",
    "            guidance_scale = gr.Slider(\n",
    "                label=\"Guidance scale for base\",\n",
    "                minimum=2,\n",
    "                maximum=14,\n",
    "                step=0.1,\n",
    "                value=8.0,\n",
    "            )\n",
    "            num_inference_steps = gr.Slider(\n",
    "                label=\"Number of inference steps for base\",\n",
    "                minimum=1,\n",
    "                maximum=32,\n",
    "                step=1,\n",
    "                value=8,\n",
    "            )\n",
    "        \n",
    "        size = gr.Slider(\n",
    "            label=\"Image size\",\n",
    "            minimum=256,\n",
    "            maximum=MAX_IMAGE_SIZE,\n",
    "            step=64,\n",
    "            value=512\n",
    "        )\n",
    "\n",
    "        with gr.Row():\n",
    "            simple_mode_button = gr.Button(\"Simple mode\", interactive=True)\n",
    "            complex_mode_button = gr.Button(\"Complex mode\", interactive=False)\n",
    "\n",
    "    gr.Examples(\n",
    "        examples=examples,\n",
    "        inputs=prompt,\n",
    "        outputs=result,\n",
    "        cache_examples=False,\n",
    "    )\n",
    "    \n",
    "    simple_mode_button.click(lambda: gr.Button(interactive=False), outputs=simple_mode_button) \\\n",
    "    .then(lambda: gr.Column(visible=False), outputs=cpu_column) \\\n",
    "    .then(lambda: gr.Column(visible=False), outputs=cpu_quantized_column if gpu_model_present else gpu_column) \\\n",
    "    .then(lambda: gr.Button(interactive=True), outputs=complex_mode_button)\n",
    "\n",
    "    complex_mode_button.click(lambda: gr.Button(interactive=False), outputs=complex_mode_button) \\\n",
    "    .then(lambda: gr.Column(visible=True), outputs=cpu_column) \\\n",
    "    .then(lambda: gr.Column(visible=True), outputs=cpu_quantized_column if gpu_model_present else gpu_column) \\\n",
    "    .then(lambda: gr.Button(interactive=True), outputs=simple_mode_button)\n",
    "\n",
    "    gr.on(\n",
    "        triggers=[randomize_seed_button.click],\n",
    "        fn=randomize_seed_fn,\n",
    "        inputs=[seed],\n",
    "        outputs=[seed],\n",
    "    )\n",
    "    \n",
    "    gr.on(\n",
    "        triggers=[\n",
    "            prompt.submit,\n",
    "            run_button.click,\n",
    "        ],\n",
    "        fn=generate_original,\n",
    "        inputs=[\n",
    "            prompt,\n",
    "            seed,\n",
    "            size,\n",
    "            guidance_scale,\n",
    "            num_inference_steps,\n",
    "            randomize_seed,\n",
    "        ],\n",
    "        outputs=[result, seed, result_cpu_time_label, result_original_prompt_label],\n",
    "    )\n",
    "\n",
    "    if quantized_model_present:\n",
    "        gr.on(\n",
    "            triggers=[\n",
    "                run_quantized_button.click,\n",
    "            ],\n",
    "            fn=generate_optimized,\n",
    "            inputs=[\n",
    "                prompt,\n",
    "                seed,\n",
    "                size,\n",
    "                guidance_scale,\n",
    "                num_inference_steps,\n",
    "                randomize_seed,\n",
    "            ],\n",
    "            outputs=[result_optimized, seed, result_optimized_time_label, result_quantized_prompt_label],\n",
    "        )\n",
    "\n",
    "    if gpu_model_present:\n",
    "        gr.on(\n",
    "            triggers=[\n",
    "                run_gpu_button.click,\n",
    "            ],\n",
    "            fn=generate_gpu,\n",
    "            inputs=[\n",
    "                prompt,\n",
    "                seed,\n",
    "                size,\n",
    "                guidance_scale,\n",
    "                num_inference_steps,\n",
    "                randomize_seed,\n",
    "            ],\n",
    "            outputs=[result_gpu, seed, result_gpu_time_label, result_gpu_prompt_label],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edce720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     demo.queue().launch(debug=True)\n",
    "# except Exception:\n",
    "#     demo.queue().launch(share=True, debug=True)\n",
    "# if you are launching remotely, specify server_name and server_port\n",
    "# demo.launch(server_name='[ip in your network]', server_port=7777)\n",
    "# Read more in the docs: https://gradio.app/docs/\n",
    "\n",
    "#if you are launching via docker use 0.0.0.0 server name:\n",
    "demo.launch(server_name='0.0.0.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d67ac2-128e-47fe-926f-ea2b9d18d9df",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#demo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a0c9d-c46e-429a-88f2-4f5ebc90d7f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
